---
title: "Credit Card Fraud: A Tidymodels Tutorial"
description: "An Imbalanced Class Problem"
twitter-card:
  image: "thumbnail.png"
date: 04-11-2023
date-modified: last-modified
categories: [R, R-code, tidymodels, machine learning, classifiers] # self-defined categories
citation:
  url: https://lsinks.github.io/posts/2023-04-11-credit-card-fraud/fraud_tutorial.html
image: "thumbnail.png"
draft: true # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
---

# 

I've used the credit card fraud dataset from Datacamp for a variety of projects, most notabily the tidymodels and imbalance class tutorials. I've used it when I was learning Tableau and also Python. One advantage of working with the same dataset is that it becomes easier to catch errors in your analysis, or even in the dataset itself.

The geographic data always seemed a bit odd. The data dictionary, from datacamp, defines 3 sets of geographic variables. That will produce a notebook like [this](https://app.datacamp.com/workspace/w/f3a94059-683b-4bc6-b354-9b98cf3d5242/edit) with the dataset and the data dictionary.

|            |                                |
|------------|--------------------------------|
| city       | City of Credit Card Holder     |
| state      | State of Credit Card Holder    |
| lat        | Latitude Location of Purchase  |
| long       | Longitude Location of Purchase |
| merch_lat  | Latitude Location of Merchant  |
| merch_long | Longitude Location of Merchant |

The original source of the data (prior to preparation by DataCamp) can be found [here](https://www.kaggle.com/kartik2112/fraud-detection?select=fraudTrain.csv) and does not include a data dictionary.

lat and merch_lat and long and merch_long were highly, but not perfectly, correlated, so I orginally dropped the merchant coordinates.

While playing with the dataset in Tableau, I became very confused about what lat/long and merch_lat/merch_long actually represented.

I puzzled over this for a long time (I finalized my Tableau dashboards in July, but never did anything with them, because I was so confused by my results. I did check the Tableau results by manually pulling out some subsets of data in R, and what I was seeing in my pretty Tableau maps was correct.

While trying to write up some thoughts about Tableau (in November), I decided to delve back into this question. I had looked at the Kaggle notebook discussions when I first starting working on this (which was probably last year!) and didn't find much info. Now, someone had posted a comment about the two coordinates and someone replied that the lat/long is the location of the card holder's home address, not the purchase location. This is so obvious (and clearly explains the results I showed in my dashboard) that it is kind of funny that I didn't figure this out. It is a good lesson in checking all your assumptions carefully when things don't make sense. I assumed that a dataset prepared by Datacamp would be correct and I assumed any weird results were because the person who coded the simulation had made some weird choices.

So, now I'm going to reanalyze the geographic data with this new definiton in mind.

# 1. Set-up steps

Loading the necessary libraries.

```{r}
#| label: loading-libraries
#| warning: false


# loading tidyverse
library(tidyverse) #core tidyverse

# visualization
library(viridis) #color scheme that is colorblind friendly
library(ggthemes) # themes for ggplot
library(gt) # to make nice tables
library(cowplot) # to make multi-panel figures
library(corrplot) # nice correlation plot

#Data Cleaning
library(skimr) #provides overview of data and missingness

#Geospatial Data
library(tidygeocoder) #converts city/state to lat/long

```

I'm setting a global theme for my figures. I'm using [cowplot](https://wilkelab.org/cowplot/index.html) to create some composite figures, and apparently you must choose a cowplot theme if you set a global theme. You can use a ggtheme on a graph by graph basis, but not globally.

```{r}
#| label: fig-options
#  setting global figure options

theme_set(theme_cowplot(12))
```

Loading the data. This is a local copy that is part of the workspace download from Datacamp.

```{r}
#| label: import-data
#  Reading in the data
fraud <- read_csv('C:/Users/drsin/OneDrive/Documents/R Projects/lsinks.github.io/posts/2023-04-11-credit-card-fraud/datacamp_workspace/credit_card_fraud.csv', show_col_types = FALSE) 
fraud
```

# 2.

I know the dataset doesn't have missing data, so I'm going to jump right in.

If lat/long is the location of the card holder's home, then I'd expect the following things to be true.

1.  Each customer should be associated with only one lat/long.
    1.  Assume that a customer in this dataset can be uniquely identified by date of birth and job
    2.  A lat/long might be associated with two customers (e.g. a couple living together with separate accounts.
2.  lat/ long should match the city/state info. I already used tidygeocoder to produce lat/long information for city/state pairs, so this should be quick.

I'm going to go through and check the assumptions.

## Assume that a customer in this dataset can be uniquely identified by date of birth and job

Four locations have 2 users, the rest only have one.

```{r}
temp <- fraud %>% select(lat, long, dob, job, city, state) %>% group_by(lat, long, dob, job, city) %>% distinct()



temp %>% group_by(lat, long) %>% count(sort = TRUE)

```

4 locations that have two users each.

```{r}
temp %>% group_by(dob) %>% count(sort = TRUE)
```

```{r}
temp %>% group_by(dob, lat, long) %>% count(sort = TRUE)
```

187 unique profiles.

## The lat/long match the city state

If the lat/long really encode the home address of the card user, then the city, state (of the card user's address) should be equivalent to the lat/long data. I can convert back and forth using a package like tidygeocoder. I did do this before, and then I calculated the distance between the home city/state and lat/long, which I thought was transaction location. I got some very large distances, which doesn't make sense if lat/long is of the home location. I don't expect to get 0, due to some imprecision in the geocoding, but I do expect a small number (that probably varies with the size of the city.) Requesting the lat/long of San Diego, CA will return the coordinates of the center of the city, while the lat/long from our dataset will reflect a specific address in San Diego.

I'm going to use the suffix \_calc for calculated values.

So what is going on? Either the original geocoding is wrong or lat/long is not the home address.

From the very first entry, I can see that Orient, Washington was given coordinates that correspond to Orient, Iowa. It is also clear that city name might be in the city, village, town or hamlet category.

For a quick check of how impactful the incorrect geocoding is, we can look at how many entries have the wrong state.

## Let's Analyze the data properly.

Reimport the data and start fresh

```{r}
fraud_geo <- read_csv('C:/Users/drsin/OneDrive/Documents/R Projects/lsinks.github.io/posts/2023-04-11-credit-card-fraud/datacamp_workspace/credit_card_fraud.csv', show_col_types = FALSE) 
```

Pull out the lat/long data.

```{r}
coord_list <- fraud_geo %>%
  distinct(lat, long)
```

Check this works with new vars

```{r}
#| label: reverse-geocode


# add_calc <- coord_list %>%
#   reverse_geocode(
#     lat = lat,
#     long = long,
#     address = addr,
#     limit = 1,
#     method = "osm",
#     full_results = TRUE
#   )
# 
# 
# write_csv(add_calc, "add_calc.csv")

```

This will be a messy enough data cleaning exercise that I was willing to cheat. I went back to the source code that generated the data set, and I looked to see how the locations were generated. I figured I could use the same method and increase the odds that I'd get the city name in the same field as my output. But all the source code seems to use a file demographic_data/locations_partitions.csv but they never generate it. So I'm unclear where the geographic information is coming from.

Anway, let's do it.

read in data

```{r}
add_calc <- read_csv("add_calc.csv")
```

```{r}

add_calc <-
  add_calc %>% select(
    lat,
    long,
    county,
    state,
    town,
    hamlet,
    suburb,
    village,
    city,
    neighbourhood,
    municipality
  ) %>% rename (
    county_calc = county,
    state_calc = state,
    town_calc = town,
    hamlet_calc = hamlet,
    suburb_calc = suburb,
    village_calc = village,
    city_calc = city,
    neighbourhood_calc = neighbourhood,
    municipality_calc = municipality
  )


```

```{r}
skim(add_calc)
```

Every one of the names contains some data, so we will keep them all.

rename geo names to calculated

```{r}
geo_set <-
  fraud_geo %>% left_join(add_calc, by = c("lat", "long")) %>% distinct(lat, long, .keep_all = TRUE) 

```

remove other var

```{r}
geo_set2 <- geo_set %>% select(
    -trans_date_trans_time,
    -amt,
    -merchant,
    -category,
    -city_pop,
    -job,
    -dob,
    -is_fraud,
    -merch_lat,
    -merch_long,
    -trans_num
  )
```

We have 183 locations to match. I don't know that some of these temrs have a legal meaning in the US (like hamlet) and I'm not sure what the hierarchy should be.

First, let's check that all the states are correct

```{r}

geo_set2$state_calc <- state.abb[match(geo_set2$state_calc, state.name)]
```

Now check

```{r}
geo_set2 <- geo_set2 %>%
  mutate(state_match = (state == state_calc))
```

value

```{r}
mean(geo_set2$state_match)
```

Okay, so we are doing better already.

```{r}
geo_set2 <- geo_set2 %>%
  mutate(city_match = (city == city_calc))
```

I'm going to match everything and then handle the nas

```{r}
geo_set2 <- geo_set2 %>%
  mutate(town_match = (city == town_calc)) %>%
  mutate(county_match = (city == county_calc)) %>%
  mutate(hamlet_match = (city == hamlet_calc)) %>%
  mutate(village_match = (city == village_calc)) %>%
  mutate(suburb_match = (city == suburb_calc)) %>%
  mutate(neighbourhood_match = (city == neighbourhood_calc)) %>%
  mutate(municipality_match = (city == municipality_calc)) 
```

Any NAs in these columns don't match.

```{r}
geo_set3 <- geo_set2 %>%

  mutate(across(ends_with("_match"), ~replace_na(.,FALSE)))

```

examine this

```{r}
skim(geo_set3)
```

neighbourhood and municipality have no matches, so delete them

```{r}
geo_set3 <- geo_set3 %>% 
  select (-neighbourhood_calc, -neighbourhood_match, -municipality_calc, -municipality_match)
```

Now let's find a city_coded using a case when. I'm going to match from what I think is the biggest to smallest geo unit.

```{r}
geo_set3 <-  geo_set3 %>%
  mutate(
    city_coded =  case_when(
      city_match == TRUE      ~ city_calc,
      town_match == TRUE      ~ town_calc,
      county_match == TRUE      ~ county_calc,
      hamlet_match == TRUE      ~ hamlet_calc,
      village_match == TRUE      ~  village_calc,
      suburb_match == TRUE      ~  suburb_calc,
      .default = "no_match"
      
    )
  )
```

How many no_match do we have?

```{r}
temp4 <- geo_set3 %>% filter (city_coded == "no_match")
```

Test some different methods

```{r}
# geo_osm <- reverse_geo(
#     lat = 48.8878,
#     long = -118.2105,
#     address = addr,
#     limit = 1,
#     method = "osm",
#     full_results = TRUE)
```

```{r}
# geo_argis <- reverse_geo(
#     lat = 48.8878,
#     long = -118.2105,
#     address = addr,
#     limit = 1,
#     method = "arcgis",
#     full_results = TRUE)
```

now mapquest

```{r}
# geo_mapquest <- reverse_geo(
#     lat = 48.8878,
#     long = -118.2105,
#     address = addr,
#     limit = 1,
#     method = "mapquest",
#     full_results = TRUE)
```

google

```{r}
# geo_google <- reverse_geo(
#     lat = 48.8878,
#     long = -118.2105,
#     address = addr,
#     limit = 1,
#     method = "google",
#     full_results = TRUE)
```

Okay, google is the only one that correctly IDs Orient, Wa

```{r}
temp4 <- temp4[,1:4]
```

Reverse geocode with google for these 88.

```{r}
# add_calc_google <- temp4 %>% reverse_geocode(
#     lat = lat,
#     long = long,
#     address = addr,
#     limit = 1,
#     method = "google",
#     full_results = TRUE)
```

save for later

```{r}
saveRDS(add_calc_google, file = "add_calc_google.rds")
```

We need to unnest the 6 column. We can also deleted some stuff.

We want locality and adminstrative_area_level_1 to start unforetunately these items don't appear in a consistent place in the matrix.

```{r}
add_calc_google <- add_calc_google[,1:6]
```

now unnest

```{r}
add_calc_google2 <- add_calc_google %>%
  unnest_longer((address_components)) %>% unnest(address_components)
```

Now let's clean things up.

```{r}
add_calc_google3 <- add_calc_google2 %>%
  filter((types == 'c("locality", "political")') | (types == 'c("administrative_area_level_1", "political")'))
```

Remove col

```{r}
add_calc_google4 <- add_calc_google3 %>%
 select(-long_name)
```

rename the locality stuff from google

```{r}
add_calc_google4 <- add_calc_google4 %>%
  mutate(types = str_replace_all(types, pattern = 'c("locality", "political")', replacement = "State"))
```

Pivot back to a reasonable shape

```{r}
add_calc_google4 <- add_calc_google4 %>%
  pivot_wider(names_from= types, values_from = short_name)
```

try renaming again

```{r}
add_calc_google4 <- add_calc_google4 %>% rename(city_calc = 'c("locality", "political")')
```

rename state

```{r}
add_calc_google4 <- add_calc_google4 %>% rename(state_calc =  'c("administrative_area_level_1", "political")')
```

test

```{r}
add_calc_google4 <-  add_calc_google4 %>%
  mutate(city_match = (city == city_calc)) %>%
  mutate(state_match = (state == state_calc))
```

See how we did

```{r}
mean(add_calc_google4$state_match)
```

now city

```{r}
mean(add_calc_google4$city_match)
```

```{r}
no_match2 <-add_calc_google4 %>%
  filter (city_match== FALSE)
```

Next, I'm going to convert city/state into latitude and longitude using the [tidygeocoder package](https://jessecambon.github.io/tidygeocoder/reference/geo.html). Also included code to save this output and then re-import it. You likely do not want to be pulling the data from the internet every time you run the code, so this gives you the option to work from a local copy. For many services, it is against terms of service to repeatedly make the same calls rather than working from a local version. I did find that I could originally pull all data from 'osm', but while double checking this code, I found that the service is now imposing some rate limit and denies some requests, leading to some NA entries. So do check your results.

Now I'm going to calculate the distance between the card holder's home and the location of the transaction. I think distance might be a feature that is related to fraud. I followed the tutorial [here](https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/) for calculating distance

Despite my assumption that distance would be correlated with fraud, the correlation value is quite low, `r fraud_distance`.

```{r}

# Code Block 14: Distance Between merchant and trans

# I believe this assuming a spherical Earth

# convert to radians
fraud <- fraud %>%
  mutate(
    lat3_radians = merch_lat / 57.29577951,
    long3_radians = merch_long / 57.29577951,
  )

# calculating distance
fraud <-
  fraud %>% mutate(distance_miles2 = 3963.0 * acos((sin(lat2_radians) * sin(lat3_radians)) + cos(lat2_radians) * cos(lat3_radians) * cos(long3_radians - long2_radians)
  ))

# calculating the correlation
fraud_distance2 <- round(cor(fraud$distance_miles2, fraud$is_fraud), 3) 
```

I'm going to visualize it anyway.

Some distances only have fraudulent transactions. This might be related to the locations that are only fraud, Figure 4.

This new feature `distances_miles` is retained, and the original variables (`city`, `state`) and the intermediate variables (address, variables used to calculate distance) are removed in Code Block 16.

I'm going to reverse geocode the trans coords

Hypothesis- wrongly encoded addresses are the large distance errors. State alone is enough to flag them?

```{r}
fraud <- fraud %>%
  left_join(address_list2, by = "address")
```

```{r}
write_csv(home_address,"home_address.csv" )
```

And looking at distance based on lat/long

```{r}
ggplot(fraud, aes(distance_miles2, is_fraud , fill = factor(is_fraud))) +
  geom_point(
    alpha = 1,
    shape = 21,
    colour = "black",
    size = 5,
    position = "jitter"
  ) +
  scale_fill_viridis(
    discrete = TRUE,
    labels = c('Not Fraud', 'Fraud'),
    name = ""
  ) +
  ggtitle("Figure 5: How far from home does fraud occur?") +
  xlab("Distance from Home (miles)") +
  ylab("Is Fraud?") 
```

How many Wrong states?

```{r}
mean(fraud_geo_prob$right_state)
```

Distribution might be clearer

```{r}


```

Other histogram

```{r}
ggplot(fraud) +
  geom_histogram(aes(distance_miles2), color = "blue", bins = 50) 
```

```{r}
#| label: distance-and-fraud-viz
# Code Block 16: Remove Extraneous/Temp Variables

# created to calculate distance
fraud <- fraud %>%
  select(-lat1_radians,-lat2_radians,-long1_radians,-long2_radians)


```

f
