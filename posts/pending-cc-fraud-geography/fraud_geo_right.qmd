---
title: "Credit Card Fraud: A Tidymodels Tutorial"
description: "An Imbalanced Class Problem"
twitter-card:
  image: "thumbnail.png"
date: 04-11-2023
date-modified: last-modified
categories: [R, R-code, mapping, geocoding, reverse geocoding, ] # self-defined categories
citation:
  url: https://lsinks.github.io/posts/2023-04-11-credit-card-fraud/fraud_tutorial.html
image: "thumbnail.png"
draft: true # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
---

## The Problem

One of the difficulties as data science student is finding appropriate datasets. Data can be real or synthetic. Synthetic datasets are generated by code. Synthetic data tend to be fairly popular with learners. Part of this is that many learning sites use synthetic datasets for teaching. It is valuable to have a dataset that is complete (enough) to practice a particular technique or method without getting bogged down in various problems that might arise from real world data. And I should specify by "completeness" I don't mean that the dataset lacks missing data, I mean the dataset contains sufficient features to solve the problem at hand. In the real world, deciding on needed features/variables and figuring out how to acquire them is often the first step in a data science or data analysis problem. Students then have access to this data as they generate portfolios and continue learning.

Sites like Kaggle are another common source of datasets. Many popular datasets are synthetic, but there is no consisent labeling or requiremnt to label datasets as synthetic. It is possible that many datasets not specified as synthetic could be generated data.

So why is this a problem? You need to have some sort of assurance that the dataset accurately reflects the true data to an appropriate level. What this level is depends on the nature of the task and your goals for the dataset. Some decision made in the code to generate the data may not be "correct" but it may still produce a dataset that is a valid test of methods for that class of problems. Other decisions might create a dataset so flawed that it isn't useful. Telling the two cases apart can be difficult when you don't have experience with real world data of the type in question.

The majority of the data used in this blog's projects are real, though in many cases it has been pre-processed or otherwise curated (#TidyTuesday data in particular tends to have some wrangling done to it before it is posted). I have, however, used a synthetic dataset for a variety of projects. This is the credit card fraud dataset from Datacamp, which is a processed version of a dataset from Kaggle. I've used it for the tidymodels and imbalance class tutorials as well as when I was learning Tableau and Python. One advantage of working with the same dataset is that it becomes easier to catch errors in your analysis, or even in the dataset itself.

This dataset passed my "smell test" even though some of the analyses produced results that I thought were odd. The [distribution of the hour of the transactions](https://lsinks.github.io/posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-date-times) (see Fig. 8 and the discussion) seemed off. The geographic data didn't raise any flags when I did [my initial analysis in R](https://lsinks.github.io/posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-geographic-data), but it should have. When I started working with this data in Tableau, [the various geographic data and transaction data was correlated in ways I couldn't explain at all](https://public.tableau.com/views/ExploratoryDataAnalysisofCreditCardFraud/Location?:language=en-US&:display_count=n&:origin=viz_share_link).

In this blog post, I'm going to discuss the geographic data and:

-   what I thought it was and what it actually is.

-   the right way to analyze this data given how it was generated

-   the right way to analyze geographic data that comes from the "real world"

-   the big mistakes I made in my initial analyses that lead me to overlook this problem and how I would prevent these errors in the future.

-   how this dataset is useful and areas where it isn't

I think it is important to always engage with our projects critically and also to learn from our mistakes.

I will be citing the code that generates the data, so if you are planning to model this data either at datacamp or at Kaggle, you may want not read any more of this post. The one thing you need to know if you are working on Datacamp's version of the dataset is that the data dictionary is wrong. The lat/long are the customer location, not the transaction location.

## What is the dataset?

The version of the credit card fraud dataset I used is provided as a sample dataset in the DataCamp Workspace. Workspace is a notebook enviroment for both businesses and learners. It currently provides a few dozen datasets for learners. These datasets provide a data dictionary and a "where to start" section with suggested analyses. The datasets almost always come from elsewhere, with a link out to the source and the license terms.

In this case, the statement of origin is "[**Source**](https://app.datacamp.com/workspace/external-link?url=https%3A%2F%2Fwww.kaggle.com%2Fkartik2112%2Ffraud-detection%3Fselect%3DfraudTrain.csv)Â of dataset. The data was partially cleaned and adapted by DataCamp." The Kaggle notebook clearly states the dataset is synthetic, though DataCamp doesn't.

The dataset contains information about roughly 300 k transactions. The features include customer information( dob, job, location), transaction information (time, date, merchant, location, amount, etc.) and whether or not the transaction was a fraud. The dataset is a subset of the data on Kaggle, both in terms of the nuimber of records and the number of features.

This dataset is highly imbalanced with 0.5% fraud, which seems realistic for this type of problem.

This is an appealing dataset to play with, since it has a variety of different types of features - time and datetime, geographic (both as city, state and latitude/longitude) as well as numerical and categorical. Each of these types require different treatments and preparation prior to modeling.

## The Geographic Data and the DataCamp Data Dictionary

As I mentioned above, the geographic data seemed really confusing when I started playing with it in Tableau.

The data dictionary defines 3 sets of geographic variables. You can look at a [sample notebook of mine with the dataset and the full data dictionary](https://app.datacamp.com/workspace/w/f3a94059-683b-4bc6-b354-9b98cf3d5242/edit).

Here is a copy of the geographic variables from the data dictionary.

|            |                                |
|------------|--------------------------------|
| city       | City of Credit Card Holder     |
| state      | State of Credit Card Holder    |
| lat        | Latitude Location of Purchase  |
| long       | Longitude Location of Purchase |
| merch_lat  | Latitude Location of Merchant  |
| merch_long | Longitude Location of Merchant |

I popped over to Kaggle and viewed the information provided with the dataset ( [here](https://www.kaggle.com/kartik2112/fraud-detection?select=fraudTrain.csv) ). It does provide a data dictionary on the training set, but the default view of the notebook is to the test set. It is easy to overlook if you are skimming the page.

Looking at the kaggle notebook, it states that the lat/long are the customer address, not the transaction address. This makes a lot of sense, and the weird Tableau viz no longer is weird.

When I started working on this, I thought maybe I could prove this from the dataset itself. I thought it would be useful to have a schema on how to check the assumptions/data dictionary; even in the real world (especially in the real world?) data isn't always what you are told it is. It is of course possible that the datacamp dictionary is correct and their modifications include renaming or creating features that follow the dictionary. So proving the meaning of lat/long isn't a meaningless exercise. However, it isn't possible to prove this with the DataCamp dataset, for a variety of reasons. However, as I'll show later, the data is consistent with lat and long representing the customer address and not the transaction location. As I was re-working my analysis and trying to prove that lat/long represented customer address, I discovered both errors in my previous analysis and that the simulator handles geographic data in a very rough way

## City, State should be the Same as Lat/Long

If Lat/Long were the customer address, they should be the same as City, State within some margin. City, State will geocode to a certain point in the city, while the lat/long might encode a specific local address. I would expect the distance between the two to be "small". But I had calculated the distance between City, State and Lat/Long and found distances greater than 1500 miles in many cases. (See everything below Fig 4 in [the Geographical Variables Section](https://lsinks.github.io/posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-geographic-data).) Here's my initial analysis:

![](/docs/posts/2023-04-11-credit-card-fraud/fraud_tutorial_files/figure-html/distance-and-fraud-1.png)

I had thought that most transactions would be close to home, so I was looking to see if fraud increased with distance. This also isn't the best visualization to answer that question, but it does show that the distance between my two home variables are not "close".

So what did I do wrong? I used using the [tidygeocoder package](https://jessecambon.github.io/tidygeocoder/reference/geo.html) to convert city, state information into latitude and longitude (this is in Code Block 13: Converting city/state data lat/long, in [the Geographical Variables Section](https://lsinks.github.io/posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-geographic-data)). I then calculated the distance between those coordinates and the provided lat/long coordinates (which I thought were transaction location remember). I calculated the distance assuming a spherical earth following this [write-up on the Haversine formula applied to calculating distances on Earth](https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/). Note that even this is an approximation; the Earth is an ellipsoid, not a sphere and this formula can be off upto 0.5%.

I'm assuming basic geometry is fine, so the issue must be in the geocoding.

### Loading Libraries

Loading the necessary libraries.

```{r}
#| label: loading-libraries
#| warning: false

# loading tidyverse/ tidymodels packages
library(tidyverse) #core tidyverse


# visualization
library(viridis) #color scheme that is colorblind friendly
library(ggthemes) # themes for ggplot
library(gt) # to make nice tables
library(cowplot) # to make multi-panel figures
library(corrplot) # nice correlation plot
library(randomcoloR)
library(RColorBrewer)


#Geospatial Data
library(tidygeocoder) #converts city/state to lat/long
library(sf) # for handling geo data
#library(mapview) # quick interactive mapping
#library(leaflet) # more mapping

```

```{r}
#| label: fig-options

theme_set(theme_cowplot(12))
```

Loading the data. This is a local copy that is part of the workspace download from Datacamp.

```{r}
#| label: import-data

fraud <-
  read_csv(
    'C:/Users/drsin/OneDrive/Documents/R Projects/lsinks.github.io/posts/2023-04-11-credit-card-fraud/datacamp_workspace/credit_card_fraud.csv',
    show_col_types = FALSE
  ) 
```

## Geocoding

The tidygeocoder package is a wrapper for [geocoding queries to a variety of different services](https://jessecambon.github.io/tidygeocoder/articles/geocoder_services.html). I used the Nominatim service and I passed it an unstructured query. Looking at the API, you can pass it specific components (city, state, country) but I passed the string "city, state" as the address. I also chose `full_results = FALSE`, which just returned the coordinates, and nothing else.

I took this list and reverse geocoded the coordinates. Reverse geocoding should be much more accurate. There is no ambiguity in lat/long coordinates, unlike the string parsing and matching for address components. (I also chose the full results this time.)

Load the results from my initial geocoding.

```{r}
#| label: loading-geocoding

addresses_geocoded <-
read_csv(
  'C:/Users/drsin/OneDrive/Documents/R Projects/lsinks.github.io/posts/2023-04-11-credit-card-fraud/datacamp_workspace/downloaded_coords.csv',
  show_col_types = FALSE
) 
```

There are 177 unique city, state pairs.

I'm going to rename the coordinates to \_calc.

```{r}
#| label: revising-addresses-geocoded

addresses_geocoded <- addresses_geocoded %>%
  mutate(address = str_replace_all(address, "\"", "")) %>%
  rename(lat_calc = lat, long_calc = long)
```

Now I'm going to reverse geocode based on the lat_calc, long_calc.

```{r}
#| label: reverse-geocode
#reverse geocode block
```

```{r}
#| label: loading-reverse-geocode
reverse <- read_csv("home_coords1.csv")
```

Take a quick look

```{r}
#| label: viewing-reverse

reverse %>%
  gt() %>%
  tab_options(container.height = px(300),
              container.padding.y = px(24))
```

From the very first entry, I can see that Orient, Washington was given coordinates that correspond to Orient, Iowa. It is also clear that city name might be in the city, village, town or hamlet category.

For a quick check of how impactful the incorrect geocoding is, we can look at how many entries have the wrong state.

```{r}
#| label: selecting-geo-reverse

reverse <- reverse %>%
  select(
    address,
    lat_home,
    long_home,
    city,
    hamlet,
    suburb,
    neighbourhood,
    village,
    town,
    county,
    state,
    country
  )
```

look at this geo subset

```{r}
#| label: viewing-reverse-subset
reverse %>%
  gt() %>%
  tab_options(container.height = px(300),
              container.padding.y = px(24))
```

A couple of things stand out. The data returned from the reverse geocode is structured in a hierarchial way. What is listed as "city" in the fraud dataset might be a hamlet, a village, a city, etc. or even a combination of the two (such as High Rolls Mountain Park, NM).

I'm going to quickly see how bad the geocoding was. I'm going to split the address back into city and state and tag them \_orig for "original".

(I always want to use str_split from stringr, because that just sounds right, but I really want to use separate from tidyR)

```{r}
#| label: making-city-state-orig

reverse <- reverse %>%
separate(address,
         into = c("city_orig", "state_orig"),
         sep = " , ")
```

take a look

```{r}
#| label: viewing-city-state-orig

reverse %>%
  gt() %>%
  tab_options(container.height = px(300),
              container.padding.y = px(24))
```

Now convert state into the abbreviation.

```{r}
#| label: state-name-to-abbrev
 
reverse$state <- state.abb[match(reverse$state, state.name)]
```

```{r}
#| label: checking-for-failures
reverse %>% filter(is.na(state) == TRUE)
```

NA arises from location that is in Canada. This can safely be encoded as an incorrect state.

```{r}
#| label: viewing-reverse-with-abbrev

reverse %>% gt() %>%
  tab_options(container.height = px(300),
              container.padding.y = px(24))
```

```{r}
#| label: creating-correct_state-boolean

reverse <- reverse %>%
  mutate(correct_state = (state_orig == state))

reverse <- reverse %>%
  mutate(correct_state = ifelse(country == "Canada", FALSE, correct_state))
                       
```

How many right?

```{r}
#| label: stats-correct_state
#sum(reverse$correct_state, na.rm = TRUE)
#mean(reverse$correct_state, na.rm = TRUE)

sum(reverse$correct_state)
mean(reverse$correct_state)
```

That's not terrible, but it isn't great. Some simple checks could have shown that the geocoding had errors.

Now, I'm going to remake Fig 5.

```{r}
#| label: joining-reverse-to-fraud

fraud <-
  fraud %>% 
  left_join(reverse, by = c("city" = "city_orig", "state" = "state_orig"))
```

Remember what I did was calculate the distance between lat/long and my calculated lat/long (now called lat_home and long_home)

```{r}
#| label: calculating-distance

# convert to radians
fraud <- fraud %>%
  mutate(
    lat1_radians = lat_home / 57.29577951,
    lat2_radians = lat / 57.29577951,
    long1_radians = long_home / 57.29577951,
    long2_radians = long / 57.29577951
  )

# calculating distance
fraud <-
  fraud %>% mutate(distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)
  ))
```

Now remake the Figure 5.

```{r}
#| label: remake-fig-5-coded-by-correct_state

ggplot(fraud, aes(distance_miles, is_fraud , fill = correct_state)) +
  geom_point(
    alpha = 1,
    shape = 21,
    colour = "black",
    size = 5,
    position = "jitter"
  ) +
  scale_fill_viridis(
    discrete = TRUE,
    labels = c('Wrong State', 'Right State'),
    name = ""
  ) +
  ggtitle("Figure 5 Remake: How far from home does fraud occur?") +
  xlab("Distance from Home (miles)") +
  ylab("Is Fraud?")
```

Let's look at the transactions that had a distance greater than 50 miles. They are in the correct state, but I think they must also be incorrectly geocoded.

```{r}
#| label: viewing-50mi-locations

fraud %>% filter((distance_miles > 50) &
                   (correct_state == TRUE)) %>%
  select(city, state, city.y, hamlet, town, village, state.y) %>%
  distinct(city, state,  city.y, hamlet, town, village, state.y) %>%
  gt() %>%
  tab_options(container.height = px(300),
              container.padding.y = px(24))
```

Okay, some definitely are. Vinton/ Winton. Not sure what the deal was with Kaktovik Alaska. It is a small town with 300 people or something. The census bureau says the town has an area of 1 sq. mile.

I expected error handling would be handled by the geocoding service; I expected an NA if there wasn't a city in that state. I didn't even check this at any level, other than for NAs.

**Conclusion: Check your work**

### Is the new definition of lat/long consistent with the rest of the data?

If lat/long is the location of the card holder's home, then I'd expect the following things to be true.

1.  Each unique customer should be associated with only one lat/long, but a lat/long might be associated with multiple customers (e.g. a couple living together with separate accounts). It is also possible that the coordinates are not highly localized- they might represent a city center rather than a specific address and thus could be the same for everyone in that city.
2.  The lat/ long should match the city/state info which is also supposed to be related to the customers address. (See above.)

Unfortunately, I can't prove that each lat/long is associated with a unique customer because I do not have any unique identifier for the customers. (The kaggle dataset includes credit card number, which would be the unique identifier.)

What I can do is show that lat/long as customer location is consistent with the other data in this dataset.

## Assume I can ID Unique Users

I'm going to assume that the combination of date of birth, job, and home location (lat/long) uniquely identifies a user within this dataset. That uses all the personally idenitifying information in the dataset. The dataset does contain city, state, but that information should be redundant with home location if the new definition is correct. For what it is worth, using city/state instead of home location gives the same result as does omitting job.

Get unique DOBs and associated coordinates. I'm creating a "ID", which is just the row number. I want to color some of my graphs by unique person, and using dob sometimes created odd scales, likely because it is a date.

```{r}
#| label: unique-dobs

unique_dobs <- fraud %>% 
  group_by(dob, job, lat, long) %>%
  count(sort = TRUE)

unique_dobs <- tibble::rowid_to_column(unique_dobs, "ID")

```

```{r}
#| label: viewing-unique-dobs

unique_dobs %>% gt() %>%
  tab_options(container.height = px(300),
              container.padding.y = px(24))
```

From this, it appears the dataset contains 187 unique persons. Again, take "unique person" with a grain of salt. This isn't a unique identifier; it is trivial to come up with scenarios where two people would have all of this information the same. I can't even prove that these are unique IDs within this dataset. However, assuming the dataset has been generated with reasonable paramaters, it is unlikely that there are more than a handful of duplicates. (The US currently has a rate of \~3 twins per 100 births, so our dataset might include 6 pairs of twins. The likelihood of them choosing the same profession and living "together" reduces that possibility more. The odds of two strangers having the same DOB, job, and home location is probably much lower than that.) I'm going to stop putting quotes around "unique", just remember that might not be.

I want to add back the states data. I'm going to filter by states later on, so having this in here helps.

```{r}
#| label: states-in-fraud

states <- fraud %>%
  select(dob, lat, long, state) %>%
  distinct()

```

Now, I'm going it to the list of unique card holders.

```{r}
#| label: join-states-to-unique_dob

unique_dobs <- unique_dobs %>%
  left_join(states)

unique_dobs %>% gt() %>%
  tab_options(container.height = px(300),
              container.padding.y = px(24))
```

The most prolific card users have \~ 4000 transactions in this time period. The least prolific only have a handful.

```{r}
#| label: unique-dobs-histogram

unique_dobs %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 15) +
  ggtitle("Distribution of Number of Transactions")
```

I might expect this distribution to be bimodal- some people use the credit card for everything (to maximize credit card rewards, for example), while some might only use it for large transactions (where they actually need credit).

How many users do we have per state?

```{r}
#| label: num-users-by-state

unique_dobs %>% 
  group_by(state) %>%
  count(sort = TRUE) %>%
  gt() %>%
  tab_options(container.height = px(300),
              container.padding.y = px(24))
```

This dataset is actually pretty sparse.

I'm going to join the ID back on to fraud, so I have access to it in all dataframes. This will allow me to have the same color scale in various graphs.

```{r}
#| label: id-joined-to-fraud

fraud <- fraud %>%
  left_join(unique_dobs) %>%
  select(-n)
```

checking

```{r}
#| label: quick-look-at-fraud
fraud %>% head() %>% gt()
```

## Mapping

Now I'm going to put my proposed unique cardholders and transactions on a single map.

### Making a Base Map

Find a shape file of the US. I took one from the census bureau.

<https://stackoverflow.com/questions/61282572/cant-read-shp-file-in-r>

```{r}
#| label: reading-census-st

census_states <- st_read(dsn = "cb_2018_us_state_500k.shp")
```

The CRS is NAD83.

See how this map looks.

```{r}
#| label: viewing-census-map
#| 
ggplot() +
  geom_sf(data = census_states)
```

It does contain us territories. Also, all of Alaska (the island chain to the far right of the graph). I'm going to filter this to match the geographic extent of the data and also to exclude territories.

```{r}
#| label: finding-min-max-merch_long

min_long <- min(min(fraud$long), min(fraud$merch_long))
max_long <- max(max(fraud$long), max(fraud$merch_long))

```

now latitude

```{r}
#| label: finding-min-max-merch_lat

min_lat <- min(min(fraud$lat), min(fraud$merch_lat))
max_lat <- max(max(fraud$lat), max(fraud$merch_lat))
```

blah

```{r}
#| label: filtering-census-map

census_states <- census_states %>%
  filter(STATEFP < 57) %>%
  st_crop(
    xmin = min_long,
    xmax = max_long,
    ymin = min_lat,
    ymax = max_lat
  )
```

Now look at the map.

```{r}
#| label: viewing-filtered-map


census_states %>%
ggplot() +
  geom_sf(aes(fill = NAME)) +
  guides(fill = "none") 
```

### Making Simple Features (SF) Objects from our data

Merchant

```{r}
#| label: making_sf_object_merchant

merchant_points <-
  st_as_sf(fraud,
           coords = c("merch_long", "merch_lat"),
           crs = 4269)
```

home points

```{r}
#| label: making_sf_object_users

home_points <-
st_as_sf(unique_dobs, coords = c("long", "lat"), crs = 4269)
```

### Mapping Users

```{r}
#| label: mapping-users

ggplot() +
  geom_sf(data = census_states) +
  geom_sf(data = home_points, alpha = 1, size = 1)
```

That's a really funny pattern. The midwest is represented only by a thin band through the center.

### Mapping Transactions

```{r}
#| label: mapping-transactions

ggplot() +
  geom_sf(data = census_states) +
  geom_sf(data = merchant_points, alpha = 0.01, size = 1)
  
```

So the transactions seem to be clustered in squares. Are these clusters centered around the home locations?

color palette

```{r}
#| label: making-187-palette

pal2 <- distinctColorPalette(k = 187, altCol = FALSE, runTsne = FALSE)

```

looking at colors

```{r}
#| label: viewing-187-palette

library(scales)
show_col(pal2 , labels = FALSE)
```

Now look at a combined map.

```{r}
#| label: viewing-combined-map

ggplot() +
  geom_sf(data = census_states) +
  geom_sf(
    data = merchant_points,
    aes(fill = ID, color = ID),
    alpha = 0.01,
    shape = 21,
    size = 1
  ) +
  geom_sf(data = home_points,
          shape = 8,
          size = 1,
          aes(color = ID)) +
  scale_fill_gradientn(colors = pal2) +
  scale_color_gradientn(colors = pal2) +
  guides(fill = "none", color = "none") 
```

Qualitatively, it seems like each cardholder is surronded by a cluster of transactions. (This visualization might be nicer as a dynamic presentation where you could zoom in, but most techniques available to a casual user don't handle this many points gracefully. Even Tableau hangs at this visualization.)

Let's look at one state. Arizona has 7 users, so that seems reasonable.

```{r}
#| label: filtering-for-AZ

census_AZ <- census_states %>%
  filter(STUSPS == "AZ")

merchant_AZ <- merchant_points %>%
  filter(state == "AZ")

home_AZ <- home_points %>%
  filter(state == "AZ")
```

create a new palette

```{r}
#| label: AZ-palette
#| 
#pal_AZ <- palette(rainbow(7))
color_rain = palette(rainbow(50))

pal_AZ <- distinctColorPalette(k = 7, altCol = FALSE, runTsne = FALSE)

show_col(pal_AZ)

#pal_AZ2= c("red", "#FFDB00", "#49FF00", "#00FF92","#0092FF", "#4900FF", "pink")
```

```{r}
#| label: AZ-map


AZ_graph <- ggplot() +
  geom_sf(data = census_AZ, fill = "white") +
  geom_sf(
    data = merchant_AZ,
    aes(fill = ID, color = ID),
    alpha = 0.05,
    shape = 21,
    size = 1
  ) +
  geom_sf(
    data = home_AZ,
    shape = 21,
    size = 3,
    aes(fill = ID),
    color = "black"
  ) +
  scale_fill_gradientn(colors = pal_AZ) +
  scale_color_gradientn(colors = pal_AZ) +
  # scale_fill_discrete() +
  guides(fill = "none", color = "none")


```

Showing Map

```{r}
#| label: showing-az-map

AZ_graph
```

So this seems pretty suggestive that transactions are contrained to a bounding box around the home coordinates of the user.

### How They Actually Sampled

```{r}
#| label: calculating-delta-coords

fraud <-
  fraud %>% 
  #select(lat, long, merch_lat, merch_long) %>%
  mutate(lat_delta = lat - merch_lat,
         long_delta = long - merch_long)
```

check uniformity COW PLOT LAT AND LONG GRAPHS

```{r}
#| label: viewing-delta-lat

ggplot(fraud, aes(lat_delta)) +
  geom_histogram()
```

```{r}
fraud %>%
  group_by(merch_lat, merch_long) %>%
  count(sort = TRUE) %>% gt() %>%
  tab_options(container.height = px(300),
              container.padding.y = px(24))
```

Distance vs. Hacky Distance.

```{r}
#| label: calc-correct-distance

# convert to radians
fraud <- fraud %>%
  mutate(
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951
  )

# calculating distance
fraud <-
  fraud %>% mutate(distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)
  ))


```

I'm going to visualize it anyway.

```{r}
#| label: distance-and-fraud
# Code Block 15: Distance from Home and Fraud
ggplot(fraud, aes(distance_miles, is_fraud , fill = factor(is_fraud))) +
  geom_point(
    alpha = 1,
    shape = 21,
    colour = "black",
    size = 5,
    position = "jitter"
  ) +
  scale_fill_viridis(
    discrete = TRUE,
    labels = c('Not Fraud', 'Fraud'),
    name = ""
  ) +
  ggtitle("Figure 5 Correct: How far from home does fraud occur?") +
  xlab("Distance from Home (miles)") +
  ylab("Is Fraud?") 
```

And as a distribution

```{r}
ggplot(fraud) +
  geom_histogram(aes(distance_miles), color = "red", bins = 50) 
```

fraud vs not fraud

```{r}

fraud %>% filter(is_fraud == 1) %>%
ggplot() +
  geom_histogram(aes(distance_miles), color = "red", bins = 50) 
```

Is not_fraud

```{r}
fraud %>% filter(is_fraud == 0) %>%
ggplot() +
  geom_histogram(aes(distance_miles), color = "red", bins = 50) 
```

## 

for the density map

```{r}
# fraud_only <- fraud %>% filter(is_fraud == TRUE) %>% select(distance_miles)
# legit_only <- fraud %>% filter(is_fraud == FALSE) %>% select(distance_miles)

fraud_only <- fraud %>% filter(is_fraud == TRUE) 
legit_only <- fraud %>% filter(is_fraud == FALSE) 
```

make into a single df

```{r}
ggplot() +
  geom_density(data = fraud_only, aes(distance_miles) , alpha = 0.5, fill = "blue") +
  geom_density(data = legit_only , aes(distance_miles),  alpha = 0.5 , fill = "yellow") +
  geom_vline(xintercept = 70) +
  theme_classic() +
  labs(title = "Evolution of Word Scores as Guessing Progresses",
       caption = "for 5 letter words") +
  xlab("Distance(miles)") +
  ylab("Density") 
```

now use the hack to calculate distance

```{r}
fraud <- fraud %>%
  mutate(distance_hacky  = (sqrt((lat-merch_lat)^2 + (long-merch_long)^2)*70))
```

graph

```{r}
ggplot(fraud) +
  geom_histogram(aes(distance_hacky), color = "red", bins = 50) +
  geom_vline(xintercept = 70) 
  
```

broken code

```{r}
{r}
m <- ggplot(fraud, aes(distance_miles, is_fraud))
m + stat_summary(fun = "mean", geom = "bar")
```

### Geocoding Services

Different geocoding services report different hierachies and have different architectures. This geocoding service reports a fixed hierarchy- if a particular level isn't defined then it returns NA. Presumably the data simulator used a certain level from the geocoding service it used, rather than switching between hamlet, village, city, etc.. It seems likely that the geocoding service used was not the one I used. I thought I could pop into the simulator code

```{r}
#| label: viewing-delta-long

ggplot(fraud, aes(long_delta)) +
  geom_histogram()
```

## A million unique merchants

On the other hand, every transaction is associated with a unique merchant location. This isn't particularly realistic, and makes visualizing the date more difficult.

Not only are there no duplicate merchant locations for specific users...

```{r}
fraud %>%
  group_by(dob, lat, long, merch_lat, merch_long) %>%
  count(sort = TRUE) %>% gt() %>%
  tab_options(container.height = px(300),
              container.padding.y = px(24))
```

But there are no duplicate merchant locations over the entire dataset.
