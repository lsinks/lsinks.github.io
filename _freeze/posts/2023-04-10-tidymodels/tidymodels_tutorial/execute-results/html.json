{
  "hash": "7de6405baa919cfe3dc3625f0bdfed0e",
  "result": {
    "markdown": "---\ntitle: \"A Tidymodels Tutorial: A Structural Approach\"\ndescription: \"Exploring the different steps for modeling\"\ntwitter-card:\n  image: \"thumbnail.png\"\nauthor:\n  - name: Louise E. Sinks\n    url: https://lsinks.github.io/\ndate: 04-10-2023\ncategories: [R, R-code, tidymodels, Machine Learning] # self-defined categories\ncitation:\n  url: https://lsinks.github.io/posts/2023-04-10-tidymodels/tidymodels_tutorial\nimage: \"thumbnail.png\"\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\n\nAs I've started working on more complicated machine learning projects, I've leaned into the tidymodels approach. Tidymodels is a highly modular approach, and I felt it reduced the number of errors, especially when evaluating many machine models and different preprocessing steps. (This is, in fact, a stated goal of the tidymodels ecosystem.)\n\nThis tutorial is more about understanding the process of modeling in tidymodels and learning about the various objects created at different steps rather than optimizing a machine learning model.\n\nThroughout this tutorial, I will use the word \"procedure\" to describe a set of steps to go from data to final predictions. I'm doing this because tidymodels uses the word workflow for specific types of objects and functions. It would be too confusing to use workflow to also describe the process/procedure.\n\nBut the tidymodels ecosystem can also be very confusing. There are several component packages in tidymodels. While it is easy to explain what a recipe object (from the recipe package) does, it became increasingly difficult for me to name and describe the objects I was creating as I started building more sophisticated machine-learning procedures. And I found it even more confusing that simple and complex procedures, while going through the same basic steps (preprocess, train, evaluate, predict), created objects with different structures and data within them. I found it confusing that `fit`, `last_fit`, `fit_resamples`, etc., did not all produce objects that contained the same information and could be acted on by the same functions. In my first attempt at using `last_fit()`, I ended up scrapping the entire ML section and redoing it with `fit()`/`predict()` because I couldn't figure out how to get the predictions out of the object created by `last_fit()`.\n\nAdding to my woes was the fact that attempting to view/print/ examine these objects, especially in notebook environments, often caused the entire project to time out. RStudio generally handles these objects more gracefully, but I've also crashed it hard. It also isn't consistent whether an object will lock-up RStudio or not. Once RStudio has locked up, restarting the program leads to an increasing number of freezes/locking up, until the computer is restarted.\n\nI've also manually numbered my code blocks and used that for referencing. I believe it is possible to hyperlink code chunks in Quarto, but I plan to replicate this project in an online notebook environment where that isn't possible. The manual numbering will make it easier to cross-reference the two. I found online notebooks really did not like displaying many tidymodels objects. That's also why there are timers around many of the display calls.\n\nSo here I'm going to go through three different procedures for modeling. I will compare and contrast the objects created as we move through the different procedures.\n\n# Loading libraries and Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 1: Loading Libraries\n# loading tidyverse/ tidymodels packages\nlibrary(tidyverse) #core tidyverse\nlibrary(tidymodels) # tidymodels framework\n\n# Modeling\nlibrary(glmnet) # elastic net logistic regression\nlibrary(themis) # provides up/down-sampling methods for the data\n```\n:::\n\n\nDetails about how the data was processed can be found at [my Credit Card fraud tutorial.](https://lsinks.github.io/posts/2023-04-11-credit-card-fraud/fraud_tutorial)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 2- loading the processed data\nfraud <- read_rds(\"fraud_processed.rds\")\nfraud$category <- factor(fraud$category)\n```\n:::\n\n\nTidymodels expects the outcome to be a factor. It also treats [the first level as the event](https://community.rstudio.com/t/tidymodels-which-factor-level-is-the-default-positive-class/100428). So, Code Block 3 handles these details.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 3: outcome to factor and relevel\n# in tidymodels, outcome should be a factor\nfraud$is_fraud <- factor(fraud$is_fraud)\nlevels(fraud$is_fraud)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"0\" \"1\"\n```\n:::\n\n```{.r .cell-code}\n#first level is the event in tidymodels, so we need to reorder\nfraud$is_fraud <- relevel(fraud$is_fraud, ref = \"1\")\nlevels(fraud$is_fraud)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"1\" \"0\"\n```\n:::\n:::\n\n\n# Resampling via rsample\n\nThe rsample package is used to [create splits and folds](https://rsample.tidymodels.org/reference/index.html) from your data. Here I use [`initial_split()`](https://rsample.tidymodels.org/reference/initial_split.html) to create a testing and training dataset. The resulting object is called an `rsplit` object and contains the original data and information about whether a record goes to testing or training. This object is not a flat dataframe but rather a nested list. The functions `testing()` and `training()` are used to create the appropriate tibbles for fitting. Other functions are available to [visualize](https://rsample.tidymodels.org/reference/tidy.rsplit.html) or [manipulate](https://rsample.tidymodels.org/articles/rsample.html) the `rsplit` object. Typing `data_split` in RStudio produces a high-level overview of the object:\n\n`<Training/Testing/Total>`\n\n`<254705/84902/339607>`\n\nI will also create some cross-validation folds using `vfold_cv()`. The resulting object is an `rset` object, which is a collection of `rsplit` objects (which [can be retrieved](https://rsample.tidymodels.org/reference/get_rsplit.html) from the `rset` object), The same methods to view or manipulate the `rsplit` object work on the rset object.\n\nBoth functions let you sample based on strata. This is highly recommended, especially for classification problems with imbalanced classes. The sample is performed separately on each class, which assures your testing/training/folds contain representative data.\n\nI did notice that the typo \"stata\" didn't kick up any sort of error. In fact, you can include any number of named parameters that don't exist, and you won't get an error. Positional matching shouldn't apply if you are using named parameters, but for what it is worth, `initial_split(fraud, prop = 0.75, mouse = is_fraud)` and `initial_split(fraud, mouse = is_fraud, prop = 0.75 )` both execute without complaint. And they both produce stratified samples, which is weird. Don't rely on this and do check that your splits and folds are properly stratified.\n\nSetting the random seed before running these functions is highly recommended for reproducibility.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 4 : Train/Test Splits & CV Folds \n# Split the data into a test and training set\n# following https://www.tidymodels.org/start/recipes/#recipe\nset.seed(222)\ndata_split <- initial_split(fraud, prop = 0.75, strata = is_fraud)     \n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\nstart_time <- Sys.time()\nset.seed(123)\nfraud_folds <- vfold_cv(train_data, v = 3, strata = is_fraud)\nend_time <- Sys.time()\n\nstart_time_display <- Sys.time()\nfraud_folds \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#  3-fold cross-validation using stratification \n# A tibble: 3 × 2\n  splits                 id   \n  <list>                 <chr>\n1 <split [169803/84902]> Fold1\n2 <split [169803/84902]> Fold2\n3 <split [169804/84901]> Fold3\n```\n:::\n\n```{.r .cell-code}\nend_time_display <- Sys.time()\n\nprint(paste(\"Making folds: \", end_time - start_time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Making folds:  0.32884407043457\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Printing fraud_folds: \", end_time_display - start_time_display))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Printing fraud_folds:  0.0849518775939941\"\n```\n:::\n:::\n\n\n# Preprocessing with recipes\n\nThe [recipes package](https://recipes.tidymodels.org/) bundles the formula, data, and feature engineering steps into a recipe object.\n\nI set the formula and training data here and then performed preprocessing/ feature engineering steps. All the feature engineering steps have the form `step_*()`. I chose the feature engineering steps based on the [Appendix](https://www.tmwr.org/pre-proc-table.html) from the [Tidy Modeling with R](https://www.tmwr.org/) book, which lists models and which feature engineering steps are required or recommended.\n\nI want to compare using the full dataset with a downsampled dataset with balanced classes, so I also created a downsample recipe. This section is where the strengths of the modularity of tidymodels start to shine. You can create several related recipes off of a base recipe. For complex projects where many preprocessing steps or different formulas are tested, decoupling the recipe step from the fitting reduces the errors that might arise from creating multiple combinations.\n\nThe recipe is built with the training dataset. This data is used to estimate some of the values of the recipe steps, such as the number of dummy variables created, but the recipe isn't \"trained\" yet. The recipe will be applied to the training data in later steps, and the necessary values for feature engineering will be calculated and stored. These values will be used on subsequent datasets, such as the testing set. This eliminates a possible source of data leakage. For example, using an imputation step based on the mean should use the mean of the training data and not the entire dataset (which would have information about the testing set within).\n\nThere are a variety of functions, [such as `prep()`, `bake()`, and `juice()`](https://recipes.tidymodels.org/reference/index.html) which can be used to apply the recipe object to datasets. These can be used in the machine learning procedures, but here we will use the workflow procedure, which handles these steps automatically. These functions are found in some tutorials online, so it is important to be aware of them. You can also use these functions to preprocess data for reasons other than modeling.\n\nThe recipe object is another [complicated object and contains a variety of objects](https://recipes.tidymodels.org/reference/recipe.html). RStudio provides a high-level summary when you view this object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 5: recipes\nrecipe_plain <- \n  recipe(is_fraud ~ ., data = train_data) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_zv(all_predictors()) \n\nrecipe_down <- \nrecipe_plain %>%\n  step_downsample(is_fraud)\n\nstart_time_display <- Sys.time()\nrecipe_down\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Inputs \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNumber of variables by role\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\noutcome:   1\npredictor: 8\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Operations \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n• Dummy variables from: all_nominal_predictors()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n• Centering and scaling for: all_numeric_predictors()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n• Zero variance filter on: all_predictors()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n• Down-sampling based on: is_fraud\n```\n:::\n\n```{.r .cell-code}\nend_time_display <- Sys.time()\nprint(paste(\"Printing recipe_down: \", end_time_display - start_time_display))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Printing recipe_down:  0.32622504234314\"\n```\n:::\n:::\n\n\n# Defining the model with parsnip\n\nThe [parsnip package](https://parsnip.tidymodels.org/) handles the modeling. However, many modeling functions and objects arise from other tidymodels packages, not parsnip, as I will discuss later. This confusion can lead to difficulties in handling fit and predictions.\n\nThe type of problem being solved and the method to solve the problem are often bundled together in parsnip, as I've done here. I set the type of problem with [`logistic_reg()`](https://parsnip.tidymodels.org/reference/logistic_reg.html). A list of types and engines can be found [here](https://www.tidymodels.org/find/parsnip/). Parameters can be set here to pass to the underlying engine later. The parsnip package is designed to create a harmonized interface to the various independent engines/packages that have been created in R, so you can set the parameters even without choosing an engine/package. [For example](https://parsnip.tidymodels.org/), all tree-based models will use \"trees\" for the number of trees. I wanted to tune the hyperparameters of the elastic net logistic regression. This can be done by setting the parameter equal to tune(). I'm not going to get into the [tune package](https://tune.tidymodels.org/index.html) in detail, but it contains a variety of functions related to tuning hyperparameters. (This is pretty much the [overview statement on the package page](https://tune.tidymodels.org/), which we will see later is deceptively incomplete.) These are passed to functions of other packages in Tidymodels (e.g., parsnip) and are not really stand-alone functions.\n\nYou use `set_engine()` to specify the particular package you want to use to solve the problem (e.g., glm).\n\nThe objects created by these functions don't have a simple name like the objects created by rsample and recipe do. The function that sets the type of problem creates \"A model specification object,\" and the [`set_engine()`](https://parsnip.tidymodels.org/reference/set_engine.html) creates \"An updated model specification.\"\n\nRStudio will again create a high-level summary of these objects, but using `View()` reveals that they are a complicated nested list. I don't think there should be a need to extract components of this object as there might be for some of the earlier objects.\n\nAt this point, you can complete your machine learning procedure entirely within parsnip. Various fitting and predicting functions are available. However, I'm going to continue to the workflows package, which will allow us to create bundles of models and fits.\n\nI should note that these are not necessarily the best choices for this problem. I chose logistic regression and downsampling because they were fast, not because they were optimal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 6: Setting engines\n\n# this is the standard logistic regression\nlogreg_spec <- \n  logistic_reg() %>% \n  set_engine(\"glm\")\n\n# elastic net regularization of logistic regression\n# this has 2 hyperparameters that we will tune\nglmnet_spec <- \n  logistic_reg(penalty = tune(),\n               mixture = tune()) %>%\n  set_engine(\"glmnet\")\n\nstart_time_display <- Sys.time()\nglmnet_spec \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet \n```\n:::\n\n```{.r .cell-code}\nend_time_display <- Sys.time()\nprint(paste(\"Printing glmnet_spec: \", end_time_display - start_time_display))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Printing glmnet_spec:  0.0121080875396729\"\n```\n:::\n:::\n\n\n# Creating a metrics set with yardstick\n\nThe [yardstick package](https://yardstick.tidymodels.org/) contains the functions to calculate a variety of metrics such as sensitivity, specificity, etc. I bundled a couple of metrics together as a metrics set, which I will pass to other functions later. The metrics set fraud_metrics contains some metrics that require probabilities, while fraud_metrics_hard only includes accuracy, which uses the hard classifications. These two metric sets will produce different results from fitting and predicting operations, which I will show you later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 7: Setting Metrics\nfraud_metrics <- metric_set(roc_auc,\n                            accuracy, sensitivity, specificity, j_index)\n\nfraud_metrics_hard <- metric_set(accuracy)\n\nstart_time_display <- Sys.time()\nfraud_metrics \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 3\n  metric      class        direction\n  <chr>       <chr>        <chr>    \n1 roc_auc     prob_metric  maximize \n2 accuracy    class_metric maximize \n3 sensitivity class_metric maximize \n4 specificity class_metric maximize \n5 j_index     class_metric maximize \n```\n:::\n\n```{.r .cell-code}\nend_time_display <- Sys.time()\nprint(paste(\"Printing fraud_metrics: \", end_time_display - start_time_display))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Printing fraud_metrics:  0.0971019268035889\"\n```\n:::\n:::\n\n\n# Bundling everything together with workflows\n\nIn my opinion, the [workflows](https://workflows.tidymodels.org/) and [workflowset](https://workflowsets.tidymodels.org/index.html) packages are the most powerful part of the tidymodels system. As I've worked through the procedure, I've created many objects: datasets with rsample, recipes, and models. I've said that modularity is an advantage, but it might be challenging to keep track of which pieces go together when faced with so many different objects. Workflows allow you to bundle your preprocessing and modeling objects together. (In theory, you can also bundle postprocessing objects, but this functionality is not available yet.)\n\nWorkflowsets allow you to bundle many workflows into a single object and pass them to fitting or predicting functions as a group. I wanted to evaluate 16 different model/preprocessing pairs in the credit card fraud tutorial. Constructing that many workflows leads to many opportunities for typos or copy/paste errors. But with `workflow_set()` you can pass the four recipes and the four model specification objects, and the function will create all 16 combinations. If you don't want all combinations, you can [manually construct a workflow_set where you set the combinations you need](https://www.tmwr.org/workflow-sets.html).\n\nHere I created a simple workflow that contains a single recipe and model specification and a simple workflow_set that contains four workflows.\n\nWhen called, the RStudio again displays high-level information for the workflow and the workflow_set. Using `View()` simple workflow shows a nested list structure. Using `View()` on the workflow_set, even the small one here, crashes RStudio for me.\n\nIt is important to keep track of whether you are using workflows or workflowsets because they have different helper functions to extract the final information from the fits.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 8: Create simple workflow to Compare Fit/Predict and last_fit\nwf_simple <-\n  workflow() %>%\n  add_recipe(recipe_plain) %>%\n  add_model(logreg_spec)\n\n# showing high-level info\nwf_simple\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 9: creating a workflow set\nwf_set_tune <-\n  workflow_set(\n    list(plain = recipe_plain, down = recipe_down),\n    list(glmnet = glmnet_spec, logreg = logreg_spec)\n  )\n\nstart_time_display <- Sys.time()\nwf_set_tune #don't call View()!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A workflow set/tibble: 4 × 4\n  wflow_id     info             option    result    \n  <chr>        <list>           <list>    <list>    \n1 plain_glmnet <tibble [1 × 4]> <opts[0]> <list [0]>\n2 plain_logreg <tibble [1 × 4]> <opts[0]> <list [0]>\n3 down_glmnet  <tibble [1 × 4]> <opts[0]> <list [0]>\n4 down_logreg  <tibble [1 × 4]> <opts[0]> <list [0]>\n```\n:::\n\n```{.r .cell-code}\nend_time_display <- Sys.time()\nprint(paste(\"Printing wf_set_tune: \", end_time_display - start_time_display))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Printing wf_set_tune:  0.146723031997681\"\n```\n:::\n:::\n\n\n# Fitting: `fit()`/`predict()` vs. `last_fit()`\n\nI'm going to show you the difference between fit()/ predict() and last_fit() using the simple workflow. These are two different procedures that should contain the same results (a fitted model on the training data and the predictions from that model for the test data).\n\n## `fit()`/`predict()`\n\nFirst, I fit the model on the training data to get the fit and then I pass that fit and the test data to `predict()` to get the predictions for test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 10: Run fit/ predict on workflow\nwflow_fit <- fit(wf_simple, data = train_data)\nwflow_predict <- predict(wflow_fit, new_data = test_data)\nwflow_predict2 <- predict(wflow_fit, new_data = test_data, type = \"prob\" )\n```\n:::\n\n\nWhat comes out of predict is super simple to understand. It is a list of predictions. No complicated nested list objects here. If I want probabilities instead of hard classification, I pass `predict()` the argument `type = \"prob\"` to get the probabilities instead.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 11:  Examine the output of predict\nhead(wflow_predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 1\n  .pred_class\n  <fct>      \n1 0          \n2 0          \n3 0          \n4 0          \n5 0          \n6 0          \n```\n:::\n\n```{.r .cell-code}\nhead(wflow_predict2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n    .pred_1 .pred_0\n      <dbl>   <dbl>\n1 0.00367     0.996\n2 0.00144     0.999\n3 0.0000262   1.00 \n4 0.00461     0.995\n5 0.0000279   1.00 \n6 0.00138     0.999\n```\n:::\n:::\n\n\nWhat about our model? Maybe I want model coefficients or to see which features are most important. There is a lot of information here, but it isn't very well structured. Again, this is a nested list. RStudio is displaying this nicely and the details can be seen using `View()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 12: Examine the outcome of fit \nwflow_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n            (Intercept)                lat_trans               long_trans  \n                7.06537                 -0.10230                 -0.01413  \n         distance_miles                      age                     hour  \n                0.06526                 -0.26818                 -0.82812  \n                weekday                  amt_log     category_food_dining  \n               -0.12721                 -1.87149                 -0.00929  \n category_gas_transport     category_grocery_net     category_grocery_pos  \n               -0.62772                 -0.29571                 -0.67063  \ncategory_health_fitness            category_home       category_kids_pets  \n                0.06286                  0.10517                  0.01683  \n      category_misc_net        category_misc_pos   category_personal_care  \n               -0.42138                 -0.13380                 -0.05152  \n  category_shopping_net    category_shopping_pos          category_travel  \n               -0.38932                 -0.16399                  0.18122  \n\nDegrees of Freedom: 254704 Total (i.e. Null);  254684 Residual\nNull Deviance:\t    16570 \nResidual Deviance: 11910 \tAIC: 11950\n```\n:::\n:::\n\n\nWhile you can use standard R operations for interacting with lists and nested data to extract the desired information from `wflow_fit`, it is much easier to use the [broom package](https://broom.tidymodels.org/). Broom is part of the core tidymodels installation, so it does not need to be installed separately. To get the model coefficients and p-values in tibble form, use `tidy()`. For high-level statistics about the model, use `glance()`. Just remember that the information you extract from the output of `fit()` relates to the model as applied to the training data. For information about the model performance as applied to the test data, you need to use the output of `predict()`. Since this output is only a vector of predictions, you need to bind it to the test dataframe and then perform analysis on the new object.\n\nSo it is pretty straightforward to get our model coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 13: Summarize wflow_fit with tidy\nwflow_fit %>% tidy() #summarizes information about model components\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 21 × 5\n   term                   estimate std.error statistic   p.value\n   <chr>                     <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)             7.07       0.0703   101.    0        \n 2 lat_trans              -0.102      0.0305    -3.36  7.94e-  4\n 3 long_trans             -0.0141     0.0306    -0.462 6.44e-  1\n 4 distance_miles          0.0653     0.0318     2.05  4.02e-  2\n 5 age                    -0.268      0.0289    -9.27  1.87e- 20\n 6 hour                   -0.828      0.0397   -20.9   1.27e- 96\n 7 weekday                -0.127      0.0288    -4.41  1.03e-  5\n 8 amt_log                -1.87       0.0510   -36.7   2.76e-294\n 9 category_food_dining   -0.00929    0.0599    -0.155 8.77e-  1\n10 category_gas_transport -0.628      0.0593   -10.6   3.62e- 26\n# ℹ 11 more rows\n```\n:::\n:::\n\n\nOr to get details of the model performance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 14: model info from wflow_fit with glance\nwflow_fit %>% glance() #reports information about the entire model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 8\n  null.deviance df.null logLik    AIC    BIC deviance df.residual   nobs\n          <dbl>   <int>  <dbl>  <dbl>  <dbl>    <dbl>       <int>  <int>\n1        16568.  254704 -5956. 11953. 12173.   11911.      254684 254705\n```\n:::\n:::\n\n\n## `last_fit()`\n\nSo, from the [tidymodels webpage](https://tune.tidymodels.org/reference/last_fit.html), `last_fit()` is described as \"`last_fit()` emulates the process where, after determining the best model, the final fit on the entire training set is needed and is then evaluated on the test set.\" (Actually this is from the `tune` subpage, which is important, though I didn't realize it.)\n\nI pass the workflow to `last_fit()` along with the data split object (with the info about testing and training) and the metrics set. In theory, the result should be the same as from `fit()`/`predict()` above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 15: Using lastfit() in hard classifier mode\nlast_fit_results <- last_fit(wflow_fit, data_split, metrics = fraud_metrics_hard)\n```\n:::\n\n\nSo, I look at the results just as I did with predict in Code Block 11. And RStudio sometimes locks up. Other times, it produces a high-level overview as expected.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 16: creating a workflow set\nstart_time_display <- Sys.time()\nhead(last_fit_results) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  <list>                 <chr>         <list>   <list>   <list>       <list>    \n1 <split [254705/84902]> train/test s… <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n\n```{.r .cell-code}\nend_time_display <- Sys.time()\nprint(paste(\"last_fit_results: \", end_time_display - start_time_display))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"last_fit_results:  0.0753130912780762\"\n```\n:::\n:::\n\n\nSo how to get the predictions out? According to the [manual page for `last_fit()`](https://tune.tidymodels.org/reference/last_fit.html), the output is \"A single row tibble that emulates the structure of [fit_resamples()](https://tune.tidymodels.org/reference/fit_resamples.html). However, a list column called .workflow is also attached with the fitted model (and recipe, if any) that used the training set.\" I also see that `last_fit()` is actually from the tune package and not from parsnip as I expected. Nothing I'm doing here involves tuning hyperparameters at all. I expected that is was a parsnip object both thematically and because you interact with `last_fit()` using `extract_fit_parsnip()`, see Code Block 23.\n\nLooking `fit_resamples()` isn't very helpful for answering this question. (Oh, but it is. It just took me another few paragraphs of writing to realize it.)\n\nI did find a [Stackoverflow discussion](#https://github.com/tidymodels/tune/issues/300) that provided the answer in their code: `last_fit1_pred <- last_fit1[[5]][[1]]`\n\nThat's not very straightforward!\n\nPull out the predictions from last_fit_pred.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 17: extracting predictions from last_fit\nlast_fit_pred <- last_fit_results[[5]][[1]]\n```\n:::\n\n\nLook at the head() of this object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 18: Examine the outcome of lastfit by head\nhead(last_fit_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  .pred_class  .row is_fraud .config             \n  <fct>       <int> <fct>    <chr>               \n1 0               1 0        Preprocessor1_Model1\n2 0               2 0        Preprocessor1_Model1\n3 0               8 0        Preprocessor1_Model1\n4 0              12 0        Preprocessor1_Model1\n5 0              13 0        Preprocessor1_Model1\n6 0              14 0        Preprocessor1_Model1\n```\n:::\n:::\n\n\nLook at the head() of the object from predict().\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 19: Examine the outcome of predict by head\nhead(wflow_predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 1\n  .pred_class\n  <fct>      \n1 0          \n2 0          \n3 0          \n4 0          \n5 0          \n6 0          \n```\n:::\n:::\n\n\nUse `identical()` to compare the two hard predictions and verify they are the same.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 20: showing that predict and the predictions in last_fit are the same\nidentical(last_fit_pred$.pred_class, wflow_predict$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nNow, let the realization of what all the stuff about the `tune` package means hit you. We now know the full secrets of `last_fit()`. It turns out that any of the helper functions for tuning functions from the tune package work on `last_fit()` because it is a tune function. I don't find the documentation for either the helper functions or `last_fit()` make that connection clear. I think that is what the reference to `fit_resamples()` on the `last_fit()` page is getting at.\n\n[Tidy Modeling with R](https://www.tmwr.org/) also contains [an example of using collect_predictions](https://www.tmwr.org/workflows.html#evaluating-the-test-set) with `last_fit(`), but most examples are with tuning functions, so obviously from the tune family. [One of the tutorials](https://www.tidymodels.org/start/tuning/#final-model) on the main tidymodels webpage does as well. But in general, extracting predictions from the test data is not demonstrated, just collecting metrics and analyzing model performance. So it is hard to google your way to the answer. This is the kind of situation I've struggled with throughout learning tidymodels and part of what motivated me to write this tutorial.\n\nSo now I get the predictions the easy way.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 21: Examine the outcome of lastfit by head\nhead(last_fit_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  .pred_class  .row is_fraud .config             \n  <fct>       <int> <fct>    <chr>               \n1 0               1 0        Preprocessor1_Model1\n2 0               2 0        Preprocessor1_Model1\n3 0               8 0        Preprocessor1_Model1\n4 0              12 0        Preprocessor1_Model1\n5 0              13 0        Preprocessor1_Model1\n6 0              14 0        Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\nlast_fit_results %>% collect_predictions()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 84,902 × 5\n   id               .pred_class  .row is_fraud .config             \n   <chr>            <fct>       <int> <fct>    <chr>               \n 1 train/test split 0               1 0        Preprocessor1_Model1\n 2 train/test split 0               2 0        Preprocessor1_Model1\n 3 train/test split 0               8 0        Preprocessor1_Model1\n 4 train/test split 0              12 0        Preprocessor1_Model1\n 5 train/test split 0              13 0        Preprocessor1_Model1\n 6 train/test split 0              14 0        Preprocessor1_Model1\n 7 train/test split 0              16 0        Preprocessor1_Model1\n 8 train/test split 0              17 0        Preprocessor1_Model1\n 9 train/test split 0              19 0        Preprocessor1_Model1\n10 train/test split 0              25 0        Preprocessor1_Model1\n# ℹ 84,892 more rows\n```\n:::\n:::\n\n\nAnd can evaluate the model performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 22: collecting metrics from lastfit collect_metrics()\nlast_fit_results %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.995 Preprocessor1_Model1\n```\n:::\n:::\n\n\nAnd extract the fit. This `extract_fit_parsnip()` result is an identical parsnip object as the workflow_fit object we got from `fit()` and can be handled the same way (i.e. via broom). You can refer back to Code Block 13 to see the results are the same. This is perhaps the key takeaway; these larger, more complex objects contain the simpler objects (workflows, parsnip objects) and they should be extracted and handled normally. Understanding this will make understanding how to handle a `workflow_set()` much easier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 23: extract model coefficients from last_fit() \nlast_fit_results %>% extract_fit_parsnip() %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 21 × 5\n   term                   estimate std.error statistic   p.value\n   <chr>                     <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)             7.07       0.0703   101.    0        \n 2 lat_trans              -0.102      0.0305    -3.36  7.94e-  4\n 3 long_trans             -0.0141     0.0306    -0.462 6.44e-  1\n 4 distance_miles          0.0653     0.0318     2.05  4.02e-  2\n 5 age                    -0.268      0.0289    -9.27  1.87e- 20\n 6 hour                   -0.828      0.0397   -20.9   1.27e- 96\n 7 weekday                -0.127      0.0288    -4.41  1.03e-  5\n 8 amt_log                -1.87       0.0510   -36.7   2.76e-294\n 9 category_food_dining   -0.00929    0.0599    -0.155 8.77e-  1\n10 category_gas_transport -0.628      0.0593   -10.6   3.62e- 26\n# ℹ 11 more rows\n```\n:::\n:::\n\n\n# Fitting multiple models at once with workflowsets\n\nI created a workflow_set back in Code Block 9. I pass `workflow_map()` this workflow_set. The next parameter is what type of fitting you want to do. Here, I used `tune_grid` and had it generate 6 grid points. For the models that don't require hyperparameter tuning, the function defaults to `fit_resamples` instead. The acceptable types of fitting functions are found `here`. It is important to note that you can only use fitting methods that operate on folds; you cannot pass `workflow_map()` the entire train or test set and have it work. This caused me a bit of frustration when I was learning this because I wanted to compare the results I got from `workflow_map()` to process all the models simultaneously to what I got when I ran each model/recipe separately. It is implemented this way to encourage proper methodology and avoid data leakage. When evaluating multiple models, you should not be evaluating with the entire dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code block 24: fitting the workflow_set \nstart_time <- Sys.time()\nset.seed(345)\ntune_results <- \n  workflow_map(\n    wf_set_tune,\n    \"tune_grid\",\n    resamples = fraud_folds,\n    grid = 6,\n    metrics = fraud_metrics,\n    verbose = TRUE,  #this gives details about how long each model/recipe takes\n    #control = control_grid(save_pred = TRUE) #save pred for each fold or not\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ni 1 of 4 tuning:     plain_glmnet\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ 1 of 4 tuning:     plain_glmnet (2m 41.2s)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ni\tNo tuning parameters. `fit_resamples()` will be attempted\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ni 2 of 4 resampling: plain_logreg\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ 2 of 4 resampling: plain_logreg (10.9s)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ni 3 of 4 tuning:     down_glmnet\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ 3 of 4 tuning:     down_glmnet (11.5s)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ni\tNo tuning parameters. `fit_resamples()` will be attempted\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ni 4 of 4 resampling: down_logreg\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ 4 of 4 resampling: down_logreg (2.9s)\n```\n:::\n\n```{.r .cell-code}\nend_time <- Sys.time()\n      print(\"Total Time for this Set: \")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Total Time for this Set: \"\n```\n:::\n\n```{.r .cell-code}\nend_time - start_time\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 3.124082 mins\n```\n:::\n:::\n\n\nNow we have another complex object. Displaying this object may or may not work. It has never worked for me and I lost a lot of time figuring that out. I followed a [tutorial from Julia Silge](https://juliasilge.com/blog/project-feederwatch/) did call that object, and it took me a long time to figure out my code was not timing out/locking up from the fitting, but rather from displaying that object.\n\nSo we are going to interact via helper functions. I'm using the other metric set I created back in Code Block 7 . Accuracy is generally a terrible metric for highly imbalanced problems; the model can achieve high accuracy by assigning everything to the majority class. Alternate metrics like [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) or [j-index](https://en.wikipedia.org/wiki/Youden%27s_J_statistic) are better choices for the imbalanced class situation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 25: table of ranked results\nrank_results(tune_results, rank_metric = \"j_index\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 70 × 9\n   wflow_id    .config      .metric  mean std_err     n preprocessor model  rank\n   <chr>       <chr>        <chr>   <dbl>   <dbl> <int> <chr>        <chr> <int>\n 1 down_logreg Preprocesso… accura… 0.749 0.0112      3 recipe       logi…     1\n 2 down_logreg Preprocesso… j_index 0.503 0.0102      3 recipe       logi…     1\n 3 down_logreg Preprocesso… roc_auc 0.852 0.00742     3 recipe       logi…     1\n 4 down_logreg Preprocesso… sensit… 0.754 0.0207      3 recipe       logi…     1\n 5 down_logreg Preprocesso… specif… 0.749 0.0114      3 recipe       logi…     1\n 6 down_glmnet Preprocesso… accura… 0.750 0.0113      3 recipe       logi…     2\n 7 down_glmnet Preprocesso… j_index 0.502 0.00986     3 recipe       logi…     2\n 8 down_glmnet Preprocesso… roc_auc 0.852 0.00771     3 recipe       logi…     2\n 9 down_glmnet Preprocesso… sensit… 0.753 0.0207      3 recipe       logi…     2\n10 down_glmnet Preprocesso… specif… 0.750 0.0115      3 recipe       logi…     2\n# ℹ 60 more rows\n```\n:::\n:::\n\n\nI can visualize this too with the `autoplot()` function. This is a ggplot type object, so I'm going to throw on a ggtitle. The legend is pretty useless- both the elastic net and regular regression are labeled log_reg (which they are) and the preprocessor is just labeled recipe and not which recipe. This could be cleaned up, but that isn't really the point of this tutorial.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 26: autoplot of best results\nautoplot(tune_results, rank_metric = \"j_index\", select_best = TRUE) +\n  ggtitle(\"Performance of Different Models\")\n```\n\n::: {.cell-output-display}\n![](tidymodels_tutorial_files/figure-html/autoplot-results-1.png){width=672}\n:::\n:::\n\n\n## Handling a model with no hyperparameters\n\nNormally, we'd want to extract the best recipe/model combination from this set. I'll do that here. Again, I'm using j-index as my metric and from the output of Code Block 25, we see `down_logreg` is the best performing model. I extract that workflow from the set of results, and pass it to `last_fit(`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 27: Validating the best model with the test data\nvalidation_results <- tune_results %>%\n  extract_workflow(\"down_logreg\") %>%\n  last_fit(split =  data_split, metrics = fraud_metrics)\n```\n:::\n\n\nNow we can use the same helper functions we did when we used `last_fit()` on the simple workflow, because we are working with a simple workflow! We pulled just the one workflow we wanted out.\n\nYou can see now that in addition to the hard classification we got from `last_fit()` before we also get the probabilities. This is driven by the metrics that make up the metrics set (see the yardstick section for more information). I use these predictions to create the ROC curve as well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 28: Metric for best model with the test data\nhead(collect_predictions(validation_results))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 7\n  id               .pred_1 .pred_0  .row .pred_class is_fraud .config           \n  <chr>              <dbl>   <dbl> <int> <fct>       <fct>    <chr>             \n1 train/test split  0.552    0.448     1 1           0        Preprocessor1_Mod…\n2 train/test split  0.197    0.803     2 0           0        Preprocessor1_Mod…\n3 train/test split  0.0329   0.967     8 0           0        Preprocessor1_Mod…\n4 train/test split  0.472    0.528    12 0           0        Preprocessor1_Mod…\n5 train/test split  0.0254   0.975    13 0           0        Preprocessor1_Mod…\n6 train/test split  0.312    0.688    14 0           0        Preprocessor1_Mod…\n```\n:::\n\n```{.r .cell-code}\nvalidation_results %>% \n  collect_predictions() %>% \n  roc_curve(is_fraud, .pred_1) %>% \n  autoplot() + \n  ggtitle(\"ROC Curve\")\n```\n\n::: {.cell-output-display}\n![](tidymodels_tutorial_files/figure-html/best-model-metrics-1.png){width=672}\n:::\n:::\n\n\n## Handling a model with hyperparameters\n\nSuppose the best model was the elastic net. I tuned the hyperparameters when I did the fitting in workflow_map(). How do I deal with that?\n\nFirst, I need to extract the best set of hyperparameters. Here we aren't extracting the workflow, we are [extracting the workflow set result](https://workflowsets.tidymodels.org/reference/extract_workflow_set_result.html), which is our set of hyperparameters. This is a really simple object, so you can view it without fear.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 29: getting-hyperparameters\nbest_hyperparam <- tune_results %>% \n    extract_workflow_set_result(\"down_glmnet\") %>%\n    select_best(metric = \"j_index\")\n\nbest_hyperparam\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n      penalty mixture .config             \n        <dbl>   <dbl> <chr>               \n1 0.000000137   0.570 Preprocessor1_Model4\n```\n:::\n:::\n\n\nOur workflow for the glmnet is incomplete because it has tune() for the two hyperparameters, instead of the values. We know the best values (at least from the limited parameter space we explored.) I first extract_workflow() just as I did for the no hyperparameter case and then call finalize_workflow(best_hyperparam). This [updates the workflow hyperparameters with the values we found](https://tune.tidymodels.org/reference/finalize_model.html). Everything is identical to the no hyperparameter case or the simple workflow/ last-fit() case. Realizing how/when to extract or reduce the more complex objects to the simpler objects is key to using tidymodels effectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 30: last_fit for a workflow with hyperparameter\nvalidation_results <- tune_results %>%\n  extract_workflow(\"down_glmnet\") %>%\n  finalize_workflow(best_hyperparam) %>%\n  last_fit(split =  data_split, metrics = fraud_metrics)\n```\n:::\n\n\nNow we can handle this object exactly as before.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 31: Metric for best model with the test data\nhead(collect_predictions(validation_results))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 7\n  id               .pred_1 .pred_0  .row .pred_class is_fraud .config           \n  <chr>              <dbl>   <dbl> <int> <fct>       <fct>    <chr>             \n1 train/test split  0.551    0.449     1 1           0        Preprocessor1_Mod…\n2 train/test split  0.217    0.783     2 0           0        Preprocessor1_Mod…\n3 train/test split  0.0342   0.966     8 0           0        Preprocessor1_Mod…\n4 train/test split  0.474    0.526    12 0           0        Preprocessor1_Mod…\n5 train/test split  0.0263   0.974    13 0           0        Preprocessor1_Mod…\n6 train/test split  0.316    0.684    14 0           0        Preprocessor1_Mod…\n```\n:::\n\n```{.r .cell-code}\nvalidation_results %>% \n  collect_predictions() %>% \n  roc_curve(is_fraud, .pred_1) %>% \n  autoplot() + \n  ggtitle(\"ROC Curve\")\n```\n\n::: {.cell-output-display}\n![](tidymodels_tutorial_files/figure-html/glm-model-metrics-1.png){width=672}\n:::\n:::\n\n\nSo that's it. I hope this clarifies some of the different procedures you can use to fit models in the tidymodels framework.\n",
    "supporting": [
      "tidymodels_tutorial_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}