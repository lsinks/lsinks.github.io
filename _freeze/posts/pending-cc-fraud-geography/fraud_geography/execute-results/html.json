{
  "hash": "98eaafdaf273b3fab9a934f94d3f1794",
  "result": {
    "markdown": "---\ntitle: \"Credit Card Fraud: A Tidymodels Tutorial\"\ndescription: \"An Imbalanced Class Problem\"\ntwitter-card:\n  image: \"thumbnail.png\"\ndate: 04-11-2023\ndate-modified: last-modified\ncategories: [R, R-code, tidymodels, machine learning, classifiers] # self-defined categories\ncitation:\n  url: https://lsinks.github.io/posts/2023-04-11-credit-card-fraud/fraud_tutorial.html\nimage: \"thumbnail.png\"\ndraft: true # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\n\n# \n\nI've used the credit card fraud dataset from Datacamp for a variety of projects, most notabily the tidymodels and imbalance class tutorials. I've used it when I was learning Tableau and also Python. One advantage of working with the same dataset is that it becomes easier to catch errors in your analysis, or even in the dataset itself.\n\nThe geographic data always seemed a bit odd. The data dictionary, from datacamp, defines 3 sets of geographic variables. That will produce a notebook like [this](https://app.datacamp.com/workspace/w/f3a94059-683b-4bc6-b354-9b98cf3d5242/edit) with the dataset and the data dictionary.\n\n|            |                                |\n|------------|--------------------------------|\n| city       | City of Credit Card Holder     |\n| state      | State of Credit Card Holder    |\n| lat        | Latitude Location of Purchase  |\n| long       | Longitude Location of Purchase |\n| merch_lat  | Latitude Location of Merchant  |\n| merch_long | Longitude Location of Merchant |\n\nThe original source of the data (prior to preparation by DataCamp) can be found [here](https://www.kaggle.com/kartik2112/fraud-detection?select=fraudTrain.csv) and does not include a data dictionary.\n\nlat and merch_lat and long and merch_long were highly, but not perfectly, correlated, so I orginally dropped the merchant coordinates.\n\nWhile playing with the dataset in Tableau, I became very confused about what lat/long and merch_lat/merch_long actually represented.\n\nI puzzled over this for a long time (I finalized my Tableau dashboards in July, but never did anything with them, because I was so confused by my results. I did check the Tableau results by manually pulling out some subsets of data in R, and what I was seeing in my pretty Tableau maps was correct.\n\nWhile trying to write up some thoughts about Tableau (in November), I decided to delve back into this question. I had looked at the Kaggle notebook discussions when I first starting working on this (which was probably last year!) and didn't find much info. Now, someone had posted a comment about the two coordinates and someone replied that the lat/long is the location of the card holder's home address, not the purchase location. This is so obvious (and clearly explains the results I showed in my dashboard) that it is kind of funny that I didn't figure this out. It is a good lesson in checking all your assumptions carefully when things don't make sense. I assumed that a dataset prepared by Datacamp would be correct and I assumed any weird results were because the person who coded the simulation had made some weird choices.\n\nSo, now I'm going to reanalyze the geographic data with this new definiton in mind.\n\n# 1. Set-up steps\n\nLoading the necessary libraries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# loading tidyverse\nlibrary(tidyverse) #core tidyverse\n\n# visualization\nlibrary(viridis) #color scheme that is colorblind friendly\nlibrary(ggthemes) # themes for ggplot\nlibrary(gt) # to make nice tables\nlibrary(cowplot) # to make multi-panel figures\nlibrary(corrplot) # nice correlation plot\n\n#Data Cleaning\nlibrary(skimr) #provides overview of data and missingness\n\n#Geospatial Data\nlibrary(tidygeocoder) #converts city/state to lat/long\n```\n:::\n\n\nI'm setting a global theme for my figures. I'm using [cowplot](https://wilkelab.org/cowplot/index.html) to create some composite figures, and apparently you must choose a cowplot theme if you set a global theme. You can use a ggtheme on a graph by graph basis, but not globally.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#  setting global figure options\n\ntheme_set(theme_cowplot(12))\n```\n:::\n\n\nLoading the data. This is a local copy that is part of the workspace download from Datacamp.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#  Reading in the data\nfraud <- read_csv('C:/Users/drsin/OneDrive/Documents/R Projects/lsinks.github.io/posts/2023-04-11-credit-card-fraud/datacamp_workspace/credit_card_fraud.csv', show_col_types = FALSE) \nfraud\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 339,607 × 15\n   trans_date_trans_time merchant        category    amt city  state   lat  long\n   <dttm>                <chr>           <chr>     <dbl> <chr> <chr> <dbl> <dbl>\n 1 2019-01-01 00:00:44   Heller, Gutman… grocery… 107.   Orie… WA     48.9 -118.\n 2 2019-01-01 00:00:51   Lind-Buckridge  enterta… 220.   Mala… ID     42.2 -112.\n 3 2019-01-01 00:07:27   Kiehn Inc       grocery…  96.3  Gren… CA     41.6 -123.\n 4 2019-01-01 00:09:03   Beier-Hyatt     shoppin…   7.77 High… NM     32.9 -106.\n 5 2019-01-01 00:21:32   Bruen-Yost      misc_pos   6.85 Free… WY     43.0 -111.\n 6 2019-01-01 00:22:06   Kunze Inc       grocery…  90.2  Hono… HI     20.1 -155.\n 7 2019-01-01 00:22:18   Nitzsche, Kess… shoppin…   4.02 Vale… NE     42.8 -101.\n 8 2019-01-01 00:22:36   Kihn, Abernath… shoppin…   3.66 West… OR     43.8 -122.\n 9 2019-01-01 00:31:51   Ledner-Pfanner… gas_tra… 102.   Thom… UT     39.0 -110.\n10 2019-01-01 00:34:10   Stracke-Lemke   grocery…  83.1  Conw… WA     48.3 -122.\n# ℹ 339,597 more rows\n# ℹ 7 more variables: city_pop <dbl>, job <chr>, dob <date>, trans_num <chr>,\n#   merch_lat <dbl>, merch_long <dbl>, is_fraud <dbl>\n```\n:::\n:::\n\n\n# 2.\n\nI know the dataset doesn't have missing data, so I'm going to jump right in.\n\nIf lat/long is the location of the card holder's home, then I'd expect the following things to be true.\n\n1.  Each customer should be associated with only one lat/long.\n    1.  Assume that a customer in this dataset can be uniquely identified by date of birth and job\n    2.  A lat/long might be associated with two customers (e.g. a couple living together with separate accounts.\n2.  lat/ long should match the city/state info. I already used tidygeocoder to produce lat/long information for city/state pairs, so this should be quick.\n\nI'm going to go through and check the assumptions.\n\n## Assume that a customer in this dataset can be uniquely identified by date of birth and job\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfraud %>% group_by(dob, job, city) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 339,607 × 15\n# Groups:   dob, job, city [187]\n   trans_date_trans_time merchant        category    amt city  state   lat  long\n   <dttm>                <chr>           <chr>     <dbl> <chr> <chr> <dbl> <dbl>\n 1 2019-01-01 00:00:44   Heller, Gutman… grocery… 107.   Orie… WA     48.9 -118.\n 2 2019-01-01 00:00:51   Lind-Buckridge  enterta… 220.   Mala… ID     42.2 -112.\n 3 2019-01-01 00:07:27   Kiehn Inc       grocery…  96.3  Gren… CA     41.6 -123.\n 4 2019-01-01 00:09:03   Beier-Hyatt     shoppin…   7.77 High… NM     32.9 -106.\n 5 2019-01-01 00:21:32   Bruen-Yost      misc_pos   6.85 Free… WY     43.0 -111.\n 6 2019-01-01 00:22:06   Kunze Inc       grocery…  90.2  Hono… HI     20.1 -155.\n 7 2019-01-01 00:22:18   Nitzsche, Kess… shoppin…   4.02 Vale… NE     42.8 -101.\n 8 2019-01-01 00:22:36   Kihn, Abernath… shoppin…   3.66 West… OR     43.8 -122.\n 9 2019-01-01 00:31:51   Ledner-Pfanner… gas_tra… 102.   Thom… UT     39.0 -110.\n10 2019-01-01 00:34:10   Stracke-Lemke   grocery…  83.1  Conw… WA     48.3 -122.\n# ℹ 339,597 more rows\n# ℹ 7 more variables: city_pop <dbl>, job <chr>, dob <date>, trans_num <chr>,\n#   merch_lat <dbl>, merch_long <dbl>, is_fraud <dbl>\n```\n:::\n:::\n\n\n1.  A lat/long might be associated with two customers (e.g. a couple living together with separate accounts.\n\n```         \n```\n\nEverything looks okay, and I am lucky because there is no missing data. I will not need to do cleaning or imputation.\n\nI see that `is_fraud` is coded as 0 or 1, and the mean of this variable is 0.00525. The number of fraudulent transactions is very low, and we should use treatments for imbalanced classes when we get to the fitting/ modeling stage.\n\n### 5.1.2. Looking at our character strings\n\ntransaction number(`trans_num`) are both strings. Transaction number should not influence fraud rate as it is a number assigned to the transaction when processed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 9: Removing Character/ String Variables\nfraud <- fraud %>%\n  select(-trans_num)\n```\n:::\n\n\n## 5.2. Looking at the geographic data\n\nThis data is coded as numeric (latitude and longitude) or character (city/state), but we can recognize it as geographic data and treat it appropriately.\n\nFirst, there are two sets of geographic data related to the merchant. The location of the merchant and where the transaction occurred. I create scatter plots of latitude and longitude separately, because I want to check the correlation between the two sources of data (merchant and transaction). I create a shared legend following the article [here](https://wilkelab.org/cowplot/articles/shared_legends.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 10: Comparing Merchant and Transaction Locations\n\n# calculate correlations\ncor_lat <- round(cor(fraud$lat, fraud$merch_lat), 3)\ncor_long <- round(cor(fraud$long, fraud$merch_long), 3)\n\n# make figure\nfig_3a <-\n  ggplot(fraud, aes(lat, merch_lat, fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5\n  ) +\n  ggtitle(\"Latitude\") +\n  ylab(\"Merchant Latitude\") +\n  xlab(\"Transaction Latitude\") +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  geom_abline(slope = 1, intercept = 0) \n\nfig_3b <-\n  ggplot(fraud, aes(long, merch_long, fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5\n  ) +\n  ggtitle(\"Longitude\") +\n  ylab(\"Merchant Longitude\") +\n  xlab(\"Transaction Longitude\") +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  geom_abline(slope = 1, intercept = 0) \n\n# create the plot with the two figs on a grid, no legend\nprow_fig_3 <- plot_grid(\n  fig_3a + theme(legend.position = \"none\"),\n  fig_3b + theme(legend.position = \"none\"),\n  align = 'vh',\n  labels = c(\"A\", \"B\"),\n  label_size = 12,\n  hjust = -1,\n  nrow = 1\n)\n\n# extract the legend from one of the figures\nlegend <- get_legend(\n  fig_3a + \n    guides(color = guide_legend(nrow = 1)) +\n    theme(legend.position = \"bottom\")\n)\n\n# add the legend to the row of figures, prow_fig_3\nplot_fig_3 <- plot_grid(prow_fig_3, legend, ncol = 1, rel_heights = c(1, .1))\n\n# title\ntitle_3 <- ggdraw() +\n  draw_label(\n    \"Figure 3. Are Merchant and Transaction Coordinates Correlated?\",\n    fontface = 'bold',\n    size = 14,\n    x = 0,\n    hjust = 0\n  ) +\n  theme(plot.margin = margin(0, 0, 0, 7))\n\n# graph everything\nplot_grid(title_3,\n          plot_fig_3,\n          ncol = 1,\n          rel_heights = c(0.1, 1))\n```\n\n::: {.cell-output-display}\n![](fraud_geography_files/figure-html/transaction-merchant-coords-1.png){width=672}\n:::\n:::\n\n\nThese two sets of data are highly correlated (for latitude = 0.994 and for longitude = 0.999) and thus are redundant. So I remove `merch_lat` and `merch_long` from the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 11: Removing merch_lat and merch_long\nfraud <- fraud %>%\n\n  rename(trans_lat = lat, trans_long = long)\n```\n:::\n\n\nNext, I will look and see if some locations are more prone to fraud.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 12: Looking at Fraud by Location\nggplot(fraud, aes(trans_long, trans_lat, fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5,\n    position = \"jitter\"\n  ) +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  ggtitle(\"Figure 4: Where does fraud occur? \") +\n  ylab(\"Latitude\") +\n  xlab(\"Longitude\") \n```\n\n::: {.cell-output-display}\n![](fraud_geography_files/figure-html/fraud-by-location-1.png){width=672}\n:::\n:::\n\n\nIt looks like there are some locations which only have fraudulent transactions.\n\nNext, I'm going to convert city/state into latitude and longitude using the [tidygeocoder package](https://jessecambon.github.io/tidygeocoder/reference/geo.html). Also included code to save this output and then re-import it. You likely do not want to be pulling the data from the internet every time you run the code, so this gives you the option to work from a local copy. For many services, it is against terms of service to repeatedly make the same calls rather than working from a local version. I did find that I could originally pull all data from 'osm', but while double checking this code, I found that the service is now imposing some rate limit and denies some requests, leading to some NA entries. So do check your results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 13: Converting city/state data lat/long\n\n# need to pass an address to geo to convert to lat/long\nfraud <- fraud %>%\n  mutate(address = str_c(city, state, sep = \" , \"))\n\n# generate a list of distinct addresses to look up\n# the dataset is large, so it is better to only look up unique address rather that the address\n# for every record\naddress_list <- fraud %>%\n  distinct(address)\n\n# this has one more than number in the cities, so there must be a city with the same name in more than one state.\n\n# Reimport the data and load it\nhome_coords <-\n  read_csv('C:/Users/drsin/OneDrive/Documents/R Projects/lsinks.github.io/posts/2023-04-11-credit-card-fraud/datacamp_workspace/downloaded_coords.csv', show_col_types = FALSE)\n\n\n# imported home coords has an extra set of quotation marks\nhome_coords <- home_coords %>%\n  mutate(address = str_replace_all(address, \"\\\"\", \"\")) %>%\n  rename(lat_home = lat, long_home = long)\n\n# use a left join on fraud and home_coords to assign the coord to every address in fraud\nfraud <- fraud %>%\n  left_join(home_coords, by = \"address\")\n```\n:::\n\n\nNow I'm going to calculate the distance between the card holder's home and the location of the transaction. I think distance might be a feature that is related to fraud. I followed the tutorial [here](https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/) for calculating distance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 14: Distance Between Home and Transaction\n\n# I believe this assuming a spherical Earth\n\n# convert to radians\nfraud <- fraud %>%\n  mutate(\n    lat1_radians = lat_home / 57.29577951,\n    lat2_radians = trans_lat / 57.29577951,\n    long1_radians = long_home / 57.29577951,\n    long2_radians = trans_long / 57.29577951\n  )\n\n# calculating distance\nfraud <-\n  fraud %>% mutate(distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)\n  ))\n\n# calculating the correlation\nfraud_distance <- round(cor(fraud$distance_miles, fraud$is_fraud), 3) \n```\n:::\n\n\nDespite my assumption that distance would be correlated with fraud, the correlation value is quite low, -0.003.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 14: Distance Between merchant and trans\n\n# I believe this assuming a spherical Earth\n\n# convert to radians\nfraud <- fraud %>%\n  mutate(\n    lat3_radians = merch_lat / 57.29577951,\n    long3_radians = merch_long / 57.29577951,\n  )\n\n# calculating distance\nfraud <-\n  fraud %>% mutate(distance_miles2 = 3963.0 * acos((sin(lat2_radians) * sin(lat3_radians)) + cos(lat2_radians) * cos(lat3_radians) * cos(long3_radians - long2_radians)\n  ))\n\n# calculating the correlation\nfraud_distance2 <- round(cor(fraud$distance_miles2, fraud$is_fraud), 3) \n```\n:::\n\n\nI'm going to visualize it anyway.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 15: Distance from Home and Fraud\nggplot(fraud, aes(distance_miles, is_fraud , fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5,\n    position = \"jitter\"\n  ) +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  ggtitle(\"Figure 5: How far from home does fraud occur?\") +\n  xlab(\"Distance from Home (miles)\") +\n  ylab(\"Is Fraud?\") \n```\n\n::: {.cell-output-display}\n![](fraud_geography_files/figure-html/distance-and-fraud-1.png){width=672}\n:::\n:::\n\n\nSome distances only have fraudulent transactions. This might be related to the locations that are only fraud, Figure 4.\n\nThis new feature `distances_miles` is retained, and the original variables (`city`, `state`) and the intermediate variables (address, variables used to calculate distance) are removed in Code Block 16.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 16: Remove Extraneous/Temp Variables\n\n# created to calculate distance\nfraud <- fraud %>%\n  select(-lat1_radians,-lat2_radians,-long1_radians,-long2_radians)\n```\n:::\n\n\n## \n",
    "supporting": [
      "fraud_geography_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}