{
  "hash": "a866fdea12c454c2c3c4edb781816648",
  "result": {
    "markdown": "---\ntitle: \"Credit Card Fraud: A Tidymodels Tutorial\"\ndescription: \"An Imbalanced Class Problem\"\ntwitter-card:\n  image: \"thumbnail.png\"\ndate: 04-11-2023\ndate-modified: last-modified\ncategories: [R, R-code, mapping, tidymodels, machine learning, classifiers] # self-defined categories\ncitation:\n  url: https://lsinks.github.io/posts/2023-04-11-credit-card-fraud/fraud_tutorial.html\nimage: \"thumbnail.png\"\ndraft: true # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\n\n## The Problem\n\nI've used the credit card fraud dataset from Datacamp for a variety of projects, most notabily the tidymodels and imbalance class tutorials. I've used it when I was learning Tableau and also Python. One advantage of working with the same dataset is that it becomes easier to catch errors in your analysis, or even in the dataset itself.\n\nThis dataset is synthetic (and the code to generate the data is freely available online). Synthetic data is common in data analysis/ data science learning communities. It is valuable to have a dataset that is complete (enough) to practice a particular technique or method without getting bogged down in various problems that might arise from real world data. And I should specify by \"completeness\" I don't mean that the dataset lacks missing data, I mean the dataset contains sufficient features to solve the problem at hand. In the real world, deciding on needed features/variables and figuring out how to acquire them is often the first step in a data science or data analysis problem.\n\nSome types of datasets aren't readily available except as synthetically generated data because they contain personal or propriatary data. Credit card fraud data is a prime example; transaction level data is not available to the general public. If real world data is available, it is, for example, as a PCA transformed dataset where the underlying variables are obscured.\n\nSynthetic data can thus extremely useful, but there are a few pitfalls. First, you need to have some sort of assurance that the dataset accurately reflects the true data to an appropriate level. So you always need to be evalauting the results and asking if they make sense. But you can't go too far down a rabbit hole and fact check every result. Some decision made in the code to generate the data may not be \"correct\" but it may still produce a dataset that is a valid test of methods for that class of problems. Other decisions might create a dataset so flawed that it isn't useful. Telling the two cases apart can be difficult when you don't have experience with real world data. Still, keeping these ideas when you work with the data is important. (Why?)\n\nThis dataset passed my \"smell test\" even though some of the analyses produced results that I thought were odd. The [distribution of the hour of the transactions](https://lsinks.github.io/posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-date-times) (see Fig. 8 and the discussion) seemed off. The geographic data didn't raise any flags when I did [my initial analysis in R](https://lsinks.github.io/posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-geographic-data), but it should have. When I started working with this data in Tableau, [the various geographic data and transaction data was correlated in ways I couldn't explain at all](https://public.tableau.com/views/ExploratoryDataAnalysisofCreditCardFraud/Location?:language=en-US&:display_count=n&:origin=viz_share_link).\n\nIn this blog post, I'm going to discuss the geographic data and:\n\n-   what I thought it was and what it actually is.\n\n-   the right way to analyze this data given how it was generated\n\n-   the right way to analyze geographic data that comes from the \"real world\"\n\n-   the big mistakes I made in my initial analyses that lead me to overlook this problem and how I would prevent these errors in the future.\n\nI think it is important to always engage with our projects critically and also to learn from our mistakes.\n\nI will be citing the code that generates the data, so if you are planning to model this data either at datacamp or at Kaggle, you may want not read any more of this post. The one thing you need to know if you are working on Datacamp's version of the dataset is that the data dictionary is wrong. The lat/long are the customer location, not the transaction location.\n\n## The Geographic Data and the Data Dictionary\n\nThe data dictionary, from datacamp, defines 3 sets of geographic variables. You can look at a [sample notebook of mine with the dataset and the data dictionary](https://app.datacamp.com/workspace/w/f3a94059-683b-4bc6-b354-9b98cf3d5242/edit).\n\nHere is a copy of the geographic variables from the data dictionary.\n\n|            |                                |\n|------------|--------------------------------|\n| city       | City of Credit Card Holder     |\n| state      | State of Credit Card Holder    |\n| lat        | Latitude Location of Purchase  |\n| long       | Longitude Location of Purchase |\n| merch_lat  | Latitude Location of Merchant  |\n| merch_long | Longitude Location of Merchant |\n\nThe source of the data (prior to preparation by DataCamp) can be found [here](https://www.kaggle.com/kartik2112/fraud-detection?select=fraudTrain.csv) and *does include a data dictionary*. (and I'm an idiot)\n\nAs I mentioned in the introduction, while playing with the dataset in Tableau, I became very confused about what lat/long and merch_lat/merch_long actually represented. This Tableau figure doesn't make much sense in light of the data dictionary. (INSERT EMBED)\n\n## Is lat/long the customer address?\n\nI puzzled over this for a long time (I finalized my Tableau dashboards in July, but never did anything with them, because I was so confused by my results. I did check the Tableau results by manually pulling out some subsets of data in R, and what I was seeing in my pretty Tableau maps was correct.\n\nWhile trying to write up some thoughts about Tableau (in November), I decided to delve back into this question. Looking at the kaggle notebook, it states that the lat/long are the customer address.\n\n### Loading Libraries\n\nLoading the necessary libraries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# loading tidyverse/ tidymodels packages\nlibrary(tidyverse) #core tidyverse\n\n\n# visualization\nlibrary(viridis) #color scheme that is colorblind friendly\nlibrary(ggthemes) # themes for ggplot\nlibrary(gt) # to make nice tables\nlibrary(cowplot) # to make multi-panel figures\nlibrary(corrplot) # nice correlation plot\nlibrary(randomcoloR)\n\n\n#Geospatial Data\nlibrary(tidygeocoder) #converts city/state to lat/long\nlibrary(sf) # for handling geo data\nlibrary(mapview) # quick interactive mapping\nlibrary(leaflet) # more mapping\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_cowplot(12))\n```\n:::\n\n\nLoading the data. This is a local copy that is part of the workspace download from Datacamp.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 4: Reading in the data\nfraud <- read_csv('C:/Users/drsin/OneDrive/Documents/R Projects/lsinks.github.io/posts/2023-04-11-credit-card-fraud/datacamp_workspace/credit_card_fraud.csv', show_col_types = FALSE) \n```\n:::\n\n\n### What do I expect logically?\n\nIf lat/long is the location of the card holder's home, then I'd expect the following things to be true.\n\n1.  Each unique customer should be associated with only one lat/long, but a lat/long might be associated with multiple customers (e.g. a couple living together with separate accounts). It is also possible that the coordinates are not highly localized- they might represent a city center rather than a specific address and thus could be the same for everyone in that city.\n2.  The lat/ long should match the city/state info which is also supposed to be related to the customers address. I had already used tidygeocoder to produce lat/long information for city/state pairs, so I thought this should be quick.\n\nUnfortunately, I can't prove that each lat/long is associated with a unique customer because I do not have any unique identifier for the customers. (The kaggle dataset includes credit card number, which would be the unique identifier.)\n\nI can make an identifier that includes as much customer data as possible (date of birth, city, state, and job),\n\n## Assume that a customer in this dataset can be uniquely identified by date of birth and job\n\nFour locations have 2 users, the rest only have one.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp <-\n  fraud %>% \n  select(lat, long, dob, job, city, state) %>% \n  group_by(lat, long, dob, job, city) %>% \n  distinct()    \n\ntemp %>% group_by(lat, long) %>% count(sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 183 × 3\n# Groups:   lat, long [183]\n     lat  long     n\n   <dbl> <dbl> <int>\n 1  33.0 -117.     2\n 2  34.2 -118.     2\n 3  37.3 -122.     2\n 4  43.0 -109.     2\n 5  20.0 -155.     1\n 6  20.1 -155.     1\n 7  32.3 -104.     1\n 8  32.7 -117.     1\n 9  32.9 -106.     1\n10  33.3 -111.     1\n# ℹ 173 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp %>% group_by(dob) %>% count(sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 187 × 2\n# Groups:   dob [187]\n   dob            n\n   <date>     <int>\n 1 1927-09-09     1\n 2 1928-04-02     1\n 3 1928-10-01     1\n 4 1929-04-07     1\n 5 1929-05-06     1\n 6 1932-03-10     1\n 7 1932-08-10     1\n 8 1935-01-29     1\n 9 1935-02-10     1\n10 1936-05-01     1\n# ℹ 177 more rows\n```\n:::\n:::\n\n\nGet unique DOBs and associated coordinates. I'm creating a \"ID\", which is just the row number. I want to color some of my graphs by person, and using dob sometimes created odd scales, likely because it is a date.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique_dobs<- fraud %>% group_by(dob, lat, long) %>% count(sort = TRUE)\nunique_dobs <- tibble::rowid_to_column(unique_dobs, \"ID\")\nunique_dobs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 187 × 5\n# Groups:   dob, lat, long [187]\n      ID dob          lat   long     n\n   <int> <date>     <dbl>  <dbl> <int>\n 1     1 1987-04-23  39.0 -110.   4386\n 2     2 1987-10-28  33.3 -111.   4383\n 3     3 1984-09-01  48.3 -122.   4381\n 4     4 1989-07-17  38.8  -93.9  4380\n 5     5 1988-10-26  38.3  -92.7  4379\n 6     6 1972-09-22  33.7 -117.   4378\n 7     7 1982-02-11  41.2 -101.   4378\n 8     8 1975-07-31  39.6 -105.   4375\n 9     9 1971-04-25  34.4 -119.   4374\n10    10 1981-10-24  33.6 -112.   4366\n# ℹ 177 more rows\n```\n:::\n:::\n\n\nThe most prolific card users have \\~ 4000 transactions in this time period. The least prolific only have a handful.\n\nGet the states data. I'm going to filter by states later on, so having this in here helps.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstates <- fraud %>% select(dob, lat, long, state) %>% distinct()\n```\n:::\n\n\nnow join\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique_dobs<- unique_dobs %>%\n  left_join(states)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(dob, lat, long)`\n```\n:::\n\n```{.r .cell-code}\nunique_dobs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 187 × 6\n# Groups:   dob, lat, long [187]\n      ID dob          lat   long     n state\n   <int> <date>     <dbl>  <dbl> <int> <chr>\n 1     1 1987-04-23  39.0 -110.   4386 UT   \n 2     2 1987-10-28  33.3 -111.   4383 AZ   \n 3     3 1984-09-01  48.3 -122.   4381 WA   \n 4     4 1989-07-17  38.8  -93.9  4380 MO   \n 5     5 1988-10-26  38.3  -92.7  4379 MO   \n 6     6 1972-09-22  33.7 -117.   4378 CA   \n 7     7 1982-02-11  41.2 -101.   4378 NE   \n 8     8 1975-07-31  39.6 -105.   4375 CO   \n 9     9 1971-04-25  34.4 -119.   4374 CA   \n10    10 1981-10-24  33.6 -112.   4366 AZ   \n# ℹ 177 more rows\n```\n:::\n:::\n\n\nNumber of users per state\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique_dobs %>% group_by(state) %>% count(sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 13 × 2\n# Groups:   state [13]\n   state     n\n   <chr> <int>\n 1 CA       45\n 2 MO       30\n 3 NE       19\n 4 OR       18\n 5 CO       13\n 6 NM       13\n 7 WA       13\n 8 WY       12\n 9 AZ        7\n10 UT        7\n11 AK        4\n12 ID        4\n13 HI        2\n```\n:::\n:::\n\n\ndistributions of the number of transactions per user\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique_dobs %>%\n  ggplot(aes(n)) +\n  geom_histogram(bins = 10)\n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nI'm going to join the ID back on to fraud, so I have access to it in all dataframes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfraud <- fraud %>%\n  left_join(unique_dobs) %>% select(-n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(state, lat, long, dob)`\n```\n:::\n:::\n\n\nchecking\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfraud %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 16\n  trans_date_trans_time merchant         category    amt city  state   lat  long\n  <dttm>                <chr>            <chr>     <dbl> <chr> <chr> <dbl> <dbl>\n1 2019-01-01 00:00:44   Heller, Gutmann… grocery… 107.   Orie… WA     48.9 -118.\n2 2019-01-01 00:00:51   Lind-Buckridge   enterta… 220.   Mala… ID     42.2 -112.\n3 2019-01-01 00:07:27   Kiehn Inc        grocery…  96.3  Gren… CA     41.6 -123.\n4 2019-01-01 00:09:03   Beier-Hyatt      shoppin…   7.77 High… NM     32.9 -106.\n5 2019-01-01 00:21:32   Bruen-Yost       misc_pos   6.85 Free… WY     43.0 -111.\n6 2019-01-01 00:22:06   Kunze Inc        grocery…  90.2  Hono… HI     20.1 -155.\n# ℹ 8 more variables: city_pop <dbl>, job <chr>, dob <date>, trans_num <chr>,\n#   merch_lat <dbl>, merch_long <dbl>, is_fraud <dbl>, ID <int>\n```\n:::\n:::\n\n\nKind of a weird pattern, but moving on. Many users have thousands of transactions in this time period.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfraud_uniform <- fraud %>% select(lat, long, merch_lat, merch_long) %>%\n  mutate(lat_delta = lat-merch_lat, long_delta = long- merch_long)\n```\n:::\n\n\ncheck uniformity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(fraud_uniform, aes(lat_delta)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(fraud_uniform, aes(long_delta)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n## A million unique merchants\n\nOn the other hand, every transaction is associated with a unique merchant location. This isn't particularly realistic, and makes visualizing the date more difficult.\n\nNot only are there no duplicate merchant locations for specific users...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfraud %>%\n  group_by(dob, lat, long, merch_lat, merch_long) %>%\n  count(sort= TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 339,607 × 6\n# Groups:   dob, lat, long, merch_lat, merch_long [339,607]\n   dob          lat  long merch_lat merch_long     n\n   <date>     <dbl> <dbl>     <dbl>      <dbl> <int>\n 1 1927-09-09  37.8 -119.      36.8      -118.     1\n 2 1927-09-09  37.8 -119.      36.8      -120.     1\n 3 1927-09-09  37.8 -119.      36.8      -120.     1\n 4 1927-09-09  37.8 -119.      36.8      -120.     1\n 5 1927-09-09  37.8 -119.      36.8      -119.     1\n 6 1927-09-09  37.8 -119.      36.8      -119.     1\n 7 1927-09-09  37.8 -119.      36.8      -118.     1\n 8 1927-09-09  37.8 -119.      36.8      -118.     1\n 9 1927-09-09  37.8 -119.      36.8      -120.     1\n10 1927-09-09  37.8 -119.      36.8      -120.     1\n# ℹ 339,597 more rows\n```\n:::\n:::\n\n\nBut there are no duplicate merchant locations over the entire dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfraud %>%\n  group_by(merch_lat, merch_long) %>%\n  count(sort= TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 339,607 × 3\n# Groups:   merch_lat, merch_long [339,607]\n   merch_lat merch_long     n\n       <dbl>      <dbl> <int>\n 1      19.0      -156.     1\n 2      19.0      -155.     1\n 3      19.0      -155.     1\n 4      19.0      -155.     1\n 5      19.0      -156.     1\n 6      19.0      -156.     1\n 7      19.0      -155.     1\n 8      19.0      -156.     1\n 9      19.0      -155.     1\n10      19.0      -156.     1\n# ℹ 339,597 more rows\n```\n:::\n:::\n\n\nThat's suspicious. But it seems to be reflected in the full dataset.\n\n## Mapping\n\nFind a shape file of the US. I took one from the census bureau.\n\n<https://stackoverflow.com/questions/61282572/cant-read-shp-file-in-r>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_states <- st_read(dsn = \"cb_2018_us_state_500k.shp\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `cb_2018_us_state_500k' from data source \n  `C:\\Users\\drsin\\OneDrive\\Documents\\R Projects\\lsinks.github.io\\posts\\pending-cc-fraud-geography\\cb_2018_us_state_500k.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 56 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1489 ymin: -14.5487 xmax: 179.7785 ymax: 71.36516\nGeodetic CRS:  NAD83\n```\n:::\n:::\n\n\nSee how this map looks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_sf(data = census_states)\n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nIt does contain us territories.\n\nAlso, all of Alaska (the island chain to the far right of the graph)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_states %>%\n  filter(NAME == \"Alaska\") %>%\nggplot() +\n  geom_sf()\n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nLet's see the extent of our fraud data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmin(fraud$long)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -165.6723\n```\n:::\n\n```{.r .cell-code}\nmin(fraud$merch_long)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -166.6716\n```\n:::\n:::\n\n\nand the lat\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmax(fraud$long)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -89.6287\n```\n:::\n\n```{.r .cell-code}\nmax(fraud$merch_long)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -88.6292\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_states %>%\n  filter(STATEFP < 57) %>%\nggplot() +\n  geom_sf(aes(fill= NAME)) +\n   guides(fill = \"none\") +\n  coord_sf(xlim = c(-166.6716, -88.6292))\n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nnow make a spatial object for the transactions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmerchant_points <- st_as_sf(fraud, coords = c(\"merch_long\", \"merch_lat\"), crs = 4269)\n```\n:::\n\n\nhome points\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhome_points<- st_as_sf(unique_dobs, coords = c(\"long\", \"lat\"), crs = 4269)\n```\n:::\n\n\nnow map this c(-166.6716, -88.6292)\n\nfilter our census states abd crop to our boundaries\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_states <- census_states %>%\n  filter(STATEFP < 57) %>%\n  st_crop( xmin = -166.6716, xmax = -88.6292, ymin = -14.5487, ymax= 71.36516)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_sf(data = census_states) +\n   guides(fill = \"none\") +\n  geom_sf(data = merchant_points, alpha = 0.01, size = 1)\n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\ncolor palette\n\n\n::: {.cell}\n\n```{.r .cell-code}\npal2 <- distinctColorPalette(k = 187, altCol = FALSE, runTsne = FALSE)\n```\n:::\n\n\nlooking at colors\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#show_col(pal2 , labels = FALSE)\n```\n:::\n\n\nplotting states\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_sf(data = census_states) +\n  geom_sf(data = merchant_points, aes(fill = ID, color = ID), alpha = 0.01, shape = 21, size = 1) +\n  geom_sf(data = home_points, shape = 8, size = 1, aes(color = ID)) +\n  scale_fill_gradientn(colors = pal2) +\n  scale_color_gradientn(colors = pal2) +\n  theme_void() +\n  guides(fill = \"none\", color = \"none\") \n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nlet's look at one state. Arizona has 7 users, so that seems reasonable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_AZ <- census_states %>%\n  filter(STUSPS == \"AZ\")\n\nmerchant_AZ <- merchant_points %>%\n  filter(state == \"AZ\")\n\nhome_AZ <- home_points %>%\n  filter(state == \"AZ\")\n```\n:::\n\n\ncreate a new palette\n\n\n::: {.cell}\n\n```{.r .cell-code}\npal_AZ <- distinctColorPalette(k = 7, altCol = FALSE, runTsne = FALSE)\n#show_col(pal_AZ)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nAZ_graph <- ggplot() +\n  geom_sf(data = census_AZ) +\n  geom_sf(data = merchant_AZ, aes(fill = ID, color = ID), alpha = 0.1, shape = 21, size = 1) +\n  geom_sf(data = home_AZ, shape = 21, size = 3, aes(fill = ID), color = \"black\") +\n  scale_fill_gradientn(colors = pal_AZ) +\n  scale_color_gradientn(colors = pal_AZ) +\n  theme_void() +\n  guides(fill = \"none\", color = \"none\") \n```\n:::\n\n\ndisplay\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAZ_graph\n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\ntable\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhome_AZ %>% select(dob, n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimple feature collection with 7 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -112.2523 ymin: 33.2887 xmax: -111.0985 ymax: 35.2563\nGeodetic CRS:  NAD83\n# A tibble: 7 × 3\n# Groups:   dob [7]\n  dob            n            geometry\n  <date>     <int>         <POINT [°]>\n1 1987-10-28  4383 (-111.0985 33.2887)\n2 1981-10-24  4366 (-112.0559 33.5623)\n3 1999-11-30  2931 (-112.1202 33.8155)\n4 1935-01-29  1469 (-111.9565 33.5494)\n5 1957-03-28  1466   (-111.95 35.2563)\n6 1955-01-20   738 (-112.2523 34.5189)\n7 1940-11-08     9 (-111.8469 33.4317)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\none_user <- fraud %>%\n  filter(dob == \"1987-10-28\")\n```\n:::\n\n\nuniform lat\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(one_user, aes(merch_lat)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\nnow long\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(one_user, aes(merch_long)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\nmake a 70 mile square\n\ntry non-sf objects\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndob_AZ <- unique_dobs %>% filter(state == \"AZ\")\n```\n:::\n\n\nget a point\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(dob_AZ)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 7\nColumns: 6\nGroups: dob, lat, long [7]\n$ ID    <int> 2, 10, 23, 98, 105, 141, 183\n$ dob   <date> 1987-10-28, 1981-10-24, 1999-11-30, 1935-01-29, 1957-03-28, 1955…\n$ lat   <dbl> 33.2887, 33.5623, 33.8155, 33.5494, 35.2563, 34.5189, 33.4317\n$ long  <dbl> -111.0985, -112.0559, -112.1202, -111.9565, -111.9500, -112.252…\n$ n     <int> 4383, 4366, 2931, 1469, 1466, 738, 9\n$ state <chr> \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nr = 1\n```\n:::\n\n\nmake a bounding box in decimal degrees\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 = as.numeric(dob_AZ[1,4])\ny1 = as.numeric(dob_AZ[1,3])\n```\n:::\n\n\nmaking a point\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoint_az1 <- sf::st_point(c(x1,y1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsq_buffer <- st_buffer(point_az1,\n              dist = 120701,\n              endCapStyle = \"SQUARE\"\n              ) %>% plot()\n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n\ngraph\n\n\n::: {.cell}\n\n```{.r .cell-code}\n ggplot() +\n  geom_sf(data = census_AZ) +\n  geom_sf(data = sq_buffer, color = \"black\", alpha = 1, size = 10)\n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\n-   geom_sf(data = merchant_AZ, aes(fill = ID, color = ID), alpha = 0.1, shape = 21, size = 1) + geom_sf(data = home_AZ, shape = 21, size = 3, aes(fill = ID), color = \"black\") + scale_fill_gradientn(colors = pal_AZ) + scale_color_gradientn(colors = pal_AZ) + theme_void() + guides(fill = \"none\", color = \"none\")\n\ndrawing 75 mile circles\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncircle_buffer  <- home_points %>%  \n      st_buffer(120701)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  \n  geom_sf(data = census_states) +\ngeom_sf(data = circle_buffer, aes(color = dob))+\n  geom_sf(data = merchant_points, aes(fill = dob, color = dob), alpha = 0.01, shape = 21, size = 1) +\n  geom_sf(data = home_points, shape = 8, size = 1, aes(color = dob)) +\n  scale_fill_gradientn(colors = pal2)+\n  scale_color_gradientn(colors = pal2)+\n   coord_sf(xlim = c(-166.6716, -88.6292)) +\n  theme_void() +\n  guides(fill = \"none\") +\n  guides(color = \"none\")\n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 14: Distance Between Home and Transaction\n\n# I believe this assuming a spherical Earth\n\n# convert to radians\nfraud <- fraud %>%\n  mutate(\n    lat1_radians = lat / 57.29577951,\n    lat2_radians = merch_lat / 57.29577951,\n    long1_radians = long / 57.29577951,\n    long2_radians = merch_long / 57.29577951\n  )\n\n# calculating distance\nfraud <-\n  fraud %>% mutate(distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)\n  ))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculating the correlation\ncor(fraud$distance_miles, fraud$is_fraud)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -1.15003e-05\n```\n:::\n:::\n\n\nI'm going to visualize it anyway.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code Block 15: Distance from Home and Fraud\nggplot(fraud, aes(distance_miles, is_fraud , fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5,\n    position = \"jitter\"\n  ) +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  ggtitle(\"Figure 5: How far from home does fraud occur?\") +\n  xlab(\"Distance from Home (miles)\") +\n  ylab(\"Is Fraud?\") \n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/distance-and-fraud-1.png){width=672}\n:::\n:::\n\n\nAnd as a distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(fraud) +\n  geom_histogram(aes(distance_miles), color = \"red\", bins = 50) \n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\nfraud vs not fraud\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfraud %>% filter(is_fraud == 1) %>%\nggplot() +\n  geom_histogram(aes(distance_miles), color = \"red\", bins = 50) \n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-52-1.png){width=672}\n:::\n:::\n\n\nIs not_fraud\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfraud %>% filter(is_fraud == 0) %>%\nggplot() +\n  geom_histogram(aes(distance_miles), color = \"red\", bins = 50) \n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-53-1.png){width=672}\n:::\n:::\n\n\n## \n\nfor the density map\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfraud_only <- fraud %>% filter(is_fraud == TRUE) %>% select(distance_miles)\nlegit_only <- fraud %>% filter(is_fraud == FALSE) %>% select(distance_miles)\n```\n:::\n\n\nmake into a single df\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_density(data = fraud_only, aes(distance_miles) , alpha = 0.5, fill = \"blue\") +\n  geom_density(data = legit_only , aes(distance_miles),  alpha = 0.5 , fill = \"yellow\") +\n  geom_vline(xintercept = 70) +\n  theme_classic() +\n  labs(title = \"Evolution of Word Scores as Guessing Progresses\",\n       caption = \"for 5 letter words\") +\n  xlab(\"Distance(miles)\") +\n  ylab(\"Density\") \n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n\nnow use the hack to calculate distance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfraud <- fraud %>%\n  mutate(distance_hacky  = (sqrt((lat-merch_lat)^2 + (long-merch_long)^2)*70))\n```\n:::\n\n\ngraph\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(fraud) +\n  geom_histogram(aes(distance_hacky), color = \"red\", bins = 50) +\n  geom_vline(xintercept = 70) \n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n:::\n\n\nlook at one guy\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfraud %>% filter(dob == \"1987-04-23\") %>%\nggplot() +\n  geom_histogram(aes(distance_hacky), color = \"red\", bins = 50) +\n  geom_vline(xintercept = 70) \n```\n\n::: {.cell-output-display}\n![](fraud_geo_right_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n\ndoes this make sense\n\n\n::: {.cell}\n\n```{.r .cell-code}\nradius = 1\n```\n:::\n",
    "supporting": [
      "fraud_geo_right_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}