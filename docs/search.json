[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Entries",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nThoughts on Building a Quarto Website\n\n\n\n\n\n\n\nR\n\n\nquarto\n\n\nturtle\n\n\n\n\nSome tips and tricks for using Quarto and RStudio to create a portfolio website and blog.\n\n\n\n\n\n\nNov 1, 2023\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to Web Scraping Using rvest\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ndata visualization\n\n\ntombstone\n\n\nregex\n\n\nweb scraping\n\n\nrvest\n\n\ndata cleaning\n\n\ntidyTuesday\n\n\n\n\nWeb scraping data with rvest to enhance the information in the Tombstone Project.\n\n\n\n\n\n\nSep 8, 2023\n\n\n30 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday 36: Visualizing Worker Demographic Information with Treemaps\n\n\n\n\n\n\n\ndata visualization\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\ntreemap\n\n\ntidyTuesday\n\n\nd3treeR\n\n\ninteractive\n\n\n\n\nUsing treemap and d3treeR to create static and dynamic treemaps\n\n\n\n\n\n\nSep 5, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday 35: Exploring Fair Use Cases\n\n\n\n\n\n\n\ndata visualization\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\ntidyTuesday\n\n\nstringr\n\n\ndata cleaning\n\n\ndual axis plot\n\n\nggplot\n\n\ncowplot\n\n\n\n\nUsing stringr::str_detect() functions to explore why variables from different datasets don’t exactly match.\n\n\n\n\n\n\nAug 29, 2023\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday: Exploring Refugee Flow with A Sankey Diagram\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\nsankey\n\n\nnetworkD3\n\n\nggsankey\n\n\nhtmlwidgets\n\n\n\n\nTidyTuesday 34: Looking at the United Nations High Commissioner for Refugees data with a Sankey Diagram. Diagram created with networkD3 in R.\n\n\n\n\n\n\nAug 28, 2023\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nMapping for the Tombstone Project\n\n\n\n\n\n\n\ndata visualization\n\n\nR\n\n\nR-code\n\n\nleaflet\n\n\nquarto\n\n\nproblem solving\n\n\nmapping\n\n\ncode-along\n\n\nspatial join\n\n\nwordPress\n\n\nleafpop\n\n\nst_jitter\n\n\n\n\nMethods for dealing with overlapping points and then styling a leaflet map with various labels and pop-ups.\n\n\n\n\n\n\nAug 15, 2023\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday Twofer (32 and 33)\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nTidyTuesday\n\n\ncorrplot\n\n\n\n\nLooking at how heat levels increase on the show The Hot Ones. Then doing some EDA on a data set on spam emails.\n\n\n\n\n\n\nAug 15, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nData Cleaning for the Tombstone Project\n\n\n\n\n\n\n\ndata visualization\n\n\nR\n\n\nR-code\n\n\nfuzzyjoin\n\n\nquarto\n\n\nleaflet\n\n\nregex\n\n\nstringr\n\n\ndata cleaning\n\n\nproblem solving\n\n\nmapping\n\n\nsf\n\n\ncode-along\n\n\n\n\nUsing StringR to clean a human created excel sheet full of typos and formatting inconsistencies. Then matching excel data to photo names.\n\n\n\n\n\n\nAug 4, 2023\n\n\n51 min\n\n\n\n\n\n\n  \n\n\n\n\nStates\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nTidyTuesday\n\n\ntidy\n\n\ngwalkr\n\n\ntableau Alternative\n\n\nexploratory data analysis\n\n\ninteractive\n\n\n\n\nUsing GWalkR to create an interactive exploratory data analysis window similar to Tableau.\n\n\n\n\n\n\nAug 1, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nA Heatmap of James Lind’s Scurvy Study\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nTidyTuesday\n\n\ntidy\n\n\nheatmap\n\n\nggplot\n\n\ntidyverse\n\n\nggthemes\n\n\n\n\nMaking a Heatmap of James Lind’s Scurvy Study using ggplot. Data cleaning and reformating handled through tidyverse packages.\n\n\n\n\n\n\nJul 25, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 28: Global Surface Temperature\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nTidyTuesday\n\n\ntidy\n\n\ngganimate\n\n\n\n\nMaking an animated graph of global temperature change over time with gganimate.\n\n\n\n\n\n\nJul 11, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 27: Historical Markers\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ndata visualization\n\n\nsf\n\n\nleaflet\n\n\n\n\nTidyTuesday: Interactive Map of Arlington Historic Markers\n\n\n\n\n\n\nJul 4, 2023\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday Revisited: Interactive Map of Arlington Historic Neighborhoods\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nData-Viz\n\n\nsf\n\n\nleaflet\n\n\n\n\nTidyTuesday: Interactive Map of Historic Neighborhoods of Arlington Virginia\n\n\n\n\n\n\nJun 29, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: US Populated Places\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ndata visualization\n\n\nopenxlsx\n\n\nstringr\n\n\nfuzzyjoin\n\n\nmapview\n\n\nsf\n\n\n\n\nTidyTuesday: Historic Neighborhoods of Arlington Virginia\n\n\n\n\n\n\nJun 27, 2023\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 25: UFO Sightings Redux\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\n\n\nTidyTuesday: UFO Sightings\n\n\n\n\n\n\nJun 20, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 18: Portal Project\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\ndata validation\n\n\nexploratory data analysis\n\n\n\n\nTidyTuesday: Rodents of Portal Arizona\n\n\n\n\n\n\nMay 2, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 17: London Marathon\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\n\n\nTidyTuesday: Exploring the London Marathon\n\n\n\n\n\n\nApr 25, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 16: Neolithic Founder Crops\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ndata visualization\n\n\n\n\nTidyTuesday: Exploring early agriculture in SW Asia\n\n\n\n\n\n\nApr 18, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCredit Card Fraud: A Tidymodels Tutorial\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ntidymodels\n\n\nmachine learning\n\n\nclassifiers\n\n\n\n\nAn Imbalanced Class Problem\n\n\n\n\n\n\nApr 11, 2023\n\n\n39 min\n\n\n\n\n\n\n  \n\n\n\n\nA Tidymodels Tutorial: A Structural Approach\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ntidymodels\n\n\nmachine learning\n\n\nhyperparameters\n\n\nworkflows\n\n\n\n\nExploring the different steps for modeling\n\n\n\n\n\n\nApr 10, 2023\n\n\n22 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge- Endangered Species\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\n30DayChartChallenge\n\n\nwaffle\n\n\n\n\nHow many species have been delisted?\n\n\n\n\n\n\nApr 4, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge- Flora and Fauna\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nturtle\n\n\n30DayChartChallenge\n\n\n\n\nHow Large are Different Types of Turtles?\n\n\n\n\n\n\nApr 3, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge -Arlington Parks\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nwaffle\n\n\n30DayChartChallenge\n\n\n\n\nWho Owns the Parks in Arlington Virgina?\n\n\n\n\n\n\nApr 2, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nSelf-Guided Learning through a Wordle Guess Generator: Part 1\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\n\n\nSkills improvement with a Wordle Guess Generator\n\n\n\n\n\n\nApr 1, 2023\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nSelf-Guided Learning through a Wordle Guess Generator: Part 2\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\n\n\nCurrent version of a Wordle Guess Generator\n\n\n\n\n\n\nApr 1, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nOne Class SVM\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nmachine learning\n\n\ncaret\n\n\nsvm\n\n\nclassifiers\n\n\nsupport vector machines\n\n\n\n\nOne Class SVM for Imbalanced Classes\n\n\n\n\n\n\nMar 30, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Daylight Savings Time\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\nData-Viz\n\n\nggimage\n\n\nmaps\n\n\n\n\nTidyTuesday: Exploring Time Zones and Daylight Savings Time\n\n\n\n\n\n\nMar 28, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 12: Programming Languages Revisited\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\nskimr\n\n\n\n\nTidyTuesday: Polishing\n\n\n\n\n\n\nMar 24, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTwitter Cards\n\n\n\n\n\n\n\nquarto\n\n\nturtle\n\n\n\n\nMaking Twitter Cards\n\n\n\n\n\n\nMar 24, 2023\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 12: Programming Languages\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\n\n\nTidyTuesday: How to comment in Various Programming Languages\n\n\n\n\n\n\nMar 21, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Blog\n\n\n\n\n\n\n\nquarto\n\n\nR\n\n\nturtle\n\n\n\n\nTrying to create a Quarto Blog\n\n\n\n\n\n\nMar 14, 2023\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "",
    "text": "This week, I participated in my first #TidyTuesday challenge. My goal was to get something out on the day of the challenge rather than perfection. I did notice that the skimr output wasn’t formatted nicely on the webpage. Today, I’m going to delve into the skimr and Quarto documentation and make a nicer version of the output. Secondly, I’m going to fix up my final figure, which is the one I shared on social media:"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#skimr-to-understand-your-data",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#skimr-to-understand-your-data",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "Skimr to understand your data",
    "text": "Skimr to understand your data\nSkimr is a package that provides statistical summaries of the variables in your dataframe. It also provides information about the missingness of each variable.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(ggrepel)\nlibrary(visdat) # visualizing missing data in dataframe\n\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 12)\n\nlanguages &lt;- tuesdata$languages\n\n\nCustomizing the skim Output\nMy main objection is that the numerical summary is too wide and has a scroll bar. I especially want the histogram to be viewable on the first screen. I also don’t particularly care about all the quartile information; min and max are enough. If I want to delve more into the stats of a variable, I will do it another way, not with skimr.\nFirst, quarto lets you expand the output of the code chunk to fill the page via the option “#| column: page”, so I’ll do that. Next, I’ll create a custom skim function that drops the p25, p50, and p75 output from the summary of the numerical variables.\n\nmy_skim &lt;- skim_with(numeric = sfl(p25 = NULL, p50 = NULL, p75 = NULL)) \n\nmy_skim(languages)\n\n\nData summary\n\n\nName\nlanguages\n\n\nNumber of rows\n4303\n\n\nNumber of columns\n49\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n21\n\n\nlogical\n4\n\n\nnumeric\n24\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n4303\n0\n\n\ntitle\n0\n1.00\n1\n56\n0\n4267\n0\n\n\ndescription\n3480\n0.19\n4\n2273\n0\n811\n0\n\n\ntype\n0\n1.00\n2\n27\n0\n40\n0\n\n\ncreators\n3203\n0.26\n2\n253\n0\n985\n0\n\n\nwebsite\n2928\n0.32\n13\n131\n0\n1368\n0\n\n\ndomain_name\n3588\n0.17\n6\n32\n0\n700\n0\n\n\nreference\n2314\n0.46\n15\n251\n0\n1955\n0\n\n\ngithub_repo\n3402\n0.21\n25\n73\n0\n897\n0\n\n\ngithub_repo_description\n3438\n0.20\n4\n419\n0\n853\n0\n\n\ngithub_language\n3829\n0.11\n1\n30\n0\n474\n0\n\n\ngithub_language_tm_scope\n3837\n0.11\n4\n34\n0\n361\n0\n\n\ngithub_language_type\n3837\n0.11\n4\n11\n0\n4\n0\n\n\ngithub_language_ace_mode\n3838\n0.11\n1\n16\n0\n96\n0\n\n\ngithub_language_file_extensions\n3833\n0.11\n1\n606\n0\n466\n0\n\n\nwikipedia\n2731\n0.37\n32\n104\n0\n1566\n0\n\n\nwikipedia_summary\n2884\n0.33\n17\n6741\n0\n1407\n0\n\n\nwikipedia_related\n3145\n0.27\n1\n1761\n0\n1059\n0\n\n\nline_comment_token\n3831\n0.11\n1\n7\n0\n23\n0\n\n\norigin_community\n1190\n0.72\n3\n305\n0\n2232\n0\n\n\nfile_type\n3213\n0.25\n2\n6\n0\n4\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n3683\n0.14\n1.00\nTRU: 617, FAL: 3\n\n\nfeatures_has_semantic_indentation\n3722\n0.14\n0.11\nFAL: 516, TRU: 65\n\n\nfeatures_has_line_comments\n3765\n0.13\n0.96\nTRU: 517, FAL: 21\n\n\nis_open_source\n3792\n0.12\n0.89\nTRU: 453, FAL: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1991.11\n111.44\n-2000\n2023\n▁▁▁▁▇\n\n\ndomain_name_registered\n3801\n0.12\n2011.33\n7.02\n1990\n2023\n▁▃▃▇▆\n\n\nisbndb\n3217\n0.25\n7.71\n33.16\n0\n400\n▇▁▁▁▁\n\n\nbook_count\n0\n1.00\n2.08\n17.34\n0\n401\n▇▁▁▁▁\n\n\nsemantic_scholar\n3545\n0.18\n3.79\n8.32\n0\n52\n▇▁▁▁▁\n\n\nlanguage_rank\n0\n1.00\n2151.00\n1242.31\n0\n4302\n▇▇▇▇▇\n\n\ngithub_repo_stars\n3414\n0.21\n2127.40\n7554.02\n0\n88526\n▇▁▁▁▁\n\n\ngithub_repo_forks\n3417\n0.21\n261.29\n1203.00\n0\n23732\n▇▁▁▁▁\n\n\ngithub_repo_updated\n3418\n0.21\n2021.39\n1.76\n2012\n2023\n▁▁▁▁▇\n\n\ngithub_repo_subscribers\n3418\n0.21\n62.34\n200.88\n0\n2910\n▇▁▁▁▁\n\n\ngithub_repo_created\n3425\n0.20\n2015.84\n3.48\n2006\n2022\n▁▅▇▇▇\n\n\ngithub_repo_issues\n3518\n0.18\n123.03\n546.26\n0\n9522\n▇▁▁▁▁\n\n\ngithub_repo_first_commit\n3567\n0.17\n2014.74\n4.99\n1987\n2022\n▁▁▁▆▇\n\n\ngithub_language_repos\n3833\n0.11\n197134.67\n1226900.57\n0\n16046489\n▇▁▁▁▁\n\n\nwikipedia_daily_page_views\n2837\n0.34\n227.13\n783.55\n-1\n13394\n▇▁▁▁▁\n\n\nwikipedia_backlinks_count\n2877\n0.33\n318.55\n1635.29\n1\n34348\n▇▁▁▁▁\n\n\nwikipedia_page_id\n2893\n0.33\n9167847.21\n13506832.90\n928\n63063548\n▇▁▁▁▁\n\n\nwikipedia_appeared\n2958\n0.31\n1991.14\n17.03\n1830\n2019\n▁▁▁▃▇\n\n\nwikipedia_created\n3040\n0.29\n2005.75\n3.77\n2001\n2020\n▇▇▂▁▁\n\n\nwikipedia_revision_count\n3130\n0.27\n330.43\n813.26\n1\n10104\n▇▁▁▁▁\n\n\nlast_activity\n0\n1.00\n2000.62\n84.60\n-900\n2023\n▁▁▁▁▇\n\n\nnumber_of_users\n0\n1.00\n13771.26\n227712.95\n0\n7179119\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n422.18\n12572.99\n0\n771996\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n1482\n0.66\n0.00\n0.00\n0\n0\n▁▁▇▁▁\n\n\n\n\n\nThis output is much nicer. It is a bit wall of text though. I wouldn’t recommend using this in reports, but it is a useful tool when doing your initial dataset analysis. (As a side note, I have noticed skimr doesn’t work well on Kaggle. It performs as expected if you are in interactive mode, but it fails when you try to save the notebook or run non-interactively.)\n\n\nStyling skim output with gt\nIf, for some reason, you did need to include output/ visualizations about missingness in a report, I’d probably recreate visualizations or tables by class of variable, especially if you have many variables, as I do here.\nHere’s an example for numeric variables, of which there are 24 in the dataset. First, we will skim the data and then use the gt package to style the resulting dataframe as a table. I used a built-in style, but each table element can be individually customized.\n\nlanguages_numeric &lt;- languages %&gt;%\n  select_if(is.numeric)\n\nlang_numeric_skim &lt;- my_skim(languages_numeric)\n\nlang_numeric_skim %&gt;%\n  select(-skim_type)   %&gt;% \n  gt() %&gt;%\n  cols_label(n_missing = \"# Missing\", complete_rate = \"Completeness\", \n             numeric.mean = \"Mean\", numeric.sd = \"Standard Deviation\",\n             numeric.p0 = \"Min\", numeric.p100 = \"Max\",\n             numeric.hist = \"Histogram\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Summary of Numerical Variables in Languages\") \n\n\n\n\n\n  \n    \n      Summary of Numerical Variables in Languages\n    \n    \n    \n      skim_variable\n      # Missing\n      Completeness\n      Mean\n      Standard Deviation\n      Min\n      Max\n      Histogram\n    \n  \n  \n    appeared\n0\n1.0000000\n1.991105e+03\n1.114434e+02\n-2000\n2023\n▁▁▁▁▇\n    domain_name_registered\n3801\n0.1166628\n2.011333e+03\n7.021132e+00\n1990\n2023\n▁▃▃▇▆\n    isbndb\n3217\n0.2523821\n7.706262e+00\n3.316421e+01\n0\n400\n▇▁▁▁▁\n    book_count\n0\n1.0000000\n2.079479e+00\n1.734465e+01\n0\n401\n▇▁▁▁▁\n    semantic_scholar\n3545\n0.1761562\n3.794195e+00\n8.316231e+00\n0\n52\n▇▁▁▁▁\n    language_rank\n0\n1.0000000\n2.151000e+03\n1.242313e+03\n0\n4302\n▇▇▇▇▇\n    github_repo_stars\n3414\n0.2066000\n2.127403e+03\n7.554016e+03\n0\n88526\n▇▁▁▁▁\n    github_repo_forks\n3417\n0.2059029\n2.612867e+02\n1.203003e+03\n0\n23732\n▇▁▁▁▁\n    github_repo_updated\n3418\n0.2056705\n2.021390e+03\n1.763285e+00\n2012\n2023\n▁▁▁▁▇\n    github_repo_subscribers\n3418\n0.2056705\n6.234237e+01\n2.008820e+02\n0\n2910\n▇▁▁▁▁\n    github_repo_created\n3425\n0.2040437\n2.015843e+03\n3.479589e+00\n2006\n2022\n▁▅▇▇▇\n    github_repo_issues\n3518\n0.1824309\n1.230344e+02\n5.462553e+02\n0\n9522\n▇▁▁▁▁\n    github_repo_first_commit\n3567\n0.1710435\n2.014739e+03\n4.985409e+00\n1987\n2022\n▁▁▁▆▇\n    github_language_repos\n3833\n0.1092261\n1.971347e+05\n1.226901e+06\n0\n16046489\n▇▁▁▁▁\n    wikipedia_daily_page_views\n2837\n0.3406925\n2.271330e+02\n7.835524e+02\n-1\n13394\n▇▁▁▁▁\n    wikipedia_backlinks_count\n2877\n0.3313967\n3.185484e+02\n1.635289e+03\n1\n34348\n▇▁▁▁▁\n    wikipedia_page_id\n2893\n0.3276784\n9.167847e+06\n1.350683e+07\n928\n63063548\n▇▁▁▁▁\n    wikipedia_appeared\n2958\n0.3125726\n1.991144e+03\n1.702650e+01\n1830\n2019\n▁▁▁▃▇\n    wikipedia_created\n3040\n0.2935162\n2.005748e+03\n3.768240e+00\n2001\n2020\n▇▇▂▁▁\n    wikipedia_revision_count\n3130\n0.2726005\n3.304314e+02\n8.132556e+02\n1\n10104\n▇▁▁▁▁\n    last_activity\n0\n1.0000000\n2.000616e+03\n8.459776e+01\n-900\n2023\n▁▁▁▁▇\n    number_of_users\n0\n1.0000000\n1.377126e+04\n2.277129e+05\n0\n7179119\n▇▁▁▁▁\n    number_of_jobs\n0\n1.0000000\n4.221838e+02\n1.257299e+04\n0\n771996\n▇▁▁▁▁\n    central_package_repository_count\n1482\n0.6555891\n0.000000e+00\n0.000000e+00\n0\n0\n▁▁▇▁▁\n  \n  \n  \n\n\n\n\n\n\nVisualizing Missingness with visdat\nThe visdat package makes ggplot- compatible missingness plots. The cluster = TRUE option groups variables that share missingness. Here we see that usually if some of the GitHub data is missing, then all of the GitHub data is missing. The percent missing is listed for each variable, and the overall missingness of the dataset is shown in the legend.\nNote vis_miss doesn’t work on very large datasets. The documentation suggests keeping the number of records below 1,000. A more extensive package for exploratory visualizations called naniar could also be used.\n\nlanguages_numeric %&gt;%\nvis_miss(cluster = TRUE) +\nggtitle(\"Missing Data in the Languages Dataset\") +\n  #theme_classic() +\n  theme(axis.text.x = element_text(size = 8, angle = 90))"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#improving-the-most-popular-language-for-each-comment-token-figure",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#improving-the-most-popular-language-for-each-comment-token-figure",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "Improving “The Most Popular Language for Each Comment Token” Figure",
    "text": "Improving “The Most Popular Language for Each Comment Token” Figure\n\njoined &lt;- read_csv(\"processed_lang.csv\" , show_col_types = FALSE)\n\nNow the original figure:\n\njoined %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\nI thought I had noted this in the previous post, but one of the tokens, ⍝ , is rendered as an empty box in the ggplot figures. I thought fixing this would be easy. First, I thought I could just pass the Unicode value for that symbol. Then, when that didn’t work, I thought I could change the font to one supporting that symbol. Supposedly, changing the font should be easy, yet after 3 hours working on it, I still had blank squares. There is a nice tutorial on changing fonts in ggplot that did not work until I found someone with the same issue. The solution is to add a line of code that doesn’t make much sense to me : windowsFonts(\"Cambria Math\" = windowsFont(\"Cambria Math\"))\nI saw a nice TidyTuesday figure on Twitter:\n\n\nMy submission for #TidyTuesday, Week 12 on programming languages. I explore jobs per users.Code: https://t.co/bV9DUHZmro pic.twitter.com/2D5YLnE5yz\n\n— Mitsuo Shiota (@mitsuoxv) March 21, 2023\n\n\nwith a caption referencing the original dataset. I’d like to add that. I generally want to increase the figure’s legibility and flip the color scale so that darker blue corresponds to more users. I also don’t think what popular means is entirely clear, so I’d like to explain more fully what I’m graphing.\n\nwindowsFonts(\"Cambria Math\" = windowsFont(\"Cambria Math\"))\njoined %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n    scale_colour_gradient(high = \"#08306b\", low = \"#6baed6\") + \n   labs(title = \"The Most Popular Language for Each Comment Token\",\n       subtitle = \"Based on # Users and Rank\",\n       caption = \"data from https://pldb.com/\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic(base_size = 16) +\n  theme(text = element_text( family = \"Cambria Math\")) +\n  theme(axis.text.x = element_text(face = \"bold\"))"
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "",
    "text": "I’m still behind on TidyTuesday, so here is TidyTuesday #34, which deals with refugee data. This data is collected by the United Nations High Commissioner for Refugees and the R package is updated twice a year."
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#loading-data-and-libraries",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#loading-data-and-libraries",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "Loading Data and Libraries",
    "text": "Loading Data and Libraries\nLibraries.\n\nlibrary(tidyverse) # who doesn't want to be tidy\nlibrary(networkD3) # for Sankey plots\nlibrary(ggsankey) # another sankey package \nlibrary(htmlwidgets) # html widgets helps to handle the networkD3 objects\nlibrary(htmltools) # for formatting html code\n\nLoading data.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 34)\npopulation &lt;- tuesdata$population"
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#exploratory-data-analysis",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#exploratory-data-analysis",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThis dataset tracks the number of refugees by year. It tracks how many people request entry at each country and breaks this information down by country of origin.\n\nhead(population)\n\n# A tibble: 6 × 16\n   year coo_name    coo   coo_iso coa_name coa   coa_iso refugees asylum_seekers\n  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n1  2010 Afghanistan AFG   AFG     Afghani… AFG   AFG            0              0\n2  2010 Iran (Isla… IRN   IRN     Afghani… AFG   AFG           30             21\n3  2010 Iraq        IRQ   IRQ     Afghani… AFG   AFG            6              0\n4  2010 Pakistan    PAK   PAK     Afghani… AFG   AFG         6398              9\n5  2010 Egypt       ARE   EGY     Albania  ALB   ALB            5              0\n6  2010 China       CHI   CHN     Albania  ALB   ALB            6              0\n# ℹ 7 more variables: returned_refugees &lt;dbl&gt;, idps &lt;dbl&gt;, returned_idps &lt;dbl&gt;,\n#   stateless &lt;dbl&gt;, ooc &lt;dbl&gt;, oip &lt;dbl&gt;, hst &lt;dbl&gt;\n\n\nWhat years have the most data?\n\npopulation %&gt;%\n  group_by(year) %&gt;%\n  count() %&gt;%\n  ggplot(aes(year, n)) +\n  geom_col()\n\n\n\n\nI expected to see spikes corresponding to specific events, but instead, it looks like the data is gradually increasing. This upward trend might be due to better record-keeping/ data collection over the years.\nI’m going to look at last year’s data.\n\npop_2022 &lt;- population %&gt;%\n  filter(year == 2022)"
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#a-sankey-diagram-to-explore-flow",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#a-sankey-diagram-to-explore-flow",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "A Sankey Diagram to Explore Flow",
    "text": "A Sankey Diagram to Explore Flow\nNow, I will look at where most refugees come from.\nThere is a technical and legal difference between refugees and asylum seekers, but I will sum them up. I also will be using refugees and asylum seekers interchangeably here.\n\npop_2022 &lt;- pop_2022 %&gt;%\n  mutate(total = refugees + asylum_seekers) %&gt;%\n  select(coo_name, coa_name, total)\n\nI’ve been interested in learning how to make Sankey diagrams in R. A Sankey diagram shows the flow between nodes; there is a great gallery of examples here. This tidytuesday data does lend itself to a Sankey diagram. There are two nodes- country of orgin and country of arrival and the flow is the number of refugees between each pair.\nI do want to highlight that the UN Refugee Agency has a separate dataset that tracks flow. The dataset we are working with is the net number of refugees. As the UNHCR webpage says:\n\nOften the net increase or decrease in stock figures between years has been used in lieu of accurate flow figures. But these may underrepresent the true magnitude of population movements if, for example, refugee arrivals and departures balance each other out.\n\nStill, for TidyTuesday, the net population is fine.\nI’m going to pull out the top 3 sources of refugees. I’m grouping by the country of origin (coo_name) and then summing the refugees going to all countries. I then take the top 3 using dpylr’s slice_max(). Note that this function replaces top_n().\n\ntop_3_source &lt;- pop_2022 %&gt;%\n  group_by(coo_name) %&gt;%\n  summarize(num_by_coo = sum(total)) %&gt;%\n  slice_max(order_by = num_by_coo, n = 3)\n\ntop_3_source_names &lt;- top_3_source$coo_name\n\ntop_3_source_names\n\n[1] \"Syrian Arab Rep.\" \"Afghanistan\"      \"Ukraine\"         \n\n\nNow, I’m going to do a related analysis and see where the top destinations are.\n\ntop_3_dest &lt;- pop_2022 %&gt;%\n  group_by(coa_name) %&gt;%\n  summarize(num_by_coa = sum(total)) %&gt;% slice_max(order_by = num_by_coa, n = 3)\n\ntop_3_dest_names &lt;- (top_3_dest$coa_name)\ntop_3_dest_names\n\n[1] \"Türkiye\"                \"Iran (Islamic Rep. of)\" \"Germany\"               \n\n\nNow I’m making a category “other” for both source and destination countries that aren’t in the top 3. I use the forcats function fct_other().\nFirst country of arrival:\n\npop_2022$coa_name = factor(pop_2022$coa_name)\n\npop_2022$coa_name &lt;- pop_2022$coa_name %&gt;%\n  fct_other(keep = top_3_dest_names, other_level = \"other\")\n\nThen, country of origin:\n\npop_2022$coo_name = factor(pop_2022$coo_name)\n\npop_2022$coo_name &lt;- pop_2022$coo_name %&gt;%\n  fct_other(keep = top_3_source_names, other_level = \"other\")\n\nFor now, I will remove the “other” category of country of origin.\n\npop_2022_no_other &lt;- pop_2022 %&gt;%\n  filter((coo_name != \"other\"))\n\nThere are 3 popular ways to make Sankey diagrams in R: ggsankey, networkD3, and plotly. There is a nice tutorial on these methods here.\nI’m going to focus on ggsankey and networkD3.\n\nggsankey method for Sankey Plots in R\nThis package is combined with ggplot to make nice static Sankey plots.\nThe package documentation does a great job explaining the parts of the Sankey chart. It does not do a great job of explaining what format the data should be in. I spent most of my time last Tuesday trying to figure this out.\nThe package includes a function make_long(), which is used to prepare the data for plotting. It labels the columns appropriately and creates the needed null/ NA entries. These entries are used to signify the end of the flow.\nThe UN refugee data is already long, so I spent a fair bit of time trying to figure out how to either label it myself or make it wide so I could use make_long().\nIt turns out that ggsankey can only be used on disaggregated data and not aggregated data such as I have here. I never found that explicitly stated in the documentation (it could be I missed that), but I only discovered it as I was trying to wrangle my data into the correct format and comparing it to the mtcars data in the example.\nWhat do I mean by aggregated vs. disaggregated?\nLet’s look at the mtcars dataframe:\n\nmtcars2 &lt;- mtcars %&gt;%\n  slice_sample(n = 10) %&gt;%\n  select(cyl, vs)\n\nmtcars2\n\n                 cyl vs\nPontiac Firebird   8  0\nFerrari Dino       6  0\nPorsche 914-2      4  0\nMerc 450SLC        8  0\nMerc 280           6  1\nMerc 230           4  1\nDatsun 710         4  1\nCamaro Z28         8  0\nMazda RX4          6  0\nMerc 450SE         8  0\n\n\nThis is disaggregated. It reports the properties of each car. The equivalent in our dataset would be if the database listed every refugee by name and specified where they were from and where they were going.\nWhat the population dataset has instead is the aggregated data. This is the mtcars equivalent:\n\nmtcars2 %&gt;% group_by(cyl, vs) %&gt;% count()\n\n# A tibble: 5 × 3\n# Groups:   cyl, vs [5]\n    cyl    vs     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     4     0     1\n2     4     1     2\n3     6     0     2\n4     6     1     1\n5     8     0     4\n\n\nSo we know we have two cars with cyl = 4 and vs = 1, but we don’t know that they are specifically the Merc 240D and the Fiat 128.\nIf all you have is the aggregated data, you can’t disaggregate it. The names of the cars are not contained in that dataframe. In this case, if you absolutely had to make a Sankey diagram using ggsankey, you could write some code to make a dummy ID to represent the missing dimension. That is, you could split the entry cyl = 4 and vs = 1 into\ncyl = 4 and vs = 1 and id = 1\ncyl = 4 and vs = 1 and id = 2\nAnd so on for each group.\nThen you’d have to dpylr::pivot_wider() and then ggsankey::make_longer(). I wouldn’t recommend this, but it is useful sometimes to think of how you’d approach a problem, even if there are easier solutions. [This :: formatting forces the function to load from the specific library on the left side of the double colon. This is useful if you have conflicting function names in different packages; this notation assure the proper one is called. It is also useful to clarify where specific functions are coming from, even if there would be no conflict.]\n\n\nnetworkD3 method for Sankey Plots in R\nI love the R Graph Gallery when I’m looking for inspiration for TidyTuesday. R Graph Gallery recommends the networkD3 package for Sankey plots, but it is an htmlwidget. I’ve been doing a lot of interactive stuff recently, and I originally wanted to get back to some good old ggplot. But the networkD3 package does create lovely Sankey plots.\nThe networkD3 method seems slightly confusing because you need to transform and label the data yourself. Like ggsankey, you need information about the nodes and how they are connected with each other. One other note- this is based on a JavaScript package (with 0 indexing). So, a few “-1” to convert the index from R (which starts at 1).\nThere is a nice post on Sankey diagrams in networkD3 on the Data to Viz blog. The code is hidden, so you need to toggle it on with the code button if you want to see it.\nThe function call is (from the manual found at CRAN):\nsankeyNetwork(Links, Nodes, Source, Target, Value, NodeID, NodeGroup = NodeID, LinkGroup = NULL, units = \"\", colourScale = JS(\"d3.scaleOrdinal(d3.schemeCategory20);\"), fontSize = 7, fontFamily = NULL, nodeWidth = 15, nodePadding = 10, margin = NULL, height = NULL, width = NULL, iterations = 32, sinksRight = TRUE)\nI need Links, Nodes, Source, Target, Value, NodeID.\nNodes is the list of countries (both origin and arrival). It needs to be a unique list of the countries stored as a dataframe.\nI converted the countries to factors earlier, but that isn’t a problem.\n\nnodes &lt;- data.frame(name =\n                      c(pop_2022_no_other$coo_name, pop_2022_no_other$coa_name) %&gt;%\n                      unique()\n                    )\n\nNext, we need the IDs for source and target, zero-indexed. The ID is generated by position in the nodes dataframe I just created. The match() function is a base R function that returns a vector of the positions of the first argument (pop_2022_no_other$coo_name) as found in the second argument (nodes$name). Note that it returns a vector of all matches. I know that nodes is a unique list, so it is only going to return a single index. This may not be true for other use cases. Again, subtract 1 for the difference in indexing.\n\npop_2022_no_other$IDsource = match(pop_2022_no_other$coo_name, nodes$name) -\n  1\npop_2022_no_other$IDtarget = match(pop_2022_no_other$coa_name,  nodes$name) -\n  1\n\nNext, create the color scales. Details about the options for the color scales are found in the D3 API documentation.\nI am going to change the colors a bit. The other category will be gray. The colors are assigned matches using the order in the nodes df. Note that this does NOT match the order in the diagram. Most Sankey plotting programs reorder the nodes to minimize crossing and to create a cleaner diagram.\n\nColourScal = 'd3.scaleOrdinal([`#946943`, `#b63e36`,`#F5B041`, `#909497`,`#383867`, `#584c77`, `#33431e`, `#a36629`, `#92462f`])'\n\nThe colors are specified in hex code here. You can find color pickers that give you the codes on the web, such as this one.\nNow I can make the diagram. I’m also specifying height and width so I can add a title and a caption using htmlwidgets. If you don’t specify height and width the figure might be truncated when the titles are applied. (This method might work for leaflet maps also. The leaflet package also lacks a method for titles.) I am using the html header 2 styling (h2) for the title, while the caption is just a normal paragraph (p).\nThe sinksRight parameter is used to put the label either outside the flows (FALSE) as I have done here. TRUE puts it inside the flow lines. Unfortunately, there is not a matching sinksLeft, so those labels will always be inside the flow.\n\n# Make the Network\nsankey &lt;- sankeyNetwork(\n  Links = pop_2022_no_other,\n  Nodes = nodes,\n  Source = \"IDsource\",\n  Target = \"IDtarget\",\n  Value = \"total\",\n  NodeID = \"name\",\n  sinksRight = FALSE,\n  colourScale = ColourScal,\n  nodeWidth = 40,\n  fontSize = 13,\n  nodePadding = 20,\n  width = 600,\n  height = 400\n)\n\nLinks is a tbl_df. Converting to a plain data frame.\n\nsankey &lt;-\n  htmlwidgets::prependContent(sankey, htmltools::tags$h2(\"Refugee Flow in 2022\"))\nsankey &lt;-\n  htmlwidgets::appendContent(sankey, htmltools::tags$p(\"from UNHCR’s refugees R package\"))\n\nsankey\n\nRefugee Flow in 2022\n\nfrom UNHCR’s refugees R package\n\n\n\nSo this is pretty clear. Because the other category is disaggregated, we get individual lines for each country of arrival (but we don’t know what countries they are.) If you mouse over the traces, you can see how many refugees are involved in each path. For example, if you mouse over the large gray band on the Afghanistan to other, you see 1.8 million refugees have gone to a single color. Some of the other lines represent only a handful of refugees. I could also aggregate the other data, creating a single band from each source to other. But I think it is neat the way it is.\n\n\nSaving an htmlwidget object\nSaving the interactive figures to a thumbnail is always tricky. Various methods are suggested- using webshot seems to be the most popular, but I haven’t had it work. (First, you save the diagram as an HTML file, and then webshot converts that to a static image). While the HTML saves fine, the webshotted png is the title and the caption with a vast white block of no figure in between. The only consistent way I’ve found is to export from RStudio’s viewer. You do have to fiddle with the height and width of your object; otherwise, the exported file will either be truncated or have non-functional scroll bars in the image."
  }
]