[
  {
    "objectID": "blogroll.html",
    "href": "blogroll.html",
    "title": "Blog Roll",
    "section": "",
    "text": "Blogs:\nR Bloggers\nTutorials I’ve followed:\nWord Games Project:\nr letter frequency in R packages via R-Bloggers\nSetting Up Webpage:\nCreating Quarto Websites by Sam Csik\nAlbert Rapp: The ultimate guide to starting a Quarto blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Louise E. Sinks",
    "section": "",
    "text": "Hello! I’m Louise Sinks.\nI’m a chemist by training. My favorite part was always analysis and modeling. I’ve recently been learning R and Python, and this webpage is a place to document my data science journey. While I did a lot of coding and modeling as a chemist, I did not formally learn about modeling and machine learning frameworks. (I wish I had! My post-doc would have been a lot smoother.)"
  },
  {
    "objectID": "posts/2023-03-14-tester-post/index.html",
    "href": "posts/2023-03-14-tester-post/index.html",
    "title": "Creating a Blog",
    "section": "",
    "text": "This is my first blog entry. I am following the tutorials here:\nhttps://ucsb-meds.github.io/creating-quarto-websites/#where-you-should-start-changing-stuff\nhttps://samanthacsik.github.io/posts/2022-10-24-quarto-blogs/\nGenerally, this process has been a nightmare. The website is being created within RStudio, then pushed to GitHub and published through GitHub pages. As I’ve made changes per the tutorial, I have repeatedly been unable to push changes to GitHub due to a variety of fatal errors and merge conflicts. Since I’m only working in a single place I have no idea where all these merge conflicts are arising from. I don’t understand how I can have everything in sync everywhere, make a local change, commit it, and then be unable to push it. I’ve had to delete the GitHub repository at least half a dozen times and recreate it from my local version because I couldn’t find any way to fix the conflicts and fatal errors. I’m not sure whose fault this is (Quarto, GitHub or RStudio). It could be my fault, but I really don’t understand why things are breaking so spectacularly. I’ve used git/ GitHub for version control of R projects before and I’ve never had an error. (I don’t really see how you can get a merge conflict if you are the only person working on a project and you are only working at a single location, but maybe I’m failing to envision some use case.)\nI decided to go with Quarto because it is now built-in to RStudio and the tutorials by Samantha Csik seemed very clear. (And to be fair, they are! Very easy to follow.) The tutorials I found for R Markdown to make a website seemed a little more involved and a little more kludgey.\nCouldn’t have done it without the best helper turtle in the world. Mac took a lot of executive naps to work on the problem.\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{e.sinks2023,\n  author = {Louise E. Sinks},\n  title = {Creating a {Blog}},\n  date = {2023-03-14},\n  url = {https://lsinks.github.io/2023-03-14_tester-post},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLouise E. Sinks. 2023. “Creating a Blog.” March 14, 2023.\nhttps://lsinks.github.io/2023-03-14_tester-post."
  },
  {
    "objectID": "posts/2023-03-21-tidytuesday-programming-languages/index.html",
    "href": "posts/2023-03-21-tidytuesday-programming-languages/index.html",
    "title": "TidyTuesday Week 12: Programming Languages",
    "section": "",
    "text": "This is my first attempt at Tidy Tuesday. The dataset today is about Programming Languages. The sample visualizations are about the comment codes.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(ggrepel)\n\nLoad the data first. There has been some cleaning done as outlined on the TidyTuesday github page.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata <- tidytuesdayR::tt_load('2023-03-21')\ntuesdata <- tidytuesdayR::tt_load(2023, week = 12)\n\nlanguages <- tuesdata$languages\n\nFirst, let’s look at how complete the data is. The skimr package produces nice summary information about the variables and their completeness.\n\nskim(languages)\n\n\nData summary\n\n\nName\nlanguages\n\n\nNumber of rows\n4303\n\n\nNumber of columns\n49\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n21\n\n\nlogical\n4\n\n\nnumeric\n24\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n4303\n0\n\n\ntitle\n0\n1.00\n1\n56\n0\n4267\n0\n\n\ndescription\n3480\n0.19\n4\n2273\n0\n811\n0\n\n\ntype\n0\n1.00\n2\n27\n0\n40\n0\n\n\ncreators\n3203\n0.26\n2\n253\n0\n985\n0\n\n\nwebsite\n2928\n0.32\n13\n131\n0\n1368\n0\n\n\ndomain_name\n3588\n0.17\n6\n32\n0\n700\n0\n\n\nreference\n2314\n0.46\n15\n251\n0\n1955\n0\n\n\ngithub_repo\n3402\n0.21\n25\n73\n0\n897\n0\n\n\ngithub_repo_description\n3438\n0.20\n4\n419\n0\n853\n0\n\n\ngithub_language\n3829\n0.11\n1\n30\n0\n474\n0\n\n\ngithub_language_tm_scope\n3837\n0.11\n4\n34\n0\n361\n0\n\n\ngithub_language_type\n3837\n0.11\n4\n11\n0\n4\n0\n\n\ngithub_language_ace_mode\n3838\n0.11\n1\n16\n0\n96\n0\n\n\ngithub_language_file_extensions\n3833\n0.11\n1\n606\n0\n466\n0\n\n\nwikipedia\n2731\n0.37\n32\n104\n0\n1566\n0\n\n\nwikipedia_summary\n2884\n0.33\n17\n6741\n0\n1407\n0\n\n\nwikipedia_related\n3145\n0.27\n1\n1761\n0\n1059\n0\n\n\nline_comment_token\n3831\n0.11\n1\n7\n0\n23\n0\n\n\norigin_community\n1190\n0.72\n3\n305\n0\n2232\n0\n\n\nfile_type\n3213\n0.25\n2\n6\n0\n4\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n3683\n0.14\n1.00\nTRU: 617, FAL: 3\n\n\nfeatures_has_semantic_indentation\n3722\n0.14\n0.11\nFAL: 516, TRU: 65\n\n\nfeatures_has_line_comments\n3765\n0.13\n0.96\nTRU: 517, FAL: 21\n\n\nis_open_source\n3792\n0.12\n0.89\nTRU: 453, FAL: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1991.11\n111.44\n-2000\n1984.00\n1997.0\n2012.00\n2023\n▁▁▁▁▇\n\n\ndomain_name_registered\n3801\n0.12\n2011.33\n7.02\n1990\n2007.00\n2013.0\n2017.00\n2023\n▁▃▃▇▆\n\n\nisbndb\n3217\n0.25\n7.71\n33.16\n0\n0.00\n0.0\n2.00\n400\n▇▁▁▁▁\n\n\nbook_count\n0\n1.00\n2.08\n17.34\n0\n0.00\n0.0\n0.00\n401\n▇▁▁▁▁\n\n\nsemantic_scholar\n3545\n0.18\n3.79\n8.32\n0\n0.00\n0.0\n3.00\n52\n▇▁▁▁▁\n\n\nlanguage_rank\n0\n1.00\n2151.00\n1242.31\n0\n1075.50\n2151.0\n3226.50\n4302\n▇▇▇▇▇\n\n\ngithub_repo_stars\n3414\n0.21\n2127.40\n7554.02\n0\n29.00\n194.0\n1071.00\n88526\n▇▁▁▁▁\n\n\ngithub_repo_forks\n3417\n0.21\n261.29\n1203.00\n0\n2.25\n16.0\n91.50\n23732\n▇▁▁▁▁\n\n\ngithub_repo_updated\n3418\n0.21\n2021.39\n1.76\n2012\n2022.00\n2022.0\n2022.00\n2023\n▁▁▁▁▇\n\n\ngithub_repo_subscribers\n3418\n0.21\n62.34\n200.88\n0\n4.00\n13.0\n44.00\n2910\n▇▁▁▁▁\n\n\ngithub_repo_created\n3425\n0.20\n2015.84\n3.48\n2006\n2013.00\n2016.0\n2019.00\n2022\n▁▅▇▇▇\n\n\ngithub_repo_issues\n3518\n0.18\n123.03\n546.26\n0\n1.00\n9.0\n61.00\n9522\n▇▁▁▁▁\n\n\ngithub_repo_first_commit\n3567\n0.17\n2014.74\n4.99\n1987\n2012.00\n2015.0\n2018.00\n2022\n▁▁▁▆▇\n\n\ngithub_language_repos\n3833\n0.11\n197134.67\n1226900.57\n0\n91.25\n725.5\n7900.25\n16046489\n▇▁▁▁▁\n\n\nwikipedia_daily_page_views\n2837\n0.34\n227.13\n783.55\n-1\n9.00\n24.0\n99.00\n13394\n▇▁▁▁▁\n\n\nwikipedia_backlinks_count\n2877\n0.33\n318.55\n1635.29\n1\n13.00\n39.0\n126.00\n34348\n▇▁▁▁▁\n\n\nwikipedia_page_id\n2893\n0.33\n9167847.21\n13506832.90\n928\n375153.75\n2114700.5\n12321223.00\n63063548\n▇▁▁▁▁\n\n\nwikipedia_appeared\n2958\n0.31\n1991.14\n17.03\n1830\n1980.00\n1994.0\n2005.00\n2019\n▁▁▁▃▇\n\n\nwikipedia_created\n3040\n0.29\n2005.75\n3.77\n2001\n2003.00\n2005.0\n2007.00\n2020\n▇▇▂▁▁\n\n\nwikipedia_revision_count\n3130\n0.27\n330.43\n813.26\n1\n35.00\n84.0\n242.00\n10104\n▇▁▁▁▁\n\n\nlast_activity\n0\n1.00\n2000.62\n84.60\n-900\n1992.00\n2006.0\n2021.00\n2023\n▁▁▁▁▇\n\n\nnumber_of_users\n0\n1.00\n13771.26\n227712.95\n0\n0.00\n20.0\n230.00\n7179119\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n422.18\n12572.99\n0\n0.00\n0.0\n0.00\n771996\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n1482\n0.66\n0.00\n0.00\n0\n0.00\n0.0\n0.00\n0\n▁▁▇▁▁\n\n\n\n\n\nThe data is pretty incomplete. Only 9 of the 49 variables are fully complete. The line comment token is only 0.110 complete and the has comments is only 0.144 complete. This variable has only 3 false values; it is likely that the missing data is skewed towards false. It is more likely that you’d complete this entry if there were a comment, than if there weren’t. It is also possible that the cleaning and prep done to prepare the #TidyTuesday dataset removed some entries which did have FALSE values for the comments.\nThere are some funny entries that appeared in the skim report, like -2000 as the year the earliest language appeared. It turns out this is Babylonian numerals, so it probably correct. This does show there is a lot more than computer languages in this dataset though.\nLooking through the variables, I see there is a “type” in the data dictionary, and it appears that “pl” means programming language. So let’s filter for that. (I couldn’t find an explanation of this variable on https://pldb.com/) It is used on various pages, but I couldn’t find the definition of the types.\nAlso, rank starts at 0, and I’d like it to start at 1.\n\nprogramming_lang <- languages %>%\n  filter(type == 'pl') %>%\n  select(-starts_with(\"github\"), -starts_with(\"wikipedia\"),\n         -description, -creators, -(website:semantic_scholar)) %>%\n  mutate(language_rank = language_rank + 1)\n\nskim(programming_lang)\n\n\nData summary\n\n\nName\nprogramming_lang\n\n\nNumber of rows\n3368\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlogical\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n3368\n0\n\n\ntitle\n0\n1.00\n1\n54\n0\n3347\n0\n\n\ntype\n0\n1.00\n2\n2\n0\n1\n0\n\n\nline_comment_token\n3002\n0.11\n1\n3\n0\n18\n0\n\n\norigin_community\n883\n0.74\n3\n176\n0\n1825\n0\n\n\nfile_type\n2609\n0.23\n4\n4\n0\n1\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n2886\n0.14\n1.00\nTRU: 482\n\n\nfeatures_has_semantic_indentation\n2917\n0.13\n0.09\nFAL: 410, TRU: 41\n\n\nfeatures_has_line_comments\n2954\n0.12\n0.97\nTRU: 401, FAL: 13\n\n\nis_open_source\n2984\n0.11\n0.85\nTRU: 328, FAL: 56\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1994.16\n17.34\n1948\n1982.0\n1994.0\n2010.0\n2022\n▁▅▇▇▇\n\n\nlanguage_rank\n0\n1.00\n2296.75\n1249.08\n1\n1243.5\n2334.5\n3423.5\n4303\n▆▆▆▆▇\n\n\nlast_activity\n0\n1.00\n2002.04\n17.91\n1951\n1989.0\n2005.0\n2019.0\n2023\n▁▂▃▆▇\n\n\nnumber_of_users\n0\n1.00\n10793.85\n190197.19\n0\n0.0\n15.0\n165.0\n5962666\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n160.22\n2692.65\n0\n0.0\n0.0\n0.0\n85206\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n939\n0.72\n0.00\n0.00\n0\n0.0\n0.0\n0.0\n0\n▁▁▇▁▁\n\n\n\n\n\nThis now produces a dataset with 0.143 completeness for features_has_comments. All non-missing entries are TRUE, which again suggests that FALSE is over represented in the missing data.\nLet’s only look at the programming languages that have data for comments.\n\nprogramming_lang <- programming_lang %>%\n  filter(features_has_comments == TRUE)\n\nskim(programming_lang)\n\n\nData summary\n\n\nName\nprogramming_lang\n\n\nNumber of rows\n482\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlogical\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n35\n0\n482\n0\n\n\ntitle\n0\n1.00\n1\n45\n0\n481\n0\n\n\ntype\n0\n1.00\n2\n2\n0\n1\n0\n\n\nline_comment_token\n120\n0.75\n1\n3\n0\n18\n0\n\n\norigin_community\n112\n0.77\n3\n105\n0\n311\n0\n\n\nfile_type\n146\n0.70\n4\n4\n0\n1\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n0\n1.00\n1.00\nTRU: 482\n\n\nfeatures_has_semantic_indentation\n57\n0.88\n0.05\nFAL: 405, TRU: 20\n\n\nfeatures_has_line_comments\n71\n0.85\n0.97\nTRU: 400, FAL: 11\n\n\nis_open_source\n305\n0.37\n0.91\nTRU: 161, FAL: 16\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n2000.17\n14.07\n1957\n1991.00\n2003.0\n2011.00\n2022\n▁▂▆▇▇\n\n\nlanguage_rank\n0\n1.00\n656.10\n559.75\n1\n201.25\n515.5\n997.25\n2994\n▇▃▂▁▁\n\n\nlast_activity\n0\n1.00\n2016.20\n8.27\n1967\n2011.00\n2022.0\n2022.00\n2023\n▁▁▁▂▇\n\n\nnumber_of_users\n0\n1.00\n62892.08\n462314.18\n0\n112.00\n437.5\n1615.25\n5962666\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n971.30\n6489.83\n0\n0.00\n0.0\n0.00\n85206\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n136\n0.72\n0.00\n0.00\n0\n0.00\n0.0\n0.00\n0\n▁▁▇▁▁\n\n\n\n\n\nThis subset is still moderately incomplete for information about comments. Only 75% of the data has the type of comment entered (#, //, etc). 86% of the entries are completed for “feature_has_line_comments” which indicates if comments must occupy a single line or if they can be made inline.\n\nprogramming_lang %>% filter(is.na(line_comment_token) == FALSE) %>%\n  group_by(line_comment_token) %>%\n  count(line_comment_token) %>%\n  ggplot(aes(fct_rev(fct_reorder(line_comment_token, n)), n)) +\n  geom_col(fill = \"dodgerblue2\") +\n  ylab(\"Count\") +\n  xlab(\"Comment Token\") +\n  ggtitle(\"Popularity of different comment tokens\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,  vjust = 0.25, hjust = 0.25))\n\n\n\n\nLet’s make a nice table of the popular comment types.\n\n# | label: table-tokens\nprogramming_lang2 <- programming_lang %>%\n  filter(is.na(line_comment_token) == FALSE) %>%\n  count(line_comment_token, sort = TRUE) \n\nprogramming_lang2 %>%\ngt() %>%\ntab_header(title = \"Most Common Comment Tokens\") %>%\ncols_label(line_comment_token = \"Token\", n = \"# of Languages that use token\")\n\n\n\n\n\n  \n    \n      Most Common Comment Tokens\n    \n    \n  \n  \n    \n      Token\n      # of Languages that use token\n    \n  \n  \n    //\n161\n    #\n70\n    ;\n49\n    --\n31\n    '\n16\n    %\n12\n    !\n7\n    *\n5\n    REM\n2\n    *>\n1\n    ---\n1\n    /\n1\n    NB.\n1\n    \\\n1\n    \\*\n1\n    __\n1\n    ~\n1\n    ⍝\n1\n  \n  \n  \n\n\n\n\nThere is a language rank, which measures the popularity of the language based on signals such as number of users and number of jobs. Let’s see the average rank of languages for each token.\n\nprogramming_lang %>% filter(is.na(line_comment_token) == FALSE) %>%\n  group_by(line_comment_token) %>%\n  summarize(avg_rank = mean(language_rank)) %>%\n  ggplot(aes((fct_reorder(line_comment_token, avg_rank)), avg_rank)) +\n  geom_col(fill = \"dodgerblue2\") +\n  ylab(\"Average Rank of Language\") +\n  xlab(\"Comment Token\") +\n  ggtitle(\"Average rank of languages using different comment tokens\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,  vjust = 0.25, hjust = 0.25))\n\n\n\n\nThe highest (average) ranked token is “*>”. What languages use this?\n\nprogramming_lang %>% filter(line_comment_token == \"*>\") %>%\n  select(title, language_rank, line_comment_token)\n\n# A tibble: 1 × 3\n  title language_rank line_comment_token\n  <chr>         <dbl> <chr>             \n1 COBOL            19 *>                \n\n\nOnly COBOL does, so the rank of this token isn’t diluted by many less popular languages. We can view the distribution of the language ranks for all the tokens.\n\nprogramming_lang %>%\n  filter(is.na(line_comment_token) == FALSE) %>%\n  ggplot(aes(line_comment_token, language_rank)) +\n  geom_boxplot(color = \"dodgerblue2\") +\n  ggtitle(\"The rank of languages by token.\") +\n  xlab(\"Token\") +\n  ylab (\"Language Rank\") +\n  theme_classic()\n\n\n\n\nOkay, let’s clean this up. I’d like it sorted by the median rank. Remeber rank is in reverse numerical order- a low number means a higher rank.\n\nprogramming_lang %>%\n  filter(is.na(line_comment_token) == FALSE) %>%\n  ggplot(aes(fct_reorder(line_comment_token, language_rank,\n                         .fun = median, .desc = FALSE), language_rank)) +\n  geom_boxplot(color = \"dodgerblue2\") +\n  ggtitle(\"The rank of languages by token\") +\n  xlab(\"Token\") +\n  ylab(\"Language Rank\") +\n    theme_classic()\n\n\n\n\nLet’s see the most popular language for each symbol. There might be a way to do this all at once, but I’m going to pull it out with joins to previous tables I’ve created.\n\nprogramming_lang3 <- programming_lang %>%\n  filter(is.na(line_comment_token) == FALSE) %>%\n  group_by(line_comment_token) %>%\n  summarize(highest_rank = min(language_rank)) \n\njoin_madness <- programming_lang2 %>%\n  left_join(programming_lang3, by = \"line_comment_token\") %>% \n  left_join(programming_lang, \n            by = c(\"highest_rank\" = \"language_rank\",\n                   \"line_comment_token\" = \"line_comment_token\")) \n\njoin_madness <- join_madness %>%\n  select(line_comment_token, n, highest_rank, title, appeared, number_of_users,\n         number_of_jobs)\n\nSo now we have a bunch of summarized data in a single dataframe. Here’s a graph. It is saying something, but I’m not sure what. When you can’t come up with a concise title, then you probably don’t know what you are trying to say…\n\njoin_madness %>%\n  ggplot(aes(highest_rank, n, size = log(number_of_users), \n             color = log(number_of_users), label = line_comment_token)) +\n  scale_y_log10() +\n  scale_x_log10() +\n  geom_text_repel(show.legend = FALSE) +\n  ggtitle(\"Popularity of tokens by language rank and usage\") +\n  xlab(\"Highest Rank of language using Token\") +\n  ylab(\"Number of Languages using token\") +\n  theme_classic()\n\n\n\n\nThis is a visualization of the highest ranked languages for each token. The number of users of the dominant language is also encoded in the size and color of the label. Having it ordered makes it difficult to tell if Java or Python is the most popular/ highest ranked language.\n\njoin_madness %>%\n  ggplot(aes(fct_rev(fct_reorder(line_comment_token, highest_rank)), n,\n             size = log(number_of_users), color = log(number_of_users),\n             label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\nHere is the same graph just ordered “alphabetically” by token.\n\njoin_madness %>%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{e.sinks2023,\n  author = {Louise E. Sinks},\n  title = {TidyTuesday {Week} 12: {Programming} {Languages}},\n  date = {2023-03-21},\n  url = {https://lsinks.github.io/posts/2023-03-21-tidytuesday-programming-languages/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLouise E. Sinks. 2023. “TidyTuesday Week 12: Programming\nLanguages.” March 21, 2023. https://lsinks.github.io/posts/2023-03-21-tidytuesday-programming-languages/."
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "",
    "text": "This week, I participated in my first #TidyTuesday challenge. My goal was to get something out on the day of the challenge rather than perfection. I did notice that the skimr output wasn’t formatted nicely on the webpage. Today, I’m going to delve into the skimr and Quarto documentation and make a nicer version of the output. Secondly, I’m going to fix up my final figure, which is the one I shared on social media:"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#skimr-to-understand-your-data",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#skimr-to-understand-your-data",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "Skimr to understand your data",
    "text": "Skimr to understand your data\nSkimr is a package that provides statistical summaries of the variables in your dataframe. It also provides information about the missingness of each variable.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(ggrepel)\nlibrary(visdat) # visualizing missing data in dataframe\n\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata <- tidytuesdayR::tt_load('2023-03-21')\ntuesdata <- tidytuesdayR::tt_load(2023, week = 12)\n\nlanguages <- tuesdata$languages\n\n\nCustomizing the skim Output\nMy main objection is that the numerical summary is too wide and has a scroll bar. I especially want the histogram to be viewable on the first screen. I also don’t particularly care about all the quartile information; min and max are enough. If I want to delve more into the stats of a variable, I will do it another way, not with skimr.\nFirst, quarto lets you expand the output of the code chunk to fill the page via the option “#| column: page”, so I’ll do that. Next, I’ll create a custom skim function that drops the p25, p50, and p75 output from the summary of the numerical variables.\n\nmy_skim <- skim_with(numeric = sfl(p25 = NULL, p50 = NULL, p75 = NULL)) \n\nmy_skim(languages)\n\n\nData summary\n\n\nName\nlanguages\n\n\nNumber of rows\n4303\n\n\nNumber of columns\n49\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n21\n\n\nlogical\n4\n\n\nnumeric\n24\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n4303\n0\n\n\ntitle\n0\n1.00\n1\n56\n0\n4267\n0\n\n\ndescription\n3480\n0.19\n4\n2273\n0\n811\n0\n\n\ntype\n0\n1.00\n2\n27\n0\n40\n0\n\n\ncreators\n3203\n0.26\n2\n253\n0\n985\n0\n\n\nwebsite\n2928\n0.32\n13\n131\n0\n1368\n0\n\n\ndomain_name\n3588\n0.17\n6\n32\n0\n700\n0\n\n\nreference\n2314\n0.46\n15\n251\n0\n1955\n0\n\n\ngithub_repo\n3402\n0.21\n25\n73\n0\n897\n0\n\n\ngithub_repo_description\n3438\n0.20\n4\n419\n0\n853\n0\n\n\ngithub_language\n3829\n0.11\n1\n30\n0\n474\n0\n\n\ngithub_language_tm_scope\n3837\n0.11\n4\n34\n0\n361\n0\n\n\ngithub_language_type\n3837\n0.11\n4\n11\n0\n4\n0\n\n\ngithub_language_ace_mode\n3838\n0.11\n1\n16\n0\n96\n0\n\n\ngithub_language_file_extensions\n3833\n0.11\n1\n606\n0\n466\n0\n\n\nwikipedia\n2731\n0.37\n32\n104\n0\n1566\n0\n\n\nwikipedia_summary\n2884\n0.33\n17\n6741\n0\n1407\n0\n\n\nwikipedia_related\n3145\n0.27\n1\n1761\n0\n1059\n0\n\n\nline_comment_token\n3831\n0.11\n1\n7\n0\n23\n0\n\n\norigin_community\n1190\n0.72\n3\n305\n0\n2232\n0\n\n\nfile_type\n3213\n0.25\n2\n6\n0\n4\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n3683\n0.14\n1.00\nTRU: 617, FAL: 3\n\n\nfeatures_has_semantic_indentation\n3722\n0.14\n0.11\nFAL: 516, TRU: 65\n\n\nfeatures_has_line_comments\n3765\n0.13\n0.96\nTRU: 517, FAL: 21\n\n\nis_open_source\n3792\n0.12\n0.89\nTRU: 453, FAL: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1991.11\n111.44\n-2000\n2023\n▁▁▁▁▇\n\n\ndomain_name_registered\n3801\n0.12\n2011.33\n7.02\n1990\n2023\n▁▃▃▇▆\n\n\nisbndb\n3217\n0.25\n7.71\n33.16\n0\n400\n▇▁▁▁▁\n\n\nbook_count\n0\n1.00\n2.08\n17.34\n0\n401\n▇▁▁▁▁\n\n\nsemantic_scholar\n3545\n0.18\n3.79\n8.32\n0\n52\n▇▁▁▁▁\n\n\nlanguage_rank\n0\n1.00\n2151.00\n1242.31\n0\n4302\n▇▇▇▇▇\n\n\ngithub_repo_stars\n3414\n0.21\n2127.40\n7554.02\n0\n88526\n▇▁▁▁▁\n\n\ngithub_repo_forks\n3417\n0.21\n261.29\n1203.00\n0\n23732\n▇▁▁▁▁\n\n\ngithub_repo_updated\n3418\n0.21\n2021.39\n1.76\n2012\n2023\n▁▁▁▁▇\n\n\ngithub_repo_subscribers\n3418\n0.21\n62.34\n200.88\n0\n2910\n▇▁▁▁▁\n\n\ngithub_repo_created\n3425\n0.20\n2015.84\n3.48\n2006\n2022\n▁▅▇▇▇\n\n\ngithub_repo_issues\n3518\n0.18\n123.03\n546.26\n0\n9522\n▇▁▁▁▁\n\n\ngithub_repo_first_commit\n3567\n0.17\n2014.74\n4.99\n1987\n2022\n▁▁▁▆▇\n\n\ngithub_language_repos\n3833\n0.11\n197134.67\n1226900.57\n0\n16046489\n▇▁▁▁▁\n\n\nwikipedia_daily_page_views\n2837\n0.34\n227.13\n783.55\n-1\n13394\n▇▁▁▁▁\n\n\nwikipedia_backlinks_count\n2877\n0.33\n318.55\n1635.29\n1\n34348\n▇▁▁▁▁\n\n\nwikipedia_page_id\n2893\n0.33\n9167847.21\n13506832.90\n928\n63063548\n▇▁▁▁▁\n\n\nwikipedia_appeared\n2958\n0.31\n1991.14\n17.03\n1830\n2019\n▁▁▁▃▇\n\n\nwikipedia_created\n3040\n0.29\n2005.75\n3.77\n2001\n2020\n▇▇▂▁▁\n\n\nwikipedia_revision_count\n3130\n0.27\n330.43\n813.26\n1\n10104\n▇▁▁▁▁\n\n\nlast_activity\n0\n1.00\n2000.62\n84.60\n-900\n2023\n▁▁▁▁▇\n\n\nnumber_of_users\n0\n1.00\n13771.26\n227712.95\n0\n7179119\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n422.18\n12572.99\n0\n771996\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n1482\n0.66\n0.00\n0.00\n0\n0\n▁▁▇▁▁\n\n\n\n\n\nThis output is much nicer. It is a bit wall of text though. I wouldn’t recommend using this in reports, but it is a useful tool when doing your initial dataset analysis. (As a side note, I have noticed skimr doesn’t work well on Kaggle. It performs as expected if you are in interactive mode, but it fails when you try to save the notebook or run non-interactively.)\n\n\nStyling skim output with gt\nIf, for some reason, you did need to include output/ visualizations about missingness in a report, I’d probably recreate visualizations or tables by class of variable, especially if you have many variables, as I do here.\nHere’s an example for numeric variables, of which there are 24 in the dataset. First, we will skim the data and then use the gt package to style the resulting dataframe as a table. I used a built-in style, but each table element can be individually customized.\n\nlanguages_numeric <- languages %>%\n  select_if(is.numeric)\n\nlang_numeric_skim <- my_skim(languages_numeric)\n\nlang_numeric_skim %>%\n  select(-skim_type)   %>% \n  gt() %>%\n  cols_label(n_missing = \"# Missing\", complete_rate = \"Completeness\", \n             numeric.mean = \"Mean\", numeric.sd = \"Standard Deviation\",\n             numeric.p0 = \"Min\", numeric.p100 = \"Max\",\n             numeric.hist = \"Histogram\") %>%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %>%\n  tab_header(title = \"Summary of Numerical Variables in Languages\") \n\n\n\n\n\n  \n    \n      Summary of Numerical Variables in Languages\n    \n    \n  \n  \n    \n      skim_variable\n      # Missing\n      Completeness\n      Mean\n      Standard Deviation\n      Min\n      Max\n      Histogram\n    \n  \n  \n    appeared\n0\n1.0000000\n1.991105e+03\n1.114434e+02\n-2000\n2023\n▁▁▁▁▇\n    domain_name_registered\n3801\n0.1166628\n2.011333e+03\n7.021132e+00\n1990\n2023\n▁▃▃▇▆\n    isbndb\n3217\n0.2523821\n7.706262e+00\n3.316421e+01\n0\n400\n▇▁▁▁▁\n    book_count\n0\n1.0000000\n2.079479e+00\n1.734465e+01\n0\n401\n▇▁▁▁▁\n    semantic_scholar\n3545\n0.1761562\n3.794195e+00\n8.316231e+00\n0\n52\n▇▁▁▁▁\n    language_rank\n0\n1.0000000\n2.151000e+03\n1.242313e+03\n0\n4302\n▇▇▇▇▇\n    github_repo_stars\n3414\n0.2066000\n2.127403e+03\n7.554016e+03\n0\n88526\n▇▁▁▁▁\n    github_repo_forks\n3417\n0.2059029\n2.612867e+02\n1.203003e+03\n0\n23732\n▇▁▁▁▁\n    github_repo_updated\n3418\n0.2056705\n2.021390e+03\n1.763285e+00\n2012\n2023\n▁▁▁▁▇\n    github_repo_subscribers\n3418\n0.2056705\n6.234237e+01\n2.008820e+02\n0\n2910\n▇▁▁▁▁\n    github_repo_created\n3425\n0.2040437\n2.015843e+03\n3.479589e+00\n2006\n2022\n▁▅▇▇▇\n    github_repo_issues\n3518\n0.1824309\n1.230344e+02\n5.462553e+02\n0\n9522\n▇▁▁▁▁\n    github_repo_first_commit\n3567\n0.1710435\n2.014739e+03\n4.985409e+00\n1987\n2022\n▁▁▁▆▇\n    github_language_repos\n3833\n0.1092261\n1.971347e+05\n1.226901e+06\n0\n16046489\n▇▁▁▁▁\n    wikipedia_daily_page_views\n2837\n0.3406925\n2.271330e+02\n7.835524e+02\n-1\n13394\n▇▁▁▁▁\n    wikipedia_backlinks_count\n2877\n0.3313967\n3.185484e+02\n1.635289e+03\n1\n34348\n▇▁▁▁▁\n    wikipedia_page_id\n2893\n0.3276784\n9.167847e+06\n1.350683e+07\n928\n63063548\n▇▁▁▁▁\n    wikipedia_appeared\n2958\n0.3125726\n1.991144e+03\n1.702650e+01\n1830\n2019\n▁▁▁▃▇\n    wikipedia_created\n3040\n0.2935162\n2.005748e+03\n3.768240e+00\n2001\n2020\n▇▇▂▁▁\n    wikipedia_revision_count\n3130\n0.2726005\n3.304314e+02\n8.132556e+02\n1\n10104\n▇▁▁▁▁\n    last_activity\n0\n1.0000000\n2.000616e+03\n8.459776e+01\n-900\n2023\n▁▁▁▁▇\n    number_of_users\n0\n1.0000000\n1.377126e+04\n2.277129e+05\n0\n7179119\n▇▁▁▁▁\n    number_of_jobs\n0\n1.0000000\n4.221838e+02\n1.257299e+04\n0\n771996\n▇▁▁▁▁\n    central_package_repository_count\n1482\n0.6555891\n0.000000e+00\n0.000000e+00\n0\n0\n▁▁▇▁▁\n  \n  \n  \n\n\n\n\n\n\nVisualizing Missingness with visdat\nThe visdat package makes ggplot- compatible missingness plots. The cluster = TRUE option groups variables that share missingness. Here we see that usually if some of the GitHub data is missing, then all of the GitHub data is missing. The percent missing is listed for each variable, and the overall missingness of the dataset is shown in the legend.\nNote vis_miss doesn’t work on very large datasets. The documentation suggests keeping the number of records below 1,000. A more extensive package for exploratory visualizations called naniar could also be used.\n\nlanguages_numeric %>%\nvis_miss(cluster = TRUE) +\nggtitle(\"Missing Data in the Languages Dataset\") +\n  #theme_classic() +\n  theme(axis.text.x = element_text(size = 8, angle = 90))"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#improving-the-most-popular-language-for-each-comment-token-figure",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#improving-the-most-popular-language-for-each-comment-token-figure",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "Improving “The Most Popular Language for Each Comment Token” Figure",
    "text": "Improving “The Most Popular Language for Each Comment Token” Figure\n\njoined <- read_csv(\"processed_lang.csv\" , show_col_types = FALSE)\n\nNow the original figure:\n\njoined %>%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\nI thought I had noted this in the previous post, but one of the tokens, ⍝ , is rendered as an empty box in the ggplot figures. I thought fixing this would be easy. First, I thought I could just pass the Unicode value for that symbol. Then, when that didn’t work, I thought I could change the font to one supporting that symbol. Supposedly, changing the font should be easy, yet after 3 hours working on it, I still had blank squares. There is a nice tutorial on changing fonts in ggplot that did not work until I found someone with the same issue. The solution is to add a line of code that doesn’t make much sense to me : windowsFonts(\"Cambria Math\" = windowsFont(\"Cambria Math\"))\nI saw a nice TidyTuesday figure on Twitter:\n\n\nMy submission for #TidyTuesday, Week 12 on programming languages. I explore jobs per users.Code: https://t.co/bV9DUHZmro pic.twitter.com/2D5YLnE5yz\n\n— Mitsuo Shiota (@mitsuoxv) March 21, 2023\n\n\nwith a caption referencing the original dataset. I’d like to add that. I generally want to increase the figure’s legibility and flip the color scale so that darker blue corresponds to more users. I also don’t think what popular means is entirely clear, so I’d like to explain more fully what I’m graphing.\n\nwindowsFonts(\"Cambria Math\" = windowsFont(\"Cambria Math\"))\njoined %>%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n    scale_colour_gradient(high = \"#08306b\", low = \"#6baed6\") + \n   labs(title = \"The Most Popular Language for Each Comment Token\",\n       subtitle = \"Based on # Users and Rank\",\n       caption = \"data from https://pldb.com/\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic(base_size = 16) +\n  theme(text = element_text( family = \"Cambria Math\")) +\n  theme(axis.text.x = element_text(face = \"bold\"))"
  },
  {
    "objectID": "posts/2023-03-25-twitter-cards/twitter-cards.html",
    "href": "posts/2023-03-25-twitter-cards/twitter-cards.html",
    "title": "Twitter Cards",
    "section": "",
    "text": "Trying to get the picture to show in a twitter card. Apparently you need to specify the image on every document, not just in the main yml doc, which is what I understood from the instructions.\nNow this works for new posts, but not old posts.\n\n\n\nCitationBibTeX citation:@online{e.sinks2023,\n  author = {Louise E. Sinks},\n  title = {Twitter {Cards}},\n  date = {2023-03-24},\n  url = {https://lsinks.github.io/posts/2023-03-25-twitter-cards/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLouise E. Sinks. 2023. “Twitter Cards.” March 24, 2023. https://lsinks.github.io/posts/2023-03-25-twitter-cards/."
  },
  {
    "objectID": "posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html",
    "href": "posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html",
    "title": "Tidy Tuesday: Daylight Savings Time",
    "section": "",
    "text": "This week’s TidyTuesday is about the timezone data from IANA timezone database.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(lubridate)\nlibrary(skimr)\nlibrary(lutz)\nlibrary(maps)\nlibrary(scales)\nlibrary(sf)\nlibrary(ggimage)\n\nThe history of this database is fascinating. It is used by many computer systems to determine the correct time based on location. To learn more, I recommend reading Daniel Rosehill’s article on the topic. For a drier history, check out the wikipedia article.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata <- tidytuesdayR::tt_load('2023-03-21')\ntuesdata <- tidytuesdayR::tt_load(2023, week = 13)\n\ntransitions <- tuesdata$transitions\ntimezones <- tuesdata$timezones\ntimezone_countries <- tuesdata$timezone_countries\ncountries <- tuesdata$countries\n\nIt is suggested that we change the begin and end variables in transitions to datetimes.\n\ntransitions <- transitions %>%\n  mutate(begin = as_datetime(begin), end = as_datetime(end))\n\nI was interested in how many countries had multiple times zones. I know the US has 4 time zones in the continental US.\n\nnum_zones <- timezone_countries %>%\n  count(country_code, sort = TRUE)\n\nnum_zones %>% \n  filter(n > 1) %>%\n  left_join(countries) %>%\n  select(place_name, n) %>%\n  filter(place_name != \"NA\") %>%\n  gt() %>%\n  cols_label(place_name = \"Country\", n = \"Number of TZs\") %>%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %>%\n  tab_header(title = \"Countries with Multiple TZs\") \n\n\n\n\n\n  \n    \n      Countries with Multiple TZs\n    \n    \n  \n  \n    \n      Country\n      Number of TZs\n    \n  \n  \n    United States\n29\n    Canada\n28\n    Russia\n27\n    Brazil\n16\n    Argentina\n12\n    Australia\n12\n    Mexico\n11\n    Kazakhstan\n7\n    Greenland\n4\n    Indonesia\n4\n    Ukraine\n4\n    Chile\n3\n    Spain\n3\n    Micronesia\n3\n    Kiribati\n3\n    Mongolia\n3\n    Malaysia\n3\n    French Polynesia\n3\n    Portugal\n3\n    US minor outlying islands\n3\n    Congo (Dem. Rep.)\n2\n    China\n2\n    Cyprus\n2\n    Germany\n2\n    Ecuador\n2\n    Marshall Islands\n2\n    New Zealand\n2\n    Papua New Guinea\n2\n    Palestine\n2\n    French Southern & Antarctic Lands\n2\n    Uzbekistan\n2\n    Vietnam\n2\n  \n  \n  \n\n\n\n\nAnd we find that the United States has 29!! time zones in the database. This was unexpected, so say the least. I thought maybe there were some times zones for territories and perhaps military bases that I did not know about. I also thought there might be some extra time zones arising from some states using daylight savings time, while others in the same area might not. I wanted to visualize where these times zones were.\n\nUS_tz <- timezone_countries %>% \n  filter(country_code == \"US\") %>%\n  left_join(timezones)\n\nJoining with `by = join_by(zone)`\n\n\nI found the lutz package created nice pictograms about when a timezone shifts from DST and back. (This package uses the same underlying database that we are using here to determine when the shifts occur.)\n\n tz_plot(US_tz$zone[21])\n\n\n\n\nI created the plots and saved them as images. I modified a function I found on stack overflow to create the file names.\n\nwd <- getwd()\nfilepath = file.path(wd)\n\n\nmake_filename = function(number){\n  # doing this, putting it all on a single line or using pipe %>%\n  # is just matter of style\n  filename = paste(\"tzplot\", number, sep=\"_\")\n  filename = paste0(filename, \".png\")\n  filename = file.path(filepath, filename)\n  \n  filename\n}\n\n#creating a variable to store the files name\nUS_tz <- US_tz %>%\n  mutate(image_name = \"tbd\")\n\nindex <- 1\nfor (index in seq(1, nrow(US_tz))) {\n  filename = make_filename(index)\n  US_tz[index , \"image_name\"] <- filename\n  # 1. Open jpeg file\n  png(filename, width = 350, height = 350, bg = \"transparent\")\n  # 2. Create the plot\n  # you need to print the plot if you call it inside a loop\n  print(tz_plot(US_tz$zone[index]))\n  # 3. Close the file\n  dev.off()\n  index = index + 1\n}\n\nNext I created a world map, inspired by the one from\n\n\nMy submission for #TidyTuesday, Week 13 on time zones. I plot time zones in the world map.Code: https://t.co/y5Cm4tuaVk pic.twitter.com/BZC3anC5Oa\n\n— Mitsuo Shiota (@mitsuoxv) March 28, 2023\n\n\nI hadn’t previously used the maps package, so I appreciate being introduced to it. The maps package only has a mainland US map, so I used the world map. (Plus, as I mentioned, I thought some of these time zones would be in other parts of the world.) I followed a tutorial on Plotting Points as Images in ggplot and used the hints about aspect ratio to make my tz_plot circles remain circular. However, that did stretch the world a bit.\n\naspect_ratio <- 1.618  \n\nus_tz_map <- map_data(\"world\") %>% \n  ggplot(aes(long, lat)) +\n  geom_polygon(aes(group = group), fill = \"white\", \n               color = \"gray30\", alpha = 0.9) +\n  geom_image(aes(x = longitude, latitude, image = image_name), \n             data = US_tz, size = 0.025, by = \"width\",\n             asp = aspect_ratio) +\n  coord_sf() +\n  labs(title = \"The United States has 29 Timezone- Mostly Redunant\",\n       caption = \"Data from: https://data.iana.org/time-zones/tz-link.html\") +\n  theme_void() +\n  theme(aspect.ratio = 1/aspect_ratio,\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = \"white\")\n    )\n\nggsave(\"thumbnail.png\", us_tz_map, width = 5 * aspect_ratio, height = 5)\nus_tz_map\n\n\n\n\nAnd what we see is there are a bunch of redundant times zone specification, especially in the Midwest.\n\nUS_tz %>%\n  select(zone, latitude, longitude) %>%\n  arrange(longitude) %>%\n  gt() %>%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %>%\n  tab_header(title = \"Countries with Multiple TZs\") \n\n\n\n\n\n  \n    \n      Countries with Multiple TZs\n    \n    \n  \n  \n    \n      zone\n      latitude\n      longitude\n    \n  \n  \n    America/Adak\n52.66667\n-177.13333\n    America/Nome\n64.56667\n-165.78333\n    Pacific/Honolulu\n21.71667\n-158.35000\n    America/Anchorage\n61.30000\n-149.91667\n    America/Yakutat\n60.35000\n-140.35000\n    America/Sitka\n57.75000\n-135.41667\n    America/Juneau\n58.41667\n-134.60000\n    America/Metlakatla\n55.73333\n-132.15000\n    America/Los_Angeles\n34.18333\n-118.80000\n    America/Boise\n44.41667\n-116.35000\n    America/Phoenix\n34.33333\n-112.46667\n    America/Denver\n40.08333\n-105.03333\n    America/North_Dakota/Beulah\n48.10000\n-102.43333\n    America/North_Dakota/Center\n48.08333\n-102.23333\n    America/North_Dakota/New_Salem\n47.53333\n-102.05000\n    America/Menominee\n45.56667\n-88.45000\n    America/Indiana/Vincennes\n39.30000\n-88.23333\n    America/Indiana/Petersburg\n39.00000\n-87.98333\n    America/Chicago\n41.85000\n-87.65000\n    America/Indiana/Tell_City\n38.13333\n-87.43333\n    America/Indiana/Knox\n42.03333\n-87.11667\n    America/Indiana/Marengo\n38.90000\n-87.01667\n    America/Indiana/Winamac\n41.13333\n-86.78333\n    America/Indiana/Indianapolis\n39.86667\n-86.63333\n    America/Kentucky/Louisville\n38.50000\n-86.31667\n    America/Kentucky/Monticello\n37.60000\n-85.78333\n    America/Indiana/Vevay\n39.60000\n-85.10000\n    America/Detroit\n43.20000\n-83.78333\n    America/New_York\n41.55000\n-74.38333\n  \n  \n  \n\n\n\n\n\n\n\nCitationBibTeX citation:@online{e.sinks2023,\n  author = {Louise E. Sinks},\n  title = {Tidy {Tuesday:} {Daylight} {Savings} {Time}},\n  date = {2023-03-28},\n  url = {https://lsinks.github.io/posts/2023-03-28-tidytuesday-timezones/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLouise E. Sinks. 2023. “Tidy Tuesday: Daylight Savings\nTime.” March 28, 2023. https://lsinks.github.io/posts/2023-03-28-tidytuesday-timezones/."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Entries",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Daylight Savings Time\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\nData-Viz\n\n\nggimage\n\n\nmaps\n\n\n\n\nTidyTuesday: Exploring Time Zones and Daylight Savings Time\n\n\n\n\n\n\nMar 28, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 12: Programming Languages Revisited\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\nData-Viz\n\n\nskimr\n\n\n\n\nTidyTuesday: Polishing\n\n\n\n\n\n\nMar 24, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nTwitter Cards\n\n\n\n\n\n\n\nQuarto\n\n\n\n\nMaking Twitter Cards\n\n\n\n\n\n\nMar 24, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 12: Programming Languages\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\n\n\nTidyTuesday: How to comment in Various Programming Languages\n\n\n\n\n\n\nMar 21, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Blog\n\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nturtle\n\n\n\n\nTrying to create a Quarto Blog\n\n\n\n\n\n\nMar 14, 2023\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  }
]