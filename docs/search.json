[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Entries",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nThoughts on Building a Quarto Website\n\n\n\n\n\n\n\nR\n\n\nquarto\n\n\nturtle\n\n\n\n\nSome tips and tricks for using Quarto and RStudio to create a portfolio website and blog.\n\n\n\n\n\n\nNov 1, 2023\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to Web Scraping Using rvest\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ndata visualization\n\n\ntombstone\n\n\nregex\n\n\nweb scraping\n\n\nrvest\n\n\ndata cleaning\n\n\ntidyTuesday\n\n\n\n\nWeb scraping data with rvest to enhance the information in the Tombstone Project.\n\n\n\n\n\n\nSep 8, 2023\n\n\n30 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday 36: Visualizing Worker Demographic Information with Treemaps\n\n\n\n\n\n\n\ndata visualization\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\ntreemap\n\n\ntidyTuesday\n\n\nd3treeR\n\n\ninteractive\n\n\n\n\nUsing treemap and d3treeR to create static and dynamic treemaps\n\n\n\n\n\n\nSep 5, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday 35: Exploring Fair Use Cases\n\n\n\n\n\n\n\ndata visualization\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\ntidyTuesday\n\n\nstringr\n\n\ndata cleaning\n\n\ndual axis plot\n\n\nggplot\n\n\ncowplot\n\n\n\n\nUsing stringr::str_detect() functions to explore why variables from different datasets don’t exactly match.\n\n\n\n\n\n\nAug 29, 2023\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday: Exploring Refugee Flow with A Sankey Diagram\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\nsankey\n\n\nnetworkD3\n\n\nggsankey\n\n\nhtmlwidgets\n\n\n\n\nTidyTuesday 34: Looking at the United Nations High Commissioner for Refugees data with a Sankey Diagram. Diagram created with networkD3 in R.\n\n\n\n\n\n\nAug 28, 2023\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nMapping for the Tombstone Project\n\n\n\n\n\n\n\ndata visualization\n\n\nR\n\n\nR-code\n\n\nleaflet\n\n\nquarto\n\n\nproblem solving\n\n\nmapping\n\n\ncode-along\n\n\nspatial join\n\n\nwordPress\n\n\nleafpop\n\n\nst_jitter\n\n\n\n\nMethods for dealing with overlapping points and then styling a leaflet map with various labels and pop-ups.\n\n\n\n\n\n\nAug 15, 2023\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday Twofer (32 and 33)\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nTidyTuesday\n\n\ncorrplot\n\n\n\n\nLooking at how heat levels increase on the show The Hot Ones. Then doing some EDA on a data set on spam emails.\n\n\n\n\n\n\nAug 15, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nData Cleaning for the Tombstone Project\n\n\n\n\n\n\n\ndata visualization\n\n\nR\n\n\nR-code\n\n\nfuzzyjoin\n\n\nquarto\n\n\nleaflet\n\n\nregex\n\n\nstringr\n\n\ndata cleaning\n\n\nproblem solving\n\n\nmapping\n\n\nsf\n\n\ncode-along\n\n\n\n\nUsing StringR to clean a human created excel sheet full of typos and formatting inconsistencies. Then matching excel data to photo names.\n\n\n\n\n\n\nAug 4, 2023\n\n\n51 min\n\n\n\n\n\n\n  \n\n\n\n\nStates\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\ntidyTuesday\n\n\ntidy\n\n\ngwalkr\n\n\ntableau Alternative\n\n\nexploratory data analysis\n\n\ninteractive\n\n\n\n\nUsing GWalkR to create an interactive exploratory data analysis window similar to Tableau.\n\n\n\n\n\n\nAug 1, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nA Heatmap of James Lind’s Scurvy Study\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\ntidyTuesday\n\n\ntidy\n\n\nheatmap\n\n\nggplot\n\n\ntidyverse\n\n\nggthemes\n\n\n\n\nMaking a Heatmap of James Lind’s Scurvy Study using ggplot. Data cleaning and reformating handled through tidyverse packages.\n\n\n\n\n\n\nJul 25, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 28: Global Surface Temperature\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nTidyTuesday\n\n\ntidy\n\n\ngganimate\n\n\n\n\nMaking an animated graph of global temperature change over time with gganimate.\n\n\n\n\n\n\nJul 11, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 27: Historical Markers\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ndata visualization\n\n\nsf\n\n\nleaflet\n\n\n\n\nTidyTuesday: Interactive Map of Arlington Historic Markers\n\n\n\n\n\n\nJul 4, 2023\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday Revisited: Interactive Map of Arlington Historic Neighborhoods\n\n\n\n\n\n\n\nR\n\n\ntidyTuesday\n\n\nR-code\n\n\ndata visualization\n\n\nsf\n\n\nleaflet\n\n\n\n\nTidyTuesday: Interactive Map of Historic Neighborhoods of Arlington Virginia\n\n\n\n\n\n\nJun 29, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: US Populated Places\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ndata visualization\n\n\nopenxlsx\n\n\nstringr\n\n\nfuzzyjoin\n\n\nmapview\n\n\nsf\n\n\n\n\nTidyTuesday: Historic Neighborhoods of Arlington Virginia\n\n\n\n\n\n\nJun 27, 2023\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 25: UFO Sightings Redux\n\n\n\n\n\n\n\nR\n\n\ntidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\n\n\nTidyTuesday: UFO Sightings\n\n\n\n\n\n\nJun 20, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 18: Portal Project\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\ndata validation\n\n\nexploratory data analysis\n\n\n\n\nTidyTuesday: Rodents of Portal Arizona\n\n\n\n\n\n\nMay 2, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 17: London Marathon\n\n\n\n\n\n\n\nR\n\n\ntidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\n\n\nTidyTuesday: Exploring the London Marathon\n\n\n\n\n\n\nApr 25, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 16: Neolithic Founder Crops\n\n\n\n\n\n\n\nR\n\n\ntidyTuesday\n\n\nR-code\n\n\ndata visualization\n\n\n\n\nTidyTuesday: Exploring early agriculture in SW Asia\n\n\n\n\n\n\nApr 18, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCredit Card Fraud: A Tidymodels Tutorial\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ntidymodels\n\n\nmachine learning\n\n\nclassifiers\n\n\n\n\nAn Imbalanced Class Problem\n\n\n\n\n\n\nApr 11, 2023\n\n\n39 min\n\n\n\n\n\n\n  \n\n\n\n\nA Tidymodels Tutorial: A Structural Approach\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ntidymodels\n\n\nmachine learning\n\n\nhyperparameters\n\n\nworkflows\n\n\n\n\nExploring the different steps for modeling\n\n\n\n\n\n\nApr 10, 2023\n\n\n22 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge- Endangered Species\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\n30DayChartChallenge\n\n\nwaffle\n\n\n\n\nHow many species have been delisted?\n\n\n\n\n\n\nApr 4, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge- Flora and Fauna\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nturtle\n\n\n30DayChartChallenge\n\n\n\n\nHow Large are Different Types of Turtles?\n\n\n\n\n\n\nApr 3, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge -Arlington Parks\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nwaffle\n\n\n30DayChartChallenge\n\n\n\n\nWho Owns the Parks in Arlington Virgina?\n\n\n\n\n\n\nApr 2, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nSelf-Guided Learning through a Wordle Guess Generator: Part 1\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\n\n\nSkills improvement with a Wordle Guess Generator\n\n\n\n\n\n\nApr 1, 2023\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nSelf-Guided Learning through a Wordle Guess Generator: Part 2\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\n\n\nCurrent version of a Wordle Guess Generator\n\n\n\n\n\n\nApr 1, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nOne Class SVM\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nmachine learning\n\n\ncaret\n\n\nsvm\n\n\nclassifiers\n\n\nsupport vector machines\n\n\n\n\nOne Class SVM for Imbalanced Classes\n\n\n\n\n\n\nMar 30, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Daylight Savings Time\n\n\n\n\n\n\n\nR\n\n\ntidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\nggimage\n\n\nmaps\n\n\n\n\nTidyTuesday: Exploring Time Zones and Daylight Savings Time\n\n\n\n\n\n\nMar 28, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 12: Programming Languages Revisited\n\n\n\n\n\n\n\nR\n\n\ntidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\nskimr\n\n\n\n\nTidyTuesday: Polishing\n\n\n\n\n\n\nMar 24, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTwitter Cards\n\n\n\n\n\n\n\nquarto\n\n\nturtle\n\n\n\n\nMaking Twitter Cards\n\n\n\n\n\n\nMar 24, 2023\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 12: Programming Languages\n\n\n\n\n\n\n\nR\n\n\ntidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\n\n\nTidyTuesday: How to comment in Various Programming Languages\n\n\n\n\n\n\nMar 21, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Blog\n\n\n\n\n\n\n\nquarto\n\n\nR\n\n\nturtle\n\n\n\n\nTrying to create a Quarto Blog\n\n\n\n\n\n\nMar 14, 2023\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "",
    "text": "This week, I participated in my first #TidyTuesday challenge. My goal was to get something out on the day of the challenge rather than perfection. I did notice that the skimr output wasn’t formatted nicely on the webpage. Today, I’m going to delve into the skimr and Quarto documentation and make a nicer version of the output. Secondly, I’m going to fix up my final figure, which is the one I shared on social media:"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#skimr-to-understand-your-data",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#skimr-to-understand-your-data",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "Skimr to understand your data",
    "text": "Skimr to understand your data\nSkimr is a package that provides statistical summaries of the variables in your dataframe. It also provides information about the missingness of each variable.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(ggrepel)\nlibrary(visdat) # visualizing missing data in dataframe\n\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 12)\n\nlanguages &lt;- tuesdata$languages\n\n\nCustomizing the skim Output\nMy main objection is that the numerical summary is too wide and has a scroll bar. I especially want the histogram to be viewable on the first screen. I also don’t particularly care about all the quartile information; min and max are enough. If I want to delve more into the stats of a variable, I will do it another way, not with skimr.\nFirst, quarto lets you expand the output of the code chunk to fill the page via the option “#| column: page”, so I’ll do that. Next, I’ll create a custom skim function that drops the p25, p50, and p75 output from the summary of the numerical variables.\n\nmy_skim &lt;- skim_with(numeric = sfl(p25 = NULL, p50 = NULL, p75 = NULL)) \n\nmy_skim(languages)\n\n\nData summary\n\n\nName\nlanguages\n\n\nNumber of rows\n4303\n\n\nNumber of columns\n49\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n21\n\n\nlogical\n4\n\n\nnumeric\n24\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n4303\n0\n\n\ntitle\n0\n1.00\n1\n56\n0\n4267\n0\n\n\ndescription\n3480\n0.19\n4\n2273\n0\n811\n0\n\n\ntype\n0\n1.00\n2\n27\n0\n40\n0\n\n\ncreators\n3203\n0.26\n2\n253\n0\n985\n0\n\n\nwebsite\n2928\n0.32\n13\n131\n0\n1368\n0\n\n\ndomain_name\n3588\n0.17\n6\n32\n0\n700\n0\n\n\nreference\n2314\n0.46\n15\n251\n0\n1955\n0\n\n\ngithub_repo\n3402\n0.21\n25\n73\n0\n897\n0\n\n\ngithub_repo_description\n3438\n0.20\n4\n419\n0\n853\n0\n\n\ngithub_language\n3829\n0.11\n1\n30\n0\n474\n0\n\n\ngithub_language_tm_scope\n3837\n0.11\n4\n34\n0\n361\n0\n\n\ngithub_language_type\n3837\n0.11\n4\n11\n0\n4\n0\n\n\ngithub_language_ace_mode\n3838\n0.11\n1\n16\n0\n96\n0\n\n\ngithub_language_file_extensions\n3833\n0.11\n1\n606\n0\n466\n0\n\n\nwikipedia\n2731\n0.37\n32\n104\n0\n1566\n0\n\n\nwikipedia_summary\n2884\n0.33\n17\n6741\n0\n1407\n0\n\n\nwikipedia_related\n3145\n0.27\n1\n1761\n0\n1059\n0\n\n\nline_comment_token\n3831\n0.11\n1\n7\n0\n23\n0\n\n\norigin_community\n1190\n0.72\n3\n305\n0\n2232\n0\n\n\nfile_type\n3213\n0.25\n2\n6\n0\n4\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n3683\n0.14\n1.00\nTRU: 617, FAL: 3\n\n\nfeatures_has_semantic_indentation\n3722\n0.14\n0.11\nFAL: 516, TRU: 65\n\n\nfeatures_has_line_comments\n3765\n0.13\n0.96\nTRU: 517, FAL: 21\n\n\nis_open_source\n3792\n0.12\n0.89\nTRU: 453, FAL: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1991.11\n111.44\n-2000\n2023\n▁▁▁▁▇\n\n\ndomain_name_registered\n3801\n0.12\n2011.33\n7.02\n1990\n2023\n▁▃▃▇▆\n\n\nisbndb\n3217\n0.25\n7.71\n33.16\n0\n400\n▇▁▁▁▁\n\n\nbook_count\n0\n1.00\n2.08\n17.34\n0\n401\n▇▁▁▁▁\n\n\nsemantic_scholar\n3545\n0.18\n3.79\n8.32\n0\n52\n▇▁▁▁▁\n\n\nlanguage_rank\n0\n1.00\n2151.00\n1242.31\n0\n4302\n▇▇▇▇▇\n\n\ngithub_repo_stars\n3414\n0.21\n2127.40\n7554.02\n0\n88526\n▇▁▁▁▁\n\n\ngithub_repo_forks\n3417\n0.21\n261.29\n1203.00\n0\n23732\n▇▁▁▁▁\n\n\ngithub_repo_updated\n3418\n0.21\n2021.39\n1.76\n2012\n2023\n▁▁▁▁▇\n\n\ngithub_repo_subscribers\n3418\n0.21\n62.34\n200.88\n0\n2910\n▇▁▁▁▁\n\n\ngithub_repo_created\n3425\n0.20\n2015.84\n3.48\n2006\n2022\n▁▅▇▇▇\n\n\ngithub_repo_issues\n3518\n0.18\n123.03\n546.26\n0\n9522\n▇▁▁▁▁\n\n\ngithub_repo_first_commit\n3567\n0.17\n2014.74\n4.99\n1987\n2022\n▁▁▁▆▇\n\n\ngithub_language_repos\n3833\n0.11\n197134.67\n1226900.57\n0\n16046489\n▇▁▁▁▁\n\n\nwikipedia_daily_page_views\n2837\n0.34\n227.13\n783.55\n-1\n13394\n▇▁▁▁▁\n\n\nwikipedia_backlinks_count\n2877\n0.33\n318.55\n1635.29\n1\n34348\n▇▁▁▁▁\n\n\nwikipedia_page_id\n2893\n0.33\n9167847.21\n13506832.90\n928\n63063548\n▇▁▁▁▁\n\n\nwikipedia_appeared\n2958\n0.31\n1991.14\n17.03\n1830\n2019\n▁▁▁▃▇\n\n\nwikipedia_created\n3040\n0.29\n2005.75\n3.77\n2001\n2020\n▇▇▂▁▁\n\n\nwikipedia_revision_count\n3130\n0.27\n330.43\n813.26\n1\n10104\n▇▁▁▁▁\n\n\nlast_activity\n0\n1.00\n2000.62\n84.60\n-900\n2023\n▁▁▁▁▇\n\n\nnumber_of_users\n0\n1.00\n13771.26\n227712.95\n0\n7179119\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n422.18\n12572.99\n0\n771996\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n1482\n0.66\n0.00\n0.00\n0\n0\n▁▁▇▁▁\n\n\n\n\n\nThis output is much nicer. It is a bit wall of text though. I wouldn’t recommend using this in reports, but it is a useful tool when doing your initial dataset analysis. (As a side note, I have noticed skimr doesn’t work well on Kaggle. It performs as expected if you are in interactive mode, but it fails when you try to save the notebook or run non-interactively.)\n\n\nStyling skim output with gt\nIf, for some reason, you did need to include output/ visualizations about missingness in a report, I’d probably recreate visualizations or tables by class of variable, especially if you have many variables, as I do here.\nHere’s an example for numeric variables, of which there are 24 in the dataset. First, we will skim the data and then use the gt package to style the resulting dataframe as a table. I used a built-in style, but each table element can be individually customized.\n\nlanguages_numeric &lt;- languages %&gt;%\n  select_if(is.numeric)\n\nlang_numeric_skim &lt;- my_skim(languages_numeric)\n\nlang_numeric_skim %&gt;%\n  select(-skim_type)   %&gt;% \n  gt() %&gt;%\n  cols_label(n_missing = \"# Missing\", complete_rate = \"Completeness\", \n             numeric.mean = \"Mean\", numeric.sd = \"Standard Deviation\",\n             numeric.p0 = \"Min\", numeric.p100 = \"Max\",\n             numeric.hist = \"Histogram\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Summary of Numerical Variables in Languages\") \n\n\n\n\n\n  \n    \n      Summary of Numerical Variables in Languages\n    \n    \n    \n      skim_variable\n      # Missing\n      Completeness\n      Mean\n      Standard Deviation\n      Min\n      Max\n      Histogram\n    \n  \n  \n    appeared\n0\n1.0000000\n1.991105e+03\n1.114434e+02\n-2000\n2023\n▁▁▁▁▇\n    domain_name_registered\n3801\n0.1166628\n2.011333e+03\n7.021132e+00\n1990\n2023\n▁▃▃▇▆\n    isbndb\n3217\n0.2523821\n7.706262e+00\n3.316421e+01\n0\n400\n▇▁▁▁▁\n    book_count\n0\n1.0000000\n2.079479e+00\n1.734465e+01\n0\n401\n▇▁▁▁▁\n    semantic_scholar\n3545\n0.1761562\n3.794195e+00\n8.316231e+00\n0\n52\n▇▁▁▁▁\n    language_rank\n0\n1.0000000\n2.151000e+03\n1.242313e+03\n0\n4302\n▇▇▇▇▇\n    github_repo_stars\n3414\n0.2066000\n2.127403e+03\n7.554016e+03\n0\n88526\n▇▁▁▁▁\n    github_repo_forks\n3417\n0.2059029\n2.612867e+02\n1.203003e+03\n0\n23732\n▇▁▁▁▁\n    github_repo_updated\n3418\n0.2056705\n2.021390e+03\n1.763285e+00\n2012\n2023\n▁▁▁▁▇\n    github_repo_subscribers\n3418\n0.2056705\n6.234237e+01\n2.008820e+02\n0\n2910\n▇▁▁▁▁\n    github_repo_created\n3425\n0.2040437\n2.015843e+03\n3.479589e+00\n2006\n2022\n▁▅▇▇▇\n    github_repo_issues\n3518\n0.1824309\n1.230344e+02\n5.462553e+02\n0\n9522\n▇▁▁▁▁\n    github_repo_first_commit\n3567\n0.1710435\n2.014739e+03\n4.985409e+00\n1987\n2022\n▁▁▁▆▇\n    github_language_repos\n3833\n0.1092261\n1.971347e+05\n1.226901e+06\n0\n16046489\n▇▁▁▁▁\n    wikipedia_daily_page_views\n2837\n0.3406925\n2.271330e+02\n7.835524e+02\n-1\n13394\n▇▁▁▁▁\n    wikipedia_backlinks_count\n2877\n0.3313967\n3.185484e+02\n1.635289e+03\n1\n34348\n▇▁▁▁▁\n    wikipedia_page_id\n2893\n0.3276784\n9.167847e+06\n1.350683e+07\n928\n63063548\n▇▁▁▁▁\n    wikipedia_appeared\n2958\n0.3125726\n1.991144e+03\n1.702650e+01\n1830\n2019\n▁▁▁▃▇\n    wikipedia_created\n3040\n0.2935162\n2.005748e+03\n3.768240e+00\n2001\n2020\n▇▇▂▁▁\n    wikipedia_revision_count\n3130\n0.2726005\n3.304314e+02\n8.132556e+02\n1\n10104\n▇▁▁▁▁\n    last_activity\n0\n1.0000000\n2.000616e+03\n8.459776e+01\n-900\n2023\n▁▁▁▁▇\n    number_of_users\n0\n1.0000000\n1.377126e+04\n2.277129e+05\n0\n7179119\n▇▁▁▁▁\n    number_of_jobs\n0\n1.0000000\n4.221838e+02\n1.257299e+04\n0\n771996\n▇▁▁▁▁\n    central_package_repository_count\n1482\n0.6555891\n0.000000e+00\n0.000000e+00\n0\n0\n▁▁▇▁▁\n  \n  \n  \n\n\n\n\n\n\nVisualizing Missingness with visdat\nThe visdat package makes ggplot- compatible missingness plots. The cluster = TRUE option groups variables that share missingness. Here we see that usually if some of the GitHub data is missing, then all of the GitHub data is missing. The percent missing is listed for each variable, and the overall missingness of the dataset is shown in the legend.\nNote vis_miss doesn’t work on very large datasets. The documentation suggests keeping the number of records below 1,000. A more extensive package for exploratory visualizations called naniar could also be used.\n\nlanguages_numeric %&gt;%\nvis_miss(cluster = TRUE) +\nggtitle(\"Missing Data in the Languages Dataset\") +\n  #theme_classic() +\n  theme(axis.text.x = element_text(size = 8, angle = 90))"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#improving-the-most-popular-language-for-each-comment-token-figure",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#improving-the-most-popular-language-for-each-comment-token-figure",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "Improving “The Most Popular Language for Each Comment Token” Figure",
    "text": "Improving “The Most Popular Language for Each Comment Token” Figure\n\njoined &lt;- read_csv(\"processed_lang.csv\" , show_col_types = FALSE)\n\nNow the original figure:\n\njoined %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\nI thought I had noted this in the previous post, but one of the tokens, ⍝ , is rendered as an empty box in the ggplot figures. I thought fixing this would be easy. First, I thought I could just pass the Unicode value for that symbol. Then, when that didn’t work, I thought I could change the font to one supporting that symbol. Supposedly, changing the font should be easy, yet after 3 hours working on it, I still had blank squares. There is a nice tutorial on changing fonts in ggplot that did not work until I found someone with the same issue. The solution is to add a line of code that doesn’t make much sense to me : windowsFonts(\"Cambria Math\" = windowsFont(\"Cambria Math\"))\nI saw a nice TidyTuesday figure on Twitter:\n\n\nMy submission for #TidyTuesday, Week 12 on programming languages. I explore jobs per users.Code: https://t.co/bV9DUHZmro pic.twitter.com/2D5YLnE5yz\n\n— Mitsuo Shiota (@mitsuoxv) March 21, 2023\n\n\nwith a caption referencing the original dataset. I’d like to add that. I generally want to increase the figure’s legibility and flip the color scale so that darker blue corresponds to more users. I also don’t think what popular means is entirely clear, so I’d like to explain more fully what I’m graphing.\n\nwindowsFonts(\"Cambria Math\" = windowsFont(\"Cambria Math\"))\njoined %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n    scale_colour_gradient(high = \"#08306b\", low = \"#6baed6\") + \n   labs(title = \"The Most Popular Language for Each Comment Token\",\n       subtitle = \"Based on # Users and Rank\",\n       caption = \"data from https://pldb.com/\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic(base_size = 16) +\n  theme(text = element_text( family = \"Cambria Math\")) +\n  theme(axis.text.x = element_text(face = \"bold\"))"
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "",
    "text": "I’m still behind on TidyTuesday, so here is TidyTuesday #34, which deals with refugee data. This data is collected by the United Nations High Commissioner for Refugees and the R package is updated twice a year."
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#loading-data-and-libraries",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#loading-data-and-libraries",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "Loading Data and Libraries",
    "text": "Loading Data and Libraries\nLibraries.\n\nlibrary(tidyverse) # who doesn't want to be tidy\nlibrary(networkD3) # for Sankey plots\nlibrary(ggsankey) # another sankey package \nlibrary(htmlwidgets) # html widgets helps to handle the networkD3 objects\nlibrary(htmltools) # for formatting html code\n\nLoading data.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 34)\npopulation &lt;- tuesdata$population"
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#exploratory-data-analysis",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#exploratory-data-analysis",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThis dataset tracks the number of refugees by year. It tracks how many people request entry at each country and breaks this information down by country of origin.\n\nhead(population)\n\n# A tibble: 6 × 16\n   year coo_name    coo   coo_iso coa_name coa   coa_iso refugees asylum_seekers\n  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n1  2010 Afghanistan AFG   AFG     Afghani… AFG   AFG            0              0\n2  2010 Iran (Isla… IRN   IRN     Afghani… AFG   AFG           30             21\n3  2010 Iraq        IRQ   IRQ     Afghani… AFG   AFG            6              0\n4  2010 Pakistan    PAK   PAK     Afghani… AFG   AFG         6398              9\n5  2010 Egypt       ARE   EGY     Albania  ALB   ALB            5              0\n6  2010 China       CHI   CHN     Albania  ALB   ALB            6              0\n# ℹ 7 more variables: returned_refugees &lt;dbl&gt;, idps &lt;dbl&gt;, returned_idps &lt;dbl&gt;,\n#   stateless &lt;dbl&gt;, ooc &lt;dbl&gt;, oip &lt;dbl&gt;, hst &lt;dbl&gt;\n\n\nWhat years have the most data?\n\npopulation %&gt;%\n  group_by(year) %&gt;%\n  count() %&gt;%\n  ggplot(aes(year, n)) +\n  geom_col()\n\n\n\n\nI expected to see spikes corresponding to specific events, but instead, it looks like the data is gradually increasing. This upward trend might be due to better record-keeping/ data collection over the years.\nI’m going to look at last year’s data.\n\npop_2022 &lt;- population %&gt;%\n  filter(year == 2022)"
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#a-sankey-diagram-to-explore-flow",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#a-sankey-diagram-to-explore-flow",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "A Sankey Diagram to Explore Flow",
    "text": "A Sankey Diagram to Explore Flow\nNow, I will look at where most refugees come from.\nThere is a technical and legal difference between refugees and asylum seekers, but I will sum them up. I also will be using refugees and asylum seekers interchangeably here.\n\npop_2022 &lt;- pop_2022 %&gt;%\n  mutate(total = refugees + asylum_seekers) %&gt;%\n  select(coo_name, coa_name, total)\n\nI’ve been interested in learning how to make Sankey diagrams in R. A Sankey diagram shows the flow between nodes; there is a great gallery of examples here. This tidytuesday data does lend itself to a Sankey diagram. There are two nodes- country of orgin and country of arrival and the flow is the number of refugees between each pair.\nI do want to highlight that the UN Refugee Agency has a separate dataset that tracks flow. The dataset we are working with is the net number of refugees. As the UNHCR webpage says:\n\nOften the net increase or decrease in stock figures between years has been used in lieu of accurate flow figures. But these may underrepresent the true magnitude of population movements if, for example, refugee arrivals and departures balance each other out.\n\nStill, for TidyTuesday, the net population is fine.\nI’m going to pull out the top 3 sources of refugees. I’m grouping by the country of origin (coo_name) and then summing the refugees going to all countries. I then take the top 3 using dpylr’s slice_max(). Note that this function replaces top_n().\n\ntop_3_source &lt;- pop_2022 %&gt;%\n  group_by(coo_name) %&gt;%\n  summarize(num_by_coo = sum(total)) %&gt;%\n  slice_max(order_by = num_by_coo, n = 3)\n\ntop_3_source_names &lt;- top_3_source$coo_name\n\ntop_3_source_names\n\n[1] \"Syrian Arab Rep.\" \"Afghanistan\"      \"Ukraine\"         \n\n\nNow, I’m going to do a related analysis and see where the top destinations are.\n\ntop_3_dest &lt;- pop_2022 %&gt;%\n  group_by(coa_name) %&gt;%\n  summarize(num_by_coa = sum(total)) %&gt;% slice_max(order_by = num_by_coa, n = 3)\n\ntop_3_dest_names &lt;- (top_3_dest$coa_name)\ntop_3_dest_names\n\n[1] \"Türkiye\"                \"Iran (Islamic Rep. of)\" \"Germany\"               \n\n\nNow I’m making a category “other” for both source and destination countries that aren’t in the top 3. I use the forcats function fct_other().\nFirst country of arrival:\n\npop_2022$coa_name = factor(pop_2022$coa_name)\n\npop_2022$coa_name &lt;- pop_2022$coa_name %&gt;%\n  fct_other(keep = top_3_dest_names, other_level = \"other\")\n\nThen, country of origin:\n\npop_2022$coo_name = factor(pop_2022$coo_name)\n\npop_2022$coo_name &lt;- pop_2022$coo_name %&gt;%\n  fct_other(keep = top_3_source_names, other_level = \"other\")\n\nFor now, I will remove the “other” category of country of origin.\n\npop_2022_no_other &lt;- pop_2022 %&gt;%\n  filter((coo_name != \"other\"))\n\nThere are 3 popular ways to make Sankey diagrams in R: ggsankey, networkD3, and plotly. There is a nice tutorial on these methods here.\nI’m going to focus on ggsankey and networkD3.\n\nggsankey method for Sankey Plots in R\nThis package is combined with ggplot to make nice static Sankey plots.\nThe package documentation does a great job explaining the parts of the Sankey chart. It does not do a great job of explaining what format the data should be in. I spent most of my time last Tuesday trying to figure this out.\nThe package includes a function make_long(), which is used to prepare the data for plotting. It labels the columns appropriately and creates the needed null/ NA entries. These entries are used to signify the end of the flow.\nThe UN refugee data is already long, so I spent a fair bit of time trying to figure out how to either label it myself or make it wide so I could use make_long().\nIt turns out that ggsankey can only be used on disaggregated data and not aggregated data such as I have here. I never found that explicitly stated in the documentation (it could be I missed that), but I only discovered it as I was trying to wrangle my data into the correct format and comparing it to the mtcars data in the example.\nWhat do I mean by aggregated vs. disaggregated?\nLet’s look at the mtcars dataframe:\n\nmtcars2 &lt;- mtcars %&gt;%\n  slice_sample(n = 10) %&gt;%\n  select(cyl, vs)\n\nmtcars2\n\n                 cyl vs\nPontiac Firebird   8  0\nFerrari Dino       6  0\nPorsche 914-2      4  0\nMerc 450SLC        8  0\nMerc 280           6  1\nMerc 230           4  1\nDatsun 710         4  1\nCamaro Z28         8  0\nMazda RX4          6  0\nMerc 450SE         8  0\n\n\nThis is disaggregated. It reports the properties of each car. The equivalent in our dataset would be if the database listed every refugee by name and specified where they were from and where they were going.\nWhat the population dataset has instead is the aggregated data. This is the mtcars equivalent:\n\nmtcars2 %&gt;% group_by(cyl, vs) %&gt;% count()\n\n# A tibble: 5 × 3\n# Groups:   cyl, vs [5]\n    cyl    vs     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     4     0     1\n2     4     1     2\n3     6     0     2\n4     6     1     1\n5     8     0     4\n\n\nSo we know we have two cars with cyl = 4 and vs = 1, but we don’t know that they are specifically the Merc 240D and the Fiat 128.\nIf all you have is the aggregated data, you can’t disaggregate it. The names of the cars are not contained in that dataframe. In this case, if you absolutely had to make a Sankey diagram using ggsankey, you could write some code to make a dummy ID to represent the missing dimension. That is, you could split the entry cyl = 4 and vs = 1 into\ncyl = 4 and vs = 1 and id = 1\ncyl = 4 and vs = 1 and id = 2\nAnd so on for each group.\nThen you’d have to dpylr::pivot_wider() and then ggsankey::make_longer(). I wouldn’t recommend this, but it is useful sometimes to think of how you’d approach a problem, even if there are easier solutions. [This :: formatting forces the function to load from the specific library on the left side of the double colon. This is useful if you have conflicting function names in different packages; this notation assure the proper one is called. It is also useful to clarify where specific functions are coming from, even if there would be no conflict.]\n\n\nnetworkD3 method for Sankey Plots in R\nI love the R Graph Gallery when I’m looking for inspiration for TidyTuesday. R Graph Gallery recommends the networkD3 package for Sankey plots, but it is an htmlwidget. I’ve been doing a lot of interactive stuff recently, and I originally wanted to get back to some good old ggplot. But the networkD3 package does create lovely Sankey plots.\nThe networkD3 method seems slightly confusing because you need to transform and label the data yourself. Like ggsankey, you need information about the nodes and how they are connected with each other. One other note- this is based on a JavaScript package (with 0 indexing). So, a few “-1” to convert the index from R (which starts at 1).\nThere is a nice post on Sankey diagrams in networkD3 on the Data to Viz blog. The code is hidden, so you need to toggle it on with the code button if you want to see it.\nThe function call is (from the manual found at CRAN):\nsankeyNetwork(Links, Nodes, Source, Target, Value, NodeID, NodeGroup = NodeID, LinkGroup = NULL, units = \"\", colourScale = JS(\"d3.scaleOrdinal(d3.schemeCategory20);\"), fontSize = 7, fontFamily = NULL, nodeWidth = 15, nodePadding = 10, margin = NULL, height = NULL, width = NULL, iterations = 32, sinksRight = TRUE)\nI need Links, Nodes, Source, Target, Value, NodeID.\nNodes is the list of countries (both origin and arrival). It needs to be a unique list of the countries stored as a dataframe.\nI converted the countries to factors earlier, but that isn’t a problem.\n\nnodes &lt;- data.frame(name =\n                      c(pop_2022_no_other$coo_name, pop_2022_no_other$coa_name) %&gt;%\n                      unique()\n                    )\n\nNext, we need the IDs for source and target, zero-indexed. The ID is generated by position in the nodes dataframe I just created. The match() function is a base R function that returns a vector of the positions of the first argument (pop_2022_no_other$coo_name) as found in the second argument (nodes$name). Note that it returns a vector of all matches. I know that nodes is a unique list, so it is only going to return a single index. This may not be true for other use cases. Again, subtract 1 for the difference in indexing.\n\npop_2022_no_other$IDsource = match(pop_2022_no_other$coo_name, nodes$name) -\n  1\npop_2022_no_other$IDtarget = match(pop_2022_no_other$coa_name,  nodes$name) -\n  1\n\nNext, create the color scales. Details about the options for the color scales are found in the D3 API documentation.\nI am going to change the colors a bit. The other category will be gray. The colors are assigned matches using the order in the nodes df. Note that this does NOT match the order in the diagram. Most Sankey plotting programs reorder the nodes to minimize crossing and to create a cleaner diagram.\n\nColourScal = 'd3.scaleOrdinal([`#946943`, `#b63e36`,`#F5B041`, `#909497`,`#383867`, `#584c77`, `#33431e`, `#a36629`, `#92462f`])'\n\nThe colors are specified in hex code here. You can find color pickers that give you the codes on the web, such as this one.\nNow I can make the diagram. I’m also specifying height and width so I can add a title and a caption using htmlwidgets. If you don’t specify height and width the figure might be truncated when the titles are applied. (This method might work for leaflet maps also. The leaflet package also lacks a method for titles.) I am using the html header 2 styling (h2) for the title, while the caption is just a normal paragraph (p).\nThe sinksRight parameter is used to put the label either outside the flows (FALSE) as I have done here. TRUE puts it inside the flow lines. Unfortunately, there is not a matching sinksLeft, so those labels will always be inside the flow.\n\n# Make the Network\nsankey &lt;- sankeyNetwork(\n  Links = pop_2022_no_other,\n  Nodes = nodes,\n  Source = \"IDsource\",\n  Target = \"IDtarget\",\n  Value = \"total\",\n  NodeID = \"name\",\n  sinksRight = FALSE,\n  colourScale = ColourScal,\n  nodeWidth = 40,\n  fontSize = 13,\n  nodePadding = 20,\n  width = 600,\n  height = 400\n)\n\nLinks is a tbl_df. Converting to a plain data frame.\n\nsankey &lt;-\n  htmlwidgets::prependContent(sankey, htmltools::tags$h2(\"Refugee Flow in 2022\"))\nsankey &lt;-\n  htmlwidgets::appendContent(sankey, htmltools::tags$p(\"from UNHCR’s refugees R package\"))\n\nsankey\n\nRefugee Flow in 2022\n\nfrom UNHCR’s refugees R package\n\n\n\nSo this is pretty clear. Because the other category is disaggregated, we get individual lines for each country of arrival (but we don’t know what countries they are.) If you mouse over the traces, you can see how many refugees are involved in each path. For example, if you mouse over the large gray band on the Afghanistan to other, you see 1.8 million refugees have gone to a single color. Some of the other lines represent only a handful of refugees. I could also aggregate the other data, creating a single band from each source to other. But I think it is neat the way it is.\n\n\nSaving an htmlwidget object\nSaving the interactive figures to a thumbnail is always tricky. Various methods are suggested- using webshot seems to be the most popular, but I haven’t had it work. (First, you save the diagram as an HTML file, and then webshot converts that to a static image). While the HTML saves fine, the webshotted png is the title and the caption with a vast white block of no figure in between. The only consistent way I’ve found is to export from RStudio’s viewer. You do have to fiddle with the height and width of your object; otherwise, the exported file will either be truncated or have non-functional scroll bars in the image."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Louise E. Sinks",
    "section": "",
    "text": "Hello! I’m Louise Sinks. Welcome to my website. I’m a chemist by training, and I’ve spent my career designing, running, and analyzing a variety of types of experiments.\nYou’ll find some of my projects for former employers in the portfolio section. Most of my research was on advanced functional materials with novel photophysical properties. In addition to spectroscopy, I employed statistical learning/ fitting methods, as interpretability is of primary importance in the sciences. Of course, being in charge of the experimental design gives you a huge advantage if you are trying to develop an interpretable model, as you can make sure you thoroughly explore the relevant parameter space. Most of my modeling and simulation work was done in Matlab, which seems to be a more popular language in academia than in industry and business.\nYou’ll also find my blog, where I do small projects to master new languages (R and Python currently). These are generally written as tutorials; throughout my career, I’ve benefitted immensely from code chunks and explanations I’ve found online, so this is my way of paying it forward. The blog posts are written in Quarto, and the raw .qmd files can be found in my GitHub repo. These can be downloaded and run in RStudio or another IDE that supports Quarto. The current projects section includes upcoming topics for the blog."
  },
  {
    "objectID": "posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html",
    "href": "posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html",
    "title": "Tidy Tuesday: Daylight Savings Time",
    "section": "",
    "text": "This week’s TidyTuesday is about the timezone data from IANA timezone database.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(lubridate)\nlibrary(skimr)\nlibrary(lutz)\nlibrary(maps)\nlibrary(scales)\nlibrary(sf)\nlibrary(ggimage)\n\nThe history of this database is fascinating. It is used by many computer systems to determine the correct time based on location. To learn more, I recommend reading Daniel Rosehill’s article on the topic. For a drier history, check out the wikipedia article.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 13)\n\ntransitions &lt;- tuesdata$transitions\ntimezones &lt;- tuesdata$timezones\ntimezone_countries &lt;- tuesdata$timezone_countries\ncountries &lt;- tuesdata$countries\n\nIt is suggested that we change the begin and end variables in transitions to datetimes.\n\ntransitions &lt;- transitions %&gt;%\n  mutate(begin = as_datetime(begin), end = as_datetime(end))\n\nI was interested in how many countries had multiple times zones. I know the US has 4 time zones in the continental US.\n\nnum_zones &lt;- timezone_countries %&gt;%\n  count(country_code, sort = TRUE)\n\nnum_zones %&gt;% \n  filter(n &gt; 1) %&gt;%\n  left_join(countries) %&gt;%\n  select(place_name, n) %&gt;%\n  filter(place_name != \"NA\") %&gt;%\n  gt() %&gt;%\n  cols_label(place_name = \"Country\", n = \"Number of TZs\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Countries with Multiple TZs\") \n\n\n\n\n\n  \n    \n      Countries with Multiple TZs\n    \n    \n    \n      Country\n      Number of TZs\n    \n  \n  \n    United States\n29\n    Canada\n28\n    Russia\n27\n    Brazil\n16\n    Argentina\n12\n    Australia\n12\n    Mexico\n11\n    Kazakhstan\n7\n    Greenland\n4\n    Indonesia\n4\n    Ukraine\n4\n    Chile\n3\n    Spain\n3\n    Micronesia\n3\n    Kiribati\n3\n    Mongolia\n3\n    Malaysia\n3\n    French Polynesia\n3\n    Portugal\n3\n    US minor outlying islands\n3\n    Congo (Dem. Rep.)\n2\n    China\n2\n    Cyprus\n2\n    Germany\n2\n    Ecuador\n2\n    Marshall Islands\n2\n    New Zealand\n2\n    Papua New Guinea\n2\n    Palestine\n2\n    French Southern & Antarctic Lands\n2\n    Uzbekistan\n2\n    Vietnam\n2\n  \n  \n  \n\n\n\n\nAnd we find that the United States has 29!! time zones in the database. This was unexpected, so say the least. I thought maybe there were some times zones for territories and perhaps military bases that I did not know about. I also thought there might be some extra time zones arising from some states using daylight savings time, while others in the same area might not. I wanted to visualize where these times zones were.\n\nUS_tz &lt;- timezone_countries %&gt;% \n  filter(country_code == \"US\") %&gt;%\n  left_join(timezones)\n\nJoining with `by = join_by(zone)`\n\n\nI found the lutz package created nice pictograms about when a timezone shifts from DST and back. (This package uses the same underlying database that we are using here to determine when the shifts occur.)\n\n tz_plot(US_tz$zone[21])\n\n\n\n\nI created the plots and saved them as images. I modified a function I found on stack overflow to create the file names.\n\nwd &lt;- getwd()\nfilepath = file.path(wd)\n\n\nmake_filename = function(number){\n  # doing this, putting it all on a single line or using pipe %&gt;%\n  # is just matter of style\n  filename = paste(\"tzplot\", number, sep=\"_\")\n  filename = paste0(filename, \".png\")\n  filename = file.path(filepath, filename)\n  \n  filename\n}\n\n#creating a variable to store the files name\nUS_tz &lt;- US_tz %&gt;%\n  mutate(image_name = \"tbd\")\n\nindex &lt;- 1\nfor (index in seq(1, nrow(US_tz))) {\n  filename = make_filename(index)\n  US_tz[index , \"image_name\"] &lt;- filename\n  # 1. Open jpeg file\n  png(filename, width = 350, height = 350, bg = \"transparent\")\n  # 2. Create the plot\n  # you need to print the plot if you call it inside a loop\n  print(tz_plot(US_tz$zone[index]))\n  # 3. Close the file\n  dev.off()\n  index = index + 1\n}\n\nNext I created a world map, inspired by the one from\n\n\nMy submission for #TidyTuesday, Week 13 on time zones. I plot time zones in the world map.Code: https://t.co/y5Cm4tuaVk pic.twitter.com/BZC3anC5Oa\n\n— Mitsuo Shiota (@mitsuoxv) March 28, 2023\n\n\nI hadn’t previously used the maps package, so I appreciate being introduced to it. The maps package only has a mainland US map, so I used the world map. (Plus, as I mentioned, I thought some of these time zones would be in other parts of the world.) I followed a tutorial on Plotting Points as Images in ggplot and used the hints about aspect ratio to make my tz_plot circles remain circular. However, that did stretch the world a bit.\n\naspect_ratio &lt;- 1.618  \n\nus_tz_map &lt;- map_data(\"world\") %&gt;% \n  ggplot(aes(long, lat)) +\n  geom_polygon(aes(group = group), fill = \"white\", \n               color = \"gray30\", alpha = 0.9) +\n  geom_image(aes(x = longitude, latitude, image = image_name), \n             data = US_tz, size = 0.025, by = \"width\",\n             asp = aspect_ratio) +\n  coord_sf() +\n  labs(title = \"The United States has 29 Timezone- Mostly Redunant\",\n       caption = \"Data from: https://data.iana.org/time-zones/tz-link.html\") +\n  theme_void() +\n  theme(aspect.ratio = 1/aspect_ratio,\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = \"white\")\n    )\n\nggsave(\"thumbnail.png\", us_tz_map, width = 5 * aspect_ratio, height = 5)\nus_tz_map\n\n\n\n\nAnd what we see is there are a bunch of redundant times zone specification, especially in the Midwest.\n\nUS_tz %&gt;%\n  select(zone, latitude, longitude) %&gt;%\n  arrange(longitude) %&gt;%\n  gt() %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Countries with Multiple TZs\") \n\n\n\n\n\n  \n    \n      Countries with Multiple TZs\n    \n    \n    \n      zone\n      latitude\n      longitude\n    \n  \n  \n    America/Adak\n52.66667\n-177.13333\n    America/Nome\n64.56667\n-165.78333\n    Pacific/Honolulu\n21.71667\n-158.35000\n    America/Anchorage\n61.30000\n-149.91667\n    America/Yakutat\n60.35000\n-140.35000\n    America/Sitka\n57.75000\n-135.41667\n    America/Juneau\n58.41667\n-134.60000\n    America/Metlakatla\n55.73333\n-132.15000\n    America/Los_Angeles\n34.18333\n-118.80000\n    America/Boise\n44.41667\n-116.35000\n    America/Phoenix\n34.33333\n-112.46667\n    America/Denver\n40.08333\n-105.03333\n    America/North_Dakota/Beulah\n48.10000\n-102.43333\n    America/North_Dakota/Center\n48.08333\n-102.23333\n    America/North_Dakota/New_Salem\n47.53333\n-102.05000\n    America/Menominee\n45.56667\n-88.45000\n    America/Indiana/Vincennes\n39.30000\n-88.23333\n    America/Indiana/Petersburg\n39.00000\n-87.98333\n    America/Chicago\n41.85000\n-87.65000\n    America/Indiana/Tell_City\n38.13333\n-87.43333\n    America/Indiana/Knox\n42.03333\n-87.11667\n    America/Indiana/Marengo\n38.90000\n-87.01667\n    America/Indiana/Winamac\n41.13333\n-86.78333\n    America/Indiana/Indianapolis\n39.86667\n-86.63333\n    America/Kentucky/Louisville\n38.50000\n-86.31667\n    America/Kentucky/Monticello\n37.60000\n-85.78333\n    America/Indiana/Vevay\n39.60000\n-85.10000\n    America/Detroit\n43.20000\n-83.78333\n    America/New_York\n41.55000\n-74.38333\n  \n  \n  \n\n\n\n\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E.},\n  title = {Tidy {Tuesday:} {Daylight} {Savings} {Time}},\n  date = {2023-03-28},\n  url = {https://lsinks.github.io/posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E. 2023. “Tidy Tuesday: Daylight Savings\nTime.” March 28, 2023. https://lsinks.github.io/posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html."
  },
  {
    "objectID": "posts/2023-06-29-tidytuesday-populated-places-leaflet/arlington-neighborhoods-leaflet.html",
    "href": "posts/2023-06-29-tidytuesday-populated-places-leaflet/arlington-neighborhoods-leaflet.html",
    "title": "Tidy Tuesday Revisited: Interactive Map of Arlington Historic Neighborhoods",
    "section": "",
    "text": "This week’s TidyTuesday is about place names as recorded by the US Board on Geographic Names. The dataset has been cleaned to include only populated places. I ended up augmenting the dataset with information about Arlington Historic neighborhoods and current neighborhood boundaries. My post with code on this project is here.\nI wanted to create an interactive map with leaflet, but I encountered two problems:\n1- I couldn’t figure out how to add my civic association map.\n2- The map that I did make worked fine when I ran it from a code chunk, but failed when I rendered the quarto document.\nI’ve solved both problems and I really enjoyed working with leaflet.\nHere are the libraries:\n\nlibrary(tidyverse) # who doesn't want to be tidy?\nlibrary(sf) # for handling geo data\nlibrary(leaflet) # interacting mapping\n\nI saved the two datasets from my previous work: historic_4269 and arlington_polygons_sf. I saved them using:\nst_write(historic_4269, \"points.shp\")\nst_write(arlington_polygons_sf, \"polygons.shp\")\nfrom the sf package.\nHere, I’m reading them in. The process does change some of the variable names. The dataset from the National Register of Historic Places had non-standard names such as Property.Name, which gets converted to a shorter name, Prprt_N, with _ instead of period.\n\nhistoric_4269 &lt;- st_read(\"points.shp\")\narlington_polygons_sf &lt;- st_read(\"polygons.shp\")\n\nI mentioned that I found tutorials here and here to make the pop-up URL using leaflet. So, following them I add the HTML anchor tag.\n\n# turn the url to HTML anchor tag\nhistoric_4269 &lt;- historic_4269 %&gt;% \n  mutate(tag = paste0(\"More Info: &lt;a href=\", Extrn_L,\"&gt;\", Extrn_L, \"&lt;/a&gt;\"))\n\nLeaflet uses background map tiles as the canvas for the map. As with all mapping, the coordinate reference system (CRS) of all your component layers needs to be the same. The two datasets I have used the CRS= 4269 projection, but this isn’t the usual CRS. The background map I chose uses the 4326 CRS, so I need to transform my data to that projection. Leaflet will give you a warning if you add layers with unexpected CRSs, so make sure to read the messages carefully and correct them.\n\nhistoric_4326 &lt;- sf::st_transform(historic_4269, crs = 4326)\narlington_polygons_sf_4326 &lt;- sf::st_transform(arlington_polygons_sf, crs = 4326) \n\nFor the issue of adding the polygon data, I was just not really thinking about things. Leaflet uses tidyverse piping, so you either need to have the dataset at the start of the pipe chain or you need to explicitly pass it as data = blah. The error message wasn’t super help to me either : addPolygons must be called with both lng and lat, or with neither. I thought that meant I needed to transform the polygons into some other type geometry format.\nSo this doesn’t work:\nleaflet_map &lt;- leaflet() %&gt;%\naddPolygons(arlington_polygons_sf_4326)\nleaflet_map\nBut this does:\n\nleaflet_map &lt;- leaflet(arlington_polygons_sf_4326) %&gt;% \n  addPolygons() \n\nleaflet_map\n\n\n\n\n\nOr this:\n\nleaflet_map &lt;- arlington_polygons_sf_4326 %&gt;% \n  leaflet() %&gt;%\n  addPolygons() \n\nleaflet_map\n\n\n\n\n\nOr this:\n\nleaflet_map &lt;- leaflet() %&gt;% \n  addPolygons(data = arlington_polygons_sf_4326) \n\nleaflet_map\n\n\n\n\n\nI chose to use the last method, since I was adding data from different sources and I thought it would be more understandable to have the data source explicitly stated in each layer call.\nTo make things a bit clearer, I set a color palette for the Arlington neighborhoods. There are 62 of them, so I used viridis, which is more suited for numerical data, but creates a pleasing effect here. There is information encoded in the colors, the purples correspond to neighborhoods starting with “A” and the yellows correspond to those at the end of the alphabet, but that isn’t really important. The choice was purely an aesthetic one.\n\npal &lt;- colorFactor(palette = \"viridis\", domain = arlington_polygons_sf_4326$CIVIC)\n\nThe final leaflet map has three layers:\n\nthe underlying map created using addProviderTiles()\nthe current Arlington neighborhoods created using addPolygons()\nthe point markers for the historic districts created using addCircleMarkers()\n\nThe neighborhood names appear when you hover over the polygon, while the name of the historic district and the link to the application submitted to be added to the National Register of Historic Places appears as a pop-up when you click on it.\nLeaflet uses ~ notation to reference variables in the data, which you can see in code below.\n\nleaflet_map &lt;- leaflet() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(\n    data = arlington_polygons_sf_4326,\n    weight = 1,\n    label = ~ CIVIC,\n    color = ~ pal(CIVIC)\n  ) %&gt;%\n  addCircleMarkers(\n    data = historic_4326,\n    popup = ~ paste0(\"&lt;b&gt;\", Prprt_N, \"&lt;/b&gt;\", \"&lt;br&gt;\", tag),\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"black\",\n    stroke = NA\n  )\nleaflet_map\n\n\n\n\n\nDatacamp has a really nice starter course on leaflet that I found very helpful for understanding leaflet conceptually as well as learning about the basic formatting options. There is also a nice set of documentation here.\nSo why was my leaflet map causing the quarto document to fail to render? Apparently, there was a issue with knitr and quarto that popped up after some updates in May 2023. It applies to packages other than leaflet as well. If you get an error message along the lines of :\nError in `add_html_caption()`: ! unused argument (xfun::grep_sub(\"^[^&lt;]*&lt;[^&gt;]+aria-labelledby[ ]*=[ ]*\\\"([^\\\"]+)\\\".*$\", \"\\\\1\", x))\nBacktrace:\n1. global .main()\n2. execute(...)\n3. rmarkdown::render(...)\n4. knitr::knit(knit_input, knit_output, envir = envir, quiet = quiet)\n5. knitr:::process_file(text, output) ...\n14. sew(res, options)\n15. knitr:::sew.list(x, options, ...)\n16. base::lapply(x, sew, options, ...)\n17. FUN(X[[i]], ...)\n18. knitr:::sew.knit_asis(x, options, ...) Execution halted.\nthen you probably have this issue. Quarto has already fixed the issue with stable release 1.3.433. The version of Quarto bundled with RStudio RStudio 2023.06.0+421 “Mountain Hydrangea” for Windows was 1.3.353 and has the problem. If you use the bundled version with RStudio, close RStudio, install the latest Quarto as a standalone program. When you open RStudio, it should automatically detect the new version and switch to that.\nTo check what version of Quarto you have, go to the terminal (not console) and type quarto check.\nLeaflet is pretty amazing. I’ve always found mapping in R to be unpleasant, but leaflet makes it easy and produces beautiful maps.\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E. and E. Sinks, Louise},\n  title = {Tidy {Tuesday} {Revisited:} {Interactive} {Map} of\n    {Arlington} {Historic} {Neighborhoods}},\n  date = {2023-06-29},\n  url = {https://lsinks.github.io/posts/2023-06-29-tidytuesday-US-populated-places-leaflet/arlington-neighborhoods-leaflet},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E., and Louise E. Sinks. 2023. “Tidy Tuesday\nRevisited: Interactive Map of Arlington Historic Neighborhoods.”\nJune 29, 2023. https://lsinks.github.io/posts/2023-06-29-tidytuesday-US-populated-places-leaflet/arlington-neighborhoods-leaflet."
  },
  {
    "objectID": "currentprojects.html",
    "href": "currentprojects.html",
    "title": "Current Projects",
    "section": "",
    "text": "I generally have a couple of big projects I’m working on that are “works in progress” and haven’t made it to a blog post yet. Links to the drafts are included, but they are just drafts.\nWhat I’m working on now:"
  },
  {
    "objectID": "currentprojects.html#family-history-project",
    "href": "currentprojects.html#family-history-project",
    "title": "Current Projects",
    "section": "Family History Project",
    "text": "Family History Project\nTombstones (in R)- I’m working on a project for my father that will culminate in a website for his genealogy research. There are a couple of different parts that I’m working on independently.\n\nLinking photos of family gravestones to an Excel sheet that records the GPS location of the tombstones. This combined dataset will be used to generate a leaflet map (and is currently called leaflet_tester). I will probably separate this into two or three portions, though.\n\nthe data cleanup and the actual matching (done and posted)\nleaflet styling (done and posted)\nweb scraping portion to add more information about some of the ancestors (done and posted)\nMy father also gave me several papers he’s written about family members. I want to write code to determine which family members are mentioned in each document and match them to his Excel sheet. (not started)\n\n\nI’ve spent a surprising large amount of time today learning how to handle “large” files. The photos are added to the tombstone repo using git’s lfs if anyone wanted to run my code. There are about 2 GB of photos, compressed to ~300 MB. The map with photos is about 500 MB, which is also too large for github pages. That’s okay, since I plan to host it on a different website devoted entirely to my father’s family history research."
  },
  {
    "objectID": "currentprojects.html#credit-card-fraud",
    "href": "currentprojects.html#credit-card-fraud",
    "title": "Current Projects",
    "section": "Credit Card Fraud",
    "text": "Credit Card Fraud\nI’m also exploring the credit card fraud project in other languages. I previously wrote two R Tidymodels tutorials using this data. They can be found here and here.\nBy using the same dataset, I can focus on learning the new language instead of thinking the higher level aspects. And I’m able to spot errors and mistakes much more easily.\nCredit Card Fraud (Tableau)- I’ve been working on replicating the credit card fraud EDA in Tableau. I have an adequate draft, but I need to go back and fix the tooltips. There were some interesting results concerning the geographic variables. I made some assumptions when I did the modeling in R that weren’t correct. I will write up some lessons I learned while working in Tableau, but I also want to see how I could have caught that while working in R.\nCredit Card Fraud (Python)- I am rebuilding my credit card fraud project in Python to improve my skills. I’m also learning how to use Python within R Studio and the ins and outs of the reticulate package."
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "",
    "text": "This week’s TidyTuesday concerns what some have called the first randomized clinical trial- a study by James Lind evaluating various treatments for scurvy. This data has been collected into the medicaldata R package from Lind’s book on scurvy."
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#choosing-a-color-palette",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#choosing-a-color-palette",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "Choosing a Color Palette",
    "text": "Choosing a Color Palette\n\npar(cex = 0.5) \ndisplay.brewer.all()"
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#first-draft-of-the-heatmap",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#first-draft-of-the-heatmap",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "First Draft of the Heatmap",
    "text": "First Draft of the Heatmap\nAnd the first draft of the heatmap. I’m coloring the heatmap by numerical severity and I’m labeling it with the descriptive severity.\n\nggplot(scurvy2, aes(treatment, symptom)) +   \n  geom_tile(aes(fill = Numerical_Severity))  +\n  geom_text(aes(label = Descriptive_Severity)) +\n  scale_fill_brewer(palette = \"YlOrRd\")\n\n\n\n\nOh, that is ugly. And ggplot grouped all the treatments, so we have overlapping text labels."
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#plotting-by-study_id",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#plotting-by-study_id",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "Plotting by study_id",
    "text": "Plotting by study_id\nLet’s fix things up. First, plot by study_id instead of treatment group. That will get me back to the 12 patients. And the legend should be removed. This is done using theme(legend.position = \"none\"). I’m also going to flip x and y. Since there are only 4 symptoms, this will give the text labels (mild, moderate, severe) more space since there will only be 4 labels across instead of 12.\n\nscurvy_heatmap &lt;- ggplot(scurvy2, aes(symptom, study_id)) +   \n  geom_tile(aes(fill = Numerical_Severity))  +\n  geom_text(aes(label = Descriptive_Severity)) +\n  scale_fill_brewer(palette = \"YlOrRd\") +\n  theme(legend.position = \"none\") \n\nscurvy_heatmap"
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#mapping-study_id-back-to-treatments",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#mapping-study_id-back-to-treatments",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "Mapping study_id Back to Treatments",
    "text": "Mapping study_id Back to Treatments\nNow I need to get back to the treatment groups. The original data frame, scurvy, has the treatment groups matched to the patient_id. The mapping can be done using scale_x_continuous(breaks = 1:12, labels = scurvy$treatment). This is pretty non-intuitive to me;I found this suggestion on Stackoverflow.\n\nscurvy_heatmap &lt;- scurvy_heatmap +\n  scale_y_continuous(breaks = 1:12, labels = scurvy$treatment)\n\nscurvy_heatmap"
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#modifying-the-style-with-theme",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#modifying-the-style-with-theme",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "Modifying the Style with theme()",
    "text": "Modifying the Style with theme()\nNow tweak the theme to clean things up. First, I’m going to start with a theme that is closest to want I want. In this case, I went with theme_void(), which actually removes all the elements. I then add the few I want back in. There is so much stuff to remove (the axis, the grid marks, the gray background). I want to retain the axis title and text. I’ll add the titles and captions too.\n\nscurvy_heatmap &lt;- scurvy_heatmap +\n  theme_void() +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = 'bold')) +\n  labs(title = \"James Lind's Study of Scurvy Treatments- 1757\", \n       caption = \"data from medicaldata R package\",\n       subtitle = \"Results at Day 6\") +\n  ylab('Treatments') + xlab('Symptoms') +\n  theme(axis.text.x = element_text(angle = 45))\n  \nscurvy_heatmap"
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#theme_void-isnt-completely-empty",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#theme_void-isnt-completely-empty",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "theme_void() Isn’t Completely Empty!",
    "text": "theme_void() Isn’t Completely Empty!\nAnd the legend came back… Apparently, theme_void does include the legend, so it re-spawned. This is a bit surprising, since the description says “A completely empty theme.”\nI’m going to quickly test this.\n\nggplot(scurvy2, aes(treatment, symptom)) + \n  geom_tile(aes(fill = Numerical_Severity)) +\n  theme_void()"
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#final-heatmap",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#final-heatmap",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "Final Heatmap",
    "text": "Final Heatmap\nOkay, so not a big deal to remove this again. The y axis label also rotated, so that needs to be rotated back.\n\nscurvy_heatmap &lt;- scurvy_heatmap +\ntheme(legend.position = \"none\") +\n  theme(axis.title.y = element_text(angle = 90))\n\nscurvy_heatmap\n\n\n\nggsave(here(\"posts\", \"2023-07-tidytuesday-scurvy\", \"thumbnail.png\"))\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "posts/2023-04-25-tidytuesday-marathon/marathon.html",
    "href": "posts/2023-04-25-tidytuesday-marathon/marathon.html",
    "title": "TidyTuesday Week 17: London Marathon",
    "section": "",
    "text": "Today’s TidyTuesday is based on a dataset about the London Marathon. The data is via a package by Nicola Rennie and there is an accompanying tutorial about how to scrape data from the web.\n\nlibrary(skimr)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(skimr)\n\nLoad the data with the TidyTuesday package in the usual way.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 17)\n\n--- Compiling #TidyTuesday Information for 2023-04-25 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `winners.csv`\n    Downloading file 2 of 2: `london_marathon.csv`\n\n\n--- Download complete ---\n\nwinners &lt;- tuesdata$winners\nlondon_marathon &lt;- tuesdata$london_marathon\n\nThere are two dataframes today: a winner’s dataframe and info about the marathon in general. Looking at the winner’s first.\n\nwinners %&gt;% skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n163\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\ndifftime\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nCategory\n0\n1\n3\n16\n0\n4\n0\n\n\nAthlete\n0\n1\n9\n26\n0\n99\n0\n\n\nNationality\n0\n1\n5\n14\n0\n24\n0\n\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nTime\n0\n1\n5187 secs\n16143 secs\n02:07:55\n158\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYear\n0\n1\n2001.61\n11.83\n1981\n1991.5\n2002\n2012\n2022\n▇▇▇▇▇\n\n\n\n\n\nAll the data is complete.\nCategory and Nationality should probably be factors rather than characters.\n\nwinners &lt;- winners %&gt;%\n  mutate(Category = as.factor(Category), Nationality = as.factor(Nationality))\n\nglimpse(winners)\n\nRows: 163\nColumns: 5\n$ Category    &lt;fct&gt; Men, Men, Men, Men, Men, Men, Men, Men, Men, Men, Men, Men…\n$ Year        &lt;dbl&gt; 1981, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989…\n$ Athlete     &lt;chr&gt; \"Dick Beardsley (Tie)\", \"Inge Simonsen (Tie)\", \"Hugh Jones…\n$ Nationality &lt;fct&gt; United States, Norway, United Kingdom, United Kingdom, Uni…\n$ Time        &lt;time&gt; 02:11:48, 02:11:48, 02:09:24, 02:09:43, 02:09:57, 02:08:1…\n\n\nWhen I check to make sure everything is correct after making the factors, I see that some athletes have (Tie) after their name. If I do something with the runner’s names later (or if I care about ties) then I need to handle this.\nThere are four categories of races in our dataset- Men, Wheelchair Men, Wheelchair Women, Women.\nI’m going to do some simple comparisons first. I’m looking for something interesting to focus on.\n\nwinners %&gt;%\n  filter(Category == \"Men\" | Category == \"Women\") %&gt;%\n  ggplot(aes(Category, Time)) +\n  geom_boxplot()\n\n\n\n\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\" | Category == \"Wheelchair Women\") %&gt;%\n  ggplot(aes(Category, Time)) +\n  geom_boxplot()\n\n\n\n\nMen are faster than women, in both classes of races.\n\nwinners %&gt;%\n  ggplot(aes(Category, Time)) +\n  geom_boxplot()\n\n\n\n\nWheelchair races are faster than the running races. The Men/Women’s races are much closer than the Wheelchair races, and don’t have outliers.\nI’m interested in ties. How many ties have there been? I’m going to make a column called “Tied” and remove (Tied) from the names. There are a few different ways to do this, but I’m going to use tidyr::separate(). I’m going to take the space too, when I separate out the name and (tie), so my name column is cleanly formatted. This is going to generate a bunch of NAs in the Tied column, but I’ll handle that in the next code chunk. I’m using \" (\" as my separator, so the Name column will not have the trailing space. The Tied column will have “Tie)” and a bunch of NAs, but I’ll clean this up in the next step. I’ve left warnings on for this code chunk, so you can see that the NAs are flagged for your notice.\nRemember that special characters like ( need to be escaped out, so the appropriate regular expression for the separator is \" \\\\(\".\nAs a side note, as of winter 2022, separate has been superseded by a family of functions separate_wider_*.\n\nwinners_tied &lt;- winners %&gt;%\n  separate(Athlete, into = c(\"Name\", \"Tied\"), \" \\\\(\")\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 161 rows [3, 4, 5, 6, 7,\n8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, ...].\n\n\nNow I’m going to clean up the Tied column.\n\nwinners_tied &lt;- winners_tied %&gt;%\n  mutate(Tied = ifelse(is.na(Tied) == TRUE, FALSE, TRUE))\n\nSo how many ties are there?\n\nwinners_tied %&gt;% filter(Tied == TRUE)\n\n# A tibble: 2 × 6\n  Category  Year Name           Tied  Nationality   Time    \n  &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;lgl&gt; &lt;fct&gt;         &lt;time&gt;  \n1 Men       1981 Dick Beardsley TRUE  United States 02:11:48\n2 Men       1981 Inge Simonsen  TRUE  Norway        02:11:48\n\n\nJust those first two, from the very first race. According to wikipedia, they crossed the finish line holding hands!\nAnyone win more than once?\n\nwinners_tied %&gt;%\n  group_by(Name) %&gt;%\n  count(Name, sort = TRUE) %&gt;%\n  filter(n &gt; 1)\n\n# A tibble: 34 × 2\n# Groups:   Name [34]\n   Name                     n\n   &lt;chr&gt;                &lt;int&gt;\n 1 David Weir               8\n 2 Tanni Grey-Thompson      6\n 3 David Holding            4\n 4 Eliud Kipchoge           4\n 5 Francesca Porcellato     4\n 6 Ingrid Kristiansen       4\n 7 Tatyana McFadden         4\n 8 António Pinto            3\n 9 Dionicio Cerón           3\n10 Heinz Frei               3\n# ℹ 24 more rows\n\n\nMore than I expected! David Weir has won the most London Marathons, with 8 wins in the Men’s Wheelchair race category. How has his race time changed over the years?\n\nwinners_tied %&gt;%\n  filter(Name == \"David Weir\") %&gt;%\n  ggplot(aes(Year, Time)) +\n  geom_point()\n\n\n\n\nIt looks like his first race was much slower than the other times he has won. It turns out he has competed in the London Marathon 23 times, and placed 5th in the 2023 Marathon which happened this weekend. His time on Saturday was 01:32:45. This is interesting, because it is quite similar to his more recent races.\nHas the field for this race gotten faster?\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\") %&gt;%\n  ggplot(aes(Year, Time)) +\n  geom_point()\n\n\n\n\nYes, and quite clearly too! How many years would he have won with this year’s time?\n\ntime_2023 &lt;- as.difftime(\"01:32:45\", format = \"%H:%M:%S\", units = \"auto\")\nslow_wins &lt;- winners %&gt;%\n  filter(Category == \"Wheelchair Men\" & Time &gt;= time_2023) \nfast_wins &lt;- winners %&gt;%\n  filter(Category == \"Wheelchair Men\" & Time &lt; time_2023) \n\nSo 12 years the wins were faster than his time this year, but 27 years were slower. And just to note, 6 of those 12 faster wins are held by David Weir…\nLet’s throw together a visual for this and make it out TidyTuesday viz. Should be simple, right?\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\") %&gt;%\n  ggplot(aes(Year, Time)) +\n  geom_point() +\n  geom_hline(yintercept = time_2023)\n\n\n\n\nHmm. ggplot is not playing well with our difftimes.\n\nstr(time_2023)\n\n 'difftime' num 1.54583333333333\n - attr(*, \"units\")= chr \"hours\"\n\nstr(winners$Time[1])\n\n 'hms' num 02:11:48\n - attr(*, \"units\")= chr \"secs\"\n\n\nApparently, our race time from winners is actually hms and not difftime. Skim reported it was a difftime. Our difftime has units of hours, while Time has units of seconds. This is probably due to be setting units to “auto” when I did the conversion. Interesting that dplyr filtering handles this smoothly, but ggplot doesn’t.\n\ntime_2023 &lt;- as.difftime(\"01:32:45\", format = \"%H:%M:%S\", units = \"sec\")\nstr(time_2023)\n\n 'difftime' num 5565\n - attr(*, \"units\")= chr \"secs\"\n\n\nI’m going to create a TRUE/FALSE column for if David Weir won that I will color code the win by.\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\") %&gt;%\n  mutate(Weir = ifelse(Athlete == \"David Weir\", TRUE, FALSE)) %&gt;%\n  ggplot(aes(Year, Time, color = Weir)) +\n  geom_point() +\n  geom_hline(yintercept = time_2023)\n\n\n\n\nOkay, now lets clean up the formatting. I need to:\n\nApply a theme. I like theme_classic() or theme_pander() as a clean base for my graphs.\nRemove the legend. This needs to go after the theme_classic/pander() call or the legend will reappear.\nAdd title, subtitle and data source\nFix the y-axis units\nChange the colors for the David Weir points.\nLabel my horizontal line.\n\n#4 is apparently quite challenging. Apparently, the time axis should be displayed as hms, so it isn’t clear to me why fractional seconds are shown. I tried a bunch of different suggestion from the web, but the top answer to this question is what worked.\nIt actually doesn’t matter if the aesthetic is difftime or hms. The key is that the label section of scale_y_time needs a formatted string generated from strftime.\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\") %&gt;%\n  mutate(Weir = ifelse(Athlete == \"David Weir\", TRUE, FALSE)) %&gt;%\n\n  ggplot(aes(Year, Time, color = Weir)) +\n  geom_point() +\n  scale_color_manual(values = c(\"black\", \"cyan2\")) +\n  geom_hline(yintercept = (time_2023),\n             color = \"cyan4\") +\n  annotate(\n    \"text\",\n    x = 1990,\n    y = time_2023 - 200,\n    label = \"David Weir's 2023 time\",\n    color = \"cyan4\"\n  ) +\n  scale_y_time(name = \"Winning Time\", labels = function(l) strftime(l, '%H:%M:%S')) +\n  labs(title = \"Race times for the London Marathon: Men's Wheelchair Race\",\n       subtitle = \"compared to David Weir's wins\",\n       caption = \"data from https://github.com/nrennie/LondonMarathon\") +\n  theme_classic(12) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E.},\n  title = {TidyTuesday {Week} 17: {London} {Marathon}},\n  date = {2023-04-25},\n  url = {https://lsinks.github.io/posts/2023-04-25-tidytuesday-marathon/marathon.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E. 2023. “TidyTuesday Week 17: London\nMarathon.” April 25, 2023. https://lsinks.github.io/posts/2023-04-25-tidytuesday-marathon/marathon.html."
  },
  {
    "objectID": "posts/2023-03-21-tidytuesday-programming-languages/index.html",
    "href": "posts/2023-03-21-tidytuesday-programming-languages/index.html",
    "title": "TidyTuesday Week 12: Programming Languages",
    "section": "",
    "text": "This is my first attempt at Tidy Tuesday. The dataset today is about Programming Languages. The sample visualizations are about the comment codes.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(ggrepel)\n\nLoad the data first. There has been some cleaning done as outlined on the TidyTuesday github page.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 12)\n\nlanguages &lt;- tuesdata$languages\n\nFirst, let’s look at how complete the data is. The skimr package produces nice summary information about the variables and their completeness.\n\nskim(languages)\n\n\nData summary\n\n\nName\nlanguages\n\n\nNumber of rows\n4303\n\n\nNumber of columns\n49\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n21\n\n\nlogical\n4\n\n\nnumeric\n24\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n4303\n0\n\n\ntitle\n0\n1.00\n1\n56\n0\n4267\n0\n\n\ndescription\n3480\n0.19\n4\n2273\n0\n811\n0\n\n\ntype\n0\n1.00\n2\n27\n0\n40\n0\n\n\ncreators\n3203\n0.26\n2\n253\n0\n985\n0\n\n\nwebsite\n2928\n0.32\n13\n131\n0\n1368\n0\n\n\ndomain_name\n3588\n0.17\n6\n32\n0\n700\n0\n\n\nreference\n2314\n0.46\n15\n251\n0\n1955\n0\n\n\ngithub_repo\n3402\n0.21\n25\n73\n0\n897\n0\n\n\ngithub_repo_description\n3438\n0.20\n4\n419\n0\n853\n0\n\n\ngithub_language\n3829\n0.11\n1\n30\n0\n474\n0\n\n\ngithub_language_tm_scope\n3837\n0.11\n4\n34\n0\n361\n0\n\n\ngithub_language_type\n3837\n0.11\n4\n11\n0\n4\n0\n\n\ngithub_language_ace_mode\n3838\n0.11\n1\n16\n0\n96\n0\n\n\ngithub_language_file_extensions\n3833\n0.11\n1\n606\n0\n466\n0\n\n\nwikipedia\n2731\n0.37\n32\n104\n0\n1566\n0\n\n\nwikipedia_summary\n2884\n0.33\n17\n6741\n0\n1407\n0\n\n\nwikipedia_related\n3145\n0.27\n1\n1761\n0\n1059\n0\n\n\nline_comment_token\n3831\n0.11\n1\n7\n0\n23\n0\n\n\norigin_community\n1190\n0.72\n3\n305\n0\n2232\n0\n\n\nfile_type\n3213\n0.25\n2\n6\n0\n4\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n3683\n0.14\n1.00\nTRU: 617, FAL: 3\n\n\nfeatures_has_semantic_indentation\n3722\n0.14\n0.11\nFAL: 516, TRU: 65\n\n\nfeatures_has_line_comments\n3765\n0.13\n0.96\nTRU: 517, FAL: 21\n\n\nis_open_source\n3792\n0.12\n0.89\nTRU: 453, FAL: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1991.11\n111.44\n-2000\n1984.00\n1997.0\n2012.00\n2023\n▁▁▁▁▇\n\n\ndomain_name_registered\n3801\n0.12\n2011.33\n7.02\n1990\n2007.00\n2013.0\n2017.00\n2023\n▁▃▃▇▆\n\n\nisbndb\n3217\n0.25\n7.71\n33.16\n0\n0.00\n0.0\n2.00\n400\n▇▁▁▁▁\n\n\nbook_count\n0\n1.00\n2.08\n17.34\n0\n0.00\n0.0\n0.00\n401\n▇▁▁▁▁\n\n\nsemantic_scholar\n3545\n0.18\n3.79\n8.32\n0\n0.00\n0.0\n3.00\n52\n▇▁▁▁▁\n\n\nlanguage_rank\n0\n1.00\n2151.00\n1242.31\n0\n1075.50\n2151.0\n3226.50\n4302\n▇▇▇▇▇\n\n\ngithub_repo_stars\n3414\n0.21\n2127.40\n7554.02\n0\n29.00\n194.0\n1071.00\n88526\n▇▁▁▁▁\n\n\ngithub_repo_forks\n3417\n0.21\n261.29\n1203.00\n0\n2.25\n16.0\n91.50\n23732\n▇▁▁▁▁\n\n\ngithub_repo_updated\n3418\n0.21\n2021.39\n1.76\n2012\n2022.00\n2022.0\n2022.00\n2023\n▁▁▁▁▇\n\n\ngithub_repo_subscribers\n3418\n0.21\n62.34\n200.88\n0\n4.00\n13.0\n44.00\n2910\n▇▁▁▁▁\n\n\ngithub_repo_created\n3425\n0.20\n2015.84\n3.48\n2006\n2013.00\n2016.0\n2019.00\n2022\n▁▅▇▇▇\n\n\ngithub_repo_issues\n3518\n0.18\n123.03\n546.26\n0\n1.00\n9.0\n61.00\n9522\n▇▁▁▁▁\n\n\ngithub_repo_first_commit\n3567\n0.17\n2014.74\n4.99\n1987\n2012.00\n2015.0\n2018.00\n2022\n▁▁▁▆▇\n\n\ngithub_language_repos\n3833\n0.11\n197134.67\n1226900.57\n0\n91.25\n725.5\n7900.25\n16046489\n▇▁▁▁▁\n\n\nwikipedia_daily_page_views\n2837\n0.34\n227.13\n783.55\n-1\n9.00\n24.0\n99.00\n13394\n▇▁▁▁▁\n\n\nwikipedia_backlinks_count\n2877\n0.33\n318.55\n1635.29\n1\n13.00\n39.0\n126.00\n34348\n▇▁▁▁▁\n\n\nwikipedia_page_id\n2893\n0.33\n9167847.21\n13506832.90\n928\n375153.75\n2114700.5\n12321223.00\n63063548\n▇▁▁▁▁\n\n\nwikipedia_appeared\n2958\n0.31\n1991.14\n17.03\n1830\n1980.00\n1994.0\n2005.00\n2019\n▁▁▁▃▇\n\n\nwikipedia_created\n3040\n0.29\n2005.75\n3.77\n2001\n2003.00\n2005.0\n2007.00\n2020\n▇▇▂▁▁\n\n\nwikipedia_revision_count\n3130\n0.27\n330.43\n813.26\n1\n35.00\n84.0\n242.00\n10104\n▇▁▁▁▁\n\n\nlast_activity\n0\n1.00\n2000.62\n84.60\n-900\n1992.00\n2006.0\n2021.00\n2023\n▁▁▁▁▇\n\n\nnumber_of_users\n0\n1.00\n13771.26\n227712.95\n0\n0.00\n20.0\n230.00\n7179119\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n422.18\n12572.99\n0\n0.00\n0.0\n0.00\n771996\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n1482\n0.66\n0.00\n0.00\n0\n0.00\n0.0\n0.00\n0\n▁▁▇▁▁\n\n\n\n\n\nThe data is pretty incomplete. Only 9 of the 49 variables are fully complete. The line comment token is only 0.110 complete and the has comments is only 0.144 complete. This variable has only 3 false values; it is likely that the missing data is skewed towards false. It is more likely that you’d complete this entry if there were a comment, than if there weren’t. It is also possible that the cleaning and prep done to prepare the #TidyTuesday dataset removed some entries which did have FALSE values for the comments.\nThere are some funny entries that appeared in the skim report, like -2000 as the year the earliest language appeared. It turns out this is Babylonian numerals, so it probably correct. This does show there is a lot more than computer languages in this dataset though.\nLooking through the variables, I see there is a “type” in the data dictionary, and it appears that “pl” means programming language. So let’s filter for that. (I couldn’t find an explanation of this variable on https://pldb.com/) It is used on various pages, but I couldn’t find the definition of the types.\nAlso, rank starts at 0, and I’d like it to start at 1.\n\nprogramming_lang &lt;- languages %&gt;%\n  filter(type == 'pl') %&gt;%\n  select(-starts_with(\"github\"), -starts_with(\"wikipedia\"),\n         -description, -creators, -(website:semantic_scholar)) %&gt;%\n  mutate(language_rank = language_rank + 1)\n\nskim(programming_lang)\n\n\nData summary\n\n\nName\nprogramming_lang\n\n\nNumber of rows\n3368\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlogical\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n3368\n0\n\n\ntitle\n0\n1.00\n1\n54\n0\n3347\n0\n\n\ntype\n0\n1.00\n2\n2\n0\n1\n0\n\n\nline_comment_token\n3002\n0.11\n1\n3\n0\n18\n0\n\n\norigin_community\n883\n0.74\n3\n176\n0\n1825\n0\n\n\nfile_type\n2609\n0.23\n4\n4\n0\n1\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n2886\n0.14\n1.00\nTRU: 482\n\n\nfeatures_has_semantic_indentation\n2917\n0.13\n0.09\nFAL: 410, TRU: 41\n\n\nfeatures_has_line_comments\n2954\n0.12\n0.97\nTRU: 401, FAL: 13\n\n\nis_open_source\n2984\n0.11\n0.85\nTRU: 328, FAL: 56\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1994.16\n17.34\n1948\n1982.0\n1994.0\n2010.0\n2022\n▁▅▇▇▇\n\n\nlanguage_rank\n0\n1.00\n2296.75\n1249.08\n1\n1243.5\n2334.5\n3423.5\n4303\n▆▆▆▆▇\n\n\nlast_activity\n0\n1.00\n2002.04\n17.91\n1951\n1989.0\n2005.0\n2019.0\n2023\n▁▂▃▆▇\n\n\nnumber_of_users\n0\n1.00\n10793.85\n190197.19\n0\n0.0\n15.0\n165.0\n5962666\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n160.22\n2692.65\n0\n0.0\n0.0\n0.0\n85206\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n939\n0.72\n0.00\n0.00\n0\n0.0\n0.0\n0.0\n0\n▁▁▇▁▁\n\n\n\n\n\nThis now produces a dataset with 0.143 completeness for features_has_comments. All non-missing entries are TRUE, which again suggests that FALSE is over represented in the missing data.\nLet’s only look at the programming languages that have data for comments.\n\nprogramming_lang &lt;- programming_lang %&gt;%\n  filter(features_has_comments == TRUE)\n\nskim(programming_lang)\n\n\nData summary\n\n\nName\nprogramming_lang\n\n\nNumber of rows\n482\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlogical\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n35\n0\n482\n0\n\n\ntitle\n0\n1.00\n1\n45\n0\n481\n0\n\n\ntype\n0\n1.00\n2\n2\n0\n1\n0\n\n\nline_comment_token\n120\n0.75\n1\n3\n0\n18\n0\n\n\norigin_community\n112\n0.77\n3\n105\n0\n311\n0\n\n\nfile_type\n146\n0.70\n4\n4\n0\n1\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n0\n1.00\n1.00\nTRU: 482\n\n\nfeatures_has_semantic_indentation\n57\n0.88\n0.05\nFAL: 405, TRU: 20\n\n\nfeatures_has_line_comments\n71\n0.85\n0.97\nTRU: 400, FAL: 11\n\n\nis_open_source\n305\n0.37\n0.91\nTRU: 161, FAL: 16\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n2000.17\n14.07\n1957\n1991.00\n2003.0\n2011.00\n2022\n▁▂▆▇▇\n\n\nlanguage_rank\n0\n1.00\n656.10\n559.75\n1\n201.25\n515.5\n997.25\n2994\n▇▃▂▁▁\n\n\nlast_activity\n0\n1.00\n2016.20\n8.27\n1967\n2011.00\n2022.0\n2022.00\n2023\n▁▁▁▂▇\n\n\nnumber_of_users\n0\n1.00\n62892.08\n462314.18\n0\n112.00\n437.5\n1615.25\n5962666\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n971.30\n6489.83\n0\n0.00\n0.0\n0.00\n85206\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n136\n0.72\n0.00\n0.00\n0\n0.00\n0.0\n0.00\n0\n▁▁▇▁▁\n\n\n\n\n\nThis subset is still moderately incomplete for information about comments. Only 75% of the data has the type of comment entered (#, //, etc). 86% of the entries are completed for “feature_has_line_comments” which indicates if comments must occupy a single line or if they can be made inline.\n\nprogramming_lang %&gt;% filter(is.na(line_comment_token) == FALSE) %&gt;%\n  group_by(line_comment_token) %&gt;%\n  count(line_comment_token) %&gt;%\n  ggplot(aes(fct_rev(fct_reorder(line_comment_token, n)), n)) +\n  geom_col(fill = \"dodgerblue2\") +\n  ylab(\"Count\") +\n  xlab(\"Comment Token\") +\n  ggtitle(\"Popularity of different comment tokens\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,  vjust = 0.25, hjust = 0.25))\n\n\n\n\nLet’s make a nice table of the popular comment types.\n\n# | label: table-tokens\nprogramming_lang2 &lt;- programming_lang %&gt;%\n  filter(is.na(line_comment_token) == FALSE) %&gt;%\n  count(line_comment_token, sort = TRUE) \n\nprogramming_lang2 %&gt;%\ngt() %&gt;%\ntab_header(title = \"Most Common Comment Tokens\") %&gt;%\ncols_label(line_comment_token = \"Token\", n = \"# of Languages that use token\")\n\n\n\n\n\n  \n    \n      Most Common Comment Tokens\n    \n    \n    \n      Token\n      # of Languages that use token\n    \n  \n  \n    //\n161\n    #\n70\n    ;\n49\n    --\n31\n    '\n16\n    %\n12\n    !\n7\n    *\n5\n    REM\n2\n    *&gt;\n1\n    ---\n1\n    /\n1\n    NB.\n1\n    \\\n1\n    \\*\n1\n    __\n1\n    ~\n1\n    ⍝\n1\n  \n  \n  \n\n\n\n\nThere is a language rank, which measures the popularity of the language based on signals such as number of users and number of jobs. Let’s see the average rank of languages for each token.\n\nprogramming_lang %&gt;% filter(is.na(line_comment_token) == FALSE) %&gt;%\n  group_by(line_comment_token) %&gt;%\n  summarize(avg_rank = mean(language_rank)) %&gt;%\n  ggplot(aes((fct_reorder(line_comment_token, avg_rank)), avg_rank)) +\n  geom_col(fill = \"dodgerblue2\") +\n  ylab(\"Average Rank of Language\") +\n  xlab(\"Comment Token\") +\n  ggtitle(\"Average rank of languages using different comment tokens\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,  vjust = 0.25, hjust = 0.25))\n\n\n\n\nThe highest (average) ranked token is “*&gt;”. What languages use this?\n\nprogramming_lang %&gt;% filter(line_comment_token == \"*&gt;\") %&gt;%\n  select(title, language_rank, line_comment_token)\n\n# A tibble: 1 × 3\n  title language_rank line_comment_token\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;             \n1 COBOL            19 *&gt;                \n\n\nOnly COBOL does, so the rank of this token isn’t diluted by many less popular languages. We can view the distribution of the language ranks for all the tokens.\n\nprogramming_lang %&gt;%\n  filter(is.na(line_comment_token) == FALSE) %&gt;%\n  ggplot(aes(line_comment_token, language_rank)) +\n  geom_boxplot(color = \"dodgerblue2\") +\n  ggtitle(\"The rank of languages by token.\") +\n  xlab(\"Token\") +\n  ylab (\"Language Rank\") +\n  theme_classic()\n\n\n\n\nOkay, let’s clean this up. I’d like it sorted by the median rank. Remeber rank is in reverse numerical order- a low number means a higher rank.\n\nprogramming_lang %&gt;%\n  filter(is.na(line_comment_token) == FALSE) %&gt;%\n  ggplot(aes(fct_reorder(line_comment_token, language_rank,\n                         .fun = median, .desc = FALSE), language_rank)) +\n  geom_boxplot(color = \"dodgerblue2\") +\n  ggtitle(\"The rank of languages by token\") +\n  xlab(\"Token\") +\n  ylab(\"Language Rank\") +\n    theme_classic()\n\n\n\n\nLet’s see the most popular language for each symbol. There might be a way to do this all at once, but I’m going to pull it out with joins to previous tables I’ve created.\n\nprogramming_lang3 &lt;- programming_lang %&gt;%\n  filter(is.na(line_comment_token) == FALSE) %&gt;%\n  group_by(line_comment_token) %&gt;%\n  summarize(highest_rank = min(language_rank)) \n\njoin_madness &lt;- programming_lang2 %&gt;%\n  left_join(programming_lang3, by = \"line_comment_token\") %&gt;% \n  left_join(programming_lang, \n            by = c(\"highest_rank\" = \"language_rank\",\n                   \"line_comment_token\" = \"line_comment_token\")) \n\njoin_madness &lt;- join_madness %&gt;%\n  select(line_comment_token, n, highest_rank, title, appeared, number_of_users,\n         number_of_jobs)\n\nSo now we have a bunch of summarized data in a single dataframe. Here’s a graph. It is saying something, but I’m not sure what. When you can’t come up with a concise title, then you probably don’t know what you are trying to say…\n\njoin_madness %&gt;%\n  ggplot(aes(highest_rank, n, size = log(number_of_users), \n             color = log(number_of_users), label = line_comment_token)) +\n  scale_y_log10() +\n  scale_x_log10() +\n  geom_text_repel(show.legend = FALSE) +\n  ggtitle(\"Popularity of tokens by language rank and usage\") +\n  xlab(\"Highest Rank of language using Token\") +\n  ylab(\"Number of Languages using token\") +\n  theme_classic()\n\n\n\n\nThis is a visualization of the highest ranked languages for each token. The number of users of the dominant language is also encoded in the size and color of the label. Having it ordered makes it difficult to tell if Java or Python is the most popular/ highest ranked language.\n\njoin_madness %&gt;%\n  ggplot(aes(fct_rev(fct_reorder(line_comment_token, highest_rank)), n,\n             size = log(number_of_users), color = log(number_of_users),\n             label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\nHere is the same graph just ordered “alphabetically” by token.\n\njoin_madness %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E.},\n  title = {TidyTuesday {Week} 12: {Programming} {Languages}},\n  date = {2023-03-21},\n  url = {https://lsinks.github.io/posts/2023-03-21-tidytuesday-programming-languages/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E. 2023. “TidyTuesday Week 12: Programming\nLanguages.” March 21, 2023. https://lsinks.github.io/posts/2023-03-21-tidytuesday-programming-languages/."
  },
  {
    "objectID": "posts/2023-04-18-tidytuesday-founder-crops/founder-crops.html",
    "href": "posts/2023-04-18-tidytuesday-founder-crops/founder-crops.html",
    "title": "TidyTuesday Week 16: Neolithic Founder Crops",
    "section": "",
    "text": "Today’s TidyTuesday relates to the use of plants in the Neolithic era and is based on a paper by Arranz-Otaegul and Roe.\nThe authors have made their data and analysis available on GitHub. The methods for generating all the figures and tables are in an RMarkdown document with some explanatory text. Having just recently looked at the code and data for one of my older papers, I now appreciate this markdown approach. Everything needed is linked together through the markdown document and the GitHub repo. My code is only as understandable with the paper notebook that resides in my former lab. Even though the code was commented, it is less clear many years later. I find markdown/ quarto a little difficult to code in. I’m not sure why- maybe the interspersed text and code is distracting. I usually code in R files for more complicated projects and then copy them into markdown/quarto. But I definitely appreciate markdown after working on today’s project.\n\nlibrary(skimr)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(gt)\n\nThe data can be downloaded through the tidytuesday package using tuesdata &lt;- tidytuesdayR::tt_load(2023, week = 16). Or the data and the code can be downloaded from the project’s Github repo. I actually don’t use the data from tidytuesday, but instead work off their analysis.\nTo briefly summarize the work of Arranz-Otaegul and Roe, in the 1980s the concept of “founder crops” was proposed. These founder crops were a set of crops that were generally all cultivated together and were wide spread as the first main agricultural crops. Arranz-Otaegul and Roe propose that with the increased data generated since the 80s, a more refined approach to the beginnings of argriculture can be taken. They break down agriculture into several different processes (selection of plants, domesitcation, plant management, etc.) which occurs over longer periods of time rather than the development agriculture occuring as a single, rapid event. Arranz-Otaegul and Roe examine the plant remains reported from 135 sites over a wide range of periods in the neolithic era. They find that founder crops don’t necessarily come as a “package”, that a variety of other plants were fairly important, and that the increased cultivation of wheat was the dominate change in the types plants used during the neolithic period. I should note, I’m a chemist, not an archeology/ archeobotanist, so this is not the most nuanced summary of their work.\nI was particularly intrigued by their conclusion about wheat, which is graphically represented in Figure 4 of their paper. I found the figure a bit unclear; stacked area plots are not common in chemistry and I’m not expert at reading them. Wheat certainly does increase with time, but it was hard for me to tell what the other crops were doing. So, for tidytuesday, I wanted to explore this specific question- how does the use of wheat change over time.\nThe file you get from the tidytuesday package is the same as the “swasia_neolithic_flora.tsv” from the paper’s repo with some minor clean-up. I don’t want to replicate all there analysis here to get to the point of being able to make the figure I want, so I saved the object flora_ts after it was generated in line 256 of their SI1.RMD file. This has the data partitioned by century and has been reshaped.\n\nflora_ts &lt;- read_rds(\"flora_ts.rds\")\n\nThere are 8 founder crops. The original analysis binned together wheat varieties and I am also binning all the legumes together, just to make the resulting graph less busy. I’m also dropping flax. This code chunk is modified from their code chunk ts-founder-crops, which starts at line 573 and ends with saving figure 4. This chunk calculates the proportion of archaeological sites at which the crop was found at in each century. (This is called an assemblage in the paper.)\n\nfounder_crops_binned &lt;- flora_ts %&gt;%\n  # aggregate the legumes and the wheat\n  mutate(founder_crop = recode(founder_crop,\n                               \"einkorn wheat\" = \"wheat\",\n                               \"emmer wheat\" = \"wheat\",\n                               \"chickpea\" = \"legumes\",\n                               \"bitter vetch\" = \"legumes\",\n                               \"lentil\" = \"legumes\",\n                               \"pea\" = \"legumes\",\n                               \"flax\" = \"flax\",\n                               .default = founder_crop)) %&gt;%\n  filter(founder_crop != \"flax\") %&gt;%\n  # Aggregate by founder crops\n  group_by(century, site_name, phase_code, founder_crop) %&gt;%\n  summarise(prop = sum(prop, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  # Add number of assemblages per century\n  group_by(century) %&gt;%\n  mutate(n_assemb = length(unique(phase_code))) %&gt;%\n  # Calculate average proportion\n  group_by(century, founder_crop) %&gt;%\n  summarise(avg_prop = sum(prop) / first(n_assemb))\n\nDrop the NAs. This corresponds to all the plants which are not founder crops.\n\nfounder_crops_only &lt;- founder_crops_binned %&gt;%\n  drop_na(founder_crop)\n\nFirst, I wanted to see roughly how common each crop was over all time.\n\nfounder_crops_only %&gt;%\n  group_by(founder_crop) %&gt;%\n  summarize(pct = round(mean(avg_prop), 2) * 100) %&gt;%\n  gt() %&gt;%\n  cols_label(founder_crop = \"Founder Crop\", pct = \"% of Sites\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Mean Frequency of Crops over Time\") \n\n\n\n\n\n  \n    \n      Mean Frequency of Crops over Time\n    \n    \n    \n      Founder Crop\n      % of Sites\n    \n  \n  \n    barley\n9\n    legumes\n6\n    wheat\n15\n  \n  \n  \n\n\n\n\nNow, I’m going to create a scatter plot with a trendline as a guide for the eyes. I’m using facet wrap, so each crop is on its own plot.\n\nfounder_crops_only  %&gt;% ggplot(aes(century / 1000, avg_prop, color = founder_crop)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(~ founder_crop, nrow = 3) +\n  scale_x_reverse(\n    breaks = scales::breaks_width(-1),\n    limits = c(11.7, 6.5),\n    expand = expansion(0)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 0.3),\n    breaks = scales::breaks_width(0.1),\n    labels = scales::label_percent(accuracy = 1),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  theme_pander() +\n  theme(legend.position = \"none\") +\n  labs(x = \"ka cal BP\", y = \"Mean proportion of assemblages\", fill = NULL) %&gt;%\n  labs(title = \"Frequency of Crops in SW Asia during the Neolithic Period\", caption = \"data from https://github.com/joeroe/SWAsiaNeolithicFounderCrops\")\n\n\n\n\nSo, it looks like barley and legumes are fairly constant with time, but wheat does increase consistently. A note about the x-axis- the times are listed as before present (BP), so a larger number is longer ago. Present is defined as January 1, 1950.\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E.},\n  title = {TidyTuesday {Week} 16: {Neolithic} {Founder} {Crops}},\n  date = {2023-04-18},\n  url = {https://lsinks.github.io/posts/2023-04-18-tidytuesday-founder-crops/founder-crops.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E. 2023. “TidyTuesday Week 16: Neolithic Founder\nCrops.” April 18, 2023. https://lsinks.github.io/posts/2023-04-18-tidytuesday-founder-crops/founder-crops.html."
  },
  {
    "objectID": "posts/2023-06-19-tidytuesday-UFOs/UFOs.html",
    "href": "posts/2023-06-19-tidytuesday-UFOs/UFOs.html",
    "title": "TidyTuesday Week 25: UFO Sightings Redux",
    "section": "",
    "text": "I haven’t been TidyTuesdaying because I’ve been learning Tableau. I’ll write more about that later; but it has been an interesting experience and provides different perspectives on data compared to what you might get from R. (I’m sure you could reproduce everything in Tableau in R and vice versa, but it is certainly easier to perform certain actions in one program over the other.)\nToday’s TidyTuesday is based on a dataset about the UFO sightings. This is an updated version of a tidytuesday challenge from 2019. The readme suggests that differences between the two datasets might be especially interesting.\n\nlibrary(skimr)\nlibrary(tidyverse)\nlibrary(ggthemes)\n\nLoad the data with the TidyTuesday package in the usual way.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 25)\n\n--- Compiling #TidyTuesday Information for 2023-06-20 ----\n\n\n--- There are 3 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `ufo_sightings.csv`\n    Downloading file 2 of 3: `places.csv`\n    Downloading file 3 of 3: `day_parts_map.csv`\n\n\n--- Download complete ---\n\nufo_sightings &lt;- tuesdata$`ufo_sightings`\nplaces &lt;- tuesdata$`places`\nday_parts_map &lt;- tuesdata$`day_parts_map`\n\nThis dataset combines information about reported UFO sightings with information about the lighting conditions at that time from sunrise-sunset. That is, was it day time, night time, twilight, etc. when the sighting occurred. This is an augmentation of the original dataset.\n\nskim(ufo_sightings)\n\n\nData summary\n\n\nName\nufo_sightings\n\n\nNumber of rows\n96429\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\nDate\n1\n\n\nlogical\n1\n\n\nnumeric\n1\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncity\n0\n1.00\n3\n26\n0\n10721\n0\n\n\nstate\n85\n1.00\n2\n31\n0\n684\n0\n\n\ncountry_code\n0\n1.00\n2\n2\n0\n152\n0\n\n\nshape\n2039\n0.98\n3\n9\n0\n24\n0\n\n\nreported_duration\n0\n1.00\n2\n25\n0\n4956\n0\n\n\nsummary\n31\n1.00\n1\n135\n0\n95898\n0\n\n\nday_part\n2563\n0.97\n5\n17\n0\n9\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nposted_date\n0\n1\n1998-03-07\n2023-05-19\n2012-08-19\n619\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nhas_images\n0\n1\n0\nFAL: 96429\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nduration_seconds\n0\n1\n31613.25\n6399774\n0\n30\n180\n600\n1987200000\n▇▁▁▁▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreported_date_time\n0\n1\n1925-12-29\n2023-05-18 19:27:00\n2012-02-05 03:00:00\n86201\n\n\nreported_date_time_utc\n0\n1\n1925-12-29\n2023-05-18 19:27:00\n2012-02-05 03:00:00\n86201\n\n\n\n\n\nThe data is fairly complete. The daypart is the least complete with only 97% completion. This variable is currently a string; it would be better as a factor.\n\nufo_sightings$day_part &lt;- as.factor(ufo_sightings$day_part)\n\nNow we can see when it is most common to see UFOs.\n\nggplot(ufo_sightings, aes(day_part)) +\n  geom_bar()\n\n\n\n\nThis is a pretty ugly plot. First, I’m going to clean up the formatting. The day-parts should be tilted so they are not overlapping, the axis labels are not clear, and I don’t like the default ggplot theme. You need to change the plot theme before you tilt the axis labels, otherwise the theme’s defaults will reset the axis label orientation.\n\nggplot(ufo_sightings, aes(day_part)) +\n  geom_bar() +\n  ylab(\"Number of Sighting\") +\n  xlab(\"\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1)) \n\n\n\n\nNow, there are two ways to order the bars. Usually, I’d use either ascending or descending order by count. But here, the day parts do have an intrinsic order- most light to least light (or the reverse) or something along the cycle. So, I need to order the factor day_part. I used the explanation here get the order and decide on the color scale. I decided that afternoon was lighter than morning.\nFirst, I’ll code the NAs as “unknown”. It is a small percentage of the total data, so dropping them is also a defensible choice. It doesn’t really impact the visualization to leave them in, and it does reveal some information about how detailed the reports are.\nI love the POSIT cheatsheets for checking syntax, especially the printed pdfs because I annotate them with my own notes. However, they can end up out of date. Recently POSIT has created HTML versions and updated most of the pdfs. So if you have a printed forcats cheatsheet branded RStudio, go get the new POSIT’s one, since some of the functions on the old cheatsheet are deprecated.\nTo convert NAs to a named level, the current function is fct_na_value_to_level rather than fct_explicit_na and the level is specified with level = \"blah\" rather than na_level = \"blah\". Both ways are shown in the code block and both will work.\n\n# old way of converting NAs to a specific level\n#ufo_sightings$day_part &lt;- fct_explicit_na(ufo_sightings$day_part, na_level = #\"unknown\")\n\n# new way of converting NAs to a specific level\nufo_sightings$day_part &lt;- fct_na_value_to_level(ufo_sightings$day_part, level = \"unknown\")\n\nNow, I need to relevel the day_parts factor. I started with morning, and then progressed through the day_parts in the order they occur. I put the unknown level last.\n\nufo_sightings$day_part &lt;- fct_relevel(ufo_sightings$day_part,c(\"morning\",\n                                      \"afternoon\", \"civil dusk\", \"nautical dusk\",\n                                      \"astronomical dusk\", \"night\",\n                                      \"astronomical dawn\", \"nautical dawn\",\n                                      \"civil dawn\", \"unknown\"))\n\nNow, I’m going to use the aesthetic fill = day_part to color my bars by time of day. I’m also going to define a manual color scale in blues, grays, and blacks to reflect the state of the sky- light blues for daylight and darker blues/blacks for night times. I made the unknown green (for little green men). I played around with both colors and values to get a gradient that I liked. Note that both gray and grey work. I kept the legend on while I was adjusting the colors because it was easier to see the gradient as compared to looking at the bar chart, but I turned it off for the final graph.\n\nggplot(ufo_sightings, aes(day_part, fill = day_part)) +\n  geom_bar() +\n  ylab(\"Number of Sighting\") +\n  xlab(\"\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1)) +\n  scale_fill_manual(values = c(\"lightskyblue2\", \"lightskyblue1\", \"skyblue2\",\n \"skyblue3\",\"lightskyblue4\", \"grey23\",\"lightskyblue4\",\"skyblue3\",\"skyblue2\",\n \"green\")) +\n  ggtitle(\"When do UFO Sightings Occur?\") +\n  theme(legend.position = \"none\")\n\n\n\n\nSo most sightings take place at night and very few take place in the early morning.\nThe sightings database also contains pictures of UFOs. Not all reports include pictures. It seems like it would be hard to get a good picture at night, so I’m wondering if most pictures are taken during daylight hours, despite having fewer over all sightings.\nhas_images is a boolean and the dataset is complete, but it seems that everything is coded FALSE? The original dataset from 2019 doesn’t include has_images, so I can’t use that to repair most of the entries.\n\nufo_sightings %&gt;%\n  group_by(day_part) %&gt;%\n  summarise(mean(has_images))\n\n# A tibble: 10 × 2\n   day_part          `mean(has_images)`\n   &lt;fct&gt;                          &lt;dbl&gt;\n 1 morning                            0\n 2 afternoon                          0\n 3 civil dusk                         0\n 4 nautical dusk                      0\n 5 astronomical dusk                  0\n 6 night                              0\n 7 astronomical dawn                  0\n 8 nautical dawn                      0\n 9 civil dawn                         0\n10 unknown                            0\n\n\nLooking at how the current dataset was prepared doesn’t help much either. The data was rescraped from National UFO Reporting Center as shown here and does not seem to include the original tidytuesday dataset at all. Searching for has_image, I find it first mentioned in line 890.\ndata_ufo_reports_clean &lt;- data_ufo_reports_durations |&gt;\ndplyr::mutate(\nstate = dplyr::coalesce(state, ascii_state),\n# Recode \"images\" to TRUE/FALSE, and replace NAs while we're at it.\nhas_images = isTRUE(images == \"Yes\"),\nLooking at the saved scraped data, found here, there is an images column that contains “Yes” or NA. It is very incomplete, with a completion rate of 1%. I also looked at the final cleaned data (post encoding to TRUE/FALSE) and it only has FALSES. So, it looks like the re-coding might have gone wrong.\nHere is the original data from the scraping.\n\nurl &lt;- \"https://github.com/jonthegeek/apis/raw/main/data/data_ufo_reports.rds\"\nufo_path &lt;- withr::local_tempfile(fileext = \".rds\")\ndownload.file(url, ufo_path)\ndata_ufo_reports_1 &lt;- readRDS(ufo_path)\n\nSee how many images are in this dataset.\n\ndata_ufo_reports_1 %&gt;%\n  filter(images == \"Yes\") %&gt;%\n  nrow()\n\n[1] 2342\n\n\nThere are only 2342 images in the database. The raw scrapped data has 144451 rows, while our cleaned processed data has 96429 rows. So a fair number of data points were dropped. It is possible that all of the records with images were dropped during the cleaning and processing.\nI’m going to join the UFO dataset with the scrapped data. I’m letting R decide what columns to join on, but the join does need to include summary, since that is one column that certainly was not cleaned/ processed.\nThe place names were cleaned (see for example lines 277 and beyond in the cleaning code). When I do an inner_join, I only get 1881 records. I’d expect to get the 96429 records in the ufo_sightings data file, since they should be contained in the original larger dataset.\n\ncombined &lt;- ufo_sightings %&gt;%\n  inner_join(data_ufo_reports_1) \n\nJoining with `by = join_by(city, state, shape, summary)`\n\ncombined %&gt;%\n  nrow()\n\n[1] 1881\n\n\nIf I were working on this for something more mission critical, I’d dig through the cleaning code more carefully, and make sure it was all ok. For now, I’m just going to check the Boolean encoding step, using my much smaller combined dataframe.\n\n# counting how many images\ncombined %&gt;%\n  filter(images == \"Yes\") %&gt;%\n  nrow()\n\n[1] 18\n\n# this is the re-coding step from the tidytuesday data\ncombined2 &lt;- combined %&gt;%\n  dplyr::mutate(\n       # Recode \"images\" to TRUE/FALSE, and replace NAs while we're at it.\n    has_images_tester = isTRUE(images == \"Yes\"))\n\n# counting how many images again\ncombined2 %&gt;%\n  filter(images == \"Yes\") %&gt;%\n  nrow()\n\n[1] 18\n\n# counting how many images after recode\ncombined2 %&gt;%\n  filter(has_images_tester == TRUE) %&gt;%\n  nrow()\n\n[1] 0\n\n\nSo, the specified code clearly doesn’t work. This is how I’d do it. I’d explicitly code both TRUE and FALSE using an ifelse clause in the mutate. Since the images contains a bunch of NAs, you need to be more careful about your test condition: images == \"Yes\" does give you the correct TRUEs, but it gives no FALSEs and retains all the NAs.\n\ncombined3 &lt;- combined2 %&gt;%\n  dplyr::mutate(\n       # Recode \"images\" to TRUE/FALSE, and replace NAs while we're at it.\n    has_images_correct1 = ifelse(images == \"Yes\", TRUE, FALSE))\n\ncombined3 %&gt;%\n  filter(has_images_correct1 == TRUE) %&gt;%\n  nrow()\n\n[1] 18\n\ncombined3 %&gt;%\n  filter(has_images_correct1 == FALSE) %&gt;%\n  nrow()\n\n[1] 0\n\n\nThe better test condition for this dataset is to use is.na(images) == FALSE (or the reverse, it doesn’t matter) and code NAs as FALSE and everything else as TRUE. This works because we have only have two values in the column (NA/ Yes). If you had other correct values, say Yes/ No/ NA, then this would not work. You’d need more complicated logic or two sets of ifelse.\n\ncombined4 &lt;- combined3 %&gt;%\n  dplyr::mutate(\n       # Recode \"images\" to TRUE/FALSE, and replace NAs while we're at it.\n    has_images_correct2 = ifelse(is.na(images) == FALSE, TRUE, FALSE))\n\ncombined4 %&gt;%\n  filter(has_images_correct2 == TRUE) %&gt;%\n  nrow()\n\n[1] 18\n\ncombined4 %&gt;%\n  filter(has_images_correct2 == FALSE) %&gt;%\n  nrow()\n\n[1] 1863\n\n\nSo this method of recoding works. My approach would be to first copy the entire processing code from line 140 onwards and update the coding of images at 890 and re-run the whole thing and see what was produced. But more detailed checking of other steps might be required, depending on the application.\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E.},\n  title = {TidyTuesday {Week} 25: {UFO} {Sightings} {Redux}},\n  date = {2023-06-20},\n  url = {https://lsinks.github.io/posts/2023-06-19-tidytuesday-UFOs/UFOs.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E. 2023. “TidyTuesday Week 25: UFO Sightings\nRedux.” June 20, 2023. https://lsinks.github.io/posts/2023-06-19-tidytuesday-UFOs/UFOs.html."
  },
  {
    "objectID": "posts/2023-08-01-tidytuesday-US-states/states.html",
    "href": "posts/2023-08-01-tidytuesday-US-states/states.html",
    "title": "States",
    "section": "",
    "text": "Today’s TidyTuesday takes some data about US states from Wikipedia.\nLast week I saw this post on Mastodon and was really intrigued by the package mentioned, GWalkr. I thought this package looked very cool, so I put it on my to try list.\n\nEmbedding Social Media Posts in Quarto\nAs a side note, I used the Quarto social embeds extension to show the Mastodon toot. I use RStudio to compose my quarto documents, so to install it for this project, I run the code given at the terminal (not console) window in RStudio. I did see that the toot overflowed the text below it, so you may need to adjust the spacing with some hard returns. The specific code to render the toot is found here.\nWhile it worked(ish) I discovered that it overflowed the following text, even with lots of space and hard returns entered after.\nI also found that the actual link to the toot kept disappearing and defaulting back to the server name only (e.g. https://fosstodon.org/) which then refused the connection.\nTo uninstall a quarto extension, use quarto remove extension from the terminal and select the one you want from the list or use quarto remove extension - NAME_OF_EXTENSION.\nSomething also broke the formatting of this post. Headers, spacing, and justification are all different. Removing the extension did not restore this, so I will dig more later into this. (I suppose it could be the vis config for GWalkR, but removing that also did not fix the problem.)\n\n\nInstalling GWalkR\nThe GWalkR package isn’t available on CRAN yet, so you’ll need to install it via the devtools package using the following code: devtools::install_url(\"https://kanaries-app.s3.ap-northeast-1.amazonaws.com/oss/gwalkr/GWalkR_latest.tar.gz\")\n\nlibrary(tidyverse)\nlibrary(GWalkR)\n\nLoading the tidytuesday data.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 31)\n\nstates &lt;- tuesdata$states\nstate_name_etymology &lt;- tuesdata$state_name_etymology\n\nGWalkR is a interactive EDA tool styled similarly to Tableau. It is a lighter weight version of Graphics Walker and is designed to work in R. To use it, you just call it on the dataframe.\nI’ve created 3 simple visualizations that I will walk you through. You can interact with any of them and change the data as you wish. This is designed to be an exploratory interactive tool. It isn’t the best option to create static, polished figures.\nAnother side note, if you are doing this in quarto, you may wish to use the #| column: page option to give the gwalkr panel more room.\n\ngwalkr(states)\n\n\n\n\n\n\n\nMaking your Viz “Stick”\nThis part is only necessary if you are publishing or sharing your visualizations. When you run gwalkr you first get the empty explorer/builder window. Once you have created the visualizations, click on the export configuration button (like &lt;&gt;) and copy the text into a code block. This initializes GWalkr to the way you created it. Without this, you end up with an empty viewer every time you run the code. I’m including the entire configuration here, since this is a semi-tutorial format, but you’d probably want to hide this in your markdown/ quarto document. This code chunk also works find when I am rendering my Quarto document, but fails when I am executing code normally.\n\nvisConfig &lt;- '[{\"visId\":\"gw_Gtm3\",\"name\":\"Bar Chart of State Pop.\",\"encodings\":{\"dimensions\":[{\"dragId\":\"gw_jx5q\",\"fid\":\"c3RhdGU=\",\"name\":\"state\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_3nGt\",\"fid\":\"cG9zdGFsX2FiYnJldmlhdGlvbg==\",\"name\":\"postal_abbreviation\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_FNnC\",\"fid\":\"Y2FwaXRhbF9jaXR5\",\"name\":\"capital_city\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_-YgP\",\"fid\":\"bGFyZ2VzdF9jaXR5\",\"name\":\"largest_city\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_QC9G\",\"fid\":\"YWRtaXNzaW9u\",\"name\":\"admission\",\"semanticType\":\"temporal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_hm9z\",\"fid\":\"ZGVtb255bQ==\",\"name\":\"demonym\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"}],\"measures\":[{\"dragId\":\"gw_FnuC\",\"fid\":\"cG9wdWxhdGlvbl8yMDIw\",\"name\":\"population_2020\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_hRWB\",\"fid\":\"dG90YWxfYXJlYV9taTI=\",\"name\":\"total_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_cUQK\",\"fid\":\"dG90YWxfYXJlYV9rbTI=\",\"name\":\"total_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_QH84\",\"fid\":\"bGFuZF9hcmVhX21pMg==\",\"name\":\"land_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_tzCn\",\"fid\":\"bGFuZF9hcmVhX2ttMg==\",\"name\":\"land_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_1ggm\",\"fid\":\"d2F0ZXJfYXJlYV9taTI=\",\"name\":\"water_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_0lnU\",\"fid\":\"d2F0ZXJfYXJlYV9rbTI=\",\"name\":\"water_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_eiwY\",\"fid\":\"bl9yZXByZXNlbnRhdGl2ZXM=\",\"name\":\"n_representatives\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_count_fid\",\"fid\":\"gw_count_fid\",\"name\":\"Row count\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\",\"computed\":true,\"expression\":{\"op\":\"one\",\"params\":[],\"as\":\"gw_count_fid\"}}],\"rows\":[{\"dragId\":\"gw_Tlb_\",\"fid\":\"c3RhdGU=\",\"name\":\"state\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"}],\"columns\":[{\"dragId\":\"gw_tHvs\",\"fid\":\"cG9wdWxhdGlvbl8yMDIw\",\"name\":\"population_2020\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"}],\"color\":[],\"opacity\":[],\"size\":[],\"shape\":[],\"radius\":[],\"theta\":[],\"details\":[],\"filters\":[],\"text\":[]},\"config\":{\"defaultAggregated\":true,\"geoms\":[\"auto\"],\"stack\":\"stack\",\"showActions\":false,\"interactiveScale\":false,\"sorted\":\"none\",\"zeroScale\":true,\"size\":{\"mode\":\"auto\",\"width\":320,\"height\":200},\"format\":{}}},{\"visId\":\"gw_uHPW\",\"name\":\"Area vs. Pop (Agg)\",\"encodings\":{\"dimensions\":[{\"dragId\":\"gw_fHm7\",\"fid\":\"c3RhdGU=\",\"name\":\"state\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_GIbq\",\"fid\":\"cG9zdGFsX2FiYnJldmlhdGlvbg==\",\"name\":\"postal_abbreviation\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_j9gr\",\"fid\":\"Y2FwaXRhbF9jaXR5\",\"name\":\"capital_city\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_UlJ3\",\"fid\":\"bGFyZ2VzdF9jaXR5\",\"name\":\"largest_city\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_7t-q\",\"fid\":\"YWRtaXNzaW9u\",\"name\":\"admission\",\"semanticType\":\"temporal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw__0zS\",\"fid\":\"ZGVtb255bQ==\",\"name\":\"demonym\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"}],\"measures\":[{\"dragId\":\"gw_5ToL\",\"fid\":\"cG9wdWxhdGlvbl8yMDIw\",\"name\":\"population_2020\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_hIf6\",\"fid\":\"dG90YWxfYXJlYV9taTI=\",\"name\":\"total_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_MxWn\",\"fid\":\"dG90YWxfYXJlYV9rbTI=\",\"name\":\"total_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_YbvT\",\"fid\":\"bGFuZF9hcmVhX21pMg==\",\"name\":\"land_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_nnL-\",\"fid\":\"bGFuZF9hcmVhX2ttMg==\",\"name\":\"land_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_zJz3\",\"fid\":\"d2F0ZXJfYXJlYV9taTI=\",\"name\":\"water_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_67yP\",\"fid\":\"d2F0ZXJfYXJlYV9rbTI=\",\"name\":\"water_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_ta24\",\"fid\":\"bl9yZXByZXNlbnRhdGl2ZXM=\",\"name\":\"n_representatives\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_count_fid\",\"fid\":\"gw_count_fid\",\"name\":\"Row count\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\",\"computed\":true,\"expression\":{\"op\":\"one\",\"params\":[],\"as\":\"gw_count_fid\"}}],\"rows\":[{\"dragId\":\"gw_YKoG\",\"fid\":\"dG90YWxfYXJlYV9taTI=\",\"name\":\"total_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"}],\"columns\":[{\"dragId\":\"gw_3HaQ\",\"fid\":\"cG9wdWxhdGlvbl8yMDIw\",\"name\":\"population_2020\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"}],\"color\":[],\"opacity\":[],\"size\":[],\"shape\":[],\"radius\":[],\"theta\":[],\"details\":[],\"filters\":[],\"text\":[]},\"config\":{\"defaultAggregated\":true,\"geoms\":[\"auto\"],\"stack\":\"stack\",\"showActions\":false,\"interactiveScale\":false,\"sorted\":\"none\",\"zeroScale\":true,\"size\":{\"mode\":\"auto\",\"width\":320,\"height\":200},\"format\":{}}},{\"visId\":\"gw_RuCs\",\"name\":\"Area vs. Pop (dis-agg)\",\"encodings\":{\"dimensions\":[{\"dragId\":\"gw_f-aN\",\"fid\":\"c3RhdGU=\",\"name\":\"state\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_T0HW\",\"fid\":\"cG9zdGFsX2FiYnJldmlhdGlvbg==\",\"name\":\"postal_abbreviation\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_Uga-\",\"fid\":\"Y2FwaXRhbF9jaXR5\",\"name\":\"capital_city\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_CZOt\",\"fid\":\"bGFyZ2VzdF9jaXR5\",\"name\":\"largest_city\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_NhIi\",\"fid\":\"YWRtaXNzaW9u\",\"name\":\"admission\",\"semanticType\":\"temporal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_kD6Y\",\"fid\":\"ZGVtb255bQ==\",\"name\":\"demonym\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"}],\"measures\":[{\"dragId\":\"gw_80B1\",\"fid\":\"cG9wdWxhdGlvbl8yMDIw\",\"name\":\"population_2020\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_oncj\",\"fid\":\"dG90YWxfYXJlYV9taTI=\",\"name\":\"total_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_p-Ch\",\"fid\":\"dG90YWxfYXJlYV9rbTI=\",\"name\":\"total_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_MtXt\",\"fid\":\"bGFuZF9hcmVhX21pMg==\",\"name\":\"land_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_fGM4\",\"fid\":\"bGFuZF9hcmVhX2ttMg==\",\"name\":\"land_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_s6UR\",\"fid\":\"d2F0ZXJfYXJlYV9taTI=\",\"name\":\"water_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_D1e8\",\"fid\":\"d2F0ZXJfYXJlYV9rbTI=\",\"name\":\"water_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_4hWE\",\"fid\":\"bl9yZXByZXNlbnRhdGl2ZXM=\",\"name\":\"n_representatives\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_count_fid\",\"fid\":\"gw_count_fid\",\"name\":\"Row count\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\",\"computed\":true,\"expression\":{\"op\":\"one\",\"params\":[],\"as\":\"gw_count_fid\"}}],\"rows\":[{\"dragId\":\"gw_g_M-\",\"fid\":\"dG90YWxfYXJlYV9taTI=\",\"name\":\"total_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"}],\"columns\":[{\"dragId\":\"gw_A9lF\",\"fid\":\"cG9wdWxhdGlvbl8yMDIw\",\"name\":\"population_2020\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"}],\"color\":[],\"opacity\":[],\"size\":[],\"shape\":[],\"radius\":[],\"theta\":[],\"details\":[{\"dragId\":\"gw_Fv7k\",\"fid\":\"c3RhdGU=\",\"name\":\"state\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"}],\"filters\":[],\"text\":[]},\"config\":{\"defaultAggregated\":true,\"geoms\":[\"auto\"],\"stack\":\"stack\",\"showActions\":false,\"interactiveScale\":false,\"sorted\":\"none\",\"zeroScale\":true,\"size\":{\"mode\":\"auto\",\"width\":320,\"height\":200},\"format\":{}}}]'\ngwalkr(data=states, visConfig=visConfig)\n\n\n\n\n\n\n\nBar Chart of State Population\nGWalkr produces a Tableau style interface. To graph something, just drag variables to X-axis and Y-axis. I did population (x) and state (y). The default setting is to autogenerate the best possible visualization, so you end up with the bar chart. You can transpose the axes using the circular arrows button at the top of the screen- probably not the best choice for this dataset though.\nIt doesn’t look like you can join data in GWalkR yet, so I’m going to do that now in R.\n\nstates_joined &lt;- states %&gt;% left_join(state_name_etymology)\n\nJoining with `by = join_by(state)`\n\n\nNow explore by starting gwalkr the same way.\n\ngwalkr(states_joined)\n\n\n\n\n\nNotice that it starts up with the visualizations I created before. This is because of the config code. It applies to all instances of GWalkr. It is a bit odd since this uses a different dataframe, but in this case it doesn’t matter.\n\n\nAggregated and Dis-Aggregated Data\nThe second and third visualization explore the idea of aggregated and dis-aggregated data. Visualize total_area vs. population- you get the very Tableau result of a single point. Tableau and similar BI platforms almost always summarize/ group/ aggregate the data automatically. You can see that in the GWalkR interface too- the X-axis and Y-Axis now have sum next to the variables. To get the scatter plot you might be expecting, drag states to “Details”, which is found in the column between the variables and the visualization. Providing a unique identifier disaggregates the data. If you hove your mouse over a point, it tells you the state, the population and the area associated with the point.\n\n\nConclusions\nI think some people think in a way where this type of interactive exploratory data analysis really works with their workflow. It is certainly faster to drag and drop a bunch of different combos to see what visualizations are most informative, especially compared to creating 15 ggplots and then deleting most of them. I also think this type of tool will probably frustrate other folks.\n\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E.},\n  title = {States},\n  date = {2023-08-01},\n  url = {https://lsinks.github.io/posts/2023-08-01-tidytuesday-US-States/states.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E. 2023. “States.” August 1, 2023. https://lsinks.github.io/posts/2023-08-01-tidytuesday-US-States/states.html."
  }
]