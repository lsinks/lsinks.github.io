[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Entries",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nThoughts on Building a Quarto Website\n\n\n\n\n\n\n\nR\n\n\nquarto\n\n\nturtle\n\n\n\n\nSome tips and tricks for using Quarto and RStudio to create a portfolio website and blog.\n\n\n\n\n\n\nNov 1, 2023\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to Web Scraping Using rvest\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ndata visualization\n\n\ntombstone\n\n\nregex\n\n\nweb scraping\n\n\nrvest\n\n\ndata cleaning\n\n\ntidyTuesday\n\n\n\n\nWeb scraping data with rvest to enhance the information in the Tombstone Project.\n\n\n\n\n\n\nSep 8, 2023\n\n\n30 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday 36: Visualizing Worker Demographic Information with Treemaps\n\n\n\n\n\n\n\ndata visualization\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\ntreemap\n\n\ntidyTuesday\n\n\nd3treeR\n\n\ninteractive\n\n\n\n\nUsing treemap and d3treeR to create static and dynamic treemaps\n\n\n\n\n\n\nSep 5, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday 35: Exploring Fair Use Cases\n\n\n\n\n\n\n\ndata visualization\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\ntidyTuesday\n\n\nstringr\n\n\ndata cleaning\n\n\ndual axis plot\n\n\nggplot\n\n\ncowplot\n\n\n\n\nUsing stringr::str_detect() functions to explore why variables from different datasets don’t exactly match.\n\n\n\n\n\n\nAug 29, 2023\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday: Exploring Refugee Flow with A Sankey Diagram\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\nsankey\n\n\nnetworkD3\n\n\nggsankey\n\n\nhtmlwidgets\n\n\n\n\nTidyTuesday 34: Looking at the United Nations High Commissioner for Refugees data with a Sankey Diagram. Diagram created with networkD3 in R.\n\n\n\n\n\n\nAug 28, 2023\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nMapping for the Tombstone Project\n\n\n\n\n\n\n\ndata visualization\n\n\nR\n\n\nR-code\n\n\nleaflet\n\n\nquarto\n\n\nproblem solving\n\n\nmapping\n\n\ncode-along\n\n\nspatial join\n\n\nwordPress\n\n\nleafpop\n\n\nst_jitter\n\n\n\n\nMethods for dealing with overlapping points and then styling a leaflet map with various labels and pop-ups.\n\n\n\n\n\n\nAug 15, 2023\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday Twofer (32 and 33)\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nTidyTuesday\n\n\ncorrplot\n\n\n\n\nLooking at how heat levels increase on the show The Hot Ones. Then doing some EDA on a data set on spam emails.\n\n\n\n\n\n\nAug 15, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nData Cleaning for the Tombstone Project\n\n\n\n\n\n\n\ndata visualization\n\n\nR\n\n\nR-code\n\n\nfuzzyjoin\n\n\nquarto\n\n\nleaflet\n\n\nregex\n\n\nstringr\n\n\ndata cleaning\n\n\nproblem solving\n\n\nmapping\n\n\nsf\n\n\ncode-along\n\n\n\n\nUsing StringR to clean a human created excel sheet full of typos and formatting inconsistencies. Then matching excel data to photo names.\n\n\n\n\n\n\nAug 4, 2023\n\n\n51 min\n\n\n\n\n\n\n  \n\n\n\n\nStates\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nTidyTuesday\n\n\ntidy\n\n\ngwalkr\n\n\ntableau Alternative\n\n\nexploratory data analysis\n\n\ninteractive\n\n\n\n\nUsing GWalkR to create an interactive exploratory data analysis window similar to Tableau.\n\n\n\n\n\n\nAug 1, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nA Heatmap of James Lind’s Scurvy Study\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nTidyTuesday\n\n\ntidy\n\n\nheatmap\n\n\nggplot\n\n\ntidyverse\n\n\nggthemes\n\n\n\n\nMaking a Heatmap of James Lind’s Scurvy Study using ggplot. Data cleaning and reformating handled through tidyverse packages.\n\n\n\n\n\n\nJul 25, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 28: Global Surface Temperature\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nTidyTuesday\n\n\ntidy\n\n\ngganimate\n\n\n\n\nMaking an animated graph of global temperature change over time with gganimate.\n\n\n\n\n\n\nJul 11, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 27: Historical Markers\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ndata visualization\n\n\nsf\n\n\nleaflet\n\n\n\n\nTidyTuesday: Interactive Map of Arlington Historic Markers\n\n\n\n\n\n\nJul 4, 2023\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday Revisited: Interactive Map of Arlington Historic Neighborhoods\n\n\n\n\n\n\n\nR\n\n\ntidyTuesday\n\n\nR-code\n\n\ndata visualization\n\n\nsf\n\n\nleaflet\n\n\n\n\nTidyTuesday: Interactive Map of Historic Neighborhoods of Arlington Virginia\n\n\n\n\n\n\nJun 29, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: US Populated Places\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ndata visualization\n\n\nopenxlsx\n\n\nstringr\n\n\nfuzzyjoin\n\n\nmapview\n\n\nsf\n\n\n\n\nTidyTuesday: Historic Neighborhoods of Arlington Virginia\n\n\n\n\n\n\nJun 27, 2023\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 25: UFO Sightings Redux\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\n\n\nTidyTuesday: UFO Sightings\n\n\n\n\n\n\nJun 20, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 18: Portal Project\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\ndata validation\n\n\nexploratory data analysis\n\n\n\n\nTidyTuesday: Rodents of Portal Arizona\n\n\n\n\n\n\nMay 2, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 17: London Marathon\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\n\n\nTidyTuesday: Exploring the London Marathon\n\n\n\n\n\n\nApr 25, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 16: Neolithic Founder Crops\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ndata visualization\n\n\n\n\nTidyTuesday: Exploring early agriculture in SW Asia\n\n\n\n\n\n\nApr 18, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCredit Card Fraud: A Tidymodels Tutorial\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ntidymodels\n\n\nmachine learning\n\n\nclassifiers\n\n\n\n\nAn Imbalanced Class Problem\n\n\n\n\n\n\nApr 11, 2023\n\n\n39 min\n\n\n\n\n\n\n  \n\n\n\n\nA Tidymodels Tutorial: A Structural Approach\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ntidymodels\n\n\nmachine learning\n\n\nhyperparameters\n\n\nworkflows\n\n\n\n\nExploring the different steps for modeling\n\n\n\n\n\n\nApr 10, 2023\n\n\n22 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge- Endangered Species\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\n30DayChartChallenge\n\n\nwaffle\n\n\n\n\nHow many species have been delisted?\n\n\n\n\n\n\nApr 4, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge- Flora and Fauna\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nturtle\n\n\n30DayChartChallenge\n\n\n\n\nHow Large are Different Types of Turtles?\n\n\n\n\n\n\nApr 3, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge -Arlington Parks\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nwaffle\n\n\n30DayChartChallenge\n\n\n\n\nWho Owns the Parks in Arlington Virgina?\n\n\n\n\n\n\nApr 2, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nSelf-Guided Learning through a Wordle Guess Generator: Part 1\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\n\n\nSkills improvement with a Wordle Guess Generator\n\n\n\n\n\n\nApr 1, 2023\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nSelf-Guided Learning through a Wordle Guess Generator: Part 2\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\n\n\nCurrent version of a Wordle Guess Generator\n\n\n\n\n\n\nApr 1, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nOne Class SVM\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nmachine learning\n\n\ncaret\n\n\nsvm\n\n\nclassifiers\n\n\nsupport vector machines\n\n\n\n\nOne Class SVM for Imbalanced Classes\n\n\n\n\n\n\nMar 30, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Daylight Savings Time\n\n\n\n\n\n\n\nR\n\n\ntidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\nggimage\n\n\nmaps\n\n\n\n\nTidyTuesday: Exploring Time Zones and Daylight Savings Time\n\n\n\n\n\n\nMar 28, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 12: Programming Languages Revisited\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\nskimr\n\n\n\n\nTidyTuesday: Polishing\n\n\n\n\n\n\nMar 24, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTwitter Cards\n\n\n\n\n\n\n\nquarto\n\n\nturtle\n\n\n\n\nMaking Twitter Cards\n\n\n\n\n\n\nMar 24, 2023\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 12: Programming Languages\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\n\n\nTidyTuesday: How to comment in Various Programming Languages\n\n\n\n\n\n\nMar 21, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Blog\n\n\n\n\n\n\n\nquarto\n\n\nR\n\n\nturtle\n\n\n\n\nTrying to create a Quarto Blog\n\n\n\n\n\n\nMar 14, 2023\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "",
    "text": "This week, I participated in my first #TidyTuesday challenge. My goal was to get something out on the day of the challenge rather than perfection. I did notice that the skimr output wasn’t formatted nicely on the webpage. Today, I’m going to delve into the skimr and Quarto documentation and make a nicer version of the output. Secondly, I’m going to fix up my final figure, which is the one I shared on social media:"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#skimr-to-understand-your-data",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#skimr-to-understand-your-data",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "Skimr to understand your data",
    "text": "Skimr to understand your data\nSkimr is a package that provides statistical summaries of the variables in your dataframe. It also provides information about the missingness of each variable.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(ggrepel)\nlibrary(visdat) # visualizing missing data in dataframe\n\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 12)\n\nlanguages &lt;- tuesdata$languages\n\n\nCustomizing the skim Output\nMy main objection is that the numerical summary is too wide and has a scroll bar. I especially want the histogram to be viewable on the first screen. I also don’t particularly care about all the quartile information; min and max are enough. If I want to delve more into the stats of a variable, I will do it another way, not with skimr.\nFirst, quarto lets you expand the output of the code chunk to fill the page via the option “#| column: page”, so I’ll do that. Next, I’ll create a custom skim function that drops the p25, p50, and p75 output from the summary of the numerical variables.\n\nmy_skim &lt;- skim_with(numeric = sfl(p25 = NULL, p50 = NULL, p75 = NULL)) \n\nmy_skim(languages)\n\n\nData summary\n\n\nName\nlanguages\n\n\nNumber of rows\n4303\n\n\nNumber of columns\n49\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n21\n\n\nlogical\n4\n\n\nnumeric\n24\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n4303\n0\n\n\ntitle\n0\n1.00\n1\n56\n0\n4267\n0\n\n\ndescription\n3480\n0.19\n4\n2273\n0\n811\n0\n\n\ntype\n0\n1.00\n2\n27\n0\n40\n0\n\n\ncreators\n3203\n0.26\n2\n253\n0\n985\n0\n\n\nwebsite\n2928\n0.32\n13\n131\n0\n1368\n0\n\n\ndomain_name\n3588\n0.17\n6\n32\n0\n700\n0\n\n\nreference\n2314\n0.46\n15\n251\n0\n1955\n0\n\n\ngithub_repo\n3402\n0.21\n25\n73\n0\n897\n0\n\n\ngithub_repo_description\n3438\n0.20\n4\n419\n0\n853\n0\n\n\ngithub_language\n3829\n0.11\n1\n30\n0\n474\n0\n\n\ngithub_language_tm_scope\n3837\n0.11\n4\n34\n0\n361\n0\n\n\ngithub_language_type\n3837\n0.11\n4\n11\n0\n4\n0\n\n\ngithub_language_ace_mode\n3838\n0.11\n1\n16\n0\n96\n0\n\n\ngithub_language_file_extensions\n3833\n0.11\n1\n606\n0\n466\n0\n\n\nwikipedia\n2731\n0.37\n32\n104\n0\n1566\n0\n\n\nwikipedia_summary\n2884\n0.33\n17\n6741\n0\n1407\n0\n\n\nwikipedia_related\n3145\n0.27\n1\n1761\n0\n1059\n0\n\n\nline_comment_token\n3831\n0.11\n1\n7\n0\n23\n0\n\n\norigin_community\n1190\n0.72\n3\n305\n0\n2232\n0\n\n\nfile_type\n3213\n0.25\n2\n6\n0\n4\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n3683\n0.14\n1.00\nTRU: 617, FAL: 3\n\n\nfeatures_has_semantic_indentation\n3722\n0.14\n0.11\nFAL: 516, TRU: 65\n\n\nfeatures_has_line_comments\n3765\n0.13\n0.96\nTRU: 517, FAL: 21\n\n\nis_open_source\n3792\n0.12\n0.89\nTRU: 453, FAL: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1991.11\n111.44\n-2000\n2023\n▁▁▁▁▇\n\n\ndomain_name_registered\n3801\n0.12\n2011.33\n7.02\n1990\n2023\n▁▃▃▇▆\n\n\nisbndb\n3217\n0.25\n7.71\n33.16\n0\n400\n▇▁▁▁▁\n\n\nbook_count\n0\n1.00\n2.08\n17.34\n0\n401\n▇▁▁▁▁\n\n\nsemantic_scholar\n3545\n0.18\n3.79\n8.32\n0\n52\n▇▁▁▁▁\n\n\nlanguage_rank\n0\n1.00\n2151.00\n1242.31\n0\n4302\n▇▇▇▇▇\n\n\ngithub_repo_stars\n3414\n0.21\n2127.40\n7554.02\n0\n88526\n▇▁▁▁▁\n\n\ngithub_repo_forks\n3417\n0.21\n261.29\n1203.00\n0\n23732\n▇▁▁▁▁\n\n\ngithub_repo_updated\n3418\n0.21\n2021.39\n1.76\n2012\n2023\n▁▁▁▁▇\n\n\ngithub_repo_subscribers\n3418\n0.21\n62.34\n200.88\n0\n2910\n▇▁▁▁▁\n\n\ngithub_repo_created\n3425\n0.20\n2015.84\n3.48\n2006\n2022\n▁▅▇▇▇\n\n\ngithub_repo_issues\n3518\n0.18\n123.03\n546.26\n0\n9522\n▇▁▁▁▁\n\n\ngithub_repo_first_commit\n3567\n0.17\n2014.74\n4.99\n1987\n2022\n▁▁▁▆▇\n\n\ngithub_language_repos\n3833\n0.11\n197134.67\n1226900.57\n0\n16046489\n▇▁▁▁▁\n\n\nwikipedia_daily_page_views\n2837\n0.34\n227.13\n783.55\n-1\n13394\n▇▁▁▁▁\n\n\nwikipedia_backlinks_count\n2877\n0.33\n318.55\n1635.29\n1\n34348\n▇▁▁▁▁\n\n\nwikipedia_page_id\n2893\n0.33\n9167847.21\n13506832.90\n928\n63063548\n▇▁▁▁▁\n\n\nwikipedia_appeared\n2958\n0.31\n1991.14\n17.03\n1830\n2019\n▁▁▁▃▇\n\n\nwikipedia_created\n3040\n0.29\n2005.75\n3.77\n2001\n2020\n▇▇▂▁▁\n\n\nwikipedia_revision_count\n3130\n0.27\n330.43\n813.26\n1\n10104\n▇▁▁▁▁\n\n\nlast_activity\n0\n1.00\n2000.62\n84.60\n-900\n2023\n▁▁▁▁▇\n\n\nnumber_of_users\n0\n1.00\n13771.26\n227712.95\n0\n7179119\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n422.18\n12572.99\n0\n771996\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n1482\n0.66\n0.00\n0.00\n0\n0\n▁▁▇▁▁\n\n\n\n\n\nThis output is much nicer. It is a bit wall of text though. I wouldn’t recommend using this in reports, but it is a useful tool when doing your initial dataset analysis. (As a side note, I have noticed skimr doesn’t work well on Kaggle. It performs as expected if you are in interactive mode, but it fails when you try to save the notebook or run non-interactively.)\n\n\nStyling skim output with gt\nIf, for some reason, you did need to include output/ visualizations about missingness in a report, I’d probably recreate visualizations or tables by class of variable, especially if you have many variables, as I do here.\nHere’s an example for numeric variables, of which there are 24 in the dataset. First, we will skim the data and then use the gt package to style the resulting dataframe as a table. I used a built-in style, but each table element can be individually customized.\n\nlanguages_numeric &lt;- languages %&gt;%\n  select_if(is.numeric)\n\nlang_numeric_skim &lt;- my_skim(languages_numeric)\n\nlang_numeric_skim %&gt;%\n  select(-skim_type)   %&gt;% \n  gt() %&gt;%\n  cols_label(n_missing = \"# Missing\", complete_rate = \"Completeness\", \n             numeric.mean = \"Mean\", numeric.sd = \"Standard Deviation\",\n             numeric.p0 = \"Min\", numeric.p100 = \"Max\",\n             numeric.hist = \"Histogram\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Summary of Numerical Variables in Languages\") \n\n\n\n\n\n  \n    \n      Summary of Numerical Variables in Languages\n    \n    \n    \n      skim_variable\n      # Missing\n      Completeness\n      Mean\n      Standard Deviation\n      Min\n      Max\n      Histogram\n    \n  \n  \n    appeared\n0\n1.0000000\n1.991105e+03\n1.114434e+02\n-2000\n2023\n▁▁▁▁▇\n    domain_name_registered\n3801\n0.1166628\n2.011333e+03\n7.021132e+00\n1990\n2023\n▁▃▃▇▆\n    isbndb\n3217\n0.2523821\n7.706262e+00\n3.316421e+01\n0\n400\n▇▁▁▁▁\n    book_count\n0\n1.0000000\n2.079479e+00\n1.734465e+01\n0\n401\n▇▁▁▁▁\n    semantic_scholar\n3545\n0.1761562\n3.794195e+00\n8.316231e+00\n0\n52\n▇▁▁▁▁\n    language_rank\n0\n1.0000000\n2.151000e+03\n1.242313e+03\n0\n4302\n▇▇▇▇▇\n    github_repo_stars\n3414\n0.2066000\n2.127403e+03\n7.554016e+03\n0\n88526\n▇▁▁▁▁\n    github_repo_forks\n3417\n0.2059029\n2.612867e+02\n1.203003e+03\n0\n23732\n▇▁▁▁▁\n    github_repo_updated\n3418\n0.2056705\n2.021390e+03\n1.763285e+00\n2012\n2023\n▁▁▁▁▇\n    github_repo_subscribers\n3418\n0.2056705\n6.234237e+01\n2.008820e+02\n0\n2910\n▇▁▁▁▁\n    github_repo_created\n3425\n0.2040437\n2.015843e+03\n3.479589e+00\n2006\n2022\n▁▅▇▇▇\n    github_repo_issues\n3518\n0.1824309\n1.230344e+02\n5.462553e+02\n0\n9522\n▇▁▁▁▁\n    github_repo_first_commit\n3567\n0.1710435\n2.014739e+03\n4.985409e+00\n1987\n2022\n▁▁▁▆▇\n    github_language_repos\n3833\n0.1092261\n1.971347e+05\n1.226901e+06\n0\n16046489\n▇▁▁▁▁\n    wikipedia_daily_page_views\n2837\n0.3406925\n2.271330e+02\n7.835524e+02\n-1\n13394\n▇▁▁▁▁\n    wikipedia_backlinks_count\n2877\n0.3313967\n3.185484e+02\n1.635289e+03\n1\n34348\n▇▁▁▁▁\n    wikipedia_page_id\n2893\n0.3276784\n9.167847e+06\n1.350683e+07\n928\n63063548\n▇▁▁▁▁\n    wikipedia_appeared\n2958\n0.3125726\n1.991144e+03\n1.702650e+01\n1830\n2019\n▁▁▁▃▇\n    wikipedia_created\n3040\n0.2935162\n2.005748e+03\n3.768240e+00\n2001\n2020\n▇▇▂▁▁\n    wikipedia_revision_count\n3130\n0.2726005\n3.304314e+02\n8.132556e+02\n1\n10104\n▇▁▁▁▁\n    last_activity\n0\n1.0000000\n2.000616e+03\n8.459776e+01\n-900\n2023\n▁▁▁▁▇\n    number_of_users\n0\n1.0000000\n1.377126e+04\n2.277129e+05\n0\n7179119\n▇▁▁▁▁\n    number_of_jobs\n0\n1.0000000\n4.221838e+02\n1.257299e+04\n0\n771996\n▇▁▁▁▁\n    central_package_repository_count\n1482\n0.6555891\n0.000000e+00\n0.000000e+00\n0\n0\n▁▁▇▁▁\n  \n  \n  \n\n\n\n\n\n\nVisualizing Missingness with visdat\nThe visdat package makes ggplot- compatible missingness plots. The cluster = TRUE option groups variables that share missingness. Here we see that usually if some of the GitHub data is missing, then all of the GitHub data is missing. The percent missing is listed for each variable, and the overall missingness of the dataset is shown in the legend.\nNote vis_miss doesn’t work on very large datasets. The documentation suggests keeping the number of records below 1,000. A more extensive package for exploratory visualizations called naniar could also be used.\n\nlanguages_numeric %&gt;%\nvis_miss(cluster = TRUE) +\nggtitle(\"Missing Data in the Languages Dataset\") +\n  #theme_classic() +\n  theme(axis.text.x = element_text(size = 8, angle = 90))"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#improving-the-most-popular-language-for-each-comment-token-figure",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#improving-the-most-popular-language-for-each-comment-token-figure",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "Improving “The Most Popular Language for Each Comment Token” Figure",
    "text": "Improving “The Most Popular Language for Each Comment Token” Figure\n\njoined &lt;- read_csv(\"processed_lang.csv\" , show_col_types = FALSE)\n\nNow the original figure:\n\njoined %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\nI thought I had noted this in the previous post, but one of the tokens, ⍝ , is rendered as an empty box in the ggplot figures. I thought fixing this would be easy. First, I thought I could just pass the Unicode value for that symbol. Then, when that didn’t work, I thought I could change the font to one supporting that symbol. Supposedly, changing the font should be easy, yet after 3 hours working on it, I still had blank squares. There is a nice tutorial on changing fonts in ggplot that did not work until I found someone with the same issue. The solution is to add a line of code that doesn’t make much sense to me : windowsFonts(\"Cambria Math\" = windowsFont(\"Cambria Math\"))\nI saw a nice TidyTuesday figure on Twitter:\n\n\nMy submission for #TidyTuesday, Week 12 on programming languages. I explore jobs per users.Code: https://t.co/bV9DUHZmro pic.twitter.com/2D5YLnE5yz\n\n— Mitsuo Shiota (@mitsuoxv) March 21, 2023\n\n\nwith a caption referencing the original dataset. I’d like to add that. I generally want to increase the figure’s legibility and flip the color scale so that darker blue corresponds to more users. I also don’t think what popular means is entirely clear, so I’d like to explain more fully what I’m graphing.\n\nwindowsFonts(\"Cambria Math\" = windowsFont(\"Cambria Math\"))\njoined %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n    scale_colour_gradient(high = \"#08306b\", low = \"#6baed6\") + \n   labs(title = \"The Most Popular Language for Each Comment Token\",\n       subtitle = \"Based on # Users and Rank\",\n       caption = \"data from https://pldb.com/\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic(base_size = 16) +\n  theme(text = element_text( family = \"Cambria Math\")) +\n  theme(axis.text.x = element_text(face = \"bold\"))"
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "",
    "text": "I’m still behind on TidyTuesday, so here is TidyTuesday #34, which deals with refugee data. This data is collected by the United Nations High Commissioner for Refugees and the R package is updated twice a year."
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#loading-data-and-libraries",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#loading-data-and-libraries",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "Loading Data and Libraries",
    "text": "Loading Data and Libraries\nLibraries.\n\nlibrary(tidyverse) # who doesn't want to be tidy\nlibrary(networkD3) # for Sankey plots\nlibrary(ggsankey) # another sankey package \nlibrary(htmlwidgets) # html widgets helps to handle the networkD3 objects\nlibrary(htmltools) # for formatting html code\n\nLoading data.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 34)\npopulation &lt;- tuesdata$population"
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#exploratory-data-analysis",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#exploratory-data-analysis",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThis dataset tracks the number of refugees by year. It tracks how many people request entry at each country and breaks this information down by country of origin.\n\nhead(population)\n\n# A tibble: 6 × 16\n   year coo_name    coo   coo_iso coa_name coa   coa_iso refugees asylum_seekers\n  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n1  2010 Afghanistan AFG   AFG     Afghani… AFG   AFG            0              0\n2  2010 Iran (Isla… IRN   IRN     Afghani… AFG   AFG           30             21\n3  2010 Iraq        IRQ   IRQ     Afghani… AFG   AFG            6              0\n4  2010 Pakistan    PAK   PAK     Afghani… AFG   AFG         6398              9\n5  2010 Egypt       ARE   EGY     Albania  ALB   ALB            5              0\n6  2010 China       CHI   CHN     Albania  ALB   ALB            6              0\n# ℹ 7 more variables: returned_refugees &lt;dbl&gt;, idps &lt;dbl&gt;, returned_idps &lt;dbl&gt;,\n#   stateless &lt;dbl&gt;, ooc &lt;dbl&gt;, oip &lt;dbl&gt;, hst &lt;dbl&gt;\n\n\nWhat years have the most data?\n\npopulation %&gt;%\n  group_by(year) %&gt;%\n  count() %&gt;%\n  ggplot(aes(year, n)) +\n  geom_col()\n\n\n\n\nI expected to see spikes corresponding to specific events, but instead, it looks like the data is gradually increasing. This upward trend might be due to better record-keeping/ data collection over the years.\nI’m going to look at last year’s data.\n\npop_2022 &lt;- population %&gt;%\n  filter(year == 2022)"
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#a-sankey-diagram-to-explore-flow",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#a-sankey-diagram-to-explore-flow",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "A Sankey Diagram to Explore Flow",
    "text": "A Sankey Diagram to Explore Flow\nNow, I will look at where most refugees come from.\nThere is a technical and legal difference between refugees and asylum seekers, but I will sum them up. I also will be using refugees and asylum seekers interchangeably here.\n\npop_2022 &lt;- pop_2022 %&gt;%\n  mutate(total = refugees + asylum_seekers) %&gt;%\n  select(coo_name, coa_name, total)\n\nI’ve been interested in learning how to make Sankey diagrams in R. A Sankey diagram shows the flow between nodes; there is a great gallery of examples here. This tidytuesday data does lend itself to a Sankey diagram. There are two nodes- country of orgin and country of arrival and the flow is the number of refugees between each pair.\nI do want to highlight that the UN Refugee Agency has a separate dataset that tracks flow. The dataset we are working with is the net number of refugees. As the UNHCR webpage says:\n\nOften the net increase or decrease in stock figures between years has been used in lieu of accurate flow figures. But these may underrepresent the true magnitude of population movements if, for example, refugee arrivals and departures balance each other out.\n\nStill, for TidyTuesday, the net population is fine.\nI’m going to pull out the top 3 sources of refugees. I’m grouping by the country of origin (coo_name) and then summing the refugees going to all countries. I then take the top 3 using dpylr’s slice_max(). Note that this function replaces top_n().\n\ntop_3_source &lt;- pop_2022 %&gt;%\n  group_by(coo_name) %&gt;%\n  summarize(num_by_coo = sum(total)) %&gt;%\n  slice_max(order_by = num_by_coo, n = 3)\n\ntop_3_source_names &lt;- top_3_source$coo_name\n\ntop_3_source_names\n\n[1] \"Syrian Arab Rep.\" \"Afghanistan\"      \"Ukraine\"         \n\n\nNow, I’m going to do a related analysis and see where the top destinations are.\n\ntop_3_dest &lt;- pop_2022 %&gt;%\n  group_by(coa_name) %&gt;%\n  summarize(num_by_coa = sum(total)) %&gt;% slice_max(order_by = num_by_coa, n = 3)\n\ntop_3_dest_names &lt;- (top_3_dest$coa_name)\ntop_3_dest_names\n\n[1] \"Türkiye\"                \"Iran (Islamic Rep. of)\" \"Germany\"               \n\n\nNow I’m making a category “other” for both source and destination countries that aren’t in the top 3. I use the forcats function fct_other().\nFirst country of arrival:\n\npop_2022$coa_name = factor(pop_2022$coa_name)\n\npop_2022$coa_name &lt;- pop_2022$coa_name %&gt;%\n  fct_other(keep = top_3_dest_names, other_level = \"other\")\n\nThen, country of origin:\n\npop_2022$coo_name = factor(pop_2022$coo_name)\n\npop_2022$coo_name &lt;- pop_2022$coo_name %&gt;%\n  fct_other(keep = top_3_source_names, other_level = \"other\")\n\nFor now, I will remove the “other” category of country of origin.\n\npop_2022_no_other &lt;- pop_2022 %&gt;%\n  filter((coo_name != \"other\"))\n\nThere are 3 popular ways to make Sankey diagrams in R: ggsankey, networkD3, and plotly. There is a nice tutorial on these methods here.\nI’m going to focus on ggsankey and networkD3.\n\nggsankey method for Sankey Plots in R\nThis package is combined with ggplot to make nice static Sankey plots.\nThe package documentation does a great job explaining the parts of the Sankey chart. It does not do a great job of explaining what format the data should be in. I spent most of my time last Tuesday trying to figure this out.\nThe package includes a function make_long(), which is used to prepare the data for plotting. It labels the columns appropriately and creates the needed null/ NA entries. These entries are used to signify the end of the flow.\nThe UN refugee data is already long, so I spent a fair bit of time trying to figure out how to either label it myself or make it wide so I could use make_long().\nIt turns out that ggsankey can only be used on disaggregated data and not aggregated data such as I have here. I never found that explicitly stated in the documentation (it could be I missed that), but I only discovered it as I was trying to wrangle my data into the correct format and comparing it to the mtcars data in the example.\nWhat do I mean by aggregated vs. disaggregated?\nLet’s look at the mtcars dataframe:\n\nmtcars2 &lt;- mtcars %&gt;%\n  slice_sample(n = 10) %&gt;%\n  select(cyl, vs)\n\nmtcars2\n\n                 cyl vs\nPontiac Firebird   8  0\nFerrari Dino       6  0\nPorsche 914-2      4  0\nMerc 450SLC        8  0\nMerc 280           6  1\nMerc 230           4  1\nDatsun 710         4  1\nCamaro Z28         8  0\nMazda RX4          6  0\nMerc 450SE         8  0\n\n\nThis is disaggregated. It reports the properties of each car. The equivalent in our dataset would be if the database listed every refugee by name and specified where they were from and where they were going.\nWhat the population dataset has instead is the aggregated data. This is the mtcars equivalent:\n\nmtcars2 %&gt;% group_by(cyl, vs) %&gt;% count()\n\n# A tibble: 5 × 3\n# Groups:   cyl, vs [5]\n    cyl    vs     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     4     0     1\n2     4     1     2\n3     6     0     2\n4     6     1     1\n5     8     0     4\n\n\nSo we know we have two cars with cyl = 4 and vs = 1, but we don’t know that they are specifically the Merc 240D and the Fiat 128.\nIf all you have is the aggregated data, you can’t disaggregate it. The names of the cars are not contained in that dataframe. In this case, if you absolutely had to make a Sankey diagram using ggsankey, you could write some code to make a dummy ID to represent the missing dimension. That is, you could split the entry cyl = 4 and vs = 1 into\ncyl = 4 and vs = 1 and id = 1\ncyl = 4 and vs = 1 and id = 2\nAnd so on for each group.\nThen you’d have to dpylr::pivot_wider() and then ggsankey::make_longer(). I wouldn’t recommend this, but it is useful sometimes to think of how you’d approach a problem, even if there are easier solutions. [This :: formatting forces the function to load from the specific library on the left side of the double colon. This is useful if you have conflicting function names in different packages; this notation assure the proper one is called. It is also useful to clarify where specific functions are coming from, even if there would be no conflict.]\n\n\nnetworkD3 method for Sankey Plots in R\nI love the R Graph Gallery when I’m looking for inspiration for TidyTuesday. R Graph Gallery recommends the networkD3 package for Sankey plots, but it is an htmlwidget. I’ve been doing a lot of interactive stuff recently, and I originally wanted to get back to some good old ggplot. But the networkD3 package does create lovely Sankey plots.\nThe networkD3 method seems slightly confusing because you need to transform and label the data yourself. Like ggsankey, you need information about the nodes and how they are connected with each other. One other note- this is based on a JavaScript package (with 0 indexing). So, a few “-1” to convert the index from R (which starts at 1).\nThere is a nice post on Sankey diagrams in networkD3 on the Data to Viz blog. The code is hidden, so you need to toggle it on with the code button if you want to see it.\nThe function call is (from the manual found at CRAN):\nsankeyNetwork(Links, Nodes, Source, Target, Value, NodeID, NodeGroup = NodeID, LinkGroup = NULL, units = \"\", colourScale = JS(\"d3.scaleOrdinal(d3.schemeCategory20);\"), fontSize = 7, fontFamily = NULL, nodeWidth = 15, nodePadding = 10, margin = NULL, height = NULL, width = NULL, iterations = 32, sinksRight = TRUE)\nI need Links, Nodes, Source, Target, Value, NodeID.\nNodes is the list of countries (both origin and arrival). It needs to be a unique list of the countries stored as a dataframe.\nI converted the countries to factors earlier, but that isn’t a problem.\n\nnodes &lt;- data.frame(name =\n                      c(pop_2022_no_other$coo_name, pop_2022_no_other$coa_name) %&gt;%\n                      unique()\n                    )\n\nNext, we need the IDs for source and target, zero-indexed. The ID is generated by position in the nodes dataframe I just created. The match() function is a base R function that returns a vector of the positions of the first argument (pop_2022_no_other$coo_name) as found in the second argument (nodes$name). Note that it returns a vector of all matches. I know that nodes is a unique list, so it is only going to return a single index. This may not be true for other use cases. Again, subtract 1 for the difference in indexing.\n\npop_2022_no_other$IDsource = match(pop_2022_no_other$coo_name, nodes$name) -\n  1\npop_2022_no_other$IDtarget = match(pop_2022_no_other$coa_name,  nodes$name) -\n  1\n\nNext, create the color scales. Details about the options for the color scales are found in the D3 API documentation.\nI am going to change the colors a bit. The other category will be gray. The colors are assigned matches using the order in the nodes df. Note that this does NOT match the order in the diagram. Most Sankey plotting programs reorder the nodes to minimize crossing and to create a cleaner diagram.\n\nColourScal = 'd3.scaleOrdinal([`#946943`, `#b63e36`,`#F5B041`, `#909497`,`#383867`, `#584c77`, `#33431e`, `#a36629`, `#92462f`])'\n\nThe colors are specified in hex code here. You can find color pickers that give you the codes on the web, such as this one.\nNow I can make the diagram. I’m also specifying height and width so I can add a title and a caption using htmlwidgets. If you don’t specify height and width the figure might be truncated when the titles are applied. (This method might work for leaflet maps also. The leaflet package also lacks a method for titles.) I am using the html header 2 styling (h2) for the title, while the caption is just a normal paragraph (p).\nThe sinksRight parameter is used to put the label either outside the flows (FALSE) as I have done here. TRUE puts it inside the flow lines. Unfortunately, there is not a matching sinksLeft, so those labels will always be inside the flow.\n\n# Make the Network\nsankey &lt;- sankeyNetwork(\n  Links = pop_2022_no_other,\n  Nodes = nodes,\n  Source = \"IDsource\",\n  Target = \"IDtarget\",\n  Value = \"total\",\n  NodeID = \"name\",\n  sinksRight = FALSE,\n  colourScale = ColourScal,\n  nodeWidth = 40,\n  fontSize = 13,\n  nodePadding = 20,\n  width = 600,\n  height = 400\n)\n\nLinks is a tbl_df. Converting to a plain data frame.\n\nsankey &lt;-\n  htmlwidgets::prependContent(sankey, htmltools::tags$h2(\"Refugee Flow in 2022\"))\nsankey &lt;-\n  htmlwidgets::appendContent(sankey, htmltools::tags$p(\"from UNHCR’s refugees R package\"))\n\nsankey\n\nRefugee Flow in 2022\n\nfrom UNHCR’s refugees R package\n\n\n\nSo this is pretty clear. Because the other category is disaggregated, we get individual lines for each country of arrival (but we don’t know what countries they are.) If you mouse over the traces, you can see how many refugees are involved in each path. For example, if you mouse over the large gray band on the Afghanistan to other, you see 1.8 million refugees have gone to a single color. Some of the other lines represent only a handful of refugees. I could also aggregate the other data, creating a single band from each source to other. But I think it is neat the way it is.\n\n\nSaving an htmlwidget object\nSaving the interactive figures to a thumbnail is always tricky. Various methods are suggested- using webshot seems to be the most popular, but I haven’t had it work. (First, you save the diagram as an HTML file, and then webshot converts that to a static image). While the HTML saves fine, the webshotted png is the title and the caption with a vast white block of no figure in between. The only consistent way I’ve found is to export from RStudio’s viewer. You do have to fiddle with the height and width of your object; otherwise, the exported file will either be truncated or have non-functional scroll bars in the image."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Louise E. Sinks",
    "section": "",
    "text": "Hello! I’m Louise Sinks. Welcome to my website. I’m a chemist by training, and I’ve spent my career designing, running, and analyzing a variety of types of experiments.\nYou’ll find some of my projects for former employers in the portfolio section. Most of my research was on advanced functional materials with novel photophysical properties. In addition to spectroscopy, I employed statistical learning/ fitting methods, as interpretability is of primary importance in the sciences. Of course, being in charge of the experimental design gives you a huge advantage if you are trying to develop an interpretable model, as you can make sure you thoroughly explore the relevant parameter space. Most of my modeling and simulation work was done in Matlab, which seems to be a more popular language in academia than in industry and business.\nYou’ll also find my blog, where I do small projects to master new languages (R and Python currently). These are generally written as tutorials; throughout my career, I’ve benefitted immensely from code chunks and explanations I’ve found online, so this is my way of paying it forward. The blog posts are written in Quarto, and the raw .qmd files can be found in my GitHub repo. These can be downloaded and run in RStudio or another IDE that supports Quarto. The current projects section includes upcoming topics for the blog."
  },
  {
    "objectID": "posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html",
    "href": "posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html",
    "title": "Tidy Tuesday: Daylight Savings Time",
    "section": "",
    "text": "This week’s TidyTuesday is about the timezone data from IANA timezone database.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(lubridate)\nlibrary(skimr)\nlibrary(lutz)\nlibrary(maps)\nlibrary(scales)\nlibrary(sf)\nlibrary(ggimage)\n\nThe history of this database is fascinating. It is used by many computer systems to determine the correct time based on location. To learn more, I recommend reading Daniel Rosehill’s article on the topic. For a drier history, check out the wikipedia article.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 13)\n\ntransitions &lt;- tuesdata$transitions\ntimezones &lt;- tuesdata$timezones\ntimezone_countries &lt;- tuesdata$timezone_countries\ncountries &lt;- tuesdata$countries\n\nIt is suggested that we change the begin and end variables in transitions to datetimes.\n\ntransitions &lt;- transitions %&gt;%\n  mutate(begin = as_datetime(begin), end = as_datetime(end))\n\nI was interested in how many countries had multiple times zones. I know the US has 4 time zones in the continental US.\n\nnum_zones &lt;- timezone_countries %&gt;%\n  count(country_code, sort = TRUE)\n\nnum_zones %&gt;% \n  filter(n &gt; 1) %&gt;%\n  left_join(countries) %&gt;%\n  select(place_name, n) %&gt;%\n  filter(place_name != \"NA\") %&gt;%\n  gt() %&gt;%\n  cols_label(place_name = \"Country\", n = \"Number of TZs\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Countries with Multiple TZs\") \n\n\n\n\n\n  \n    \n      Countries with Multiple TZs\n    \n    \n    \n      Country\n      Number of TZs\n    \n  \n  \n    United States\n29\n    Canada\n28\n    Russia\n27\n    Brazil\n16\n    Argentina\n12\n    Australia\n12\n    Mexico\n11\n    Kazakhstan\n7\n    Greenland\n4\n    Indonesia\n4\n    Ukraine\n4\n    Chile\n3\n    Spain\n3\n    Micronesia\n3\n    Kiribati\n3\n    Mongolia\n3\n    Malaysia\n3\n    French Polynesia\n3\n    Portugal\n3\n    US minor outlying islands\n3\n    Congo (Dem. Rep.)\n2\n    China\n2\n    Cyprus\n2\n    Germany\n2\n    Ecuador\n2\n    Marshall Islands\n2\n    New Zealand\n2\n    Papua New Guinea\n2\n    Palestine\n2\n    French Southern & Antarctic Lands\n2\n    Uzbekistan\n2\n    Vietnam\n2\n  \n  \n  \n\n\n\n\nAnd we find that the United States has 29!! time zones in the database. This was unexpected, so say the least. I thought maybe there were some times zones for territories and perhaps military bases that I did not know about. I also thought there might be some extra time zones arising from some states using daylight savings time, while others in the same area might not. I wanted to visualize where these times zones were.\n\nUS_tz &lt;- timezone_countries %&gt;% \n  filter(country_code == \"US\") %&gt;%\n  left_join(timezones)\n\nJoining with `by = join_by(zone)`\n\n\nI found the lutz package created nice pictograms about when a timezone shifts from DST and back. (This package uses the same underlying database that we are using here to determine when the shifts occur.)\n\n tz_plot(US_tz$zone[21])\n\n\n\n\nI created the plots and saved them as images. I modified a function I found on stack overflow to create the file names.\n\nwd &lt;- getwd()\nfilepath = file.path(wd)\n\n\nmake_filename = function(number){\n  # doing this, putting it all on a single line or using pipe %&gt;%\n  # is just matter of style\n  filename = paste(\"tzplot\", number, sep=\"_\")\n  filename = paste0(filename, \".png\")\n  filename = file.path(filepath, filename)\n  \n  filename\n}\n\n#creating a variable to store the files name\nUS_tz &lt;- US_tz %&gt;%\n  mutate(image_name = \"tbd\")\n\nindex &lt;- 1\nfor (index in seq(1, nrow(US_tz))) {\n  filename = make_filename(index)\n  US_tz[index , \"image_name\"] &lt;- filename\n  # 1. Open jpeg file\n  png(filename, width = 350, height = 350, bg = \"transparent\")\n  # 2. Create the plot\n  # you need to print the plot if you call it inside a loop\n  print(tz_plot(US_tz$zone[index]))\n  # 3. Close the file\n  dev.off()\n  index = index + 1\n}\n\nNext I created a world map, inspired by the one from\n\n\nMy submission for #TidyTuesday, Week 13 on time zones. I plot time zones in the world map.Code: https://t.co/y5Cm4tuaVk pic.twitter.com/BZC3anC5Oa\n\n— Mitsuo Shiota (@mitsuoxv) March 28, 2023\n\n\nI hadn’t previously used the maps package, so I appreciate being introduced to it. The maps package only has a mainland US map, so I used the world map. (Plus, as I mentioned, I thought some of these time zones would be in other parts of the world.) I followed a tutorial on Plotting Points as Images in ggplot and used the hints about aspect ratio to make my tz_plot circles remain circular. However, that did stretch the world a bit.\n\naspect_ratio &lt;- 1.618  \n\nus_tz_map &lt;- map_data(\"world\") %&gt;% \n  ggplot(aes(long, lat)) +\n  geom_polygon(aes(group = group), fill = \"white\", \n               color = \"gray30\", alpha = 0.9) +\n  geom_image(aes(x = longitude, latitude, image = image_name), \n             data = US_tz, size = 0.025, by = \"width\",\n             asp = aspect_ratio) +\n  coord_sf() +\n  labs(title = \"The United States has 29 Timezone- Mostly Redunant\",\n       caption = \"Data from: https://data.iana.org/time-zones/tz-link.html\") +\n  theme_void() +\n  theme(aspect.ratio = 1/aspect_ratio,\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = \"white\")\n    )\n\nggsave(\"thumbnail.png\", us_tz_map, width = 5 * aspect_ratio, height = 5)\nus_tz_map\n\n\n\n\nAnd what we see is there are a bunch of redundant times zone specification, especially in the Midwest.\n\nUS_tz %&gt;%\n  select(zone, latitude, longitude) %&gt;%\n  arrange(longitude) %&gt;%\n  gt() %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Countries with Multiple TZs\") \n\n\n\n\n\n  \n    \n      Countries with Multiple TZs\n    \n    \n    \n      zone\n      latitude\n      longitude\n    \n  \n  \n    America/Adak\n52.66667\n-177.13333\n    America/Nome\n64.56667\n-165.78333\n    Pacific/Honolulu\n21.71667\n-158.35000\n    America/Anchorage\n61.30000\n-149.91667\n    America/Yakutat\n60.35000\n-140.35000\n    America/Sitka\n57.75000\n-135.41667\n    America/Juneau\n58.41667\n-134.60000\n    America/Metlakatla\n55.73333\n-132.15000\n    America/Los_Angeles\n34.18333\n-118.80000\n    America/Boise\n44.41667\n-116.35000\n    America/Phoenix\n34.33333\n-112.46667\n    America/Denver\n40.08333\n-105.03333\n    America/North_Dakota/Beulah\n48.10000\n-102.43333\n    America/North_Dakota/Center\n48.08333\n-102.23333\n    America/North_Dakota/New_Salem\n47.53333\n-102.05000\n    America/Menominee\n45.56667\n-88.45000\n    America/Indiana/Vincennes\n39.30000\n-88.23333\n    America/Indiana/Petersburg\n39.00000\n-87.98333\n    America/Chicago\n41.85000\n-87.65000\n    America/Indiana/Tell_City\n38.13333\n-87.43333\n    America/Indiana/Knox\n42.03333\n-87.11667\n    America/Indiana/Marengo\n38.90000\n-87.01667\n    America/Indiana/Winamac\n41.13333\n-86.78333\n    America/Indiana/Indianapolis\n39.86667\n-86.63333\n    America/Kentucky/Louisville\n38.50000\n-86.31667\n    America/Kentucky/Monticello\n37.60000\n-85.78333\n    America/Indiana/Vevay\n39.60000\n-85.10000\n    America/Detroit\n43.20000\n-83.78333\n    America/New_York\n41.55000\n-74.38333\n  \n  \n  \n\n\n\n\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E.},\n  title = {Tidy {Tuesday:} {Daylight} {Savings} {Time}},\n  date = {2023-03-28},\n  url = {https://lsinks.github.io/posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E. 2023. “Tidy Tuesday: Daylight Savings\nTime.” March 28, 2023. https://lsinks.github.io/posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html."
  },
  {
    "objectID": "posts/2023-06-29-tidytuesday-populated-places-leaflet/arlington-neighborhoods-leaflet.html",
    "href": "posts/2023-06-29-tidytuesday-populated-places-leaflet/arlington-neighborhoods-leaflet.html",
    "title": "Tidy Tuesday Revisited: Interactive Map of Arlington Historic Neighborhoods",
    "section": "",
    "text": "This week’s TidyTuesday is about place names as recorded by the US Board on Geographic Names. The dataset has been cleaned to include only populated places. I ended up augmenting the dataset with information about Arlington Historic neighborhoods and current neighborhood boundaries. My post with code on this project is here.\nI wanted to create an interactive map with leaflet, but I encountered two problems:\n1- I couldn’t figure out how to add my civic association map.\n2- The map that I did make worked fine when I ran it from a code chunk, but failed when I rendered the quarto document.\nI’ve solved both problems and I really enjoyed working with leaflet.\nHere are the libraries:\n\nlibrary(tidyverse) # who doesn't want to be tidy?\nlibrary(sf) # for handling geo data\nlibrary(leaflet) # interacting mapping\n\nI saved the two datasets from my previous work: historic_4269 and arlington_polygons_sf. I saved them using:\nst_write(historic_4269, \"points.shp\")\nst_write(arlington_polygons_sf, \"polygons.shp\")\nfrom the sf package.\nHere, I’m reading them in. The process does change some of the variable names. The dataset from the National Register of Historic Places had non-standard names such as Property.Name, which gets converted to a shorter name, Prprt_N, with _ instead of period.\n\nhistoric_4269 &lt;- st_read(\"points.shp\")\narlington_polygons_sf &lt;- st_read(\"polygons.shp\")\n\nI mentioned that I found tutorials here and here to make the pop-up URL using leaflet. So, following them I add the HTML anchor tag.\n\n# turn the url to HTML anchor tag\nhistoric_4269 &lt;- historic_4269 %&gt;% \n  mutate(tag = paste0(\"More Info: &lt;a href=\", Extrn_L,\"&gt;\", Extrn_L, \"&lt;/a&gt;\"))\n\nLeaflet uses background map tiles as the canvas for the map. As with all mapping, the coordinate reference system (CRS) of all your component layers needs to be the same. The two datasets I have used the CRS= 4269 projection, but this isn’t the usual CRS. The background map I chose uses the 4326 CRS, so I need to transform my data to that projection. Leaflet will give you a warning if you add layers with unexpected CRSs, so make sure to read the messages carefully and correct them.\n\nhistoric_4326 &lt;- sf::st_transform(historic_4269, crs = 4326)\narlington_polygons_sf_4326 &lt;- sf::st_transform(arlington_polygons_sf, crs = 4326) \n\nFor the issue of adding the polygon data, I was just not really thinking about things. Leaflet uses tidyverse piping, so you either need to have the dataset at the start of the pipe chain or you need to explicitly pass it as data = blah. The error message wasn’t super help to me either : addPolygons must be called with both lng and lat, or with neither. I thought that meant I needed to transform the polygons into some other type geometry format.\nSo this doesn’t work:\nleaflet_map &lt;- leaflet() %&gt;%\naddPolygons(arlington_polygons_sf_4326)\nleaflet_map\nBut this does:\n\nleaflet_map &lt;- leaflet(arlington_polygons_sf_4326) %&gt;% \n  addPolygons() \n\nleaflet_map\n\n\n\n\n\nOr this:\n\nleaflet_map &lt;- arlington_polygons_sf_4326 %&gt;% \n  leaflet() %&gt;%\n  addPolygons() \n\nleaflet_map\n\n\n\n\n\nOr this:\n\nleaflet_map &lt;- leaflet() %&gt;% \n  addPolygons(data = arlington_polygons_sf_4326) \n\nleaflet_map\n\n\n\n\n\nI chose to use the last method, since I was adding data from different sources and I thought it would be more understandable to have the data source explicitly stated in each layer call.\nTo make things a bit clearer, I set a color palette for the Arlington neighborhoods. There are 62 of them, so I used viridis, which is more suited for numerical data, but creates a pleasing effect here. There is information encoded in the colors, the purples correspond to neighborhoods starting with “A” and the yellows correspond to those at the end of the alphabet, but that isn’t really important. The choice was purely an aesthetic one.\n\npal &lt;- colorFactor(palette = \"viridis\", domain = arlington_polygons_sf_4326$CIVIC)\n\nThe final leaflet map has three layers:\n\nthe underlying map created using addProviderTiles()\nthe current Arlington neighborhoods created using addPolygons()\nthe point markers for the historic districts created using addCircleMarkers()\n\nThe neighborhood names appear when you hover over the polygon, while the name of the historic district and the link to the application submitted to be added to the National Register of Historic Places appears as a pop-up when you click on it.\nLeaflet uses ~ notation to reference variables in the data, which you can see in code below.\n\nleaflet_map &lt;- leaflet() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(\n    data = arlington_polygons_sf_4326,\n    weight = 1,\n    label = ~ CIVIC,\n    color = ~ pal(CIVIC)\n  ) %&gt;%\n  addCircleMarkers(\n    data = historic_4326,\n    popup = ~ paste0(\"&lt;b&gt;\", Prprt_N, \"&lt;/b&gt;\", \"&lt;br&gt;\", tag),\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"black\",\n    stroke = NA\n  )\nleaflet_map\n\n\n\n\n\nDatacamp has a really nice starter course on leaflet that I found very helpful for understanding leaflet conceptually as well as learning about the basic formatting options. There is also a nice set of documentation here.\nSo why was my leaflet map causing the quarto document to fail to render? Apparently, there was a issue with knitr and quarto that popped up after some updates in May 2023. It applies to packages other than leaflet as well. If you get an error message along the lines of :\nError in `add_html_caption()`: ! unused argument (xfun::grep_sub(\"^[^&lt;]*&lt;[^&gt;]+aria-labelledby[ ]*=[ ]*\\\"([^\\\"]+)\\\".*$\", \"\\\\1\", x))\nBacktrace:\n1. global .main()\n2. execute(...)\n3. rmarkdown::render(...)\n4. knitr::knit(knit_input, knit_output, envir = envir, quiet = quiet)\n5. knitr:::process_file(text, output) ...\n14. sew(res, options)\n15. knitr:::sew.list(x, options, ...)\n16. base::lapply(x, sew, options, ...)\n17. FUN(X[[i]], ...)\n18. knitr:::sew.knit_asis(x, options, ...) Execution halted.\nthen you probably have this issue. Quarto has already fixed the issue with stable release 1.3.433. The version of Quarto bundled with RStudio RStudio 2023.06.0+421 “Mountain Hydrangea” for Windows was 1.3.353 and has the problem. If you use the bundled version with RStudio, close RStudio, install the latest Quarto as a standalone program. When you open RStudio, it should automatically detect the new version and switch to that.\nTo check what version of Quarto you have, go to the terminal (not console) and type quarto check.\nLeaflet is pretty amazing. I’ve always found mapping in R to be unpleasant, but leaflet makes it easy and produces beautiful maps.\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E. and E. Sinks, Louise},\n  title = {Tidy {Tuesday} {Revisited:} {Interactive} {Map} of\n    {Arlington} {Historic} {Neighborhoods}},\n  date = {2023-06-29},\n  url = {https://lsinks.github.io/posts/2023-06-29-tidytuesday-US-populated-places-leaflet/arlington-neighborhoods-leaflet},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E., and Louise E. Sinks. 2023. “Tidy Tuesday\nRevisited: Interactive Map of Arlington Historic Neighborhoods.”\nJune 29, 2023. https://lsinks.github.io/posts/2023-06-29-tidytuesday-US-populated-places-leaflet/arlington-neighborhoods-leaflet."
  }
]