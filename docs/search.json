[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Entries",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nTidy Tuesday Revisited: Interactive Map of Arlington Historic Neighborhoods\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nData-Viz\n\n\nsf\n\n\nleaflet\n\n\n\n\nTidyTuesday: Interactive Map of Historic Neighborhoods of Arlington Virginia\n\n\n\n\n\n\nJun 29, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: US Populated Places\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nData-Viz\n\n\nopenxlsx\n\n\nstringr\n\n\nfuzzyjoin\n\n\nmapview\n\n\nsf\n\n\n\n\nTidyTuesday: Historic Neighborhoods of Arlington Virginia\n\n\n\n\n\n\nJun 27, 2023\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 25: UFO Sightings Redux\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\nData-Viz\n\n\n\n\nTidyTuesday: UFO Sightings\n\n\n\n\n\n\nJun 20, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 18: Portal Project\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\nData-Viz\n\n\ndata validation\n\n\nexploratory data analysis\n\n\n\n\nTidyTuesday: Rodents of Portal Arizona\n\n\n\n\n\n\nMay 2, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 17: London Marathon\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\nData-Viz\n\n\n\n\nTidyTuesday: Exploring the London Marathon\n\n\n\n\n\n\nApr 25, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 16: Neolithic Founder Crops\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nData-Viz\n\n\n\n\nTidyTuesday: Exploring early agriculture in SW Asia\n\n\n\n\n\n\nApr 18, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCredit Card Fraud: A Tidymodels Tutorial\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ntidymodels\n\n\nMachine Learning\n\n\nclassifiers\n\n\n\n\nAn Imbalanced Class Problem\n\n\n\n\n\n\nApr 11, 2023\n\n\n39 min\n\n\n\n\n\n\n  \n\n\n\n\nA Tidymodels Tutorial: A Structural Approach\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ntidymodels\n\n\nMachine Learning\n\n\n\n\nExploring the different steps for modeling\n\n\n\n\n\n\nApr 10, 2023\n\n\n22 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge- Endangered Species\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\n30DayChartChallenge\n\n\nwaffle\n\n\n\n\nHow many species have been delisted?\n\n\n\n\n\n\nApr 4, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge- Flora and Fauna\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\nturtle\n\n\n30DayChartChallenge\n\n\n\n\nHow Large are Different Types of Turtles?\n\n\n\n\n\n\nApr 3, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge -Arlington Parks\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\nWaffle\n\n\n30DayChartChallenge\n\n\n\n\nWho Owns the Parks in Arlington Virgina?\n\n\n\n\n\n\nApr 2, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nSelf-Guided Learning through a Wordle Guess Generator: Part 1\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\n\n\nSkills improvement with a Wordle Guess Generator\n\n\n\n\n\n\nApr 1, 2023\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nSelf-Guided Learning through a Wordle Guess Generator: Part 2\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\n\n\nCurrent version of a Wordle Guess Generator\n\n\n\n\n\n\nApr 1, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nOne Class SVM\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\nMachine Learning\n\n\ncaret\n\n\nSVM\n\n\nClassifiers\n\n\n\n\nOne Class SVM for Imbalanced Classes\n\n\n\n\n\n\nMar 30, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Daylight Savings Time\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\nData-Viz\n\n\nggimage\n\n\nmaps\n\n\n\n\nTidyTuesday: Exploring Time Zones and Daylight Savings Time\n\n\n\n\n\n\nMar 28, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 12: Programming Languages Revisited\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\nData-Viz\n\n\nskimr\n\n\n\n\nTidyTuesday: Polishing\n\n\n\n\n\n\nMar 24, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTwitter Cards\n\n\n\n\n\n\n\nQuarto\n\n\n\n\nMaking Twitter Cards\n\n\n\n\n\n\nMar 24, 2023\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 12: Programming Languages\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\n\n\nTidyTuesday: How to comment in Various Programming Languages\n\n\n\n\n\n\nMar 21, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Blog\n\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nturtle\n\n\n\n\nTrying to create a Quarto Blog\n\n\n\n\n\n\nMar 14, 2023\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-06-27-tidytuesday-US-populated-places/arlington-neighborhoods.html",
    "href": "posts/2023-06-27-tidytuesday-US-populated-places/arlington-neighborhoods.html",
    "title": "Tidy Tuesday: US Populated Places",
    "section": "",
    "text": "Today’s TidyTuesday is about place names as recorded by the US Board on Geographic Names. The dataset has been cleaned to include only populated places.\nThis week will involve more libraries than normal, since I am going to play with mapping.\n\nlibrary(tidyverse) # who doesn't want to be tidy?\nlibrary(ggthemes) # more themes for ggplot\nlibrary(gt) # For nice tables\nlibrary(ggrepel) # to help position labels in ggplot graphs\nlibrary(openxlsx) # importing excel files from a URL\nlibrary(fuzzyjoin) # for joining on inexact matches\nlibrary(sf) # for handling geo data\nlibrary(mapview) # quick interactive mapping\nlibrary(leaflet) # more mapping\n\nLoad dataset in the usual way.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 26) \nus_place_names &lt;- tuesdata$`us_place_names` \nus_place_history &lt;- tuesdata$`us_place_history`\n\nI’d like to look at the places local to me. The dataset contains two dataframes- one with geographic details about the location and the other with some commentary like description and history.\n\nva &lt;- us_place_names %&gt;% filter(state_name == \"Virginia\")\nva &lt;- va %&gt;% filter(county_name == \"Arlington\")\nva_joined &lt;- va %&gt;% left_join(us_place_history, by = join_by(feature_id))\n\nI don’t need city, state, and county number since I am dealing with a single city/county. So I am removing them from the dataset and then viewing what I have.\n\nva_joined %&gt;% select(-state_name,-county_name,-county_numeric) %&gt;%\n  gt()\n\n\n\n\n\n  \n    \n    \n      feature_id\n      feature_name\n      date_created\n      date_edited\n      prim_lat_dec\n      prim_long_dec\n      description\n      history\n    \n  \n  \n    1471986\nOverlee Knolls\n1979-09-28\n2022-06-07\n38.88956\n-77.14776\nNA\nNA\n    1492448\nAddison Heights\n1979-09-28\n2022-06-07\n38.85567\n-77.06026\nNA\nNA\n    1492455\nAlcova Heights\n1979-09-28\n2022-06-07\n38.86456\n-77.09720\nNA\nNA\n    1492483\nArlington Forest\n1979-09-28\n2022-06-07\n38.86872\n-77.11303\nNA\nNA\n    1492484\nArlington Heights\n1979-09-28\n2022-06-07\n38.86956\n-77.09220\nNA\nNA\n    1492485\nArlington Village\n1979-09-28\n2022-06-07\n38.86178\n-77.08526\nNA\nNA\n    1492487\nArna Valley\n1979-09-28\n2022-06-07\n38.84428\n-77.07637\nNA\nNA\n    1492496\nAurora Hills\n1979-09-28\n2022-06-07\n38.85150\n-77.06414\nNA\nNA\n    1492512\nBarcroft\n1979-09-28\n2022-06-07\n38.85595\n-77.10387\nNA\nNA\n    1492597\nBluemont Junction\n1979-09-28\n2022-06-07\n38.87483\n-77.13331\nNA\nNA\n    1492606\nBon Air\n1979-09-28\n2022-06-07\n38.87317\n-77.12665\nNA\nNA\n    1492659\nBuckingham\n1979-09-28\n2022-06-07\n38.87345\n-77.10665\nNA\nNA\n    1492771\nClaremont\n1979-09-28\n2022-06-07\n38.84317\n-77.10470\nNA\nNA\n    1492797\nColumbia Forest\n1979-09-28\n2022-06-07\n38.85400\n-77.11026\nNA\nNA\n    1492798\nColumbia Heights\n1979-09-28\n2022-06-07\n38.85761\n-77.12109\nNA\nNA\n    1492877\nDouglass Park\n1979-09-28\n2022-06-07\n38.84983\n-77.09303\nNA\nNA\n    1492958\nFort Barnard Heights\n1979-09-28\n2022-06-07\n38.84650\n-77.08942\nNA\nNA\n    1493006\nGlencarlyn\n1979-09-28\n2011-05-11\n38.86178\n-77.12915\nNA\nNA\n    1493353\nNorth Fairlington\n1979-09-28\n2022-06-07\n38.83650\n-77.09720\nNA\nNA\n    1493397\nParkglen\n1979-09-28\n2022-06-07\n38.85595\n-77.11637\nNA\nNA\n    1493586\nShirlington\n1979-09-28\n2022-06-07\n38.84178\n-77.08831\nNA\nNA\n    1493630\nSouth Fairlington\n1979-09-28\n2022-06-07\n38.83261\n-77.08970\nNA\nNA\n    1493744\nVirginia Heights\n1979-09-28\n2022-06-07\n38.85095\n-77.11637\nNA\nNA\n    1493745\nVirginia Highlands\n1979-09-28\n2022-06-07\n38.85845\n-77.06470\nNA\nNA\n    1493784\nWestmont\n1979-09-28\n2022-06-07\n38.86261\n-77.09192\nNA\nNA\n    1495188\nAllencrest\n1979-09-28\n2022-06-07\n38.89344\n-77.15026\nNA\nNA\n    1495260\nBerkshire\n1979-09-28\n2022-06-07\n38.89789\n-77.15137\nNA\nNA\n    1495429\nCountry Club Hills\n1979-09-28\n2022-06-07\n38.91400\n-77.13081\nNA\nNA\n    1495430\nCountry Club Manor\n1979-09-28\n2022-06-07\n38.91372\n-77.13776\nNA\nNA\n    1495438\nCrescent Hills\n1979-09-28\n2022-06-07\n38.90483\n-77.14581\nNA\nNA\n    1495472\nDominion Hills\n1979-09-28\n2022-06-07\n38.87595\n-77.14109\nNA\nNA\n    1495490\nEast Falls Church\n1979-09-28\n2022-06-07\n38.88733\n-77.15442\nNA\nNA\n    1495579\nGarden City\n1979-09-28\n2022-06-07\n38.90011\n-77.13526\nNA\nNA\n    1495641\nHalls Hill\n1979-09-28\n2022-06-07\n38.89761\n-77.12859\nNA\nNA\n    1495692\nHighview Park\n1979-09-28\n2022-06-07\n38.89372\n-77.12748\nNA\nNA\n    1495804\nLacey Forest\n1979-09-28\n2022-06-07\n38.88289\n-77.12915\nNA\nNA\n    1495821\nLarchmont\n1979-09-28\n2022-06-07\n38.88650\n-77.12776\nNA\nNA\n    1495887\nMadison Manor\n1979-09-28\n2022-06-07\n38.88039\n-77.14720\nNA\nNA\n    1496037\nOakwood\n1979-09-28\n2022-06-07\n38.89733\n-77.16248\nNA\nNA\n    1496271\nStratford Hills\n1979-09-28\n2022-06-07\n38.90872\n-77.14053\nNA\nNA\n    1496293\nTara\n1979-09-28\n2022-06-07\n38.89039\n-77.13498\nNA\nNA\n    1496368\nWalker Chapel\n1979-09-28\n2022-06-07\n38.92150\n-77.12942\nNA\nNA\n    1496386\nWest Arlington\n1979-09-28\n2022-06-07\n38.89400\n-77.16831\nNA\nNA\n    1496394\nWestover\n1979-09-28\n2022-06-07\n38.88706\n-77.13942\nNA\nNA\n    1496421\nWilliamsburg Village\n1979-09-28\n2022-06-07\n38.90511\n-77.15498\nNA\nNA\n    1496434\nWoodland Acres\n1979-09-28\n2022-06-07\n38.91261\n-77.14526\nNA\nNA\n    1499060\nArlingwood\n1979-09-28\n2022-06-07\n38.92761\n-77.12192\nNA\nNA\n    1499086\nBallston\n1979-09-28\n2011-05-11\n38.88011\n-77.11387\nNA\nNA\n    1499108\nBeechwood Hills\n1979-09-28\n2022-06-07\n38.90900\n-77.10998\nNA\nNA\n    1499116\nBellevue Forest\n1979-09-28\n2022-06-07\n38.91428\n-77.11359\nNA\nNA\n    1499157\nBrandon Village\n1979-09-28\n2022-06-07\n38.87567\n-77.11581\nNA\nNA\n    1499172\nBroyhill Forest\n1979-09-28\n2022-06-07\n38.91539\n-77.12248\nNA\nNA\n    1499245\nCherrydale\n1979-09-28\n2022-06-07\n38.89706\n-77.10831\nNA\nNA\n    1499266\nClarendon\n1979-09-28\n2022-06-07\n38.88595\n-77.09692\nNA\nNA\n    1499290\nColonial Village\n1979-09-28\n2022-06-07\n38.89317\n-77.08609\nNA\nNA\n    1499313\nCrystal Spring Knolls\n1979-09-28\n2022-06-07\n38.90344\n-77.10498\nNA\nNA\n    1499349\nDominion Heights\n1979-09-28\n2022-06-07\n38.89289\n-77.10776\nNA\nNA\n    1499354\nDover\n1979-09-28\n2022-06-07\n38.90678\n-77.10581\nNA\nNA\n    1499439\nFort Myer Heights\n1979-09-28\n2022-06-07\n38.89206\n-77.07942\nNA\nNA\n    1499560\nHighlands\n1979-09-28\n2022-06-07\n38.89817\n-77.08303\nNA\nNA\n    1499652\nLee Heights\n1979-09-28\n2022-06-07\n38.90206\n-77.11720\nNA\nNA\n    1499696\nLyon Park\n1979-09-28\n2022-06-07\n38.88067\n-77.09026\nNA\nNA\n    1499697\nLyon Village\n1979-09-28\n2022-06-07\n38.89483\n-77.09498\nNA\nNA\n    1499930\nRadnor Heights\n1979-09-28\n2022-06-07\n38.88900\n-77.07303\nNA\nNA\n    1499964\nRivercrest\n1979-09-28\n2022-06-07\n38.92206\n-77.11915\nNA\nNA\n    1499969\nRiverwood\n1979-09-28\n2022-06-07\n38.90539\n-77.10248\nNA\nNA\n    1499990\nRosslyn\n1979-09-28\n2022-06-07\n38.89678\n-77.07248\nNA\nNA\n    1500349\nWoodmont\n1979-09-28\n2022-06-07\n38.90067\n-77.09498\nNA\nNA\n    1779110\nBrockwood\n1998-02-05\n2022-06-07\n38.87761\n-77.12887\nNA\nNA\n    1779112\nCountry Club Grove\n1998-02-05\n2022-06-07\n38.91956\n-77.12942\nNA\nNA\n    1779118\nEast Arlington (historical)\n1998-02-05\n2022-06-07\n38.87345\n-77.06220\nNA\nNA\n    1779119\nGreen Valley\n1998-02-05\n2022-06-07\n38.85511\n-77.08859\nNA\nNA\n    1779147\nMillburn Terrace\n1998-02-05\n2022-06-07\n38.90067\n-77.13831\nNA\nNA\n    1783506\nArlington\n1998-03-02\n2022-06-07\n38.89039\n-77.08414\nNA\nNA\n    2646878\nCrystal City\n2010-08-26\n2018-11-14\n38.85535\n-77.05090\nNA\nNA\n  \n  \n  \n\n\n\n\nThere is no historical or descriptive data for any of the features in Arlington. Many of these are historical sites or are otherwise of interest. I’d like to augment this data with some context. Arlington has 23 neighborhoods that are on the National Register of Historic Places. The National Register does have scanned applications available for post 2012 applications, but most of the historic neighborhoods were designated prior to that. The National Register does also have a spreadsheet with links to the National archives, which contains the pre-2012 applications.\nI normally like to use tidyverse packages, but read_excel won’t work with URLs. There are workarounds, but it is easier just to use the openxlsx package. The read.xlsx function works as you’d expect but you do need to specify the sheet to read in.\n\nnational_historic &lt;-\n  read.xlsx(\n    'https://www.nps.gov/subjects/nationalregister/upload/national-register-listed-20230119.xlsx' ,\n    sheet = 1\n  )\n\nTaking only my local historic sites. This dataset is annoying because some entries are in all CAPS (like state), but others are in titlecase (like City/County). Some, like building category are in both. To use the entire dataset some string cleaning and formating might be necessary, but for this case, I don’t need to do this.\n\narlington_historic &lt;- national_historic %&gt;%\n  filter(State == \"VIRGINIA\" & County == \"Arlington\")\n\nLooking at the data, it neighborhoods seem to be encoded as districts.\n\narlington_historic_districts &lt;- arlington_historic %&gt;%\n  filter(Category.of.Property == \"DISTRICT\")\n\nArlington County has a website listing historic neighborhoods, and I know there should be 23. The National Register has 29 local entries. I should also note that only 17 of the Arlington neighborhoods appeared in our place names dataset.\nOn to figure out what the extra 3 historic places are. Apparently forts are also districts. There are also applications for boundary increases. To do this I am going to use the stringr function str_detect to find “Boundary Increase” and “Fort” and use the negate = TRUE flag to return everything that doesn’t match.\n\narlington_historic_districts2 &lt;- arlington_historic_districts %&gt;%\n  filter(str_detect(Property.Name, \"Boundary Increase\", negate = TRUE)) %&gt;%\n  filter(str_detect(Property.Name, \"Fort\", negate = TRUE))  \n\n\narlington_historic_districts2 %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      Reference.number\n      Property.Name\n      Status\n      Request.Type\n      Restricted.Address\n      Category.of.Property\n      State\n      County\n      City\n      Street.&.Number\n      External.Link\n      Federal.Agencies\n      Level.of.Significance.-.International\n      Level.of.Significance.-.Local\n      Level.of.Significance.-.National\n      Level.of.Significance.-.Not.Indicated\n      Level.of.Significance.-.State\n      Listed.Date\n      Name.of.Multiple.Property.Listing\n      NHL.Designated.Date\n      Other.Names\n      Park.Name\n      Status.Date\n      Area.of.Significance\n    \n  \n  \n    _05001344\nArlington Forest Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by Carlin Springs Rd., George Mason Dr., Henderson Rd., Aberdeen St., Columbus St., Granada, Galveston and 2nd\nhttps://catalog.archives.gov/id/77834749\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38688\nNA\nNA\n VDHR File No.000-7808\nNA\n38688\n ARCHITECTURE; COMMUNICATIONS\n    _08000063\nArlington Heights Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by Arlington Blvd., S. Fillmore St., S. Walter Reed Dr., columbia Pk., & S. Glebe Rd.\nhttps://catalog.archives.gov/id/41678540\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n39499\n Garden Apartments, Apartment Houses and Apartment Complexes in Arlington County, Virginia MPS\nNA\n 000-3383\nNA\n39499\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _14000146\nArlington National Cemetery Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\n1 Memorial Ave.\nNA\n DEPARTMENT OF THE ARMY\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\n41740\nNA\nNA\n Arlington National Cemetery; DHR #000-0042\nNA\n41740\n MILITARY; LANDSCAPE ARCHITECTURE; POLITICS/GOVERNMENT; ARCHITECTURE\n    _03000215\nArlington Village Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nS 13th St., S 13 Rd., S 16th St., S Barton S., S. Cleveland St. and Edgewood St.\nhttps://catalog.archives.gov/id/41679618\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37722\nNA\nNA\n 000-0024\nNA\n37722\n COMMUNITY PLANNING AND DEVELOPMENT\n    _03000561\nAshton Heights Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by Wilson Bvd., N. Irving St., Arlington Bvd., N. Oxford St., N. Piedmont & N. Oakland Sts.\nhttps://catalog.archives.gov/id/41679598\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37795\nNA\nNA\n 000-7819\nNA\n37795\n ARCHITECTURE; COMMERCE\n    _08001018\nAurora Highlands Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by 16th St. S., S. Eads St., 26th St. S., and S. Joyce St.\nhttps://catalog.archives.gov/id/77834759\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n39743\nNA\nNA\n 000-9706\nNA\n39743\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _98001649\nBuckingham Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by N. 5th, N. Oxford, and N. 2nd Sts., and N. Glebe Rd.\nhttps://catalog.archives.gov/id/41679602\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n36181\nNA\nNA\n DHR File # 00-0025\nNA\n36181\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT; LANDSCAPE ARCHITECTURE\n    _03000461\nCherrydale Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by Lorcom Ln., N. Utah and N. Taylor Sts., and I-66\nhttps://catalog.archives.gov/id/41679592\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37763\nNA\nNA\n VDHR File Number 000-7821\nNA\n37763\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _06000751\nClaremont Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by S. Dinwiddie St., S. Chesterfield Rd., S. Buchanan St., 25th St. S, 24th St. S, 23rd St. S and 22nd St. S\nhttps://catalog.archives.gov/id/77834757\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38960\nNA\nNA\n 000-9700\nNA\n38960\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _04000047\nColumbia Forest Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by 11th, S. Edison, S. Dinwiddie, S. Columbus, S. George Mason, and S. Frederick St.\nhttps://catalog.archives.gov/id/41679620\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38028\nNA\nNA\n VDHR # 000-9416\nNA\n38028\n COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n    _12000239\nDominion Hills Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by N. Four Mile Run Dr., N. McKinley Rd., N. Larrimore, N. Madison, N. Montana Sts., & 9th St. N.\nhttps://catalog.archives.gov/id/77834753\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n41023\n Historic Residential Suburbs in the United States, 1830-1960 MPS\nNA\n VDHR FILE NUMBER: 000-4212\nNA\n41023\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _99000368\nFairlington Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by Quaker Lane, King St., I-395, S. Walter Reed Dr., and S. Abingdon St.\nhttps://catalog.archives.gov/id/41679636\nNA\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\n36248\nNA\nNA\n DHR File No. 000-5772\nNA\n36248\n MILITARY; COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n    _04000049\nGlebewood Village Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nN. Brandywine St. Bet. Lee Hwy and 10th Place N, 21St Rd. bet. N. Brandywine St. and N. Glebe Rd.\nhttps://catalog.archives.gov/id/41679622\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38028\nNA\nNA\n 000-9414\nNA\n38028\n COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n    _08000910\nGlencarlyn Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by S. Carlin Springs Rd., Arlington Blvd., 5th Rd. S., Glencarlyn Park\nhttps://catalog.archives.gov/id/77834761\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n39709\nNA\nNA\n 000-9704\nNA\n39709\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _11000548\nHighland Park-Overlee Knolls\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by 22nd St. N., N. Lexington St., 16th St. N., N. Longfellow St., McKinley Rd., I-66 & N. Quantico St.\nhttps://catalog.archives.gov/id/77834763\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n40773\n Historic Residential Suburbs in the United States, 1830-1960 MPS\nNA\n Fostoria/VDHR File Number OOO-9703\nNA\n40773\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _04000109\nLee Gardens North Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\n2300-2341 N. 11th St.\nhttps://catalog.archives.gov/id/41678536\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38043\n Garden Apartments, Apartment Houses and Apartment Complexes in Arlington County, Virginia MPS\nNA\n 000-9411; Woodbury Park Apartments\nNA\n38043\n COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n    _03000437\nLyon Park Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by 10th St. N, Arlington Blvd., and N. Irving St.\nhttps://catalog.archives.gov/id/41679594\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37937\nNA\nNA\n 000-7820\nNA\n37937\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _02000512\nLyon Village Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by Lee Hwy, N. Veitch St., N. Franklin Rd., N. Highland St., N. Fillmore St., and N. Kirkwood Rd.\nhttps://catalog.archives.gov/id/41679590\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37386\nNA\nNA\n VDHR File No. 000-7822\nNA\n37386\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _03000460\nMaywood Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by Lorcom Ln., Spout Run Parkway, I-66, Lee Highway, N. Oakland St., N. Nelson St., and N. Lincoln St.\nhttps://catalog.archives.gov/id/41679596\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37763\nNA\nNA\n VDHR File Number 000-5056\nNA\n37763\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _08000064\nMonroe Courts Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\n1041-1067 N. Nelson and 1036-1062 & 1033-1055 N. Monroe Sts.\nhttps://catalog.archives.gov/id/77834751\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n39499\nNA\nNA\n 000-4105\nNA\n39499\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _04000112\nPenrose Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by Arlington Blvd., S. Courthouse Rd., S. Fillmore St., S. Barton St. S, and Columbia Pike\nhttps://catalog.archives.gov/id/41679600\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38306\nNA\nNA\n VDHR File Number 000-8823\nNA\n38306\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT; BLACK\n    _08000065\nVirginia Heights Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by 10th Pl. S., S. Frederick St. & S. George Mason Dr.\nhttps://catalog.archives.gov/id/77834755\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n39499\nNA\nNA\n 000-9701\nNA\n39499\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _03000451\nWalter Reed Gardens Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\n2900-2906 13th St. S, 2900-2914 13th Rd S, 1301-1319 S. Walter Reed Dr.\nhttps://catalog.archives.gov/id/41678548\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37763\n Garden Apartments, Apartment Houses and Apartment Complexes in Arlington County, Virginia MPS\nNA\n Commons of Arlington; 000-8824\nNA\n37763\n COMMUNITY PLANNING AND DEVELOPMENT\n    _04000111\nWaverly Hills Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by 20th Rd. N, N. Utah St, I-66, N. Glebe Rd. and N. Vermont St.\nhttps://catalog.archives.gov/id/41679624\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38043\nNA\nNA\n VDHR File Number 000-9413\nNA\n38043\n COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n    _06000345\nWestover Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by McKinley Rd., N. Washington Blvd., N. 16th St., N. Jefferson St., N. 11th St. and N. Fairfax Dr.\nhttps://catalog.archives.gov/id/41678538\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38839\n Garden Apartments, Apartment Houses and Apartment Complexes in Arlington County, Virginia MPS\nNA\n 000-0032\nNA\n38839\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n  \n  \n  \n\n\n\n\nI still have too many entries. It turns out that Arlington National Cemetary is also encoded as a DISTRICT. There is also an entry for Walter Reed Gardens Historic District. Arlington County has this listed as a building on their site (and the other entries like Calvert Manor are noted as buildings in the National Register.)\nI could remove these two items manually, but they will be removed when I join it to the place names dataset, since neither one appears in the populated place names.\nJoining the two datasets will require some sort of string manipulation since the place names are not the same. The place names dataset contains just the place names (“Addison Heights”), while the historic sites data contains the phrase “Historic District” appended to the end. In addition, some place names don’t exactly match the historic district names (“Overlee Knolls” and “Highland Park/ Overlee Knolls Historic district”).\nSo I want to do some fuzzy matching and luckily (of course!) there is an R package for that.\nHowever, the populated place names data contains “Arlington” which will match to a ton of different neighborhoods (Arlington Forest, Arlington Heights, etc.) I’m going to change Arlington to Arlington County.\n\nva_joined2 &lt;- va_joined %&gt;%\n  mutate(feature_name = ifelse(feature_name == \"Arlington\", \"Arlington County\", feature_name))\n\nI also know that North and South Fairlington, while separate places in the populated place names, are a single historic district called Fairlington. I’m going to make both North and South Fairlington entry in the historical sites dataframe. I’m not removing the original Fairlington entry because I know I’m going to filter it out with my joins later. But this is the kind of thing that could lead to errors/ extraneous entries later on, so if you do something like this, just make sure you do clean it up later.\n\nsouth_fairlington &lt;- arlington_historic_districts2 %&gt;% \n  filter(Property.Name == \"Fairlington Historic District\") %&gt;%\n  mutate(Property.Name = \"South Fairlington\")\n\nnorth_fairlington &lt;- arlington_historic_districts2 %&gt;%\n  filter(Property.Name == \"Fairlington Historic District\") %&gt;%\n  mutate(Property.Name = \"North Fairlington\")\n\narlington_historic_districts3 &lt;- arlington_historic_districts2 %&gt;%\n  rbind(south_fairlington) %&gt;%\n  rbind(north_fairlington)\n\nOkay, on to fuzzyjoining. The name from the populated places names dataset should be a subset of the name from the historic district dataset. I’m going to illustrate this in a very simply way using str_detect(). “Overlee Knolls” is the first entry in the populated places dataset. I’m going to use this as the pattern to search for in the Historic places dataset. The expected returned neighborhood is “Highland Park/ Overlee Knolls Historic district”.\n\nva_joined2$feature_name[1]\n\n[1] \"Overlee Knolls\"\n\narlington_historic_districts %&gt;%\n  filter(str_detect(Property.Name, va_joined2$feature_name[1])) %&gt;%\n  gt()\n\n\n\n\n\n  \n    \n    \n      Reference.number\n      Property.Name\n      Status\n      Request.Type\n      Restricted.Address\n      Category.of.Property\n      State\n      County\n      City\n      Street.&.Number\n      External.Link\n      Federal.Agencies\n      Level.of.Significance.-.International\n      Level.of.Significance.-.Local\n      Level.of.Significance.-.National\n      Level.of.Significance.-.Not.Indicated\n      Level.of.Significance.-.State\n      Listed.Date\n      Name.of.Multiple.Property.Listing\n      NHL.Designated.Date\n      Other.Names\n      Park.Name\n      Status.Date\n      Area.of.Significance\n    \n  \n  \n    _11000548\nHighland Park-Overlee Knolls\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by 22nd St. N., N. Lexington St., 16th St. N., N. Longfellow St., McKinley Rd., I-66 & N. Quantico St.\nhttps://catalog.archives.gov/id/77834763\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n40773\n Historic Residential Suburbs in the United States, 1830-1960 MPS\nNA\n Fostoria/VDHR File Number OOO-9703\nNA\n40773\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n  \n  \n  \n\n\n\n\nI’ve decided I only want to look at the historic areas in the populated place names. I’m choosing an inner join so I will only get entries that exist in BOTH the populated places and the historic register. This is 17 items (from manually comparing the populated places to the Arlington County website). I’m going to map these places on top of current Arlington County neighborhood groups/civic associates. I’m interested in how current neighborhood compare to the historic districts. (Note that I could have done this without the populated places dataset at all, but this is the Tidytuesday dataset and it is what lead me to my question.)\nThere are a few different ways to use fuzzyjoins. I found this discussion on stackoverflow to be a good starting point. I chose to use the match_fun version, since I had already prototyped with str_detect. The only thing that wasn’t clear to me is which dataframe would be sent to str_detect as the pattern and which was the string. That is, for\nfuzzy_inner_join(x, y, by = c(x$name1 = y$name2), match_fun = str_detect) would I get\nstr_detect(string = x$name1,  pattern = y$name2)\nor\nstr_detect(string = y$name2,  pattern = x$name1)\n?\nMaybe it is clear to others from the stackoverflow example or the fuzzyjoin manual, but it wasn’t clear to me, so I ended up trying it both ways. It turns out that the dataframes are passed to str_detect in the order they are listed, which makes sense (and is probably the convention, but I had never seen it explicitly stated). [To be absolutely clear, what happens is the first case (str_detect(string = x$name1,  pattern = y$name2))]\n\nhistoric_pop_places &lt;-\n  arlington_historic_districts3 %&gt;% fuzzy_inner_join(va_joined2,\n                                                     by = c(\"Property.Name\" = \"feature_name\"),\n                                                     match_fun = str_detect)\n\nFor what I plan to do, I need the place name and the location. I want the reason the place is important and a link to the historic registry application. I started this project wanting to know why these places were important! I’m leaving in both sets of place names, just so I can visually check that my dataset is correct.\n\nhistoric_pop_places &lt;- historic_pop_places %&gt;%\n  select(\n    Property.Name,\n    feature_name,\n    Area.of.Significance,\n    prim_lat_dec,\n    prim_long_dec,\n    External.Link\n  )\ngt(historic_pop_places)\n\n\n\n\n\n  \n    \n    \n      Property.Name\n      feature_name\n      Area.of.Significance\n      prim_lat_dec\n      prim_long_dec\n      External.Link\n    \n  \n  \n    Arlington Forest Historic District\nArlington Forest\n ARCHITECTURE; COMMUNICATIONS\n38.86872\n-77.11303\nhttps://catalog.archives.gov/id/77834749\n    Arlington Heights Historic District\nArlington Heights\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.86956\n-77.09220\nhttps://catalog.archives.gov/id/41678540\n    Arlington Village Historic District\nArlington Village\n COMMUNITY PLANNING AND DEVELOPMENT\n38.86178\n-77.08526\nhttps://catalog.archives.gov/id/41679618\n    Aurora Highlands Historic District\nHighlands\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.89817\n-77.08303\nhttps://catalog.archives.gov/id/77834759\n    Buckingham Historic District\nBuckingham\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT; LANDSCAPE ARCHITECTURE\n38.87345\n-77.10665\nhttps://catalog.archives.gov/id/41679602\n    Cherrydale Historic District\nCherrydale\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.89706\n-77.10831\nhttps://catalog.archives.gov/id/41679592\n    Claremont Historic District\nClaremont\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.84317\n-77.10470\nhttps://catalog.archives.gov/id/77834757\n    Columbia Forest Historic District\nColumbia Forest\n COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n38.85400\n-77.11026\nhttps://catalog.archives.gov/id/41679620\n    Dominion Hills Historic District\nDominion Hills\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.87595\n-77.14109\nhttps://catalog.archives.gov/id/77834753\n    Glencarlyn Historic District\nGlencarlyn\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.86178\n-77.12915\nhttps://catalog.archives.gov/id/77834761\n    Highland Park-Overlee Knolls\nOverlee Knolls\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.88956\n-77.14776\nhttps://catalog.archives.gov/id/77834763\n    Lyon Park Historic District\nLyon Park\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.88067\n-77.09026\nhttps://catalog.archives.gov/id/41679594\n    Lyon Village Historic District\nLyon Village\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.89483\n-77.09498\nhttps://catalog.archives.gov/id/41679590\n    Virginia Heights Historic District\nVirginia Heights\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.85095\n-77.11637\nhttps://catalog.archives.gov/id/77834755\n    Westover Historic District\nWestover\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.88706\n-77.13942\nhttps://catalog.archives.gov/id/41678538\n    South Fairlington\nSouth Fairlington\n MILITARY; COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n38.83261\n-77.08970\nhttps://catalog.archives.gov/id/41679636\n    North Fairlington\nNorth Fairlington\n MILITARY; COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n38.83650\n-77.09720\nhttps://catalog.archives.gov/id/41679636\n  \n  \n  \n\n\n\n\nAurora Highlands and Highlands are the same place- the description of Aurora Highlands from Wikipedia matches the description in the application to be entered on the National Historic Register.\nNow, I found a map of all the Civic associations in Arlington on the county’s open data page. Data can be downloaded in a variety of formats, including shape files or geoJSON. I chose to download the shapefile and extracted the zip to my project directory (not shown).\nThe R Graph Gallery (which is a great resource and source of inspiration) has a great section on mapping, but unfortunately one of the needed packages is being retired. The code below still works but you will get a very long message telling you to migrate away from rgal.\n\n# library(sp)\n# library(rgdal)\n# my_spdf &lt;- readOGR( \n#  dsn = \"Civic_poly.shp\" , \n#  verbose=FALSE\n#)\n\nSo, here is another way to read in the shape file using the sf package. This contains the polygons that define the boundaries of modern neighborhoods in Arlington. There are a lot of neighborhoods!\n\narlington_polygons &lt;- st_read(dsn = \"Civic_poly.shp\")\n\nMapping points (which is what we have in our TidyTuesday dataset- we have the lat/long of the “official feature location”) and polygons from the Arlington County dataset involved a few steps. Shape files can be encoded using different coordinate reference systems (CRS) and care needs to be taken that all the map layers are using the same CRS. I found the mapview package invaluable during this process, as it is simple to create an interactive map. This made trouble shooting incredibly easy.\nGenerally, the first step for handling shape files in R is to convert them to simple features objects. Here, I’m using the sf_package. With a shape file, you generally don’t need to pass the coordinates or CRS, since that data is encoded in the shape file in a way that is easily detectable by the function.\n\narlington_polygons_sf &lt;- st_as_sf(arlington_polygons)\nmapview(arlington_polygons_sf)\n\n\n\n\n\n\nThe generated map looks perfect. Arlington is in the right place in the world mapview(arlington_polygons_sf)and the civic association map looks as it should.\nFor the point data, the conversion does require additional parameters (description of parameters here). Specifically, the coordinates for point data need to be specified. The order for this is longitude, latitude, which I did not do properly the first name, since in spoken English, you usually say latitude/longitude. The mapview map made that very easy to troubleshoot when I saw my points were all in Antarctica. The pop up made it clear that latitude and longitude were flipped. I also need the CRS for this dataset if I’m going to map it with the polygon data. (You also need a CRS for mapview to place your points on a map- without a CRS you get the pattern, but not the geolocation.)\nThe CRS is not specified in the data dictionary for TidyTuesday. There are two likely choices, 4326 and 4269. In this application, there isn’t actually a significant difference. With the mapview data you can select and deselect the layers and see both sets of points are in the same place on this map.\n\nhistoric_4269 &lt;- st_as_sf(historic_pop_places, coords = c(5:4), crs = 4269)\nhistoric_4326 &lt;- st_as_sf(historic_pop_places, coords = c(5:4), crs = 4326)\n\nmapview(historic_4269) + mapview(historic_4326) + mapview(arlington_polygons_sf)\n\n\n\n\n\n\nGoing back to the original data source, it notes that “Datum is NAD83”. This means that the CRS = 4269 as found at the EPGS registry.\nThe Arlington polygons dataset is also NAD83/ 4269, so you can go directly to plotting. If they were different CRSs then you would need to transform them to the same projection, such as with:\npoints_transformed &lt;- sf::st_transform(points_wrong, crs = sf::st_crs(arlington_polygons_sf))\n\nmapview(arlington_polygons_sf ,\n  col.regions = \"purple\") + mapview(historic_4269, col.regions = \"blue\")\n\n\n\n\n\n\nSo many of these historic neighborhoods don’t appear to correspond strongly to modern day neighborhoods. Several of them appear the borders of multiple neighborhood groups. And for example, the location of the Westover feature from the populated places names is actually in the Tara-Leeway Heights neighborhood according to the county’s description of civic association boundaries.\nNow I’m going to make a static ggplot map. Mapview is great for exploratory data analysis, but it isn’t as highly customizable as other graphing packages. I’m displaying the populated place names/ historic district using geom_sf_text(). This needs to be passed both the data and the label, and despite being passed the data it still needs the full variable name (historic_4269$feature_name not feature_name). The units for nudging the text are depend on what crs is used? I just played around until I got the label to move and 800 was not the kind of number I was expecting. (I was thinking 0 to 1 like for hjust.)\n\nggplot() +\n  geom_sf(data = arlington_polygons_sf) +\n  geom_sf(data = historic_4269, alpha = 0.5) +\n  theme_void() +\n  geom_sf_text(\n    data = historic_4269,\n    label = historic_4269$feature_name,\n    size = 2.0,\n    nudge_y = 850\n  ) +\n  labs(title = \"Historic Districs in Arlington compared to modern neighborhoods\") +\n  labs(caption = \"Data from: US Board of Geographic Names,  \\nArlington County, VA - Official GIS Open Data Portal,  \\nand the National Register of Historic Places\") +\n  theme(plot.title = element_text(size = 10),\n        plot.caption = element_text(size = 6, hjust = 0))\n\n\n\n\nAs a further project, I’d like to make an interactive map with a pop-up giving a clickable link to the National Archives page on the historic district application. The mapview version does have the pop-up, but the link isn’t live.\nI found tutorial here to make the pop-up URL using leaflet, but I can’t figure out how to add my polygons. I add the points just fine. It also fails causes the quarto document to fail during render, though it works just fine as regular code.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {Tidy {Tuesday:} {US} {Populated} {Places}},\n  date = {2023-06-27},\n  url = {https://lsinks.github.io/posts/2023-06-27-tidytuesday-US-populated-places/arlington-neighborhoods},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “Tidy Tuesday: US Populated\nPlaces.” June 27, 2023. https://lsinks.github.io/posts/2023-06-27-tidytuesday-US-populated-places/arlington-neighborhoods."
  },
  {
    "objectID": "posts/2023-04-25-tidytuesday-marathon/marathon.html",
    "href": "posts/2023-04-25-tidytuesday-marathon/marathon.html",
    "title": "TidyTuesday Week 17: London Marathon",
    "section": "",
    "text": "Today’s TidyTuesday is based on a dataset about the London Marathon. The data is via a package by Nicola Rennie and there is an accompanying tutorial about how to scrape data from the web.\n\nlibrary(skimr)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(skimr)\n\nLoad the data with the TidyTuesday package in the usual way.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 17)\n\n--- Compiling #TidyTuesday Information for 2023-04-25 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `winners.csv`\n    Downloading file 2 of 2: `london_marathon.csv`\n\n\n--- Download complete ---\n\nwinners &lt;- tuesdata$winners\nlondon_marathon &lt;- tuesdata$london_marathon\n\nThere are two dataframes today: a winner’s dataframe and info about the marathon in general. Looking at the winner’s first.\n\nwinners %&gt;% skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n163\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\ndifftime\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nCategory\n0\n1\n3\n16\n0\n4\n0\n\n\nAthlete\n0\n1\n9\n26\n0\n99\n0\n\n\nNationality\n0\n1\n5\n14\n0\n24\n0\n\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nTime\n0\n1\n5187 secs\n16143 secs\n02:07:55\n158\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYear\n0\n1\n2001.61\n11.83\n1981\n1991.5\n2002\n2012\n2022\n▇▇▇▇▇\n\n\n\n\n\nAll the data is complete.\nCategory and Nationality should probably be factors rather than characters.\n\nwinners &lt;- winners %&gt;%\n  mutate(Category = as.factor(Category), Nationality = as.factor(Nationality))\n\nglimpse(winners)\n\nRows: 163\nColumns: 5\n$ Category    &lt;fct&gt; Men, Men, Men, Men, Men, Men, Men, Men, Men, Men, Men, Men…\n$ Year        &lt;dbl&gt; 1981, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989…\n$ Athlete     &lt;chr&gt; \"Dick Beardsley (Tie)\", \"Inge Simonsen (Tie)\", \"Hugh Jones…\n$ Nationality &lt;fct&gt; United States, Norway, United Kingdom, United Kingdom, Uni…\n$ Time        &lt;time&gt; 02:11:48, 02:11:48, 02:09:24, 02:09:43, 02:09:57, 02:08:1…\n\n\nWhen I check to make sure everything is correct after making the factors, I see that some athletes have (Tie) after their name. If I do something with the runner’s names later (or if I care about ties) then I need to handle this.\nThere are four categories of races in our dataset- Men, Wheelchair Men, Wheelchair Women, Women.\nI’m going to do some simple comparisons first. I’m looking for something interesting to focus on.\n\nwinners %&gt;%\n  filter(Category == \"Men\" | Category == \"Women\") %&gt;%\n  ggplot(aes(Category, Time)) +\n  geom_boxplot()\n\n\n\n\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\" | Category == \"Wheelchair Women\") %&gt;%\n  ggplot(aes(Category, Time)) +\n  geom_boxplot()\n\n\n\n\nMen are faster than women, in both classes of races.\n\nwinners %&gt;%\n  ggplot(aes(Category, Time)) +\n  geom_boxplot()\n\n\n\n\nWheelchair races are faster than the running races. The Men/Women’s races are much closer than the Wheelchair races, and don’t have outliers.\nI’m interested in ties. How many ties have there been? I’m going to make a column called “Tied” and remove (Tied) from the names. There are a few different ways to do this, but I’m going to use tidyr::separate(). I’m going to take the space too, when I separate out the name and (tie), so my name column is cleanly formatted. This is going to generate a bunch of NAs in the Tied column, but I’ll handle that in the next code chunk. I’m using \" (\" as my separator, so the Name column will not have the trailing space. The Tied column will have “Tie)” and a bunch of NAs, but I’ll clean this up in the next step. I’ve left warnings on for this code chunk, so you can see that the NAs are flagged for your notice.\nRemember that special characters like ( need to be escaped out, so the appropriate regular expression for the separator is \" \\\\(\".\nAs a side note, as of winter 2022, separate has been superseded by a family of functions separate_wider_*.\n\nwinners_tied &lt;- winners %&gt;%\n  separate(Athlete, into = c(\"Name\", \"Tied\"), \" \\\\(\")\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 161 rows [3, 4, 5, 6, 7,\n8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, ...].\n\n\nNow I’m going to clean up the Tied column.\n\nwinners_tied &lt;- winners_tied %&gt;%\n  mutate(Tied = ifelse(is.na(Tied) == TRUE, FALSE, TRUE))\n\nSo how many ties are there?\n\nwinners_tied %&gt;% filter(Tied == TRUE)\n\n# A tibble: 2 × 6\n  Category  Year Name           Tied  Nationality   Time    \n  &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;lgl&gt; &lt;fct&gt;         &lt;time&gt;  \n1 Men       1981 Dick Beardsley TRUE  United States 02:11:48\n2 Men       1981 Inge Simonsen  TRUE  Norway        02:11:48\n\n\nJust those first two, from the very first race. According to wikipedia, they crossed the finish line holding hands!\nAnyone win more than once?\n\nwinners_tied %&gt;%\n  group_by(Name) %&gt;%\n  count(Name, sort = TRUE) %&gt;%\n  filter(n &gt; 1)\n\n# A tibble: 34 × 2\n# Groups:   Name [34]\n   Name                     n\n   &lt;chr&gt;                &lt;int&gt;\n 1 David Weir               8\n 2 Tanni Grey-Thompson      6\n 3 David Holding            4\n 4 Eliud Kipchoge           4\n 5 Francesca Porcellato     4\n 6 Ingrid Kristiansen       4\n 7 Tatyana McFadden         4\n 8 António Pinto            3\n 9 Dionicio Cerón           3\n10 Heinz Frei               3\n# ℹ 24 more rows\n\n\nMore than I expected! David Weir has won the most London Marathons, with 8 wins in the Men’s Wheelchair race category. How has his race time changed over the years?\n\nwinners_tied %&gt;%\n  filter(Name == \"David Weir\") %&gt;%\n  ggplot(aes(Year, Time)) +\n  geom_point()\n\n\n\n\nIt looks like his first race was much slower than the other times he has won. It turns out he has competed in the London Marathon 23 times, and placed 5th in the 2023 Marathon which happened this weekend. His time on Saturday was 01:32:45. This is interesting, because it is quite similar to his more recent races.\nHas the field for this race gotten faster?\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\") %&gt;%\n  ggplot(aes(Year, Time)) +\n  geom_point()\n\n\n\n\nYes, and quite clearly too! How many years would he have won with this year’s time?\n\ntime_2023 &lt;- as.difftime(\"01:32:45\", format = \"%H:%M:%S\", units = \"auto\")\nslow_wins &lt;- winners %&gt;%\n  filter(Category == \"Wheelchair Men\" & Time &gt;= time_2023) \nfast_wins &lt;- winners %&gt;%\n  filter(Category == \"Wheelchair Men\" & Time &lt; time_2023) \n\nSo 12 years the wins were faster than his time this year, but 27 years were slower. And just to note, 6 of those 12 faster wins are held by David Weir…\nLet’s throw together a visual for this and make it out TidyTuesday viz. Should be simple, right?\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\") %&gt;%\n  ggplot(aes(Year, Time)) +\n  geom_point() +\n  geom_hline(yintercept = time_2023)\n\n\n\n\nHmm. ggplot is not playing well with our difftimes.\n\nstr(time_2023)\n\n 'difftime' num 1.54583333333333\n - attr(*, \"units\")= chr \"hours\"\n\nstr(winners$Time[1])\n\n 'hms' num 02:11:48\n - attr(*, \"units\")= chr \"secs\"\n\n\nApparently, our race time from winners is actually hms and not difftime. Skim reported it was a difftime. Our difftime has units of hours, while Time has units of seconds. This is probably due to be setting units to “auto” when I did the conversion. Interesting that dplyr filtering handles this smoothly, but ggplot doesn’t.\n\ntime_2023 &lt;- as.difftime(\"01:32:45\", format = \"%H:%M:%S\", units = \"sec\")\nstr(time_2023)\n\n 'difftime' num 5565\n - attr(*, \"units\")= chr \"secs\"\n\n\nI’m going to create a TRUE/FALSE column for if David Weir won that I will color code the win by.\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\") %&gt;%\n  mutate(Weir = ifelse(Athlete == \"David Weir\", TRUE, FALSE)) %&gt;%\n  ggplot(aes(Year, Time, color = Weir)) +\n  geom_point() +\n  geom_hline(yintercept = time_2023)\n\n\n\n\nOkay, now lets clean up the formatting. I need to:\n\nApply a theme. I like theme_classic() or theme_pander() as a clean base for my graphs.\nRemove the legend. This needs to go after the theme_classic/pander() call or the legend will reappear.\nAdd title, subtitle and data source\nFix the y-axis units\nChange the colors for the David Weir points.\nLabel my horizontal line.\n\n#4 is apparently quite challenging. Apparently, the time axis should be displayed as hms, so it isn’t clear to me why fractional seconds are shown. I tried a bunch of different suggestion from the web, but the top answer to this question is what worked.\nIt actually doesn’t matter if the aesthetic is difftime or hms. The key is that the label section of scale_y_time needs a formatted string generated from strftime.\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\") %&gt;%\n  mutate(Weir = ifelse(Athlete == \"David Weir\", TRUE, FALSE)) %&gt;%\n\n  ggplot(aes(Year, Time, color = Weir)) +\n  geom_point() +\n  scale_color_manual(values = c(\"black\", \"cyan2\")) +\n  geom_hline(yintercept = (time_2023),\n             color = \"cyan4\") +\n  annotate(\n    \"text\",\n    x = 1990,\n    y = time_2023 - 200,\n    label = \"David Weir's 2023 time\",\n    color = \"cyan4\"\n  ) +\n  scale_y_time(name = \"Winning Time\", labels = function(l) strftime(l, '%H:%M:%S')) +\n  labs(title = \"Race times for the London Marathon: Men's Wheelchair Race\",\n       subtitle = \"compared to David Weir's wins\",\n       caption = \"data from https://github.com/nrennie/LondonMarathon\") +\n  theme_classic(12) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {TidyTuesday {Week} 17: {London} {Marathon}},\n  date = {2023-04-25},\n  url = {https://lsinks.github.io/posts/2023-04-25-tidytuesday-marathon/marathon},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “TidyTuesday Week 17: London\nMarathon.” April 25, 2023. https://lsinks.github.io/posts/2023-04-25-tidytuesday-marathon/marathon."
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "",
    "text": "I will walk through a classification problem from importing the data, cleaning, exploring, fitting, choosing a model, and finalizing the model.\nI wanted to create a project that could serve as a template for other two-class classification problems. I also wanted to fully use the tidymodels framework, particularly more advanced functionalities like workflowsets. There are some great tutorials on tidymodels, in particular Olivier Gimenez’s tutorial on Kaggle’s Titanic competition. This tutorial steps through each model individually, while I wanted to use the more streamlined approach offered by workflowsets. I also found myself confused as I started doing more advanced procedures in tidymodels, despite having read the book Tidy Modeling with R multiple times and working through several tutorials on Julia Silge’s excellent blog. I ended up writing my own tutorial on tidymodels objects that goes through the differences in the various ways to perform fitting and the various objects produced.\nIn addition to providing a template for the machine learning portion, I wanted to create nice figures and tables that could also be re-used.\nI will also have a different version of this code on Datacamp. I’ve numbered the code chunks manually to aid in comparison between the two versions. I start numbering at 2, because Code Block 1 will be installing libraries at the Datacamp workspace. There are some important differences between the RStudio environment and online notebooks/workspaces.\nPlease feel free to copy and use any of my code in your work. I’d appreciate an acknowledgment or link back if you find this tutorial useful."
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-strings",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-strings",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "5.1. Looking at the strings",
    "text": "5.1. Looking at the strings\nStrings are usually not a useful format for classification problems. The strings should be converted to factors, dropped, or otherwise transformed.\n5.1.1. Strings to Factors (Code Block 6 - 8)\n\ncategory, Category of Merchant\njob, Job of Credit Card Holder\n\n5.1.2. Strings as Strings (Code Block 9)\n\nmerchant, Merchant Name\ntrans_num, Transaction Number\n\nI’m not going to retain these, as they are either unlikely to have predictive power (trans_num) or are highly correlated with other predictors (merchant with merch_lat/merch_long.)\n5.2. Strings to Geospatial Data (Code Block 13)\nWe have plenty of geospatial data as lat/long pairs, so I want to convert city/state to lat/long so I can compare to the other geospatial variables. This will also make it easier to compute new variables like the distance the transaction is from the home location. I will transform and explore this when I handle the other geospatial data.\n\ncity, City of Credit Card Holder\nstate, State of Credit Card Holder\n\nThings to consider as we walk through the data:\n\nDo we have typos that lead to duplicate entries : VA/ Va. / Virginia?\nDo we have excessive # of categories? Do we want to combine some?\nShould they be ordered?\n\n\n5.1.1. Exploring the factors: how is the compactness of categories?\nThe predictors category and job are transformed into factors.\n\n# Code Block 6: Converting Strings to Factors\nfraud$category &lt;- factor(fraud$category)\nfraud$job &lt;- factor(fraud$job)\n\nFrom the skim output, I see that category has 14 unique values, and job has 163 unique values. The dataset is quite large, with 339,607 records, so these variables don’t have an excessive number of levels at first glance. However, it is worth seeing if I can compact the levels to a smaller number.\n\nWhy do we care about the number of categories and whether they are “excessive”?\nConsider the extreme case where a dataset had categories that only contained one record each. There is simply insufficient data to make correct predictions using category as a predictor on new data with that category label. Additionally, if your modeling uses dummy variables, having an extremely large number of categories will lead to the production of a huge number of predictors, which can slow down the fitting. This is fine if all the predictors are useful, but if they aren’t useful (as in the case of having only one record for a category), trimming them will improve the speed and quality of the data fitting.\nIf I had subject matter expertise, I could manually combine categories. For example, in this dataset, the three largest categories in job are surveying-related and perhaps could be combined. If you don’t have subject matter expertise, or if performing this task would be too labor intensive, then you can use cutoffs based on the amount of data in a category. If the majority of the data exists in only a few categories, then it might be reasonable to keep those categories and lump everything else in an “other” category or perhaps even drop the data points in smaller categories. As a side note, the forcats package has a variety of tools to handle consolidating and dropping levels based on different cutoffs if this is the approach you decide to take.\nOne way to evaluate the compactness of a factor is to group the data by category and look at a table of counts. I like the gt package for making attractive tables in R. (Uncomment the line in Code Block 7 #gt:::as.tags.gt_tbl(table_3a) to see the table.) The tabular data also shows that there aren’t typos leading to duplicate categories.\nAnother way to evaluate the compactness is to make a cumulative plot. This looks at the proportion of data that is described as you add categories. I’m using the cowplot package to make multipanel figures. I want to look at both factors at once; this is fine for exploratory data analysis, but I wouldn’t recommend it for a report or presentation, since there is no connection between the two variables.\n\n# Code Block 7: Exploring the Compactness of the Categories\n\n# Exploring the jobs factor\n# bin and count the data and return sorted\ntable_3a_data &lt;- fraud %&gt;% count(job, sort = TRUE) \n\n# creating a table to go with this, but not displaying it\ntable_3a &lt;- table_3a_data %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Jobs of Card Holders\") %&gt;%\n  cols_label(job = \"Jobs\", n = \"Count\") %&gt;%\n  opt_stylize(style = 1,\n              color = \"green\",\n              add_row_striping = TRUE)\n#gt:::as.tags.gt_tbl(table_3a)  #displays the table \n\nfig_1a &lt;- ggplot(table_3a_data, aes(\n  x = 1:nlevels(fraud$job),\n  y = (cumsum(n) * 100 / nrow(fraud))\n)) +\n  geom_point(color = \"darkcyan\") +\n  geom_hline(yintercept = 80) +  #marker for 80% of the data\n  xlab(\"jobs index\") +\n  ylab(\"% of Total\") +\n  ylim(0, 100) # +\n  #ggtitle(\"Jobs of Card Holder\")  #use if standalone graph\n                       \n\n# same as above, but just for the category variable\ntable_3b_data &lt;- fraud %&gt;% count(category, sort = TRUE)\ntable_3b &lt;- table_3b_data %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Transaction Category in Credit Card Fraud\") %&gt;%\n  cols_label(category = \"Category\", n = \"Count\") %&gt;%\n  opt_stylize(style = 1,\n              color = \"blue\",\n              add_row_striping = TRUE) #%&gt;%\n#gt:::as.tags.gt_tbl(table_3b)\n\nfig_1b &lt;- ggplot(table_3b_data, aes(\n  x = 1:nlevels(fraud$category),\n  y = (cumsum(n) * 100 / nrow(fraud))\n)) +\n  geom_point(color = \"darkcyan\") +\n  geom_hline(yintercept = 80) +\n  xlab(\"category index\") +\n  ylab(\"% of Total\") +\n  ylim(0, 100) #+\n#ggtitle(\"Jobs of Card Holder\") #use if standalone graph\n\n\n#this makes the panel grid and labels it\nplot_fig_1 &lt;-\n  plot_grid(fig_1a,\n            fig_1b,\n            labels = c('A', 'B'),\n            label_size = 14)\n\n#This creates the figure title\ntitle_1 &lt;- ggdraw() +\n  draw_label(\n    \"Figure 1: Exploring Categorical Variables\",\n    fontface = 'bold',\n    x = 0,\n    hjust = 0,\n    size = 14\n  ) +\n  theme(# add margin on the left of the drawing canvas,\n    # so title is aligned with left edge of first plot\n    plot.margin = margin(0, 0, 0, 7))\n\n#this combines the panel grid, title, and displays both\nplot_grid(title_1,\n          plot_fig_1,\n          ncol = 1,\n          # rel_heights values control vertical title margins\n          rel_heights = c(0.1, 1))\n\n\n\n\nIf you look at Figure 1A, roughly 75-80 categories have to be included to capture 80% of the data. For Figure 1B, roughly ten categories have to be included. Ideally, you’d like a very steep curve initially (where a “small number” of categories cover the “majority” of the data) and then a long, shallow tail approaching 100% that corresponds to the data to be binned in “other” or dropped. There aren’t hard and fast rules on making these decisions. I decided to use 80% as my threshold. Both of these curves look relatively shallow to me, so I decided not to do any binning, grouping, or dropping of levels.\nI decided to look at all the categories of transactions just to see which ones were the most common.\n\n# Code Block 8: Exploring the Category factor\nggplot(fraud, aes(fct_infreq(category))) +\n  geom_bar(color = \"darkcyan\", fill = \"darkcyan\") +\n  ggtitle(\"Figure 2: Types of Transactions\") +\n  coord_flip() +\n  ylab(\"Count\") +\n  xlab(\"Merchant Type\")\n\n\n\n\nGas/transport was the most common category, and grocery was the second most common, both of which make sense. The least common category was travel. Nothing seemed unusual in the ranking.\n\n\n\n5.1.2. Looking at our character strings\nMerchant name (merchant) and transaction number(trans_num) are both strings. Transaction number should not influence fraud rate as it is a number assigned to the transaction when processed. I will drop it from our dataset. Merchant name could be correlated with fraud, for example, if a company’s employee was involved. However, this data is also represented by the location and category. If a location/category is found to have higher levels of fraud, then a more detailed examination of those transactions can be performed, including the merchant name. Here, I also remove it from the dataset.\n\n# Code Block 9: Removing Character/ String Variables\nfraud &lt;- fraud %&gt;%\n  select(-merchant,-trans_num)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-geographic-data",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-geographic-data",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "5.2. Looking at the geographic data",
    "text": "5.2. Looking at the geographic data\nThis data is coded as numeric (latitude and longitude) or character (city/state), but we can recognize it as geographic data and treat it appropriately.\nFirst, there are two sets of geographic data related to the merchant. The location of the merchant and where the transaction occurred. I create scatter plots of latitude and longitude separately, because I want to check the correlation between the two sources of data (merchant and transaction). I create a shared legend following the article here.\n\n# Code Block 10: Comparing Merchant and Transaction Locations\n\n# calculate correlations\ncor_lat &lt;- round(cor(fraud$lat, fraud$merch_lat), 3)\ncor_long &lt;- round(cor(fraud$long, fraud$merch_long), 3)\n\n# make figure\nfig_3a &lt;-\n  ggplot(fraud, aes(lat, merch_lat, fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5\n  ) +\n  ggtitle(\"Latitude\") +\n  ylab(\"Merchant Latitude\") +\n  xlab(\"Transaction Latitude\") +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  geom_abline(slope = 1, intercept = 0) \n\nfig_3b &lt;-\n  ggplot(fraud, aes(long, merch_long, fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5\n  ) +\n  ggtitle(\"Longitude\") +\n  ylab(\"Merchant Longitude\") +\n  xlab(\"Transaction Longitude\") +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  geom_abline(slope = 1, intercept = 0) \n\n# create the plot with the two figs on a grid, no legend\nprow_fig_3 &lt;- plot_grid(\n  fig_3a + theme(legend.position = \"none\"),\n  fig_3b + theme(legend.position = \"none\"),\n  align = 'vh',\n  labels = c(\"A\", \"B\"),\n  label_size = 12,\n  hjust = -1,\n  nrow = 1\n)\n\n# extract the legend from one of the figures\nlegend &lt;- get_legend(\n  fig_3a + \n    guides(color = guide_legend(nrow = 1)) +\n    theme(legend.position = \"bottom\")\n)\n\n# add the legend to the row of figures, prow_fig_3\nplot_fig_3 &lt;- plot_grid(prow_fig_3, legend, ncol = 1, rel_heights = c(1, .1))\n\n# title\ntitle_3 &lt;- ggdraw() +\n  draw_label(\n    \"Figure 3. Are Merchant and Transaction Coordinates Correlated?\",\n    fontface = 'bold',\n    size = 14,\n    x = 0,\n    hjust = 0\n  ) +\n  theme(plot.margin = margin(0, 0, 0, 7))\n\n# graph everything\nplot_grid(title_3,\n          plot_fig_3,\n          ncol = 1,\n          rel_heights = c(0.1, 1))\n\n\n\n\nThese two sets of data are highly correlated (for latitude = 0.994 and for longitude = 0.999) and thus are redundant. So I remove merch_lat and merch_long from the dataset.\n\n# Code Block 11: Removing merch_lat and merch_long\nfraud &lt;- fraud %&gt;%\n  select(-merch_lat,-merch_long) %&gt;%\n  rename(lat_trans = lat, long_trans = long)\n\nNext, I will look and see if some locations are more prone to fraud.\n\n# Code Block 12: Looking at Fraud by Location\nggplot(fraud, aes(long_trans, lat_trans, fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5,\n    position = \"jitter\"\n  ) +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  ggtitle(\"Figure 4: Where does fraud occur? \") +\n  ylab(\"Latitude\") +\n  xlab(\"Longitude\") \n\n\n\n\nIt looks like there are some locations which only have fraudulent transactions.\nNext, I’m going to convert city/state into latitude and longitude using the tidygeocoder package. Also included code to save this output and then re-import it. You likely do not want to be pulling the data from the internet every time you run the code, so this gives you the option to work from a local copy. For many services, it is against terms of service to repeatedly make the same calls rather than working from a local version. I did find that I could originally pull all data from ‘osm’, but while double checking this code, I found that the service is now imposing some rate limit and denies some requests, leading to some NA entries. So do check your results.\n\n# Code Block 13: Converting city/state data lat/long\n\n# need to pass an address to geo to convert to lat/long\nfraud &lt;- fraud %&gt;%\n  mutate(address = str_c(city, state, sep = \" , \"))\n\n# generate a list of distinct addresses to look up\n# the dataset is large, so it is better to only look up unique address rather that the address\n# for every record\naddress_list &lt;- fraud %&gt;%\n  distinct(address)\n\n# this has one more than number in the cities, so there must be a city with the same name in more than one state.\n\n#I don't want to run this api call everytime I open the notebook, so I downloaded the data and will reimport it and load it\n# Below is the code to run the call.  Uncomment it.\n# gets coordinates for city,states\n#home_coords &lt;-\n#  geo(address_list$address,\n#      method = \"osm\",\n#      full_results = FALSE)\n#write.csv(\"home_coords.csv\", home_coords)\n#home_coords &lt;- home_coords %&gt;%\n#  rename(lat_home = lat, long_home = long)\n\n# I downloaded it using the gui interface provided by datacamp when you view the object. This adds an extra set of \"\" compared to write.csv.\n\n# Reimport the data and load it\nhome_coords &lt;-\n  read_csv('datacamp_workspace/downloaded_coords.csv', show_col_types = FALSE)\n\n\n# imported home coords has an extra set of quotation marks\nhome_coords &lt;- home_coords %&gt;%\n  mutate(address = str_replace_all(address, \"\\\"\", \"\")) %&gt;%\n  rename(lat_home = lat, long_home = long)\n\n# use a left join on fraud and home_coords to assign the coord to every address in fraud\nfraud &lt;- fraud %&gt;%\n  left_join(home_coords, by = \"address\")\n\nNow I’m going to calculate the distance between the card holder’s home and the location of the transaction. I think distance might be a feature that is related to fraud. I followed the tutorial here for calculating distance\n\n# Code Block 14: Distance Between Home and Transaction\n\n# I believe this assuming a spherical Earth\n\n# convert to radians\nfraud &lt;- fraud %&gt;%\n  mutate(\n    lat1_radians = lat_home / 57.29577951,\n    lat2_radians = lat_trans / 57.29577951,\n    long1_radians = long_home / 57.29577951,\n    long2_radians = long_trans / 57.29577951\n  )\n\n# calculating distance\nfraud &lt;-\n  fraud %&gt;% mutate(distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)\n  ))\n\n# calculating the correlation\nfraud_distance &lt;- round(cor(fraud$distance_miles, fraud$is_fraud), 3) \n\nDespite my assumption that distance would be correlated with fraud, the correlation value is quite low, -0.003.\nI’m going to visualize it anyway.\n\n# Code Block 15: Distance from Home and Fraud\nggplot(fraud, aes(distance_miles, is_fraud , fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5,\n    position = \"jitter\"\n  ) +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  ggtitle(\"Figure 5: How far from home does fraud occur?\") +\n  xlab(\"Distance from Home (miles)\") +\n  ylab(\"Is Fraud?\") \n\n\n\n\nSome distances only have fraudulent transactions. This might be related to the locations that are only fraud, Figure 4.\nThis new feature distances_miles is retained, and the original variables (city, state) and the intermediate variables (address, variables used to calculate distance) are removed in Code Block 16.\n\n# Code Block 16: Remove Extraneous/Temp Variables\n\n# created to calculate distance\nfraud &lt;- fraud %&gt;%\n  select(-lat1_radians,-lat2_radians,-long1_radians,-long2_radians)\n\n#remove city and state and address, replaced by lat/long\nfraud &lt;- fraud %&gt;%\n  select(-city, -state, -address)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-dates",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-dates",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "5.3. Looking at the dates",
    "text": "5.3. Looking at the dates\nDate\ndob, Date of Birth of Credit Card Holder\nQuestions:\n\nWhat is the date range, and does it make sense?\nDo we have improbably old or young people?\nDo we have historic or futuristic transaction dates?\n\nI calculate the age from the dob and visualize them both.\n\n# Code Block 17: Looking at dob\n\n#summary(fraud$dob) #if you wanted a printed summary stats\n\nfig_6a &lt;- ggplot(fraud, aes(dob)) +\n  geom_histogram(color = \"darkcyan\",\n                 fill = \"darkcyan\" ,\n                 bins = 10) +\n  #ggtitle(\"How old are card Holders?\") +\n  ylab(\"Count\") +\n  xlab(\"Date of Birth\") \n\nfraud &lt;- fraud %&gt;%\n  #mutate (age = trunc((dob %--% today()) / years(1))) #if you wanted to calculate age relative to today\n  mutate(age = trunc((\n    dob %--% min(fraud$trans_date_trans_time)\n  ) / years(1)))\n#summary(fraud$age) #if you wanted a printed summary stats\n\n\nfig_6b &lt;- ggplot(fraud, aes(age)) +\n  geom_histogram(color = \"darkcyan\",\n                 fill = \"darkcyan\",\n                 bins = 10) +\n  #ggtitle(\"How old are card holders?\") +\n  ylab(\"Count\") +\n  xlab(\"Age\") \nplot_fig_6 &lt;- plot_grid(fig_6a, fig_6b, labels = c('A', 'B'))\n\ntitle_6 &lt;- ggdraw() +\n  draw_label(\n    \"Figure 6. How old are the card holders?\",\n    fontface = 'bold',\n    x = 0,\n    hjust = 0\n  ) +\n  theme(# add margin on the left of the drawing canvas,\n    # so title is aligned with left edge of first plot\n    plot.margin = margin(0, 0, 0, 7))\nplot_grid(title_6,\n          plot_fig_6,\n          ncol = 1,\n          # rel_heights values control vertical title margins\n          rel_heights = c(0.1, 1))\n\n\n\ntable_4_data &lt;- fraud %&gt;% count(age)\n\ntable_4 &lt;- table_4_data %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Ages of Card Holders\") %&gt;%\n  cols_label(age = \"Ages\", n = \"Count\") %&gt;%\n  opt_stylize(style = 1,\n              color = \"green\",\n              add_row_striping = TRUE)\ngt:::as.tags.gt_tbl(table_4)\n\n\n\n\n\n  \n    \n      Ages of Card Holders\n    \n    \n    \n      Ages\n      Count\n    \n  \n  \n    17\n2190\n    18\n2198\n    19\n6592\n    20\n741\n    21\n2207\n    22\n31\n    23\n3663\n    25\n3659\n    26\n6587\n    27\n4378\n    28\n2926\n    29\n8772\n    30\n17525\n    31\n16093\n    32\n2940\n    33\n10220\n    34\n11675\n    35\n2204\n    36\n10218\n    37\n9489\n    38\n8049\n    39\n5120\n    40\n8017\n    41\n1467\n    42\n6592\n    43\n5847\n    44\n14619\n    45\n14608\n    46\n13888\n    47\n8015\n    48\n1500\n    49\n5135\n    50\n1482\n    51\n12449\n    52\n7330\n    53\n5855\n    54\n2922\n    55\n3684\n    56\n735\n    57\n7340\n    58\n2201\n    59\n7318\n    60\n2194\n    61\n2951\n    62\n5878\n    63\n5143\n    64\n5115\n    65\n3657\n    66\n737\n    67\n4391\n    68\n6582\n    70\n1461\n    72\n8\n    73\n2939\n    75\n1463\n    76\n1472\n    77\n753\n    78\n2951\n    79\n5118\n    80\n11\n    81\n1465\n    82\n2926\n    83\n4393\n    86\n744\n    89\n2929\n    90\n3652\n    91\n2193\n  \n  \n  \n\n\n\n\nThe ages seem reasonable (calculated relative to the earliest date of transactions). There are a few thousand 17-year-olds, which is too young to have their own credit card, but it is plausible that they would be an authorized user on their parents’ card. age seems a more reasonable variable than dob, so dob is also dropped from the dataset. For example, scammers might be more likely to target 90-year-olds. The age is the feature that leads to them being targeted, not the birth year. The birth year is related to age through the current date- in 10 years, a new cohort of birth years would be targeted if age is the important feature. So the age feature is more robust to passing time than dob.\n\n# Code Block 18: Removing dob\n\nfraud &lt;- fraud %&gt;%\n  select(-dob)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-date-times",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-date-times",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "5.4. Looking at the date-times",
    "text": "5.4. Looking at the date-times\ndate-time\ntrans_date_trans_time, Transaction DateTime\nQuestions\nWould processing the date-times yield more useful predictors?\nFirst, I want to look at variation in the number of transactions with date-time. I chose to use a histogram with bins corresponding to one month widths.\n\n# Code Block 19: Looking at Transaction Date/ Times\n\nggplot(fraud, aes(trans_date_trans_time)) +\n  geom_histogram(color = \"darkcyan\",\n                 fill = \"darkcyan\",\n                 bins = 24) + #24 months in dataset\n  ggtitle(\"Figure 7: When do Transactions occur\") +\n  ylab(\"Count\") +\n  xlab(\"Date/ Time\")\n\n\n\n\nNext, I will break the transaction date-time into day of the week, hour, and the date only. I’m doing this here with lubridate functions, but I could also do this in the model building section, when I create recipes by using step_date(). I will also graph transactions by day of the week.\n\n# Code Block 20: \n\nfraud &lt;- fraud %&gt;%\n  mutate(\n    date_only = date(trans_date_trans_time),\n    hour = hour(trans_date_trans_time),\n    weekday = wday(trans_date_trans_time)\n  )\n\nggplot(fraud, aes(weekday)) +\n  geom_histogram(\n    color = \"darkcyan\",\n    fill = \"darkcyan\",\n    binwidth = 1,\n    center = 0.5\n  ) +\n  ggtitle(\"Figure 7: On what days do transactions occur?\") +\n  ylab(\"Count\") +\n  xlab(\"Weekday\")\n\n\n\n\nMonday has the highest number of transactions; this could be due to businesses processing orders that came in over the weekend. By default, lubridate codes the day of the week as a number where 1 means Monday, 7 means Sunday.\nNow, I look at what time of day do most transactions occur?\n\n# Code Block 21: What time do transactions occur\nfig_8a &lt;- ggplot(fraud, aes(hour)) +\n  geom_boxplot(color = \"darkcyan\") +\n  #ggtitle(\"What hour do transactions occur\") +\n  ylab(\"Count\") +\n  xlab(\"Hour\")\n\n\nfig_8b &lt;- ggplot(fraud, aes(hour)) +\n  geom_bar(fill = \"darkcyan\") +\n  #ggtitle(\"What hour do transactions occur\") +\n  ylab(\"Count\") +\n  xlab(\"Hour\") \n\nplot_fig_8 &lt;- plot_grid(fig_8a, fig_8b, labels = c('A', 'B'))\n\ntitle_8 &lt;- ggdraw() +\n  draw_label(\n    \"Figure 8. When do transactions occur?\",\n    fontface = 'bold',\n    x = 0,\n    hjust = 0\n  ) +\n  theme(\n    plot.margin = margin(0, 0, 0, 7))\nplot_grid(title_8,\n          plot_fig_8,\n          ncol = 1,\n          # rel_heights values control vertical title margins\n          rel_heights = c(0.1, 1))\n\n\n\n\nThis data honestly looks funny to me. I might expect that most transactions would occur during normal business hours (~9-5) or more occur during lunch or after work, but what we see is a lower number of transactions from midnight to ~ 2 pm and then a higher number of transactions from 2 pm until midnight. The odd pattern could be a sign that something is wrong with the data (perhaps timezones aren’t being encoded properly?), or it could be simply a lack of subject matter knowledge (for example, transactions are pre-authorized at the time of sale and processed later, and the transaction time is the processing time, not the sale time.) Of course, this is also a synthetic dataset, so this pattern may be simply the result of user input choices when the set was generated. If this were a real dataset, I’d chase this down.\nAnd I made a table too, just to look at this data in another way.\n\n# Code Block 22:\ntable_5_data &lt;- fraud %&gt;% count(hour)\n\ntable_5 &lt;- table_5_data %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Transactions by Time of Day\") %&gt;%\n  cols_label(hour = \"Hour\", n = \"Count\") %&gt;%\n  opt_stylize(style = 1,\n              color = \"green\",\n              add_row_striping = TRUE)\ngt:::as.tags.gt_tbl(table_5)\n\n\n\n\n\n  \n    \n      Transactions by Time of Day\n    \n    \n    \n      Hour\n      Count\n    \n  \n  \n    0\n11039\n    1\n11241\n    2\n11019\n    3\n11227\n    4\n10904\n    5\n11023\n    6\n11145\n    7\n11094\n    8\n11123\n    9\n10997\n    10\n11123\n    11\n11016\n    12\n17168\n    13\n17125\n    14\n16879\n    15\n17169\n    16\n17465\n    17\n17011\n    18\n17021\n    19\n17277\n    20\n17298\n    21\n17267\n    22\n17460\n    23\n17516\n  \n  \n  \n\n\n\n\nStill weird.\nI discard the original variable and keep the new variables.\n\n# Code Block 23:\n#removing the original variable and keeping the component variables.\nfraud &lt;- fraud %&gt;%\n  select(-trans_date_trans_time)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-numerical-variables",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-numerical-variables",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "5.5. Looking at the numerical variables",
    "text": "5.5. Looking at the numerical variables\nNumerical\namt, transaction amount\nQuestions\nWould transforming this data produce a more normal distribution?\nGenerally, more normal or at least more symmetric data tends to be fitted better, especially when using model-fitting algorithms that arise from statistics rather than pure machine learning.\nI compare the original data with the log-transformed data.\n\n# Code Block 24:\nfig_9a &lt;- ggplot(fraud, aes(amt)) +\n  geom_histogram(color = \"darkcyan\", fill = \"darkcyan\", bins = 50) +\n  #ggtitle(\"Amount of Transaction\") +\n  ylab(\"Count\") +\n  xlab(\"purchase amount ($)\")\n\nfig_9b &lt;- ggplot(fraud, aes(log(amt))) +\n  geom_histogram(color = \"darkcyan\", fill = \"darkcyan\", bins = 50) +\n  #ggtitle(\"log(Amount) of Transaction\") +\n  ylab(\"Count\") +\n  xlab(\"log(purchase amount) ($)\")\n\nplot_fig_9 &lt;-\n  plot_grid(fig_9a, fig_9b, labels = c('A', 'B'), label_size = 12)\ntitle_9 &lt;- ggdraw() +\n  draw_label(\n    \"Figure 9. Distribution of amount and log(amount)\",\n    fontface = 'bold',\n    x = 0,\n    hjust = 0\n  ) +\n  theme(\n    plot.margin = margin(0, 0, 0, 7))\nplot_grid(title_9,\n          plot_fig_9,\n          ncol = 1,\n          rel_heights = c(0.1, 1))\n\n\n\n\nThe transformed data is more symmetric so that the transformed variable will be retained.\n\n# Code Block 25:\nfraud &lt;- fraud %&gt;%\n  mutate(amt_log = log(amt))\n\nI do a final clean-up of variables next. I remove some variables that I don’t think will impact fraud- the population of the home city and the location of the home. I don’t think the home should have an impact on fraud; it is where the card is used, not where it is billed, that should matter. I suppose you could have a neighborhood where all the mail was being stolen, and cards were compromised that way, but I think most cards get compromised at the point of sale.\nI also removed the date. The date itself is unlikely to be related to fraud. It is possible that special dates are correlated with fraud, like a holiday or a big sports match. Engineering a holiday feature could be a future improvement.\nThere is a possibility that job type could have an impact on fraud; for example, a trucker might be more likely to have his/her card stolen just because they are always on the road and visiting a wide variety of places where they would use the card. Or this could come in as an interaction term with distance; distance from home and the occupation trucker might have no correlation, but the distance from home and the occupation teacher might have because it would be a more unusual event for that job. However, some model fitting fails to converge when job is included, and it takes a long time for the models that it does work for. So I remove it too.\n\n# Code Block 26:\n# removed in related clusters, so easy to comment out if you want to add back a group\n\n# remove amt and keep log transformed version\nfraud &lt;- fraud %&gt;%\n  select(-amt)\n\n# home location and home city pop shouldn't impact fraud\nfraud &lt;- fraud %&gt;%\n  select(-city_pop,-lat_home,-long_home)\n\n# remove date\nfraud &lt;- fraud %&gt;% select(-date_only)\n\n# remove jobs\nfraud &lt;- fraud %&gt;%\n  select(-job)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#splitting-the-data",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#splitting-the-data",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.1. Splitting the data",
    "text": "7.1. Splitting the data\nFirst, preparation work. Here, I split the data into a testing and training set. I also create folds for cross-validation from the training set.\n\n# Code Block 30 : Train/Test Splits & CV Folds \n\n# Split the data into a test and training set\nset.seed(222)\ndata_split &lt;-\n  initial_split(fraud, prop = 0.75, strata = is_fraud)\n\n# Create data frames for the two sets:\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\nstart_time &lt;- Sys.time()\n\nset.seed(123)\nfraud_folds &lt;- vfold_cv(train_data, v = 3, strata = is_fraud)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#creating-recipes",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#creating-recipes",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.2. Creating recipes",
    "text": "7.2. Creating recipes\nNext, I create recipes that do preprocessing of the data- making dummy variables, normalizing, and removing variables that only contain one value (step_zv(all_predictors())). The processing will be applied to both the training and testing data as you move through the workflow.\nI used the chart found in Appendix A of the Tidy Modeling with R by Max Kuhn and Julia Silge to choose the preprocessing of data. Some models require specific types of preprocessing, others don’t require it, but it can produce better or faster fitting, and in other cases, the preprocessing isn’t required and probably doesn’t help. The chart breaks this down for each category of preprocessing model by model. The same preprocessing steps were required or recommended for the models I chose, so I used them across the board. You can create recipes for different models and build a workflow manually to match the models to the proper recipe. This process is covered extensively in Chapter 15 of Tidy Modeling with R.\nI use the selector functions (all_nominal_predictors(), all_numerical_predictors(), etc.) available in the tidymodels framework. A listing of all selector functions usable in tidymodels can be found here. Using selector functions when handling groups of features reduces the chance of mistakes and typos.\nI then modify this recipe to handle the imbalanced class problem. I use SMOTE and ROSE hybrid methods to balance the classes. These methods create synthetic data for the minority class and downsample the majority class to balance the classes. I also use downsample, which throws away majority class records to balance the two classes. A good overview is here, and it also provides a tutorial for handling this type of problem with caret, rather than tidymodels. These recipe steps require the themis package.\n\n# Code Block 31: creating recipes\n\nrecipe_plain &lt;-\n  recipe(is_fraud ~ ., data = train_data) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nrecipe_rose &lt;-\n  recipe_plain %&gt;%\n  step_rose(is_fraud)\n\nrecipe_smote &lt;-\n  recipe_plain %&gt;%\n  step_smote(is_fraud)\n\nrecipe_down &lt;-\n  recipe_plain %&gt;%\n  step_downsample(is_fraud)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#setting-the-model-engines",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#setting-the-model-engines",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.3. Setting the model engines",
    "text": "7.3. Setting the model engines\nNext, I set the engines for the models. I tune the hyperparameters of the elastic net logistic regression and the lightgbm. Random Forest also has tuning parameters, but the random forest model is pretty slow to fit, and adding tuning parameters makes it even slower. If none of the other models worked well, then tuning RF would be a good idea.\n\n# Code Block 32: Setting engines\n\n#this is the standard logistic regression\nlogreg_spec &lt;-\n  logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\n#elastic net regularization of logistic regression\n#this has 2 hyperparameters that we will tune\nglmnet_spec &lt;-\n  logistic_reg(penalty = tune(),\n               mixture = tune()) %&gt;%\n  set_engine(\"glmnet\")\n\n#random forest also has tunable hyperparameters, but we won't\nrf_spec &lt;-\n  rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n#This is a boosted gradient method with 6 tuning parameters\nlightgbm_spec &lt;-\n  boost_tree(\n    mtry = tune(),\n    trees = tune(),\n    tree_depth = tune(),\n    learn_rate = tune(),\n    min_n = tune(),\n    loss_reduction = tune()\n  ) %&gt;%\n  set_engine(engine = \"lightgbm\") %&gt;%\n  set_mode(mode = \"classification\")"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#creating-a-metrics-set",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#creating-a-metrics-set",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.4. Creating a metrics set",
    "text": "7.4. Creating a metrics set\nLastly, I create a metrics set in Code Block 33. Accuracy is generally a terrible metric for highly imbalanced problems; the model can achieve high accuracy by assigning everything to the majority class. Alternate metrics like sensitivity or j-index are better choices for the imbalanced class situation.\n\n# Code Block 33: Setting Metrics\n\nfraud_metrics &lt;-\n  metric_set(roc_auc, accuracy, sensitivity, specificity, j_index)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#creating-the-workflow_set",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#creating-the-workflow_set",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.5. Creating the workflow_set",
    "text": "7.5. Creating the workflow_set\nNext, I create the workflow_set. This is where tidymodels shines. I feed it the 4 recipes and the 4 engines, and it makes all the permutations to fit. (As I mentioned earlier, you can manually create a workflow_set where you assign specific recipes to specific models, but here all recipes work with all models.)\n\n# Code block 34:\nwf_set_tune &lt;-\n  workflow_set(\n    list(plain = recipe_plain,\n         rose = recipe_rose,\n         smote = recipe_smote,\n         down = recipe_down),\n    list(glmnet = glmnet_spec,\n      lightgmb = lightgbm_spec,\n      rf = rf_spec,\n      logreg = logreg_spec\n     )\n  )"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#fitting-all-the-models",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#fitting-all-the-models",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.6. Fitting all the models",
    "text": "7.6. Fitting all the models\nI now run these 16 models. I pass workflow_map() the workflow_set from Code Block 34. The next parameter is what type of fitting you want to do. Here, I used tune_grid and had it generate 6 grid points. For the models that don’t require hyperparameter tuning, the function defaults to fit_resamples instead. The acceptable types of fitting functions are found here. It is important to note that you can only use fitting methods that operate on folds; you cannot pass workflow_map() the entire train or test set and have it work.\nI’m using the verbose option when fitting. This shows how long each model takes. When I first started, I had no idea how long various models would take. I’m running this on an older, low-end laptop (Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz 2.71 GHz, 32 GB RAM).\nI would recommend 10 folds rather than 3 if you have the time. Similarly, 6 grids points is a very low number.\n\n# Code block 35: \nset.seed(345)\ntune_results &lt;-\n  workflow_map(\n    wf_set_tune,\n    \"tune_grid\",\n    resamples = fraud_folds,\n    grid = 6,\n    metrics = fraud_metrics,\n    verbose = TRUE\n    )\n\ni  1 of 16 tuning:     plain_glmnet\n\n\n✔  1 of 16 tuning:     plain_glmnet (2m 28.3s)\n\n\ni  2 of 16 tuning:     plain_lightgmb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔  2 of 16 tuning:     plain_lightgmb (4m 58.1s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni  3 of 16 resampling: plain_rf\n\n\n✔  3 of 16 resampling: plain_rf (51.7s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni  4 of 16 resampling: plain_logreg\n\n\n✔  4 of 16 resampling: plain_logreg (9.9s)\n\n\ni  5 of 16 tuning:     rose_glmnet\n\n\n✔  5 of 16 tuning:     rose_glmnet (4m 1.2s)\n\n\ni  6 of 16 tuning:     rose_lightgmb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔  6 of 16 tuning:     rose_lightgmb (11m 46s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni  7 of 16 resampling: rose_rf\n\n\n✔  7 of 16 resampling: rose_rf (24m 23.1s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni  8 of 16 resampling: rose_logreg\n\n\n✔  8 of 16 resampling: rose_logreg (18.1s)\n\n\ni  9 of 16 tuning:     smote_glmnet\n\n\n✔  9 of 16 tuning:     smote_glmnet (5m 45.1s)\n\n\ni 10 of 16 tuning:     smote_lightgmb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 10 of 16 tuning:     smote_lightgmb (7m 24.4s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 11 of 16 resampling: smote_rf\n\n\n✔ 11 of 16 resampling: smote_rf (2m 41.2s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 12 of 16 resampling: smote_logreg\n\n\n✔ 12 of 16 resampling: smote_logreg (12.4s)\n\n\ni 13 of 16 tuning:     down_glmnet\n\n\n✔ 13 of 16 tuning:     down_glmnet (9.9s)\n\n\ni 14 of 16 tuning:     down_lightgmb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 14 of 16 tuning:     down_lightgmb (2m 25.6s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 15 of 16 resampling: down_rf\n\n\n✔ 15 of 16 resampling: down_rf (8.4s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 16 of 16 resampling: down_logreg\n\n\n✔ 16 of 16 resampling: down_logreg (2.7s)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#evaluating-the-models",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#evaluating-the-models",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.7. Evaluating the models",
    "text": "7.7. Evaluating the models\nI viewed the results of the fitting as both a table and graphically using autoplot(). The default autoplot legend is unclear, so you’ll want to do both, as I did. The legend doesn’t label by recipe (only that a recipe was used for preprocessing) and folds related categories into one. Here you see that elastic net logistic regression and logistic regression are both labeled log_reg.\nThe object we have now, tune_results, is incredibly large and complicated. Using View() on it has crashed RStudio for me. This object should be interacted with through helper functions. For more information about this, please see my other tutorial on tidymodels.\n\n#|label: rank-results-table\n# Code Block 35\nrank_results(tune_results, rank_metric = \"j_index\")\n\n# A tibble: 280 × 9\n   wflow_id       .config   .metric  mean std_err     n preprocessor model  rank\n   &lt;chr&gt;          &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n 1 down_lightgmb  Preproce… accura… 0.957 0.00240     3 recipe       boos…     1\n 2 down_lightgmb  Preproce… j_index 0.915 0.00483     3 recipe       boos…     1\n 3 down_lightgmb  Preproce… roc_auc 0.992 0.00100     3 recipe       boos…     1\n 4 down_lightgmb  Preproce… sensit… 0.959 0.00407     3 recipe       boos…     1\n 5 down_lightgmb  Preproce… specif… 0.956 0.00241     3 recipe       boos…     1\n 6 smote_lightgmb Preproce… accura… 0.967 0.00170     3 recipe       boos…     2\n 7 smote_lightgmb Preproce… j_index 0.915 0.00310     3 recipe       boos…     2\n 8 smote_lightgmb Preproce… roc_auc 0.990 0.00102     3 recipe       boos…     2\n 9 smote_lightgmb Preproce… sensit… 0.948 0.00399     3 recipe       boos…     2\n10 smote_lightgmb Preproce… specif… 0.967 0.00172     3 recipe       boos…     2\n# ℹ 270 more rows\n\n\n\n# Code Block 36\nautoplot(tune_results, rank_metric = \"j_index\", select_best = TRUE) +\n  ggtitle(\"Figure 11: Performance of various models\")\n\n\n\n\nThe best performing model / recipe pair by j-index is the downsampled lightgmb (down_lightgmb).\nTo see how this model/recipe performs across tuning parameters, we can use extract_workflow_set_result and autoplot. If you wanted to refine the hyperparameters more, you could use these results to narrow the search parameters to areas with the best performance.\n\n# Code block 37: \n\nresults_down_gmb &lt;- tune_results %&gt;%\n  extract_workflow_set_result(\"down_lightgmb\")\nautoplot(results_down_gmb) +\n  theme_pander(8) +\n  ggtitle(\"Figure 12: Perfomance of different hyperparameters\")\n\n\n\n\nIn this case, I’m just going to extract the best set of hyperparameters and move on. This is done using the extract_workflow_set_result and select_best(metric = \"j_index\"). There are other ways to select the best hyperparameters. The list of selectors is found here.\n\n# Code block 38: \nbest_hyperparameters &lt;- tune_results %&gt;%\n  extract_workflow_set_result(\"down_lightgmb\") %&gt;%\n  select_best(metric = \"j_index\")\n\nAnd here I look at the selected hyperparameters.\n\n# Code block 39: \nprint(best_hyperparameters)\n\n# A tibble: 1 × 7\n   mtry trees min_n tree_depth learn_rate loss_reduction .config             \n  &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1    19  1679    35         15     0.0279        0.00249 Preprocessor1_Model6\n\n\nNow, I am going to use the convenience functions finalize_workflow() and last_fit() to add the best hyperparameters to the workflow, train the model/recipe on the entire training set, and then predict on the entire test set. There is a lot of stuff going on here at once (Code Block 40)!\n\n# Code Block 40: Validating the model with the test data\nvalidation_results &lt;- tune_results %&gt;%\n  extract_workflow(\"down_lightgmb\") %&gt;%\n  finalize_workflow(best_hyperparameters) %&gt;%\n  last_fit(split =  data_split, metrics = fraud_metrics)\n\nLastly, I look at the metrics and ROC curve for the test data.\n\n# Code Block 41: Looking at the validation metrics from the test data.\ncollect_metrics(validation_results)\n\n# A tibble: 5 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.962 Preprocessor1_Model1\n2 sensitivity binary         0.950 Preprocessor1_Model1\n3 specificity binary         0.963 Preprocessor1_Model1\n4 j_index     binary         0.912 Preprocessor1_Model1\n5 roc_auc     binary         0.992 Preprocessor1_Model1\n\nvalidation_results %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(is_fraud, .pred_1) %&gt;%\n  autoplot() +\n  ggtitle(\"Figure 13: ROC Curve\")\n\n\n\n\nJust for fun, let’s see how much money this model would have save our credit card company. I’m going to assume the cost of fraud is the cost of the transaction. I calculate the total cost of all the fraudulent transactions in the test dataset. I then calculate the cost based on the model predictions. Any truly fraudulent transactions that were not caught, cost the value of the transaction. Legitimate transactions that were marked as fraud were assigned $0 cost. This likely isn’t true. There is the cost of having to deal with customers calling because the transaction was declined or the cost sending out texts for suspicious transactions, but this cost is very small relative to the cost of a fraudulent transaction. I got the idea from this paper: Zhang, D. , Bhandari, B. and Black, D. (2020) Credit Card Fraud Detection Using Weighted Support Vector Machine. Applied Mathematics, 11, 1275-1291. doi: 10.4236/am.2020.1112087.\nI’m using the list method to access predictions, but you could also use collect_predictions().\n\n#code block 42: Calculating how much fraud cost the company\n\nval &lt;- validation_results[[5]][[1]]\n\nval %&gt;% conf_mat(truth = is_fraud, estimate = .pred_class)\n\n          Truth\nPrediction     1     0\n         1   435  3161\n         0    23 81283\n\nval &lt;-\n  #I'm going to bind this to the test data and I want unique names\n  val %&gt;% rename(is_fraud2  = is_fraud) \n\ncost &lt;- test_data %&gt;%\n  cbind(val)\n\ncost &lt;- cost %&gt;%\n  select(is_fraud, amt_log, pred = .pred_class, is_fraud2) \n\ncost &lt;- cost %&gt;%\n  #cost for missing fraud in prediction\n  mutate(cost_act = ifelse((is_fraud == 1 &\n                              pred == 0), amt_log, 0)) %&gt;%\n  #cost of all fraud\n  mutate(cost_potential = ifelse((is_fraud == 1), amt_log, 0))\n\nmissed_fraud_cost &lt;- round(sum(exp(cost$cost_act)), 2)\nall_fraud_cost &lt;- round(sum(exp(cost$cost_potential)), 2)\n\nsavings &lt;- 100 * round((sum(exp(cost$cost_act)) / sum(exp(cost$cost_potential))), 2)\n\nMy model had dramatic costs savings for the imaginary credit card company! The losses from the model were 27 % of the potential losses."
  },
  {
    "objectID": "posts/2023-04-04-chart-challenge-4/day4.html",
    "href": "posts/2023-04-04-chart-challenge-4/day4.html",
    "title": "30 Day Chart Challenge- Endangered Species",
    "section": "",
    "text": "It is Day 4 of the #30DayChartChallenge. More info can be found at the challenge’s Github page. Today’s theme is history. But this is a subtheme of “comparisions”, so I’d like to avoid doing a simple time series.\nI decided to look at the endangered species list the US Fish and Wildlife Service maintains. They have a bunch of data spread over multiple tables. I decided to look at the 5 year review data. A 5 year review is the assessment to decide if a species remains list or delisted. The dataset also contains the year the species was first listed. So I’d like to compare how many species have been listed vs. delisted.\nThe key to the different listing types is found here.\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(skimr)\nlibrary(waffle)\n\nToday, I’m was going to load the data directly from the website. I’ve been downloading it and reading it in from a local folder, but I thought it would be nice to download directly. However, the data uses a “blob:” url, which is not donwloadable directly. There is a way around this but then you have to process some JSON data. I”ll come back to this later, but for now, I’m just going to use a csv.\n\nendangered_df &lt;- read_csv(\"five_year.csv\", show_col_types = FALSE)\n\n\nendangered_df_sub &lt;- endangered_df %&gt;%\n  select(name = `Common Name`, \n         status = `ESA Listing Status`, \n         date = `Listing Date`,\n         rec = `5YSR Recommendation`)\n\nLet’s see what kind of categories we have.\n\nendangered_df_sub &lt;- endangered_df_sub %&gt;%\n  mutate(status = factor(status), rec = factor(rec))\n\nSkim this bad boy.\n\nskim(endangered_df_sub) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      skim_type\n      skim_variable\n      n_missing\n      complete_rate\n      character.min\n      character.max\n      character.empty\n      character.n_unique\n      character.whitespace\n      factor.ordered\n      factor.n_unique\n      factor.top_counts\n      numeric.mean\n      numeric.sd\n      numeric.p0\n      numeric.p25\n      numeric.p50\n      numeric.p75\n      numeric.p100\n      numeric.hist\n    \n  \n  \n    character\nname\n0\n1\n3\n51\n0\n1159\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n    factor\nstatus\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n8\nE: 1173, T: 316, DM: 35, DNS: 3\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n    factor\nrec\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n7\nNo : 1389, Del: 49, Dow: 40, Del: 27\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n    numeric\ndate\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1993.5\n12.4735\n1967\n1987\n1993\n1999\n2017\n▂▃▇▂▃\n  \n  \n  \n\n\n\n\nrec\n\nsummary(endangered_df_sub$rec)\n\n                    Delist: The listed entity does not meet the statutory definition of a species \n                                                                                                8 \nDelist: The species does not meet the definition of an endangered species or a threatened species \n                                                                                               49 \n                                                                   Delist: The species is extinct \n                                                                                               27 \n                                                                                    Downlist to T \n                                                                                               40 \n                                                                              No change in Status \n                                                                                             1389 \n                                                                        Revision of listed entity \n                                                                                                2 \n                                                                                      Uplist to E \n                                                                                               18 \n\n\nThe recommendations don’t always match the current status. I’m assuming the recommendations will be enacted/adopted eventually, so I am using them as the correct current status.\nWe have 7 levels in recommendations. We need to consolidate them. I’m going to combine “Delist: The listed entity does not meet the statutory definition of a species” and “Delist: The species does not meet the definition of an endangered species or a threatened species” into a level called delisted. The delisting because the species is extinct will be made into a level called extinct later.\n\nendangered_df_sub &lt;- endangered_df_sub %&gt;%\n  mutate(condensed = fct_collapse(rec, delisted = c(\"Delist: The listed entity does not meet the statutory definition of a species\",\n    \"Delist: The species does not meet the definition of an endangered species or a threatened species\")\n  ))\n\nI’m going to count both “Downlist to threatened” and “uplist to Endangered” as endangered. I don’t know the original listing level, so it doesn’t make too much difference to me.\n\nendangered_df_sub &lt;- endangered_df_sub %&gt;%\n  mutate(condensed = fct_collapse(condensed, endangered = c(\"Downlist to T\",\n    \"Uplist to E\")  ))\n\nNow, I’m pulling in the status for the entries that have “No change in Status” as the recommendation. I’m using a case_when and listing every combination. I could get this done if fewer lines if I used or statements (E or T is endangered), but I left it more granular in case I wanted to come back and change the levels. Maybe later I do care about the different between threatened and endangered and want to break them out separately.\n\nendangered_df_sub &lt;- endangered_df_sub %&gt;%\n  mutate(condensed = case_when(\n    condensed == \"No change in Status\" & status == \"E\" ~ \"endangered\",\n    condensed == \"No change in Status\" & status == \"T\" ~ \"endangered\",\n    condensed == \"No change in Status\" & status == \"RT\" ~ \"delisted\",\n    condensed == \"No change in Status\" & status == \"D3A\" ~ \"extinct\",\n    condensed == \"No change in Status\" & status == \"DM\" ~ \"delisted\",\n    condensed == \"No change in Status\" & status == \"DP\" ~ \"delisted\",\n    condensed == \"No change in Status\" & status == \"DR\" ~ \"delisted\",\n    condensed == \"No change in Status\" & status == \"DNS\" ~ \"delisted\",\n    condensed != \"No change in Status\" ~ condensed)\n    )\n\nNow I’m going to group my extincts.\n\nendangered_df_sub &lt;- endangered_df_sub %&gt;%\n  mutate(condensed = \n           fct_collapse(condensed, extinct = \n                          c(\"Delist: The species is extinct\", \"extinct\")))\n\nI’m not sure what : Revision of listed entity means. I’m going to see if there are comments back in the full dataset.\n\nendangered_df %&gt;% \n  filter(`5YSR Recommendation` == \"Revision of listed entity\") %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      Scientific Name\n      Common Name\n      Where Listed\n      ESA Listing Status\n      Lead Region\n      Listing Date\n      Most Recently Completed 5YSR\n      5YSR Recommendation\n      Notice of In Progress 5YSR\n      Notice Date of In Progress 5YSR\n      Group\n    \n  \n  \n    Rangifer tarandus ssp. caribou\nCaribou DPS, Southern Mountain\n&lt;div&gt;Southern Mountain DPS&lt;/div&gt;\nE\n1\n1983\n2019-10-02\nRevision of listed entity\nNo Five Year Review In Progress\nNA\nMammals\n    Cereus eriophorus var. fragrans\nPrickly-apple, fragrant\n&lt;div&gt;&lt;/div&gt;\nE\n4\n1985\n2021-10-19\nRevision of listed entity\nNo Five Year Review In Progress\nNA\nFlowering Plants\n  \n  \n  \n\n\n\n\nI’m not seeing any explanation. There is not an entry in the code key either.\nOkay, now for a visualization. This actually seems perfect for a waffle. I’ve had bad luck with the waffle package, but know how to make it output something now. So, I will try waffling again. I did try a different package (ggwaffle) that also doesn’t work. It does let you use a dataframe, but it also doesn’t handle large numbers well. It soes let you downsample the data if the numbers are too large, but I’d rather just process the data myself to make it waffle.\nSo, first I need to summarize the data to get the counts per class.\n\nprogress &lt;- endangered_df_sub %&gt;%\n  count(condensed)\n\nprogress %&gt;% \n  gt() %&gt;%\n  cols_label(condensed = \"Status\", n = \"Number of species\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Progess of Endangered/Threatened species\")\n\n\n\n\n\n  \n    \n      Progess of Endangered/Threatened species\n    \n    \n    \n      Status\n      Number of species\n    \n  \n  \n    extinct\n27\n    delisted\n64\n    endangered\n1440\n    Revision of listed entity\n2\n  \n  \n  \n\n\n\n\nNow let’s change to percentages for optimal waffling\n\nnum_species &lt;- nrow(endangered_df_sub)\nprogress_percent &lt;- progress %&gt;%\n  mutate(n = ( n/num_species) * 100)\n\nprogress_percent &lt;- progress_percent %&gt;%\n  mutate(n = round(n,1))\n\ngt(progress_percent) %&gt;%\ncols_label(condensed = \"Status\", n = \"% of species\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Progess of Endangered/Threatened species\") \n\n\n\n\n\n  \n    \n      Progess of Endangered/Threatened species\n    \n    \n    \n      Status\n      % of species\n    \n  \n  \n    extinct\n1.8\n    delisted\n4.2\n    endangered\n93.9\n    Revision of listed entity\n0.1\n  \n  \n  \n\n\n\n#Values below 1 won't show in a waffle graph anyway, so remove them.\nprogress_percent &lt;- progress_percent %&gt;%\n  filter(n &gt;= 1)\n\nThe waffle package won’t work with dataframes for me, so make it a vector.\n\nprogress_vec = deframe(progress_percent)\n\n\nwaffle::waffle(progress_vec, colors = c(\"black\", \"darkgreen\", \"darkred\"),\n               title = \"How has the US done with our Endangered species?\",\n               xlab = \"1 square = 1%\") \n\n\n\n\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {30 {Day} {Chart} {Challenge-} {Endangered} {Species}},\n  date = {2023-04-04},\n  url = {https://lsinks.github.io/posts/2023-04-04-chart-challenge-4/day4},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “30 Day Chart Challenge- Endangered\nSpecies.” April 4, 2023. https://lsinks.github.io/posts/2023-04-04-chart-challenge-4/day4."
  },
  {
    "objectID": "posts/2023-04-03-chart-challenge-3/day3.html",
    "href": "posts/2023-04-03-chart-challenge-3/day3.html",
    "title": "30 Day Chart Challenge- Flora and Fauna",
    "section": "",
    "text": "It is Day 3 of the #30DayChartChallenge. More info can be found at the challenge’s Github page. Today’s theme is flora and fauna. I found a trove of fascinating data at Global Assessment of Reptile Distributions. I chose the dataset on body size/ mass from the paper: “Different solutions lead to similar life history traits across the great divides of the amniote tree of life.” by Shai Meiri, Gopal Murali, Anna Zimin, Lior Shak, Yuval Itescu, Gabriel Caetano, and Uri Roll (Journal of Biological Research-Thessaloniki), 202128: 3.\nOkay, let’s go. I’m going to keep the libraries to a minimum. That’s always my goal, but yet I ended up with 6!\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(gt)\nlibrary(ggthemes)\nlibrary(cowplot)\nlibrary(magick)\n\n\nfauna &lt;- read_csv(\"animals.csv\", show_col_types = FALSE)\n\nI’m going to change the class, clade, order, family and bionomial_2020 to factors. I’m keeping those columns and the mass and discarding the rest.\n\nfauna_cleaned &lt;- fauna %&gt;%\n  select(Class:`body mass (g)`) %&gt;%\n  select(-`binomial_(original files)`) %&gt;%\n  rename(mass_g = `body mass (g)`, name = binomial_2020) %&gt;%\n  mutate(Class = factor(Class),\n         Clade = factor(Clade),\n         order = factor(order),\n         family = factor(family),\n         name = factor(name))\n\nSo, let’s see what kind of data we have.\n\ntable1 &lt;- fauna_cleaned %&gt;%\n  count(Class)\n\ngt(table1)\n\n\n\n\n\n  \n    \n    \n      Class\n      n\n    \n  \n  \n    Aves\n9534\n    Mammalia\n5840\n    Reptilia\n11240\n  \n  \n  \n\n\n\n\nWe have data on more than just reptiles, the dataset includes information about birds and mammals as well. But I’m only interested in reptiles.\n\nreptiles &lt;- fauna_cleaned %&gt;%\n  filter(Class == \"Reptilia\")\n\n\ntable2 &lt;- reptiles %&gt;%\n  count(Clade, order)\n\ngt(table2)\n\n\n\n\n\n  \n    \n    \n      Clade\n      order\n      n\n    \n  \n  \n    Crocodylia\nCrocodylia\n24\n    Rhynchocephalia\nRhynchocephalia\n1\n    Squamata\nSquamata (Amphisbaenia)\n195\n    Squamata\nSquamata (Sauria)\n6868\n    Squamata\nSquamata (Serpentes)\n3837\n    Testudines\nTestudines\n315\n  \n  \n  \n\n\n\n\nEveryone knows that turtles are the best type of reptile, so let’s filter even further.\n\nturtles &lt;- reptiles %&gt;% \n  filter(Clade == \"Testudines\")\n\ntable3 &lt;- turtles %&gt;%\n  count(order, family)\n\ngt(table3)\n\n\n\n\n\n  \n    \n    \n      order\n      family\n      n\n    \n  \n  \n    Testudines\nCarettochelyidae\n1\n    Testudines\nChelidae\n53\n    Testudines\nCheloniidae\n6\n    Testudines\nChelydridae\n3\n    Testudines\nDermatemydidae\n1\n    Testudines\nDermochelyidae\n1\n    Testudines\nEmydidae\n47\n    Testudines\nGeoemydidae\n69\n    Testudines\nKinosternidae\n24\n    Testudines\nPelomedusidae\n18\n    Testudines\nPodocnemididae\n8\n    Testudines\nTestudinidae\n54\n    Testudines\nTrionychidae\n30\n  \n  \n  \n\n\n\n\nLet’s take a look at how big (or mighty, as some might say) the different families of turtles are. There is a very large range of masses so I’m using a log scale.\n\nggplot(turtles, aes(x = family, y = mass_g, color = family)) +\n  scale_y_log10() +\n  geom_boxplot() +\n  ggtitle(\"Mightiness of Different Families of Turtle and Tortoise\") +\n  ylab(\"mass (g)\") +\n  theme(legend.position = \"none\" , \n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\nThe heaviest turtles are the giantic leatherback turtles, a species of SEA TURTLE that weigh hundred of kg (around 1,000 lbs). The smallest turtle is the African dwarf mud turtle (Pelusios nanus), family Pelomedusidae, which full grown weighs under 100 grams.\nNow let’s look at everyone’s favorite family of turtles, Emydidae. This family is often known as pond or marsh turtles. Filter them out, and add a category for Eastern Box Turtles. I called the variable box_turtle, but I’m only marking Eastern. There are other types of box turtles though not all of them are in Emydidae. Asian box turtle species were reassigned to the family Geoemydidae.\nThe common name box turtle, arises from the fact that these species have a hinge on the bottom shell, and can close up/ box up completely. Other turtles and tortoises can only pull their bits within their shell.\n\n\n\nA closed Eastern Box Turtle\n\n\n\npond_turtles &lt;- turtles %&gt;%\n  filter(family == 'Emydidae') %&gt;%\n  mutate(box_turtle = ifelse(name == \"Terrapene carolina\", TRUE, FALSE)) \n\nOkay, let’s look at mightiness of the turtles in this family.\n\nturtle_plot &lt;- pond_turtles %&gt;%\n  ggplot(aes(x = fct_reorder(name, mass_g), y = mass_g, fill = box_turtle)) +\n  scale_fill_manual(values=c(\"#999999\", \"#E69F00\")) +\n  geom_col(width = 0.7, position = position_dodge(10)) +\n  coord_flip() +\n  ylab(\"mass (g)\") +\n  xlab(\"\") +\n  ggtitle(\"Mightiness of Different Turtles in family Emydidae\") +\n  labs(caption = \"Data from https://doi.org/10.1186/s40709-021-00134-9\") +\n  theme_classic() +\n  theme(axis.text = element_text(size = 6)) +\n  theme(legend.position = \"none\")\n\n\n#found how to add an image to my graph on stack overflow\n#https://stackoverflow.com/questions/63442933/how-can-i-add-a-logo-to-a-ggplot-visualisation\n\nimg &lt;- image_read(\"pqtk5r.jpg\")\n\n# Set the canvas where you are going to draw the plot and the image\nggdraw() +\n# Draw the plot in the canvas setting the x and y positions, which go from 0,0\n# (lower left corner) to 1,1 (upper right corner) and set the width and height of\n# the plot. It's advisable that x + width = 1 and y + height = 1, to avoid clipping # the plot\ndraw_plot(turtle_plot,x = 0, y = 0.15, width = 1, height = 0.85) +\n# Draw image in the canvas using the same concept as for the plot. Might need to \n# play with the x, y, width and height values to obtain the desired result\ndraw_image(img, x = 0.6, y = 0.35, width = 0.45, height = 0.45) \n\n\n\n\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {30 {Day} {Chart} {Challenge-} {Flora} and {Fauna}},\n  date = {2023-04-03},\n  url = {https://lsinks.github.io/posts/2023-04-03-chart-challenge-3/day3},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “30 Day Chart Challenge- Flora and\nFauna.” April 3, 2023. https://lsinks.github.io/posts/2023-04-03-chart-challenge-3/day3."
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-2/current_wordle.html",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-2/current_wordle.html",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 2",
    "section": "",
    "text": "This is the current version of the word game guesser. I discussed how I used this project to hone my coding skills in the companion blog post to this one.\nI’m not going to walk through this in much detail, but I’m going to point out some of the major lessons I learned as I revised the code. Again, both the initial version and this version are on GitHub in all their messiness.\nI learned how to put functions in a separate file and call them from my many script. This can make long code much easier to read. Here, I’ve included the helper functions in-line and commented out the source(\"code/helper-functions.R\") in the main code. I’ve also set up switchable troubleshooting help with verbose and debug_detail parameters in my functions. Setting them to TRUE provide more info as the functions are executed.\n\n#|label: helper-functions\nConstruct_Freq_Table &lt;- function(word_list) {\n\n#scoring code uses the counting code from\n\n#https://www.r-bloggers.com/2018/12/rrrrs-in-r-letter-frequency-in-r-package-names/  \n# making the frequency table ----\n\nletters &lt;- unlist(strsplit(word_list[,1], split = \"\"))\nchar_frequencies &lt;- as.data.frame(table(letters))\n\n#normalized\ncommon &lt;- max(char_frequencies[,2])\ny = (char_frequencies[,2]/common)\nchar_frequencies$normalized &lt;- y\nreturn(char_frequencies)\n}\n\nScoring_Word &lt;- function(word, freqs = char_frequencies,\n                         verbose = FALSE, debug_detail = FALSE){\n  letter_vec &lt;-  unlist(strsplit(word, split = \"\"))\n    if (verbose == TRUE)\n    {message(\"I'm in Scoring_words message and scoring: \", word)}\n  \n  value &lt;- 0\n  for (i in 1:length(letter_vec)) {\n    position &lt;- letter_vec[i]== freqs$letters\n    value[i] &lt;- freqs$normalized[position]\n    if (debug_detail == TRUE)\n    {\n      print(\"I am in the scoring loop calculating value: \")\n      print(i)\n      print(sum(value))\n      \n    }\n    \n    if (i == length(letter_vec)) {\n      \n      return(total &lt;- sum(value))\n    }\n    \n  }\n  }\n  \n\nScoring_Word_Unique &lt;- function(word, freqs = char_frequencies, \n                                verbose = FALSE, debug_detail = FALSE){\n  # This does only score on unique letters\n  letter_vec &lt;-  unlist(strsplit(word, split = \"\"))\n  unique_letter_vec &lt;- unique(letter_vec)\n  #unique_letter_vec &lt;- letter_vec\n  if (verbose == TRUE)\n  {message(\"I'm in Scoring_words_Unique and scoring: \", word)}\n  \n  value &lt;- 0\n  if (length(unique_letter_vec) == 0) {\n    return(value)\n  } else{\n    for (i in 1:length(unique_letter_vec)) {\n           position &lt;- unique_letter_vec[i] == freqs$letters\n          value[i] &lt;- freqs$normalized[position]\n      if (debug_detail == TRUE)\n      {\n        print(\"I am in the unique scoring loop calculating value: \")\n        print(i)\n        print(sum(value))\n      }\n      \n      if (i == length(unique_letter_vec)) {\n        \n        return(total &lt;- sum(value))\n      }\n      \n    }\n  }\n}\n\nRemoving_Letters &lt;- function(word, chosen_word, \n                              verbose = FALSE, debug_detail = FALSE) {\n  lvec &lt;- gsub(paste0(\"[\", chosen_word, \"]\"), \"\", word)  \n  return(lvec)}\n\nI finally did figure out how to make variables the types I wanted. I also replaced several loops with map functions from purrr. I also made a reshaped version of my dataframe using pivot_longer from tidyr. Reshaping data is a really useful skill, but might be a bit confusing at first. So I certainly wanted to make sure I could do it correctly. The reshaped data is used to make a nice density plot later.\n\n# Loading libraries and data ----\nlibrary(\"tidyverse\")\n\n\n#from https://www-cs-faculty.stanford.edu/~knuth/sgb-words.txt\nword_list &lt;- \n  read.table(\"C:/Users/drsin/OneDrive/Documents/R Projects/Word-Games/input/sgb-words.txt\") \n\n# Functions ----\n#source(\"code/helper-functions.R\")\n\n# calculate letter frequencies from word list\nchar_frequencies &lt;- Construct_Freq_Table(word_list)\n\n# Initialize the word_scores dataframe ----\nnum_words &lt;- nrow(word_list)\n#num_words &lt;- 5\nword_scores &lt;- data.frame(word_name = word_list[1:num_words,1],\n                    word_length = rep(0, times = num_words),\n                    word_guess1 = rep(0, times = num_words),\n                    word_guess2 = rep(0, times = num_words),\n                    word_guess3 = rep(0, times = num_words),\n                    word_guess4 = rep(0, times = num_words),\n                    score = rep(0, times = num_words), \n                    score_guess1 = rep(0, times = num_words),\n                    score_guess2 = rep(0, times = num_words),\n                    score_guess3 = rep(0, times = num_words),\n                    score_guess4 = rep(0, times = num_words)\n                                                )\n#fill in word lengths.  This is so code can be expended to longer words\nword_scores$word_length &lt;-  str_length(word_scores$word_name)\n\n# Calculates the initial scores for all words -----\n\nword_scores &lt;- word_scores %&gt;% \n  mutate(score = map_dbl(word_name, Scoring_Word))\n\nword_scores &lt;- word_scores %&gt;%\n  mutate(score_guess1 = map_dbl(word_name, Scoring_Word_Unique))\n\n\n# Finding the best first word\ntop_words &lt;- word_scores %&gt;%\n arrange(desc(score_guess1))\nword_1 &lt;- top_words$word_name[1]\n\n# Scoring for second guess\nword_scores &lt;- word_scores %&gt;%\n  mutate(word_guess2 = \n           map_chr(word_name, Removing_Letters, chosen_word = word_1))\nword_scores &lt;- word_scores %&gt;%\n  mutate(score_guess2 = map_dbl(word_guess2, Scoring_Word_Unique))\n\ntop_words &lt;- word_scores %&gt;%\n  arrange(desc(score_guess2))\n\nword_2 &lt;- top_words$word_name[1]\n\n# Scoring for third guess\nword_scores &lt;- word_scores %&gt;% \n  mutate(word_guess3 =\n           map_chr(word_guess2, Removing_Letters, chosen_word = word_2))\nword_scores &lt;- word_scores %&gt;%\n  mutate(score_guess3 = map_dbl(word_guess3, Scoring_Word_Unique))\n\n\ntop_words &lt;- word_scores %&gt;%\n  arrange(desc(score_guess3))\nword_3 &lt;- top_words$word_name[1]\n\n# Scoring for fourth guess\nword_scores &lt;- word_scores %&gt;%\n  mutate(word_guess4 = \n           map_chr(word_guess3, Removing_Letters, chosen_word = word_3))\nword_scores &lt;- word_scores %&gt;%\n  mutate(score_guess4 = map_dbl(word_guess4, Scoring_Word_Unique))\n\n\ntop_words &lt;- word_scores %&gt;%\n  arrange(desc(score_guess4))\n\nword_4 &lt;- top_words$word_name[1]\n\n# subsetting this dataframe and reshaping it.\n# This is used to make a density plot later.\nword_scores2 &lt;- word_scores %&gt;%\n   select(word_name, score_guess1, score_guess2, score_guess3, score_guess4)\nword_scores_reshaped &lt;- \n  pivot_longer(word_scores2, cols = 2:5, \n               names_to = \"score_type\", values_to = \"score\")\n\nNice visualizations were definitely not part of the initial code. In the next chunk, I make some prettier visualizations.\n\n### This is now just visualizing what we've done. ------\n\n#plotting the frequency of the letters in our word_set\nggplot(char_frequencies, \n       aes(x = fct_rev(fct_reorder(letters,  normalized)), y = normalized )) +\n  geom_col() +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Frequencies of Letters\", caption = \"from 5 letter words\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  xlab(\"Letter\") +\n  ylab(\"Frequency\") +\n    scale_y_continuous(expand = c(0, 0))\n\n\n\n## This looks at the distribution of scores as guessing occurs.  Initially, you have a\n\nword_scores_reshaped$score_type &lt;- as.factor(word_scores_reshaped$score_type)\n\nggplot(word_scores_reshaped, aes(score, fill = score_type)) +\n  geom_density(alpha = 0.5) +\n  theme_classic() +\n  labs(title = \"Evolution of Word Scores as Guessing Progresses\",\n       caption = \"for 5 letter words\") +\n  xlab(\"Score\") +\n  ylab(\"Density\") +\n  labs(fill = \"\") +\n  theme(legend.position = c(0.7, 0.8)) +\n  scale_x_continuous( expand = c(0, 0)) +\n  scale_y_continuous( expand = c(0, 0)) \n\n\n\n## Now we are visualizing what letters are picked in each guess\nguess &lt;- rep(\"not guessed\", times = 26)\nchar_frequencies &lt;- cbind(char_frequencies, guess)\n\n# this is done in reverse order because some letters are guessed in more than\n# one word and I'd like them marked at the earliest guess.\nletter_vec &lt;-  unlist(strsplit(word_4, split = \"\"))\nprint(letter_vec)\n\n[1] \"w\" \"h\" \"a\" \"c\" \"k\"\n\nfor (i in 1:length(letter_vec)) {\n  position &lt;- letter_vec[i] == char_frequencies$letters\n  char_frequencies$guess[position] &lt;- \"Guess 4\"\n}\n\nletter_vec &lt;-  unlist(strsplit(word_3, split = \"\"))\nprint(letter_vec)\n\n[1] \"d\" \"u\" \"m\" \"p\" \"y\"\n\nfor (i in 1:length(letter_vec)) {\n  position &lt;- letter_vec[i] == char_frequencies$letters\n  char_frequencies$guess[position] &lt;- \"Guess 3\"\n}\n\nletter_vec &lt;-  unlist(strsplit(word_2, split = \"\"))\nprint(letter_vec)\n\n[1] \"u\" \"n\" \"t\" \"i\" \"l\"\n\nfor (i in 1:length(letter_vec)) {\n  position &lt;- letter_vec[i] == char_frequencies$letters\n  char_frequencies$guess[position] &lt;- \"Guess 2\"\n}\n\n\nletter_vec &lt;-  unlist(strsplit(word_1, split = \"\"))\nprint(letter_vec)\n\n[1] \"a\" \"r\" \"o\" \"s\" \"e\"\n\nfor (i in 1:length(letter_vec)) {\n  position &lt;- letter_vec[i] == char_frequencies$letters\n  char_frequencies$guess[position] &lt;- \"Guess 1\"\n}\n\n\nggplot(char_frequencies, aes(\n  x = fct_rev(fct_reorder(letters,  normalized)),\n  y = normalized,\n  fill = guess)) +\n  geom_col() +\n  ggtitle(\"When Letters are Guessed\") +\n  ylab(\"Normalized Counts\") +\n  xlab(\"Letter\") +\n  theme_classic() +\n  theme(legend.position = c(0.6, 0.6)) +\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\nSo that’s the current state of this project. This was a really useful project for me and it really strengthened by R and Tidyverse skills.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {Self-Guided {Learning} Through a {Wordle} {Guess}\n    {Generator:} {Part} 2},\n  date = {2023-04-01},\n  url = {https://lsinks.github.io/posts/2023-04-01-self-guided-learning-wordle-guesser-part-2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “Self-Guided Learning Through a Wordle\nGuess Generator: Part 2.” April 1, 2023. https://lsinks.github.io/posts/2023-04-01-self-guided-learning-wordle-guesser-part-2."
  },
  {
    "objectID": "posts/2023-03-30-One-Class-SVM/one-class-svm.html",
    "href": "posts/2023-03-30-One-Class-SVM/one-class-svm.html",
    "title": "One Class SVM",
    "section": "",
    "text": "I’ve recently been playing around with classification models, specifically on data sets with a skewed class distribution. In imbalanced classification problems, one class occurs infrequently. The minority class is often the class of interest (think fraudulent transaction, positive disease diagnosis, or intruder detection). Sometimes these applications are framed as a two-class classification problem, but other times they are called anomaly, outlier, or novelty detection.\nImbalanced classification problems are tricky for a couple of reasons. Models can achieve high accuracy by classifying everything as the dominant class. You can somewhat mitigate this problem by choosing models based on other metrics, such as sensitivity. You can also downsample the data to balance the classes (which throws out a lot of data) or upsample the infrequent class using a technique like SMOTE or ROSE to create synthetic data points.\nCollecting enough labeled data can also be expensive in highly imbalanced classes. Techniques like SMOTE won’t help if you only have 2 of a class in the dataset; the model needs “sufficient” data to learn from.\nAnother way to handle a minority class is to use a one-class classifier. One-class classifiers are one of the most widely used methods in anomaly detection because it does not require extensive labeled data for training. This method can either be semi-supervised, where only the normal (major) class is used for training, or unsupervised, where the method can handle anomalies in the training class. The one-class SVM is a popular implementation of one-class classifiers.\nHere I’m going to use a toy dataset from Datacamp. They have told me that all datasets used in their courses can be used outside Datacamp.\nI’m using some specialty packages here, specifically e1071 and caret for the machine learning.\nlibrary(tidyverse)\nlibrary(skimr) # for EDA\nlibrary(corrplot) # for cool correlation graph\nlibrary(gt) # for tables\nlibrary(e1071) # for svm\nlibrary(caret) # for data split\nthyroid &lt;- read.csv(\"~/R Projects/SVM/thyroid.csv\", header = TRUE)"
  },
  {
    "objectID": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#exploratory-data-analysis",
    "href": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#exploratory-data-analysis",
    "title": "One Class SVM",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThe dataset explores thyroid disease as a function of thyroid hormone levels. I’m using a custom skim function to tailor the output. More info on that can be found here.\n\nmy_skim &lt;- skim_with(numeric = sfl(p25 = NULL, p50 = NULL, p75 = NULL)) \nthyroid_skim &lt;- my_skim(thyroid)\n\nthyroid_skim %&gt;%\n  select(-skim_type)   %&gt;% \n  gt() %&gt;%\n  cols_label(n_missing = \"# Missing\", complete_rate = \"Completeness\", \n             numeric.mean = \"Mean\", numeric.sd = \"Standard Deviation\",\n             numeric.p0 = \"Min\", numeric.p100 = \"Max\",\n             numeric.hist = \"Histogram\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Summary of Variables in Thyroid\") \n\n\n\n\n\n  \n    \n      Summary of Variables in Thyroid\n    \n    \n  \n  \n    \n      skim_variable\n      # Missing\n      Completeness\n      Mean\n      Standard Deviation\n      Min\n      Max\n      Histogram\n    \n  \n  \n    label\n0\n1\n0.0220000\n0.1467567\n0.000000\n1.00000000\n▇▁▁▁▁\n    TSH\n0\n1\n-0.6881938\n0.4455654\n-4.532599\n-0.02173999\n▁▁▁▃▇\n    T3\n0\n1\n-6.5046015\n1.3994315\n-9.268609\n-1.43659510\n▅▇▇▁▁\n    TT4\n0\n1\n-1.7235631\n0.4421667\n-5.350910\n-0.37417607\n▁▁▁▇▁\n    T4U\n0\n1\n-1.4666057\n0.4495771\n-6.164484\n0.00000000\n▁▁▁▇▂\n    FTI\n0\n1\n-1.0093125\n0.2522809\n-3.569533\n-0.17950862\n▁▁▁▇▂\n    TBG\n0\n1\n-1.7932517\n0.4318577\n-6.636603\n0.00000000\n▁▁▁▇▁\n  \n  \n  \n\n\n\n\nWe see that the dataset is complete with no missing values. All data types are numeric. About 2% of the patients are diagnosed with thyroid disease.\nI like to look at a correlation plot to get an overview of how the predictors relate to each other and the outcome. The correlation plot created by corrplot() has the title truncated in a lot of notebook/ markdown environments. The solution, which I found here, is to add a margin.\n\n# examining correlation between variables categories\n# moving the outcome to the first column to start\n# will be sorted by decreasing correlation with outcome\nthyroid %&gt;%\n    dplyr::select(label, everything()) %&gt;%\n    cor %&gt;%\n        {.[order(abs(.[, 1]), decreasing = TRUE), \n       order(abs(.[, 1]), decreasing = TRUE)]} %&gt;% \n    corrplot( type = 'lower', tl.col = 'black', \n            addCoef.col = 'black', cl.ratio = 0.2, tl.srt = 45, \n            col = COL2('PuOr', 10), diag = FALSE , mar = c(0,0,2,0),\n            title = \" Correlations between Thyroid Disease and hormone levels\")\n\n\n\n\nMany of the features are strongly correlated with the outcome. So, we can expect to get reasonably decent results from our model."
  },
  {
    "objectID": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#setting-up-for-ml-with-caret",
    "href": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#setting-up-for-ml-with-caret",
    "title": "One Class SVM",
    "section": "Setting up for ML with caret",
    "text": "Setting up for ML with caret\nI’m using the e1071 package for SVM, which is not supported by tidymodels, so I will use caret as the wrapper for a lot of the machine modeling workflow. First, I’m going to make a train and test split. createDataPartition will stratify the sampling over the two classes if you pass it the vector of labels. Stratification is usually critical with an imbalanced dataset; you don’t want a scenario where the train or test dataset has most of the minority class observations.\n\n# Relabel the classes to TRUE if it is normal data and FALSE if it is\n# an anomaly.  (That is, it is false that the outlier data is normal).  \n# makes it easier to compare with the output of the SVM model.  \nthyroid &lt;- thyroid %&gt;%\n  mutate(label = ifelse(label == 0, TRUE, FALSE))\n\n# create data split for test and training\n# will be split among strata\nset.seed(2346)\ninTrain &lt;- createDataPartition(thyroid$label, p = 0.6, list = FALSE) \n\n# formatting the data as required for svm()\ntrain_predictors &lt;- thyroid[inTrain, 2:7]\ntrain_labels &lt;- thyroid[inTrain, 1]\n\n# Creating the test set\ntest &lt;- thyroid[-inTrain,]\n\n# formatting the data as required for svm()\ntest_predictors &lt;- test[,2:7]\ntest_labels &lt;- test[,1]\n\n#double checking that the test and train sets do contain ~2% disease\n# or rather 98% normal.\nmean(train_labels)\n\n[1] 0.9767055\n\nmean(test_labels)\n\n[1] 0.9799499"
  },
  {
    "objectID": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#two-class-svm",
    "href": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#two-class-svm",
    "title": "One Class SVM",
    "section": "Two-class SVM",
    "text": "Two-class SVM\nFirst, I’m going to fit the data with a traditional 2 class classifier. I’m using SVM for the classification. The option type ='C-classification' performs normal classification. I’m not going to get into the details of SVM here, but for more information check out this tutorial. I’m also not going to tune any hyper-parameters.\n\n# fitting SVM on training data \ntwo_class_svm_model &lt;- svm(train_predictors, y = train_labels,\n               type = 'C-classification',\n               scale = TRUE,\n               kernel = \"radial\")\n\n# now predicting both classes on train and test data\ntwo_class_svm_predtrain &lt;- predict(two_class_svm_model,train_predictors)\ntwo_class_svm_predtest &lt;- predict(two_class_svm_model,test_predictors)\n\n\n# code below here will be provided\n# seeing how well the model did\ntwo_class_confTrain &lt;- table(Predicted = two_class_svm_predtrain, Reference = train_labels)\ntwo_class_confTest &lt;- table(Predicted = two_class_svm_predtest, Reference = test_labels)\n\n# printing out the results\nprint(\"These are the predictions on the training data:\")\n\n[1] \"These are the predictions on the training data:\"\n\nprint(two_class_confTrain)\n\n         Reference\nPredicted FALSE TRUE\n    FALSE    12    0\n    TRUE      2  587\n\nprint(\"These are the predictions on the test data:\")\n\n[1] \"These are the predictions on the test data:\"\n\nprint(two_class_confTest)\n\n         Reference\nPredicted FALSE TRUE\n    FALSE     6    0\n    TRUE      2  391\n\n\nWe see that the two-class classifier does very well! In the test data set, it correctly predicts 397/ 399 data points. However, it misidentified a quarter of the disease patients as having normal thyroid. This is as I mentioned above- models can generally achieve good accuracy, but by over predicting the majority class. This result could potentially be unacceptable for a healthcare application."
  },
  {
    "objectID": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#one-class-svm",
    "href": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#one-class-svm",
    "title": "One Class SVM",
    "section": "One-class SVM",
    "text": "One-class SVM\nNow, let’s compare this to the one-class classifier. I will use the one-class classifier in supervised mode; that is, I will pass it labeled data, but only for the normal class. Then I will predict and calculate metrics based on both classes. There are a few different ways we can prepare this data. For ease of comparison with the regular classifier, I will use the same splits but filter out the anomalies from the training data. You might instead filter out all the outliers from the training set and add them to the test set, so you can get a better idea of how the model works for outlier detection. However, I want an apples-to-apples comparison, so I’m not doing that here. The regular and one class SVM will be predicting on the same test data set.\n\n# subset the labeled data into the two classes\n# the normal class should be called \"train_normal\" and the anomaly\n# class should be called \"test_outlier\"\n\ntrain_normal_class &lt;- subset(thyroid[inTrain, ], label == TRUE)\n\n\ntrain_normal_class_pred &lt;- train_normal_class[,2:7]\ntrain_normal_class_label &lt;- train_normal_class[,1]\n\n\n# fitting one class SVM on training data- no labels needed! \none_class_svm_model &lt;- svm(train_normal_class_pred, y = NULL,\n               type = 'one-classification',\n               nu = 0.10,\n               scale = TRUE,\n               kernel = \"radial\")\n\n# now predicting both classes on train and test data\none_class_svm_predtrain &lt;- predict(one_class_svm_model,train_normal_class_pred)\none_class_svm_predtest &lt;- predict(one_class_svm_model,test_predictors)\n\n\n# code below here will be provided\n# seeing how well the model did\none_class_confTrain &lt;- table(Predicted = one_class_svm_predtrain,\n                             Reference = train_normal_class_label)\none_class_confTest &lt;- table(Predicted = one_class_svm_predtest,\n                            Reference = test_labels)\n\n# printing out the results\nprint(\"These are the predictions on the normal class training data only:\")\n\n[1] \"These are the predictions on the normal class training data only:\"\n\nprint(one_class_confTrain)\n\n         Reference\nPredicted TRUE\n    FALSE   61\n    TRUE   526\n\nprint(\"These are the predictions on the test data with both classes:\")\n\n[1] \"These are the predictions on the test data with both classes:\"\n\nprint(one_class_confTest)\n\n         Reference\nPredicted FALSE TRUE\n    FALSE     8   40\n    TRUE      0  351\n\n\nThis model doesn’t do quite as well, but it is pretty impressive given that it only learned on normal data. It correctly predicted 359/399 data points in the test set. It incorrectly classified 44 cases as abnormal when they were normal, but correctly found all 8 disease cases.\nSo now I’ve showed you how to use a one-class SVM to predict outliers. This is an incredible useful tool to keep in mind for classification tasks."
  },
  {
    "objectID": "posts/2023-03-25-twitter-cards/twitter-cards.html",
    "href": "posts/2023-03-25-twitter-cards/twitter-cards.html",
    "title": "Twitter Cards",
    "section": "",
    "text": "Trying to get the picture to show in a twitter card. Apparently you need to specify the image on every document, not just in the main yml doc, which is what I understood from the instructions.\nNow this works for new posts, but not old posts.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {Twitter {Cards}},\n  date = {2023-03-24},\n  url = {https://lsinks.github.io/posts/2023-03-25-twitter-cards/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “Twitter Cards.” March 24, 2023. https://lsinks.github.io/posts/2023-03-25-twitter-cards/."
  },
  {
    "objectID": "posts/2023-03-21-tidytuesday-programming-languages/index.html",
    "href": "posts/2023-03-21-tidytuesday-programming-languages/index.html",
    "title": "TidyTuesday Week 12: Programming Languages",
    "section": "",
    "text": "This is my first attempt at Tidy Tuesday. The dataset today is about Programming Languages. The sample visualizations are about the comment codes.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(ggrepel)\n\nLoad the data first. There has been some cleaning done as outlined on the TidyTuesday github page.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 12)\n\nlanguages &lt;- tuesdata$languages\n\nFirst, let’s look at how complete the data is. The skimr package produces nice summary information about the variables and their completeness.\n\nskim(languages)\n\n\nData summary\n\n\nName\nlanguages\n\n\nNumber of rows\n4303\n\n\nNumber of columns\n49\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n21\n\n\nlogical\n4\n\n\nnumeric\n24\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n4303\n0\n\n\ntitle\n0\n1.00\n1\n56\n0\n4267\n0\n\n\ndescription\n3480\n0.19\n4\n2273\n0\n811\n0\n\n\ntype\n0\n1.00\n2\n27\n0\n40\n0\n\n\ncreators\n3203\n0.26\n2\n253\n0\n985\n0\n\n\nwebsite\n2928\n0.32\n13\n131\n0\n1368\n0\n\n\ndomain_name\n3588\n0.17\n6\n32\n0\n700\n0\n\n\nreference\n2314\n0.46\n15\n251\n0\n1955\n0\n\n\ngithub_repo\n3402\n0.21\n25\n73\n0\n897\n0\n\n\ngithub_repo_description\n3438\n0.20\n4\n419\n0\n853\n0\n\n\ngithub_language\n3829\n0.11\n1\n30\n0\n474\n0\n\n\ngithub_language_tm_scope\n3837\n0.11\n4\n34\n0\n361\n0\n\n\ngithub_language_type\n3837\n0.11\n4\n11\n0\n4\n0\n\n\ngithub_language_ace_mode\n3838\n0.11\n1\n16\n0\n96\n0\n\n\ngithub_language_file_extensions\n3833\n0.11\n1\n606\n0\n466\n0\n\n\nwikipedia\n2731\n0.37\n32\n104\n0\n1566\n0\n\n\nwikipedia_summary\n2884\n0.33\n17\n6741\n0\n1407\n0\n\n\nwikipedia_related\n3145\n0.27\n1\n1761\n0\n1059\n0\n\n\nline_comment_token\n3831\n0.11\n1\n7\n0\n23\n0\n\n\norigin_community\n1190\n0.72\n3\n305\n0\n2232\n0\n\n\nfile_type\n3213\n0.25\n2\n6\n0\n4\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n3683\n0.14\n1.00\nTRU: 617, FAL: 3\n\n\nfeatures_has_semantic_indentation\n3722\n0.14\n0.11\nFAL: 516, TRU: 65\n\n\nfeatures_has_line_comments\n3765\n0.13\n0.96\nTRU: 517, FAL: 21\n\n\nis_open_source\n3792\n0.12\n0.89\nTRU: 453, FAL: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1991.11\n111.44\n-2000\n1984.00\n1997.0\n2012.00\n2023\n▁▁▁▁▇\n\n\ndomain_name_registered\n3801\n0.12\n2011.33\n7.02\n1990\n2007.00\n2013.0\n2017.00\n2023\n▁▃▃▇▆\n\n\nisbndb\n3217\n0.25\n7.71\n33.16\n0\n0.00\n0.0\n2.00\n400\n▇▁▁▁▁\n\n\nbook_count\n0\n1.00\n2.08\n17.34\n0\n0.00\n0.0\n0.00\n401\n▇▁▁▁▁\n\n\nsemantic_scholar\n3545\n0.18\n3.79\n8.32\n0\n0.00\n0.0\n3.00\n52\n▇▁▁▁▁\n\n\nlanguage_rank\n0\n1.00\n2151.00\n1242.31\n0\n1075.50\n2151.0\n3226.50\n4302\n▇▇▇▇▇\n\n\ngithub_repo_stars\n3414\n0.21\n2127.40\n7554.02\n0\n29.00\n194.0\n1071.00\n88526\n▇▁▁▁▁\n\n\ngithub_repo_forks\n3417\n0.21\n261.29\n1203.00\n0\n2.25\n16.0\n91.50\n23732\n▇▁▁▁▁\n\n\ngithub_repo_updated\n3418\n0.21\n2021.39\n1.76\n2012\n2022.00\n2022.0\n2022.00\n2023\n▁▁▁▁▇\n\n\ngithub_repo_subscribers\n3418\n0.21\n62.34\n200.88\n0\n4.00\n13.0\n44.00\n2910\n▇▁▁▁▁\n\n\ngithub_repo_created\n3425\n0.20\n2015.84\n3.48\n2006\n2013.00\n2016.0\n2019.00\n2022\n▁▅▇▇▇\n\n\ngithub_repo_issues\n3518\n0.18\n123.03\n546.26\n0\n1.00\n9.0\n61.00\n9522\n▇▁▁▁▁\n\n\ngithub_repo_first_commit\n3567\n0.17\n2014.74\n4.99\n1987\n2012.00\n2015.0\n2018.00\n2022\n▁▁▁▆▇\n\n\ngithub_language_repos\n3833\n0.11\n197134.67\n1226900.57\n0\n91.25\n725.5\n7900.25\n16046489\n▇▁▁▁▁\n\n\nwikipedia_daily_page_views\n2837\n0.34\n227.13\n783.55\n-1\n9.00\n24.0\n99.00\n13394\n▇▁▁▁▁\n\n\nwikipedia_backlinks_count\n2877\n0.33\n318.55\n1635.29\n1\n13.00\n39.0\n126.00\n34348\n▇▁▁▁▁\n\n\nwikipedia_page_id\n2893\n0.33\n9167847.21\n13506832.90\n928\n375153.75\n2114700.5\n12321223.00\n63063548\n▇▁▁▁▁\n\n\nwikipedia_appeared\n2958\n0.31\n1991.14\n17.03\n1830\n1980.00\n1994.0\n2005.00\n2019\n▁▁▁▃▇\n\n\nwikipedia_created\n3040\n0.29\n2005.75\n3.77\n2001\n2003.00\n2005.0\n2007.00\n2020\n▇▇▂▁▁\n\n\nwikipedia_revision_count\n3130\n0.27\n330.43\n813.26\n1\n35.00\n84.0\n242.00\n10104\n▇▁▁▁▁\n\n\nlast_activity\n0\n1.00\n2000.62\n84.60\n-900\n1992.00\n2006.0\n2021.00\n2023\n▁▁▁▁▇\n\n\nnumber_of_users\n0\n1.00\n13771.26\n227712.95\n0\n0.00\n20.0\n230.00\n7179119\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n422.18\n12572.99\n0\n0.00\n0.0\n0.00\n771996\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n1482\n0.66\n0.00\n0.00\n0\n0.00\n0.0\n0.00\n0\n▁▁▇▁▁\n\n\n\n\n\nThe data is pretty incomplete. Only 9 of the 49 variables are fully complete. The line comment token is only 0.110 complete and the has comments is only 0.144 complete. This variable has only 3 false values; it is likely that the missing data is skewed towards false. It is more likely that you’d complete this entry if there were a comment, than if there weren’t. It is also possible that the cleaning and prep done to prepare the #TidyTuesday dataset removed some entries which did have FALSE values for the comments.\nThere are some funny entries that appeared in the skim report, like -2000 as the year the earliest language appeared. It turns out this is Babylonian numerals, so it probably correct. This does show there is a lot more than computer languages in this dataset though.\nLooking through the variables, I see there is a “type” in the data dictionary, and it appears that “pl” means programming language. So let’s filter for that. (I couldn’t find an explanation of this variable on https://pldb.com/) It is used on various pages, but I couldn’t find the definition of the types.\nAlso, rank starts at 0, and I’d like it to start at 1.\n\nprogramming_lang &lt;- languages %&gt;%\n  filter(type == 'pl') %&gt;%\n  select(-starts_with(\"github\"), -starts_with(\"wikipedia\"),\n         -description, -creators, -(website:semantic_scholar)) %&gt;%\n  mutate(language_rank = language_rank + 1)\n\nskim(programming_lang)\n\n\nData summary\n\n\nName\nprogramming_lang\n\n\nNumber of rows\n3368\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlogical\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n3368\n0\n\n\ntitle\n0\n1.00\n1\n54\n0\n3347\n0\n\n\ntype\n0\n1.00\n2\n2\n0\n1\n0\n\n\nline_comment_token\n3002\n0.11\n1\n3\n0\n18\n0\n\n\norigin_community\n883\n0.74\n3\n176\n0\n1825\n0\n\n\nfile_type\n2609\n0.23\n4\n4\n0\n1\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n2886\n0.14\n1.00\nTRU: 482\n\n\nfeatures_has_semantic_indentation\n2917\n0.13\n0.09\nFAL: 410, TRU: 41\n\n\nfeatures_has_line_comments\n2954\n0.12\n0.97\nTRU: 401, FAL: 13\n\n\nis_open_source\n2984\n0.11\n0.85\nTRU: 328, FAL: 56\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1994.16\n17.34\n1948\n1982.0\n1994.0\n2010.0\n2022\n▁▅▇▇▇\n\n\nlanguage_rank\n0\n1.00\n2296.75\n1249.08\n1\n1243.5\n2334.5\n3423.5\n4303\n▆▆▆▆▇\n\n\nlast_activity\n0\n1.00\n2002.04\n17.91\n1951\n1989.0\n2005.0\n2019.0\n2023\n▁▂▃▆▇\n\n\nnumber_of_users\n0\n1.00\n10793.85\n190197.19\n0\n0.0\n15.0\n165.0\n5962666\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n160.22\n2692.65\n0\n0.0\n0.0\n0.0\n85206\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n939\n0.72\n0.00\n0.00\n0\n0.0\n0.0\n0.0\n0\n▁▁▇▁▁\n\n\n\n\n\nThis now produces a dataset with 0.143 completeness for features_has_comments. All non-missing entries are TRUE, which again suggests that FALSE is over represented in the missing data.\nLet’s only look at the programming languages that have data for comments.\n\nprogramming_lang &lt;- programming_lang %&gt;%\n  filter(features_has_comments == TRUE)\n\nskim(programming_lang)\n\n\nData summary\n\n\nName\nprogramming_lang\n\n\nNumber of rows\n482\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlogical\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n35\n0\n482\n0\n\n\ntitle\n0\n1.00\n1\n45\n0\n481\n0\n\n\ntype\n0\n1.00\n2\n2\n0\n1\n0\n\n\nline_comment_token\n120\n0.75\n1\n3\n0\n18\n0\n\n\norigin_community\n112\n0.77\n3\n105\n0\n311\n0\n\n\nfile_type\n146\n0.70\n4\n4\n0\n1\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n0\n1.00\n1.00\nTRU: 482\n\n\nfeatures_has_semantic_indentation\n57\n0.88\n0.05\nFAL: 405, TRU: 20\n\n\nfeatures_has_line_comments\n71\n0.85\n0.97\nTRU: 400, FAL: 11\n\n\nis_open_source\n305\n0.37\n0.91\nTRU: 161, FAL: 16\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n2000.17\n14.07\n1957\n1991.00\n2003.0\n2011.00\n2022\n▁▂▆▇▇\n\n\nlanguage_rank\n0\n1.00\n656.10\n559.75\n1\n201.25\n515.5\n997.25\n2994\n▇▃▂▁▁\n\n\nlast_activity\n0\n1.00\n2016.20\n8.27\n1967\n2011.00\n2022.0\n2022.00\n2023\n▁▁▁▂▇\n\n\nnumber_of_users\n0\n1.00\n62892.08\n462314.18\n0\n112.00\n437.5\n1615.25\n5962666\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n971.30\n6489.83\n0\n0.00\n0.0\n0.00\n85206\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n136\n0.72\n0.00\n0.00\n0\n0.00\n0.0\n0.00\n0\n▁▁▇▁▁\n\n\n\n\n\nThis subset is still moderately incomplete for information about comments. Only 75% of the data has the type of comment entered (#, //, etc). 86% of the entries are completed for “feature_has_line_comments” which indicates if comments must occupy a single line or if they can be made inline.\n\nprogramming_lang %&gt;% filter(is.na(line_comment_token) == FALSE) %&gt;%\n  group_by(line_comment_token) %&gt;%\n  count(line_comment_token) %&gt;%\n  ggplot(aes(fct_rev(fct_reorder(line_comment_token, n)), n)) +\n  geom_col(fill = \"dodgerblue2\") +\n  ylab(\"Count\") +\n  xlab(\"Comment Token\") +\n  ggtitle(\"Popularity of different comment tokens\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,  vjust = 0.25, hjust = 0.25))\n\n\n\n\nLet’s make a nice table of the popular comment types.\n\n# | label: table-tokens\nprogramming_lang2 &lt;- programming_lang %&gt;%\n  filter(is.na(line_comment_token) == FALSE) %&gt;%\n  count(line_comment_token, sort = TRUE) \n\nprogramming_lang2 %&gt;%\ngt() %&gt;%\ntab_header(title = \"Most Common Comment Tokens\") %&gt;%\ncols_label(line_comment_token = \"Token\", n = \"# of Languages that use token\")\n\n\n\n\n\n  \n    \n      Most Common Comment Tokens\n    \n    \n  \n  \n    \n      Token\n      # of Languages that use token\n    \n  \n  \n    //\n161\n    #\n70\n    ;\n49\n    --\n31\n    '\n16\n    %\n12\n    !\n7\n    *\n5\n    REM\n2\n    *&gt;\n1\n    ---\n1\n    /\n1\n    NB.\n1\n    \\\n1\n    \\*\n1\n    __\n1\n    ~\n1\n    ⍝\n1\n  \n  \n  \n\n\n\n\nThere is a language rank, which measures the popularity of the language based on signals such as number of users and number of jobs. Let’s see the average rank of languages for each token.\n\nprogramming_lang %&gt;% filter(is.na(line_comment_token) == FALSE) %&gt;%\n  group_by(line_comment_token) %&gt;%\n  summarize(avg_rank = mean(language_rank)) %&gt;%\n  ggplot(aes((fct_reorder(line_comment_token, avg_rank)), avg_rank)) +\n  geom_col(fill = \"dodgerblue2\") +\n  ylab(\"Average Rank of Language\") +\n  xlab(\"Comment Token\") +\n  ggtitle(\"Average rank of languages using different comment tokens\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,  vjust = 0.25, hjust = 0.25))\n\n\n\n\nThe highest (average) ranked token is “*&gt;”. What languages use this?\n\nprogramming_lang %&gt;% filter(line_comment_token == \"*&gt;\") %&gt;%\n  select(title, language_rank, line_comment_token)\n\n# A tibble: 1 × 3\n  title language_rank line_comment_token\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;             \n1 COBOL            19 *&gt;                \n\n\nOnly COBOL does, so the rank of this token isn’t diluted by many less popular languages. We can view the distribution of the language ranks for all the tokens.\n\nprogramming_lang %&gt;%\n  filter(is.na(line_comment_token) == FALSE) %&gt;%\n  ggplot(aes(line_comment_token, language_rank)) +\n  geom_boxplot(color = \"dodgerblue2\") +\n  ggtitle(\"The rank of languages by token.\") +\n  xlab(\"Token\") +\n  ylab (\"Language Rank\") +\n  theme_classic()\n\n\n\n\nOkay, let’s clean this up. I’d like it sorted by the median rank. Remeber rank is in reverse numerical order- a low number means a higher rank.\n\nprogramming_lang %&gt;%\n  filter(is.na(line_comment_token) == FALSE) %&gt;%\n  ggplot(aes(fct_reorder(line_comment_token, language_rank,\n                         .fun = median, .desc = FALSE), language_rank)) +\n  geom_boxplot(color = \"dodgerblue2\") +\n  ggtitle(\"The rank of languages by token\") +\n  xlab(\"Token\") +\n  ylab(\"Language Rank\") +\n    theme_classic()\n\n\n\n\nLet’s see the most popular language for each symbol. There might be a way to do this all at once, but I’m going to pull it out with joins to previous tables I’ve created.\n\nprogramming_lang3 &lt;- programming_lang %&gt;%\n  filter(is.na(line_comment_token) == FALSE) %&gt;%\n  group_by(line_comment_token) %&gt;%\n  summarize(highest_rank = min(language_rank)) \n\njoin_madness &lt;- programming_lang2 %&gt;%\n  left_join(programming_lang3, by = \"line_comment_token\") %&gt;% \n  left_join(programming_lang, \n            by = c(\"highest_rank\" = \"language_rank\",\n                   \"line_comment_token\" = \"line_comment_token\")) \n\njoin_madness &lt;- join_madness %&gt;%\n  select(line_comment_token, n, highest_rank, title, appeared, number_of_users,\n         number_of_jobs)\n\nSo now we have a bunch of summarized data in a single dataframe. Here’s a graph. It is saying something, but I’m not sure what. When you can’t come up with a concise title, then you probably don’t know what you are trying to say…\n\njoin_madness %&gt;%\n  ggplot(aes(highest_rank, n, size = log(number_of_users), \n             color = log(number_of_users), label = line_comment_token)) +\n  scale_y_log10() +\n  scale_x_log10() +\n  geom_text_repel(show.legend = FALSE) +\n  ggtitle(\"Popularity of tokens by language rank and usage\") +\n  xlab(\"Highest Rank of language using Token\") +\n  ylab(\"Number of Languages using token\") +\n  theme_classic()\n\n\n\n\nThis is a visualization of the highest ranked languages for each token. The number of users of the dominant language is also encoded in the size and color of the label. Having it ordered makes it difficult to tell if Java or Python is the most popular/ highest ranked language.\n\njoin_madness %&gt;%\n  ggplot(aes(fct_rev(fct_reorder(line_comment_token, highest_rank)), n,\n             size = log(number_of_users), color = log(number_of_users),\n             label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\nHere is the same graph just ordered “alphabetically” by token.\n\njoin_madness %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {TidyTuesday {Week} 12: {Programming} {Languages}},\n  date = {2023-03-21},\n  url = {https://lsinks.github.io/posts/2023-03-21-tidytuesday-programming-languages/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “TidyTuesday Week 12: Programming\nLanguages.” March 21, 2023. https://lsinks.github.io/posts/2023-03-21-tidytuesday-programming-languages/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Louise E. Sinks",
    "section": "",
    "text": "Hello! I’m Louise Sinks.\nI’m a chemist by training. My favorite part was always analysis and modeling. I’ve recently been learning R and Python, and this webpage is a place to document my data science journey. While I did a lot of coding and modeling as a chemist, I did not formally learn about modeling and machine learning frameworks. (I wish I had! My post-doc would have been a lot smoother.)"
  },
  {
    "objectID": "blogroll.html",
    "href": "blogroll.html",
    "title": "Blog Roll",
    "section": "",
    "text": "Blogs:\nR Bloggers\nTutorials I’ve followed:\nWord Games Project:\nr letter frequency in R packages via R-Bloggers\nSetting Up Webpage:\nCreating Quarto Websites by Sam Csik\nAlbert Rapp: The ultimate guide to starting a Quarto blog"
  },
  {
    "objectID": "posts/2023-03-14-tester-post/index.html",
    "href": "posts/2023-03-14-tester-post/index.html",
    "title": "Creating a Blog",
    "section": "",
    "text": "This is my first blog entry. I am following the tutorials here:\nhttps://ucsb-meds.github.io/creating-quarto-websites/#where-you-should-start-changing-stuff\nhttps://samanthacsik.github.io/posts/2022-10-24-quarto-blogs/\nGenerally, this process has been a nightmare. The website is being created within RStudio, then pushed to GitHub and published through GitHub pages. As I’ve made changes per the tutorial, I have repeatedly been unable to push changes to GitHub due to a variety of fatal errors and merge conflicts. Since I’m only working in a single place I have no idea where all these merge conflicts are arising from. I don’t understand how I can have everything in sync everywhere, make a local change, commit it, and then be unable to push it. I’ve had to delete the GitHub repository at least half a dozen times and recreate it from my local version because I couldn’t find any way to fix the conflicts and fatal errors. I’m not sure whose fault this is (Quarto, GitHub or RStudio). It could be my fault, but I really don’t understand why things are breaking so spectacularly. I’ve used git/ GitHub for version control of R projects before and I’ve never had an error. (I don’t really see how you can get a merge conflict if you are the only person working on a project and you are only working at a single location, but maybe I’m failing to envision some use case.)\nI decided to go with Quarto because it is now built-in to RStudio and the tutorials by Samantha Csik seemed very clear. (And to be fair, they are! Very easy to follow.) The tutorials I found for R Markdown to make a website seemed a little more involved and a little more kludgey.\nCouldn’t have done it without the best helper turtle in the world. Mac took a lot of executive naps to work on the problem.\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {Creating a {Blog}},\n  date = {2023-03-14},\n  url = {https://lsinks.github.io/2023-03-14_tester-post},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “Creating a Blog.” March 14, 2023.\nhttps://lsinks.github.io/2023-03-14_tester-post."
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "",
    "text": "This week, I participated in my first #TidyTuesday challenge. My goal was to get something out on the day of the challenge rather than perfection. I did notice that the skimr output wasn’t formatted nicely on the webpage. Today, I’m going to delve into the skimr and Quarto documentation and make a nicer version of the output. Secondly, I’m going to fix up my final figure, which is the one I shared on social media:"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#skimr-to-understand-your-data",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#skimr-to-understand-your-data",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "Skimr to understand your data",
    "text": "Skimr to understand your data\nSkimr is a package that provides statistical summaries of the variables in your dataframe. It also provides information about the missingness of each variable.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(ggrepel)\nlibrary(visdat) # visualizing missing data in dataframe\n\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 12)\n\nlanguages &lt;- tuesdata$languages\n\n\nCustomizing the skim Output\nMy main objection is that the numerical summary is too wide and has a scroll bar. I especially want the histogram to be viewable on the first screen. I also don’t particularly care about all the quartile information; min and max are enough. If I want to delve more into the stats of a variable, I will do it another way, not with skimr.\nFirst, quarto lets you expand the output of the code chunk to fill the page via the option “#| column: page”, so I’ll do that. Next, I’ll create a custom skim function that drops the p25, p50, and p75 output from the summary of the numerical variables.\n\nmy_skim &lt;- skim_with(numeric = sfl(p25 = NULL, p50 = NULL, p75 = NULL)) \n\nmy_skim(languages)\n\n\nData summary\n\n\nName\nlanguages\n\n\nNumber of rows\n4303\n\n\nNumber of columns\n49\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n21\n\n\nlogical\n4\n\n\nnumeric\n24\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n4303\n0\n\n\ntitle\n0\n1.00\n1\n56\n0\n4267\n0\n\n\ndescription\n3480\n0.19\n4\n2273\n0\n811\n0\n\n\ntype\n0\n1.00\n2\n27\n0\n40\n0\n\n\ncreators\n3203\n0.26\n2\n253\n0\n985\n0\n\n\nwebsite\n2928\n0.32\n13\n131\n0\n1368\n0\n\n\ndomain_name\n3588\n0.17\n6\n32\n0\n700\n0\n\n\nreference\n2314\n0.46\n15\n251\n0\n1955\n0\n\n\ngithub_repo\n3402\n0.21\n25\n73\n0\n897\n0\n\n\ngithub_repo_description\n3438\n0.20\n4\n419\n0\n853\n0\n\n\ngithub_language\n3829\n0.11\n1\n30\n0\n474\n0\n\n\ngithub_language_tm_scope\n3837\n0.11\n4\n34\n0\n361\n0\n\n\ngithub_language_type\n3837\n0.11\n4\n11\n0\n4\n0\n\n\ngithub_language_ace_mode\n3838\n0.11\n1\n16\n0\n96\n0\n\n\ngithub_language_file_extensions\n3833\n0.11\n1\n606\n0\n466\n0\n\n\nwikipedia\n2731\n0.37\n32\n104\n0\n1566\n0\n\n\nwikipedia_summary\n2884\n0.33\n17\n6741\n0\n1407\n0\n\n\nwikipedia_related\n3145\n0.27\n1\n1761\n0\n1059\n0\n\n\nline_comment_token\n3831\n0.11\n1\n7\n0\n23\n0\n\n\norigin_community\n1190\n0.72\n3\n305\n0\n2232\n0\n\n\nfile_type\n3213\n0.25\n2\n6\n0\n4\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n3683\n0.14\n1.00\nTRU: 617, FAL: 3\n\n\nfeatures_has_semantic_indentation\n3722\n0.14\n0.11\nFAL: 516, TRU: 65\n\n\nfeatures_has_line_comments\n3765\n0.13\n0.96\nTRU: 517, FAL: 21\n\n\nis_open_source\n3792\n0.12\n0.89\nTRU: 453, FAL: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1991.11\n111.44\n-2000\n2023\n▁▁▁▁▇\n\n\ndomain_name_registered\n3801\n0.12\n2011.33\n7.02\n1990\n2023\n▁▃▃▇▆\n\n\nisbndb\n3217\n0.25\n7.71\n33.16\n0\n400\n▇▁▁▁▁\n\n\nbook_count\n0\n1.00\n2.08\n17.34\n0\n401\n▇▁▁▁▁\n\n\nsemantic_scholar\n3545\n0.18\n3.79\n8.32\n0\n52\n▇▁▁▁▁\n\n\nlanguage_rank\n0\n1.00\n2151.00\n1242.31\n0\n4302\n▇▇▇▇▇\n\n\ngithub_repo_stars\n3414\n0.21\n2127.40\n7554.02\n0\n88526\n▇▁▁▁▁\n\n\ngithub_repo_forks\n3417\n0.21\n261.29\n1203.00\n0\n23732\n▇▁▁▁▁\n\n\ngithub_repo_updated\n3418\n0.21\n2021.39\n1.76\n2012\n2023\n▁▁▁▁▇\n\n\ngithub_repo_subscribers\n3418\n0.21\n62.34\n200.88\n0\n2910\n▇▁▁▁▁\n\n\ngithub_repo_created\n3425\n0.20\n2015.84\n3.48\n2006\n2022\n▁▅▇▇▇\n\n\ngithub_repo_issues\n3518\n0.18\n123.03\n546.26\n0\n9522\n▇▁▁▁▁\n\n\ngithub_repo_first_commit\n3567\n0.17\n2014.74\n4.99\n1987\n2022\n▁▁▁▆▇\n\n\ngithub_language_repos\n3833\n0.11\n197134.67\n1226900.57\n0\n16046489\n▇▁▁▁▁\n\n\nwikipedia_daily_page_views\n2837\n0.34\n227.13\n783.55\n-1\n13394\n▇▁▁▁▁\n\n\nwikipedia_backlinks_count\n2877\n0.33\n318.55\n1635.29\n1\n34348\n▇▁▁▁▁\n\n\nwikipedia_page_id\n2893\n0.33\n9167847.21\n13506832.90\n928\n63063548\n▇▁▁▁▁\n\n\nwikipedia_appeared\n2958\n0.31\n1991.14\n17.03\n1830\n2019\n▁▁▁▃▇\n\n\nwikipedia_created\n3040\n0.29\n2005.75\n3.77\n2001\n2020\n▇▇▂▁▁\n\n\nwikipedia_revision_count\n3130\n0.27\n330.43\n813.26\n1\n10104\n▇▁▁▁▁\n\n\nlast_activity\n0\n1.00\n2000.62\n84.60\n-900\n2023\n▁▁▁▁▇\n\n\nnumber_of_users\n0\n1.00\n13771.26\n227712.95\n0\n7179119\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n422.18\n12572.99\n0\n771996\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n1482\n0.66\n0.00\n0.00\n0\n0\n▁▁▇▁▁\n\n\n\n\n\nThis output is much nicer. It is a bit wall of text though. I wouldn’t recommend using this in reports, but it is a useful tool when doing your initial dataset analysis. (As a side note, I have noticed skimr doesn’t work well on Kaggle. It performs as expected if you are in interactive mode, but it fails when you try to save the notebook or run non-interactively.)\n\n\nStyling skim output with gt\nIf, for some reason, you did need to include output/ visualizations about missingness in a report, I’d probably recreate visualizations or tables by class of variable, especially if you have many variables, as I do here.\nHere’s an example for numeric variables, of which there are 24 in the dataset. First, we will skim the data and then use the gt package to style the resulting dataframe as a table. I used a built-in style, but each table element can be individually customized.\n\nlanguages_numeric &lt;- languages %&gt;%\n  select_if(is.numeric)\n\nlang_numeric_skim &lt;- my_skim(languages_numeric)\n\nlang_numeric_skim %&gt;%\n  select(-skim_type)   %&gt;% \n  gt() %&gt;%\n  cols_label(n_missing = \"# Missing\", complete_rate = \"Completeness\", \n             numeric.mean = \"Mean\", numeric.sd = \"Standard Deviation\",\n             numeric.p0 = \"Min\", numeric.p100 = \"Max\",\n             numeric.hist = \"Histogram\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Summary of Numerical Variables in Languages\") \n\n\n\n\n\n  \n    \n      Summary of Numerical Variables in Languages\n    \n    \n  \n  \n    \n      skim_variable\n      # Missing\n      Completeness\n      Mean\n      Standard Deviation\n      Min\n      Max\n      Histogram\n    \n  \n  \n    appeared\n0\n1.0000000\n1.991105e+03\n1.114434e+02\n-2000\n2023\n▁▁▁▁▇\n    domain_name_registered\n3801\n0.1166628\n2.011333e+03\n7.021132e+00\n1990\n2023\n▁▃▃▇▆\n    isbndb\n3217\n0.2523821\n7.706262e+00\n3.316421e+01\n0\n400\n▇▁▁▁▁\n    book_count\n0\n1.0000000\n2.079479e+00\n1.734465e+01\n0\n401\n▇▁▁▁▁\n    semantic_scholar\n3545\n0.1761562\n3.794195e+00\n8.316231e+00\n0\n52\n▇▁▁▁▁\n    language_rank\n0\n1.0000000\n2.151000e+03\n1.242313e+03\n0\n4302\n▇▇▇▇▇\n    github_repo_stars\n3414\n0.2066000\n2.127403e+03\n7.554016e+03\n0\n88526\n▇▁▁▁▁\n    github_repo_forks\n3417\n0.2059029\n2.612867e+02\n1.203003e+03\n0\n23732\n▇▁▁▁▁\n    github_repo_updated\n3418\n0.2056705\n2.021390e+03\n1.763285e+00\n2012\n2023\n▁▁▁▁▇\n    github_repo_subscribers\n3418\n0.2056705\n6.234237e+01\n2.008820e+02\n0\n2910\n▇▁▁▁▁\n    github_repo_created\n3425\n0.2040437\n2.015843e+03\n3.479589e+00\n2006\n2022\n▁▅▇▇▇\n    github_repo_issues\n3518\n0.1824309\n1.230344e+02\n5.462553e+02\n0\n9522\n▇▁▁▁▁\n    github_repo_first_commit\n3567\n0.1710435\n2.014739e+03\n4.985409e+00\n1987\n2022\n▁▁▁▆▇\n    github_language_repos\n3833\n0.1092261\n1.971347e+05\n1.226901e+06\n0\n16046489\n▇▁▁▁▁\n    wikipedia_daily_page_views\n2837\n0.3406925\n2.271330e+02\n7.835524e+02\n-1\n13394\n▇▁▁▁▁\n    wikipedia_backlinks_count\n2877\n0.3313967\n3.185484e+02\n1.635289e+03\n1\n34348\n▇▁▁▁▁\n    wikipedia_page_id\n2893\n0.3276784\n9.167847e+06\n1.350683e+07\n928\n63063548\n▇▁▁▁▁\n    wikipedia_appeared\n2958\n0.3125726\n1.991144e+03\n1.702650e+01\n1830\n2019\n▁▁▁▃▇\n    wikipedia_created\n3040\n0.2935162\n2.005748e+03\n3.768240e+00\n2001\n2020\n▇▇▂▁▁\n    wikipedia_revision_count\n3130\n0.2726005\n3.304314e+02\n8.132556e+02\n1\n10104\n▇▁▁▁▁\n    last_activity\n0\n1.0000000\n2.000616e+03\n8.459776e+01\n-900\n2023\n▁▁▁▁▇\n    number_of_users\n0\n1.0000000\n1.377126e+04\n2.277129e+05\n0\n7179119\n▇▁▁▁▁\n    number_of_jobs\n0\n1.0000000\n4.221838e+02\n1.257299e+04\n0\n771996\n▇▁▁▁▁\n    central_package_repository_count\n1482\n0.6555891\n0.000000e+00\n0.000000e+00\n0\n0\n▁▁▇▁▁\n  \n  \n  \n\n\n\n\n\n\nVisualizing Missingness with visdat\nThe visdat package makes ggplot- compatible missingness plots. The cluster = TRUE option groups variables that share missingness. Here we see that usually if some of the GitHub data is missing, then all of the GitHub data is missing. The percent missing is listed for each variable, and the overall missingness of the dataset is shown in the legend.\nNote vis_miss doesn’t work on very large datasets. The documentation suggests keeping the number of records below 1,000. A more extensive package for exploratory visualizations called naniar could also be used.\n\nlanguages_numeric %&gt;%\nvis_miss(cluster = TRUE) +\nggtitle(\"Missing Data in the Languages Dataset\") +\n  #theme_classic() +\n  theme(axis.text.x = element_text(size = 8, angle = 90))"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#improving-the-most-popular-language-for-each-comment-token-figure",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#improving-the-most-popular-language-for-each-comment-token-figure",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "Improving “The Most Popular Language for Each Comment Token” Figure",
    "text": "Improving “The Most Popular Language for Each Comment Token” Figure\n\njoined &lt;- read_csv(\"processed_lang.csv\" , show_col_types = FALSE)\n\nNow the original figure:\n\njoined %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\nI thought I had noted this in the previous post, but one of the tokens, ⍝ , is rendered as an empty box in the ggplot figures. I thought fixing this would be easy. First, I thought I could just pass the Unicode value for that symbol. Then, when that didn’t work, I thought I could change the font to one supporting that symbol. Supposedly, changing the font should be easy, yet after 3 hours working on it, I still had blank squares. There is a nice tutorial on changing fonts in ggplot that did not work until I found someone with the same issue. The solution is to add a line of code that doesn’t make much sense to me : windowsFonts(\"Cambria Math\" = windowsFont(\"Cambria Math\"))\nI saw a nice TidyTuesday figure on Twitter:\n\n\nMy submission for #TidyTuesday, Week 12 on programming languages. I explore jobs per users.Code: https://t.co/bV9DUHZmro pic.twitter.com/2D5YLnE5yz\n\n— Mitsuo Shiota (@mitsuoxv) March 21, 2023\n\n\nwith a caption referencing the original dataset. I’d like to add that. I generally want to increase the figure’s legibility and flip the color scale so that darker blue corresponds to more users. I also don’t think what popular means is entirely clear, so I’d like to explain more fully what I’m graphing.\n\nwindowsFonts(\"Cambria Math\" = windowsFont(\"Cambria Math\"))\njoined %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n    scale_colour_gradient(high = \"#08306b\", low = \"#6baed6\") + \n   labs(title = \"The Most Popular Language for Each Comment Token\",\n       subtitle = \"Based on # Users and Rank\",\n       caption = \"data from https://pldb.com/\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic(base_size = 16) +\n  theme(text = element_text( family = \"Cambria Math\")) +\n  theme(axis.text.x = element_text(face = \"bold\"))"
  },
  {
    "objectID": "posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html",
    "href": "posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html",
    "title": "Tidy Tuesday: Daylight Savings Time",
    "section": "",
    "text": "This week’s TidyTuesday is about the timezone data from IANA timezone database.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(lubridate)\nlibrary(skimr)\nlibrary(lutz)\nlibrary(maps)\nlibrary(scales)\nlibrary(sf)\nlibrary(ggimage)\n\nThe history of this database is fascinating. It is used by many computer systems to determine the correct time based on location. To learn more, I recommend reading Daniel Rosehill’s article on the topic. For a drier history, check out the wikipedia article.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 13)\n\ntransitions &lt;- tuesdata$transitions\ntimezones &lt;- tuesdata$timezones\ntimezone_countries &lt;- tuesdata$timezone_countries\ncountries &lt;- tuesdata$countries\n\nIt is suggested that we change the begin and end variables in transitions to datetimes.\n\ntransitions &lt;- transitions %&gt;%\n  mutate(begin = as_datetime(begin), end = as_datetime(end))\n\nI was interested in how many countries had multiple times zones. I know the US has 4 time zones in the continental US.\n\nnum_zones &lt;- timezone_countries %&gt;%\n  count(country_code, sort = TRUE)\n\nnum_zones %&gt;% \n  filter(n &gt; 1) %&gt;%\n  left_join(countries) %&gt;%\n  select(place_name, n) %&gt;%\n  filter(place_name != \"NA\") %&gt;%\n  gt() %&gt;%\n  cols_label(place_name = \"Country\", n = \"Number of TZs\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Countries with Multiple TZs\") \n\n\n\n\n\n  \n    \n      Countries with Multiple TZs\n    \n    \n  \n  \n    \n      Country\n      Number of TZs\n    \n  \n  \n    United States\n29\n    Canada\n28\n    Russia\n27\n    Brazil\n16\n    Argentina\n12\n    Australia\n12\n    Mexico\n11\n    Kazakhstan\n7\n    Greenland\n4\n    Indonesia\n4\n    Ukraine\n4\n    Chile\n3\n    Spain\n3\n    Micronesia\n3\n    Kiribati\n3\n    Mongolia\n3\n    Malaysia\n3\n    French Polynesia\n3\n    Portugal\n3\n    US minor outlying islands\n3\n    Congo (Dem. Rep.)\n2\n    China\n2\n    Cyprus\n2\n    Germany\n2\n    Ecuador\n2\n    Marshall Islands\n2\n    New Zealand\n2\n    Papua New Guinea\n2\n    Palestine\n2\n    French Southern & Antarctic Lands\n2\n    Uzbekistan\n2\n    Vietnam\n2\n  \n  \n  \n\n\n\n\nAnd we find that the United States has 29!! time zones in the database. This was unexpected, so say the least. I thought maybe there were some times zones for territories and perhaps military bases that I did not know about. I also thought there might be some extra time zones arising from some states using daylight savings time, while others in the same area might not. I wanted to visualize where these times zones were.\n\nUS_tz &lt;- timezone_countries %&gt;% \n  filter(country_code == \"US\") %&gt;%\n  left_join(timezones)\n\nJoining with `by = join_by(zone)`\n\n\nI found the lutz package created nice pictograms about when a timezone shifts from DST and back. (This package uses the same underlying database that we are using here to determine when the shifts occur.)\n\n tz_plot(US_tz$zone[21])\n\n\n\n\nI created the plots and saved them as images. I modified a function I found on stack overflow to create the file names.\n\nwd &lt;- getwd()\nfilepath = file.path(wd)\n\n\nmake_filename = function(number){\n  # doing this, putting it all on a single line or using pipe %&gt;%\n  # is just matter of style\n  filename = paste(\"tzplot\", number, sep=\"_\")\n  filename = paste0(filename, \".png\")\n  filename = file.path(filepath, filename)\n  \n  filename\n}\n\n#creating a variable to store the files name\nUS_tz &lt;- US_tz %&gt;%\n  mutate(image_name = \"tbd\")\n\nindex &lt;- 1\nfor (index in seq(1, nrow(US_tz))) {\n  filename = make_filename(index)\n  US_tz[index , \"image_name\"] &lt;- filename\n  # 1. Open jpeg file\n  png(filename, width = 350, height = 350, bg = \"transparent\")\n  # 2. Create the plot\n  # you need to print the plot if you call it inside a loop\n  print(tz_plot(US_tz$zone[index]))\n  # 3. Close the file\n  dev.off()\n  index = index + 1\n}\n\nNext I created a world map, inspired by the one from\n\n\nMy submission for #TidyTuesday, Week 13 on time zones. I plot time zones in the world map.Code: https://t.co/y5Cm4tuaVk pic.twitter.com/BZC3anC5Oa\n\n— Mitsuo Shiota (@mitsuoxv) March 28, 2023\n\n\nI hadn’t previously used the maps package, so I appreciate being introduced to it. The maps package only has a mainland US map, so I used the world map. (Plus, as I mentioned, I thought some of these time zones would be in other parts of the world.) I followed a tutorial on Plotting Points as Images in ggplot and used the hints about aspect ratio to make my tz_plot circles remain circular. However, that did stretch the world a bit.\n\naspect_ratio &lt;- 1.618  \n\nus_tz_map &lt;- map_data(\"world\") %&gt;% \n  ggplot(aes(long, lat)) +\n  geom_polygon(aes(group = group), fill = \"white\", \n               color = \"gray30\", alpha = 0.9) +\n  geom_image(aes(x = longitude, latitude, image = image_name), \n             data = US_tz, size = 0.025, by = \"width\",\n             asp = aspect_ratio) +\n  coord_sf() +\n  labs(title = \"The United States has 29 Timezone- Mostly Redunant\",\n       caption = \"Data from: https://data.iana.org/time-zones/tz-link.html\") +\n  theme_void() +\n  theme(aspect.ratio = 1/aspect_ratio,\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = \"white\")\n    )\n\nggsave(\"thumbnail.png\", us_tz_map, width = 5 * aspect_ratio, height = 5)\nus_tz_map\n\n\n\n\nAnd what we see is there are a bunch of redundant times zone specification, especially in the Midwest.\n\nUS_tz %&gt;%\n  select(zone, latitude, longitude) %&gt;%\n  arrange(longitude) %&gt;%\n  gt() %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Countries with Multiple TZs\") \n\n\n\n\n\n  \n    \n      Countries with Multiple TZs\n    \n    \n  \n  \n    \n      zone\n      latitude\n      longitude\n    \n  \n  \n    America/Adak\n52.66667\n-177.13333\n    America/Nome\n64.56667\n-165.78333\n    Pacific/Honolulu\n21.71667\n-158.35000\n    America/Anchorage\n61.30000\n-149.91667\n    America/Yakutat\n60.35000\n-140.35000\n    America/Sitka\n57.75000\n-135.41667\n    America/Juneau\n58.41667\n-134.60000\n    America/Metlakatla\n55.73333\n-132.15000\n    America/Los_Angeles\n34.18333\n-118.80000\n    America/Boise\n44.41667\n-116.35000\n    America/Phoenix\n34.33333\n-112.46667\n    America/Denver\n40.08333\n-105.03333\n    America/North_Dakota/Beulah\n48.10000\n-102.43333\n    America/North_Dakota/Center\n48.08333\n-102.23333\n    America/North_Dakota/New_Salem\n47.53333\n-102.05000\n    America/Menominee\n45.56667\n-88.45000\n    America/Indiana/Vincennes\n39.30000\n-88.23333\n    America/Indiana/Petersburg\n39.00000\n-87.98333\n    America/Chicago\n41.85000\n-87.65000\n    America/Indiana/Tell_City\n38.13333\n-87.43333\n    America/Indiana/Knox\n42.03333\n-87.11667\n    America/Indiana/Marengo\n38.90000\n-87.01667\n    America/Indiana/Winamac\n41.13333\n-86.78333\n    America/Indiana/Indianapolis\n39.86667\n-86.63333\n    America/Kentucky/Louisville\n38.50000\n-86.31667\n    America/Kentucky/Monticello\n37.60000\n-85.78333\n    America/Indiana/Vevay\n39.60000\n-85.10000\n    America/Detroit\n43.20000\n-83.78333\n    America/New_York\n41.55000\n-74.38333\n  \n  \n  \n\n\n\n\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {Tidy {Tuesday:} {Daylight} {Savings} {Time}},\n  date = {2023-03-28},\n  url = {https://lsinks.github.io/posts/2023-03-28-tidytuesday-timezones/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “Tidy Tuesday: Daylight Savings\nTime.” March 28, 2023. https://lsinks.github.io/posts/2023-03-28-tidytuesday-timezones/."
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 1",
    "section": "",
    "text": "The best way to learn a programming language is to code. Obvious, but sometimes it can be daunting to start a project when you are a novice. Influenced by my time at a start-up, I’ve found the best approach is to create a minimum viable product. This is the smallest working part of the project. By continuing to revise and iterate the code, you can rapidly detect weaknesses in your knowledge and take active steps to fix them.\nMy learning project was a Wordle Guess Generator. I will show you how I used this project to create a self-guided learning plan. Motivated partly by my desire to have good guesses for Septle (a seven-letter guessing game), this project has been a crucial part of my journey to learn R. Practicing and learning coding skills were more important to me than devising the optimal strategy; if you are interested in “the best” Wordle strategy, then you will probably find much better answers by Googling.\nFor those who don’t know, Wordle is a word-guessing gamer that gives you five guesses to identify a 5-letter word. Correct letters are shown in green or yellow with each guess, depending on whether they are appropriately placed or not. Incorrect letters are shown in gray.\nI wasn’t interested in coming up with the global “best” answer. I wanted to come up with some guesses that fit my gameplay style. I guess three words in a row without trying to solve the game. Then, with a fair number of letters known, I attempt to solve the word.\nThe code has undergone several significant overhauls as I’ve needed to learn new things and used this project as the test bed. And here it is again, as a vehicle to learn quarto/ markdown. Here I will show you the very first ugly code and how I critiqued and revised it over several months. I’ve put it all on GitHub. The project is unstructured, the code is ugly, and all the versioning was by filename."
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#self-guided-learning-the-ever-evolving-wordle-guess-generator",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#self-guided-learning-the-ever-evolving-wordle-guess-generator",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 1",
    "section": "",
    "text": "The best way to learn a programming language is to code. Obvious, but sometimes it can be daunting to start a project when you are a novice. Influenced by my time at a start-up, I’ve found the best approach is to create a minimum viable product. This is the smallest working part of the project. By continuing to revise and iterate the code, you can rapidly detect weaknesses in your knowledge and take active steps to fix them.\nMy learning project was a Wordle Guess Generator. I will show you how I used this project to create a self-guided learning plan. Motivated partly by my desire to have good guesses for Septle (a seven-letter guessing game), this project has been a crucial part of my journey to learn R. Practicing and learning coding skills were more important to me than devising the optimal strategy; if you are interested in “the best” Wordle strategy, then you will probably find much better answers by Googling.\nFor those who don’t know, Wordle is a word-guessing gamer that gives you five guesses to identify a 5-letter word. Correct letters are shown in green or yellow with each guess, depending on whether they are appropriately placed or not. Incorrect letters are shown in gray.\nI wasn’t interested in coming up with the global “best” answer. I wanted to come up with some guesses that fit my gameplay style. I guess three words in a row without trying to solve the game. Then, with a fair number of letters known, I attempt to solve the word.\nThe code has undergone several significant overhauls as I’ve needed to learn new things and used this project as the test bed. And here it is again, as a vehicle to learn quarto/ markdown. Here I will show you the very first ugly code and how I critiqued and revised it over several months. I’ve put it all on GitHub. The project is unstructured, the code is ugly, and all the versioning was by filename."
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#brainstorming-the-project.",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#brainstorming-the-project.",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 1",
    "section": "1. Brainstorming the project.",
    "text": "1. Brainstorming the project.\nBrainstorm what you want this project to do. Try to have a very clear statement about the project’s goal at the top. If you can think of self-contained modules, mark that. If you can sketch out the scaffold of the code- great! If you have some ideas about the results, even better. Put everything you can think of down. I had been thinking about this project for a while, so this brainstorming sheet is neat. Neatness and organization are not important at this point; getting stuff written down is.\nMy goal was “I want 3 initial guesses for Septle. Ideally, I’ll maximize the number of unique letters, and I want to preferentially pick from the most frequently used letters.”\nI decided my scoring would be based on the frequency letters occur in English; more common letters get a higher score than uncommon letters. To generate the score for the word, I proposed the following process:\n\nRead in letter frequencies\nScoring [the letters, originally I thought it would be based on rank, with the most common letter getting a 26 and the least common getting a 1. That is what the confusing little sketch is attempting to convey. A is not the most common letter, nor is Z the least common.]\nRead in the list of words\nCalculate the scores [of the words]\nPick the best word as a first guess [best meaning highest scoring]\n\nI also talk about how the frequency distribution of letters will likely differ between five and seven-letter words. I suggested looking at the frequency of letters in modern text instead of just frequency lists from linguists. I noted that certain letters are more likely to be in a specific position, and it could be valuable to constrain the guess to typical positions. An example is that “y” is almost always at the end of a word in English, so “sadly” might be a better guess than “yacht” for that reason. You are likelier to lock down a letter with a positionally accurate guess. I also said that I wanted a 4th Wordle guess. There are a lot of ideas here! That’s okay because I winnow them down in the next step.\n\n\n\nThe initial brainstorming session"
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#minimum-viable-product-what-is-the-smallest-program-that-works",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#minimum-viable-product-what-is-the-smallest-program-that-works",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 1",
    "section": "2. Minimum Viable Product: What is the smallest program that works?",
    "text": "2. Minimum Viable Product: What is the smallest program that works?\nPull out the minimum viable product from your brainstorming. What is the smallest result that would satisfy your goals? Is there a way to make things a bit smaller? I would size it so you can get working code to accomplish the goal in a few hours.\nI chose to generate four guesses for Wordle. I also decided to generate my frequency table from the word list itself. I found a five-letter word list that I could download and read in, but all the letter frequency tables I found were on blogs or in articles, and the only way I could see to get them into my program was to type them in and I was too lazy to do that. I decided to implement the process I outlined in the brainstorming session and calculate four guesses."
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#write-some-bad-code.",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#write-some-bad-code.",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 1",
    "section": "3. Write some bad code.",
    "text": "3. Write some bad code.\nWrite some code that does that thing. It will be ugly. If you can’t figure something out, do it the wrong way. Just get something running.\nThis first attempt took me 3-4 hours to write.\nI googled how to calculate the frequency of letters in words (in R) and found this helpful tutorial.\n\nThe Minimal Viable Product: Running Code\nI cleaned up the commenting/formatting of the initial code just for this post. I also added library(tidyverse)- apparently, I was just loading libraries through the gui back then. If you want to see the true initial version, it is on GitHub.\nHere’s what my MVP does:\n\nI found a list of five letter words online and imported it.\nI calculated how often each letter occurred over the word list.\n(This, or the scaled version, was the score assigned to each letter.)\nI looped through the word list and calculated two scores for each word, one using all letters and one using only the unique letters.\nI picked the highest-scoring word as my first guess.\nI then went through the word list and calculated a second score for the words minus the letters already guessed in the first guess (and ignoring duplicated letters).\nI picked the highest-scoring word as my second guess.\nI repeated steps 5 & 6 to pick a third guess.\n\nOkay, so let’s look at some bad code. I will flag a few things as we go through, but I’m certain you can find much more that is not optimal.\nHere’s loading the data and writing two scoring functions. I first wrote these in the code but had time to convert them to functions in this initial work session. It was an opportunity to practice function writing, but it was not critical to the minimum viable product.\nI have lots of print statements commented out; this is a very simple way to debug and see how you are proceeding through the code. There are more sophisticated tools in R Studio, but I didn’t want to figure out how to use them at this moment. I use the global variable char_frequencies for the value of each letter. I create this variable in the next code chunk.\n\nlibrary(tidyverse)\n\nsgb.words &lt;- \n  read.delim(\"C:/Users/drsin/OneDrive/Documents/R Projects/wordle/sgb-words.txt\",\n             sep = \"\")\n\n#probably want this instead because it assumes no headers\n#test6 &lt;- read.table(file.choose())\n\nScoring_Word &lt;- function(word){\n  #i'm not handling duplicate letters at all right now\n  letter_vec &lt;-  unlist(strsplit(word, split = \"\"))\n  value &lt;- 0\n  for (i in 1:5) {\n    position &lt;- letter_vec[i] == char_frequencies$letters\n    value[i] &lt;- y[position]\n   # print(i)\n    if (i == 5) {\n     # print(\"I am here\")\n     # print(sum(value))\n      return(total &lt;- sum(value))\n      }\n    \n  }\n} \n\n\nScoring_Word_Unique &lt;- function(word){\n\n # print(word)\n  letter_vec &lt;-  unlist(strsplit(word, split = \"\"))\n  unique_letter_vec &lt;- unique(letter_vec)\n  #print(unique_letter_vec)\n  #print(length(unique_letter_vec))\n  \n  value &lt;- 0\n  if (length(unique_letter_vec) == 0) {\n    return(value)\n  } else{\n      for (i in 1:length(unique_letter_vec)) {\n      position &lt;- unique_letter_vec[i] == char_frequencies$letters\n      value[i] &lt;- y[position]\n    # print(i)\n    # print(value)\n    if (i == length(unique_letter_vec)) {\n      # print(\"I am here\")\n      # print(sum(value))\n      return(total &lt;- sum(value))\n    }\n    \n  }\n  }\n}\n\nI did run through most of the code with five words initially, and then later the whole word list, when I was more confident that things worked.\nI calculate how often each letter appears in the list and create the scaled version. I created two incredibly ugly graphs: one of the raw counts for each letter and one of the scaled frequencies. This is also a moment to do a quick reality check on the results- are the most and least common letters what you’d expect?\n\nstart_time &lt;- Sys.time()\n\nletters &lt;- unlist(strsplit(sgb.words[,1], split = \"\"))\nchar_frequencies &lt;- as.data.frame(table(letters))\n#char_frequencies\n\nggplot(char_frequencies, \n        aes(x = fct_reorder(char_frequencies[,1], char_frequencies[,2])\n                              , char_frequencies[,2] )) +\n   geom_col() +\n   theme_classic()\n\n\n\ncommon &lt;- max(char_frequencies[,2])\ny = (char_frequencies[,2]/common)\n\nggplot(char_frequencies, \n       aes(x =  fct_reorder(char_frequencies[,1], char_frequencies[,2]), y )) +\n  geom_col() +\n  theme_classic()\n\n\n\n\nNow I calculate the scores for the (hand-picked) words I’ve been playing with. I also hand-calculated these scores using the values from char_frequencies to ensure my scoring functions did what I thought they were.\nI initialized an object to store the words, scores, and guesses. You can also tell that I had no idea what data types my objects were since I called them a list. small_list is a matrix/array of characters, and none of my zeros are numbers. I wanted a dataframe, but I didn’t know how to do that. I didn’t have a strong reason to prefer a dataframe other than it was widely used in the courses I was taking at Datacamp.\nThis chunk also pulls out a single word and sends it to score to check that it works before I loop through the entire object and calculate all the scores.\nYou can also see I hard-coded the number of words (again… I did this in the prior code chunk too.)\n\n#calculate the score for crone\ncrone_score &lt;- Scoring_Word(\"crone\")\n#might_score &lt;- Scoring_Word (\"might\")\n#sadly_score &lt;- Scoring_Word (\"sadly\")\nnum_words &lt;- 5756\n#num_words &lt;- 5\nsmall_list &lt;- cbind(word_name = sgb.words[1:num_words,1], \n                    score =rep(0, times = num_words), \n                    unique_score = rep(0, times = num_words),\n                    post_word_one_unique = rep(0, times = num_words),\n                    post_word_two_unique = rep(0, times = num_words),\n                    post_word_three_unique = rep(0, times = num_words)\n                                                )\nword &lt;- small_list[[1,1]]\nScoring_Word(word)\n\n[1] 3.40422\n\nind2 &lt;- 0\n\nfor (ind2 in 1:num_words){\n  #print(small_list[[ind2,1]])\n  score_ind2 &lt;- Scoring_Word(small_list[[ind2,1]])\n  small_list[[ind2,2]] &lt;- score_ind2\n}\n\n#u_crone_score &lt;- Scoring_Word_Unique(\"crone\")\n#u_there_core &lt;- Scoring_Word_Unique (\"there\")\n#sadly_score &lt;- Scoring_Word (\"sadly\")\n\nind2 &lt;- 0\nfor (ind2 in 1:num_words){\n # print(small_list[[ind2,1]])\n  score_ind2 &lt;- Scoring_Word_Unique(small_list[[ind2,1]])\n # print(score_ind2)\n  small_list[[ind2,3]] &lt;- score_ind2\n}\n\nIn my attempt to sort the word scores and pick out the highest-scoring works, I created an unnecessary number of temporary variables. I forced one of these objects to be a dataframe, but I didn’t check the types of the individual components. Note that all my numbers are still characters. It is funny that things worked even though they were the wrong type.\n\nsmall_list1 &lt;- small_list\nsmall_df &lt;- as.data.frame(small_list1)\ntop_words &lt;- small_df %&gt;%\n arrange(desc(unique_score))\n\nword_1 &lt;- top_words$word_name[1]\n\nNow I calculate the second and third guesses. I wanted to penalize duplicate letters, so I used the unique letter scoring function and removed the letters from the first guess. I couldn’t figure out how to do that automatically, so I hardcoded to remove the letters “a”, “r”, “o”, “s”, and “e” from the words before I sent them to be scored. This is precisely the kind of situation where you can get bogged down doing things “properly” and end up stuck. I quickly attempted to figure it out and then did it incorrectly. You can also see that I have a bunch of stuff commented out that didn’t work and a bunch of print statements for debugging. This is not pretty code.\nThen I loop through the list again and repeat for the last guess. Again, hardcoded in the letters to remove from the first and second guesses.\n\n#now we need a function that sees if a word has the letters of the word_1\n#and removes them and then calculates the word score\n#top word is arose\n# Word 1= arose -----\nind3 &lt;- 1\nfor (ind3 in 1:num_words) {\n # print(top_words$word_name[ind3])\n  test &lt;- small_list[[ind3,1]]\n  lvec &lt;- gsub(\"[a r o s e]\", \"\", test)  #this actually works.  How do I use the string?\n  #lvec &lt;-  unlist(strsplit(word_1, split = \"\"))\n  #lvec&lt;- \"t|h|e|i|r\" #how do I contruct this automatically\n\n  #new_let &lt;- str_remove_all(pattern= lvec, string= test)\n # print(lvec)\n  score_ind3 &lt;- Scoring_Word_Unique(lvec)\n # print(\"writing score\")\n # print(c(ind3, \" \", score_ind3, \"for the word \", test, \"sent as \", lvec))\n  \n  small_list[[ind3,4]] &lt;- score_ind3\n  #print (c(\"output of small list \", top_words[[ind3,4]]))\n}\n\nsmall_df2 &lt;- as.data.frame(small_list)\ntop_words2 &lt;- small_df2 %&gt;%\n  arrange(desc(post_word_one_unique))\n\n\nword_2 &lt;- top_words2$word_name[1]\n\n# top word 2 is until\n\nind4 &lt;- 1\nfor (ind4 in 1:num_words) {\n  # print(top_words$word_name[ind3])\n  test &lt;- small_list[[ind4,1]]\n  lvec &lt;- gsub(\"[u n t i l a r o s e]\", \"\", test)  #this actually works.  How do I use the string?\n  #lvec &lt;-  unlist(strsplit(word_1, split = \"\"))\n  #lvec&lt;- \"t|h|e|i|r\" #how do I contruct this automatically\n  \n  #new_let &lt;- str_remove_all(pattern= lvec, string= test)\n  # print(lvec)\n  score_ind4 &lt;- Scoring_Word_Unique(lvec)\n  # print(\"writing score\")\n  # print(c(ind3, \" \", score_ind3, \"for the word \", test, \"sent as \", lvec))\n  \n  end_time &lt;- Sys.time()\n  end_time - start_time\n  \n  small_list[[ind4,5]] &lt;- score_ind4\n  #print (c(\"output of small list \", top_words[[ind3,4]]))\n}\n\nsmall_df3&lt;- as.data.frame(small_list)\ntop_words2 &lt;- small_df3 %&gt;%\n  arrange(desc(post_word_two_unique))\n\n\nword_3 &lt;- top_words2$word_name[1]\n\nLastly, I calculated the total score of these three words compared to my hand-picked words.\n\na = Scoring_Word_Unique(\"arose\") + \n  Scoring_Word_Unique(\"until\") + \n  Scoring_Word_Unique(\"dumpy\")\na\n\n[1] 8.013518\n\nb = Scoring_Word_Unique(\"crone\") +\n  Scoring_Word_Unique(\"mighty\") +\n  Scoring_Word_Unique(\"sadly\")\nb\n\n[1] 8.081767\n\n\nNote that there is an error here too. By calling Scoring_Words_Unique on individual words, I did not penalize duplicate letters. Thus “u” appears in two words. The correct scoring call should have been:\n\nc = Scoring_Word_Unique(\"aroseuntildumpy\")\nc\n\n[1] 7.654468\n\n\nBut the program works! It generated three reasonable guesses for Wordle that use common letters. (Note that by my scoring rules, the manually chosen set of words is a better choice.)"
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#critique-the-code",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#critique-the-code",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 1",
    "section": "4. Critique the code",
    "text": "4. Critique the code\nThis step is the critical step to accelerate your learning. You need to review your code and list everything you know is not done properly and everything confusing you. This should be done at the end of every session or at the start of the next. Be honest with yourself. If you don’t understand something, put it on your list, even if your code works. The point of this exercise is to increase your coding proficiency.\nThis is my list from that code we just reviewed. I created this the next day before I started work. Note my versioning by file name.\n\nSave current code as frequency_072222 and work on a new copy. This was functional code last night, so I want to keep it.\nImport is wrong because it takes the first word as a header.\nI need more concise variable names. Also, I create a bunch of temp variables that aren’t needed.\nI manually subtract out (hard-coded) words picked in previous cycles. I need that done on the fly.\nOnce 4 is done, I’d like to write a function to generate however many guesses you ask for.\nI’d like to look at the histogram of the scores as you cycle through the guesses.\nI’m very unclear on when I need lists, dataframes, tibbles, etc., for the different functions.\nGive credit to the website where I took the string split code from.\nSome functions from the other code are out of date, so I should update them. [I got warnings in R Studio about this, which I didn’t understand.]\nUpdate scoring_word to have flexible word length.\n\nAgain, there is a lot more wrong with this code, but this is the list of things I could identify with the knowledge I had at the time.\nThe next day, I created a chart of all my variables, their sizes, and their types. I also proposed new, clearer names for them, though this wasn’t implemented until a few days later. I mostly played around with how long it took to calculate the initial score and unique score using separate or combined loops. I used sys.time() to benchmark, which didn’t seem terribly consistent in its output. However, there didn’t seem to be a huge difference between the methods, so I returned it to two loops for clarity. At the end of this day, I had four items on my task list after critiquing and reviewing. The last one was, “I need to figure out git_hub or come up with a better way of versioning than the date.”\nProving that point, I put the wrong date in all the file names the next time I worked on the code. In that session, I devoted most of the time to figuring out item 4 on my task list. I wrote a function called remove_letters, which could be used instead of hard coding. I also played around with reshaping the data using pivot_longer from tidyr. I created a histogram of word scores as letters are removed from consideration, which required the reshaped data. Reshaping data can be tricky, so this was a good opportunity to work through that process. (Again, versioning by name, this was called “frequency_072422_tidyr”, in case I really messed up.)"
  },
  {
    "objectID": "posts/2023-04-02-chart-challenge-2/parks.html",
    "href": "posts/2023-04-02-chart-challenge-2/parks.html",
    "title": "30 Day Chart Challenge -Arlington Parks",
    "section": "",
    "text": "When I looked at Twitter this morning, my feed was filled with amazing charts. Apparently, April is month for the #30DayChartChallenge. More info can be found at the challenge’s Github page. Each day, the challenge specifies a type of chart, but not a dataset. Today’s challenge is to create a waffle chart. I’d never heard of a waffle chart, but luckily, R has a package for that!\nKeeping it simple and just using tidyverse and waffle packages today. (Spoiler, I had incredible difficulties with this package.)\n\nlibrary(waffle)\nlibrary(tidyverse)\n\nA waffle chart is similar to a pie chart, but since it is constructed from squares, instead of wedges, it is a bit easier to correctly judge the relative areas.\nI discovered that Arlington County has a website with a bunch of open source data, so I poked around there to find a dataset for today’s challenge. I decided to use the dataset on parks and acreage. In addition to having local and federally owned parks, Arlington is park of a consortium of Northern Virginia jurisdictions that also operate regional parks.\n\nparks &lt;- read_csv(\"parks.csv\")\n\nRows: 10 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): Profile Year, County Owned Parkland (Acreage), NOVA Parks (Acreage)...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe dataset contains 3 years worth of data (2019-2021) and the total number of acres owned by the county, NOVA parks, and the National Park Service. It also includes the number of parks own by NOVA parks and the county, but not the number of NPS parks. I did try to find the number of NPS parks and the answer isn’t easily obtainable. The NPS owns one park in Arlington (Arlington House), but there are a couple of multi-state trails that might go through the county, but I wasn’t interested in pulling up maps to check.\nThe dataset comes as an excel sheet and the column names aren’t nice. I’ve been working with so many datasets designed for R or SQL that it was a shock to see long column names with spaces and punctuation. I had to look up how to handle that! The answer is backticks, as below.\n\nparks_2021 &lt;- parks %&gt;%\n  filter(`Profile Year` == 2021)\n\nI’m going to rename the columns and in this filtered dataset. There isn’t much year to year change in this dataset (there is one extra park in 2020), so I’m not going to do anything with the entire dataset. If I were, I’d rename the parent dataset.\n\nparks_2021 &lt;- parks_2021 %&gt;%\n  transmute(year = `Profile Year`,\n         county_acres = `County Owned Parkland (Acreage)`,\n         NOVA_acres = `NOVA Parks (Acreage)`,\n         NPS_acres = `National Parks Service (Acreage)`) \n\nparks_2021 &lt;- parks_2021 %&gt;%\n  select(-year)\n\nNow let’s get waffling!\nAllegedly, you can both pass waffle dataframes or use the provided geom_waffle, but neither was successful for me. With respect to the geom_waffle, I found a suggestion to install waffle from the github repo and restart R Studio, but that didn’t work for me. Others stated that re-installing R Studio fixed the problem, but my commitment to waffle charts is not that great.\nAs to passing it dataframes, waffle constantly complained about 'list' object cannot be coerced to type 'double' even when using code from other folk’s tutorial. Passing waffle a simple vector did work.\n\n# Vector\nx &lt;- c(30, 25, 20, 5)\n\n# Waffle chart\nwaffle(x, rows = 8)\n\n\n\n\nSo, I converted my dataframe to a vector. First, I reshaped it to long rather than wide. (Neither the long nor the wide df waffled.)\n\nparks_2021_long &lt;- pivot_longer(parks_2021, cols = 1:3, names_to = \"owner\", values_to = \"acreage\")\n\nThen I converted it to a vector. This should be a named vector of numbers.\n\nparks_2021_vec = deframe(parks_2021_long)\nclass(parks_2021_vec)\n\n[1] \"numeric\"\n\nprint(parks_2021_vec)\n\ncounty_acres   NOVA_acres    NPS_acres \n         924          136          417 \n\nstr(parks_2021_vec)\n\n Named num [1:3] 924 136 417\n - attr(*, \"names\")= chr [1:3] \"county_acres\" \"NOVA_acres\" \"NPS_acres\"\n\n\nLet’s waffle it. When I first waffled it, I got 4 categories instead of 3. I found an example that said you needed to explicitly pass it 3 colors or else it would fill in the blank space with a 4th color. Then you get the correct labels, but no chart!\n\n#This waffles, it seems like nonsense to me\n\nwaffle(parks_2021_vec, colors = c(\"#FFC0CB\", \"#FFC0AB\", \"green\"))\n\n\n\n\nSo now we reached the sad state of affairs where I type in the values to make this work. And that also does not work.\n\nx &lt;- c(county = 924, nova = 136, nps = 417)\n\n# Waffle chart\nwaffle(x , rows = 10)\n\n\n\n\nSmall numbers work\n\ny &lt;- c(county = 9.24, nova = 1.36, nps = 4.17)\n\n# Waffle chart\nwaffle(y ,  rows = 10)\n\n\n\n\nIf I convert everything to percentages…\n\ntotal = 924 + 136 + 417\ny &lt;- c(county = (924/total)*100, nova = (136/total)*100, nps = (417/total)*100)\n\n# Waffle chart\nwaffle(y ,  rows = 10)\n\n\n\n\nI don’t find any documentation about the size of the numbers. It is not a requirement that the totals must add up to 100 (100%); small numbers adding up to anything works. Waffle charts are not only for proportional data, but can also be used to track progress. There is nothing in the documentation on CRAN that gives a clue about this, nor did I see anything in the tutorials I looked at.\nI’m going to pretty up the chart and call it a day. I thought this would take me about 20 minutes to make a nice chart and instead I’ve spent several hours and I don’t even understand what went wrong. Also, the named vector does work when I adjust the size of the numbers to “smaller” values. I picked nice forest colors since we are talking about parks.\n\nparks_2021_percent = (parks_2021_vec / total) * 100\n\n\nwaffle(parks_2021_percent, colors = c(\"darkgreen\", \"darkseagreen\", \"darkolivegreen\"), title = \"Who owns the parks in Arlington, Virginia?\")\n\n\n\n\nI don’t really understand the waffle package. I don’t find the graphic particularly understandable. I’d like there to be some sort of indication about what each square represents. And I find it very annoying that there are not 100 squares. I know this is a rounding issue, but given that the dataset, by the nature of how it was created, should equal 100%, the chart is just confusing. And for what it is worth, I had to repeatedly restart RStudio, because code chunks would just fail to do anything. They’d run, but there would be no output, not even warnings or errors.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {30 {Day} {Chart} {Challenge} {-Arlington} {Parks}},\n  date = {2023-04-02},\n  url = {https://lsinks.github.io/posts/2023-04-02-chart-challenge-2/parks},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “30 Day Chart Challenge -Arlington\nParks.” April 2, 2023. https://lsinks.github.io/posts/2023-04-02-chart-challenge-2/parks."
  },
  {
    "objectID": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html",
    "href": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html",
    "title": "A Tidymodels Tutorial: A Structural Approach",
    "section": "",
    "text": "As I’ve started working on more complicated machine learning projects, I’ve leaned into the tidymodels approach. Tidymodels is a highly modular approach, and I felt it reduced the number of errors, especially when evaluating many machine models and different preprocessing steps. (This is, in fact, a stated goal of the tidymodels ecosystem.)\nThis tutorial is more about understanding the process of modeling in tidymodels and learning about the various objects created at different steps rather than optimizing a machine learning model.\nThroughout this tutorial, I will use the word “procedure” to describe a set of steps to go from data to final predictions. I’m doing this because tidymodels uses the word workflow for specific types of objects and functions. It would be too confusing to use workflow to also describe the process/procedure.\nBut the tidymodels ecosystem can also be very confusing. There are several component packages in tidymodels. While it is easy to explain what a recipe object (from the recipe package) does, it became increasingly difficult for me to name and describe the objects I was creating as I started building more sophisticated machine-learning procedures. And I found it even more confusing that simple and complex procedures, while going through the same basic steps (preprocess, train, evaluate, predict), created objects with different structures and data within them. I found it confusing that fit, last_fit, fit_resamples, etc., did not all produce objects that contained the same information and could be acted on by the same functions. In my first attempt at using last_fit(), I ended up scrapping the entire ML section and redoing it with fit()/predict() because I couldn’t figure out how to get the predictions out of the object created by last_fit().\nAdding to my woes was the fact that attempting to view/print/ examine these objects, especially in notebook environments, often caused the entire project to time out. RStudio generally handles these objects more gracefully, but I’ve also crashed it hard. It also isn’t consistent whether an object will lock-up RStudio or not. Once RStudio has locked up, restarting the program leads to an increasing number of freezes/locking up, until the computer is restarted.\nI’ve also manually numbered my code blocks and used that for referencing. I believe it is possible to hyperlink code chunks in Quarto, but I plan to replicate this project in an online notebook environment where that isn’t possible. The manual numbering will make it easier to cross-reference the two. I found online notebooks really did not like displaying many tidymodels objects. That’s also why there are timers around many of the display calls.\nSo here I’m going to go through three different procedures for modeling. I will compare and contrast the objects created as we move through the different procedures."
  },
  {
    "objectID": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#fitpredict",
    "href": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#fitpredict",
    "title": "A Tidymodels Tutorial: A Structural Approach",
    "section": "fit()/predict()",
    "text": "fit()/predict()\nFirst, I fit the model on the training data to get the fit and then I pass that fit and the test data to predict() to get the predictions for test.\n\n# Code block 10: Run fit/ predict on workflow\nwflow_fit &lt;- fit(wf_simple, data = train_data)\nwflow_predict &lt;- predict(wflow_fit, new_data = test_data)\nwflow_predict2 &lt;- predict(wflow_fit, new_data = test_data, type = \"prob\" )\n\nWhat comes out of predict is super simple to understand. It is a list of predictions. No complicated nested list objects here. If I want probabilities instead of hard classification, I pass predict() the argument type = \"prob\" to get the probabilities instead.\n\n# Code block 11:  Examine the output of predict\nhead(wflow_predict)\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 0          \n3 0          \n4 0          \n5 0          \n6 0          \n\nhead(wflow_predict2)\n\n# A tibble: 6 × 2\n    .pred_1 .pred_0\n      &lt;dbl&gt;   &lt;dbl&gt;\n1 0.00367     0.996\n2 0.00144     0.999\n3 0.0000262   1.00 \n4 0.00461     0.995\n5 0.0000279   1.00 \n6 0.00138     0.999\n\n\nWhat about our model? Maybe I want model coefficients or to see which features are most important. There is a lot of information here, but it isn’t very well structured. Again, this is a nested list. RStudio is displaying this nicely and the details can be seen using View().\n\n# Code block 12: Examine the outcome of fit \nwflow_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n            (Intercept)                lat_trans               long_trans  \n                7.06537                 -0.10230                 -0.01413  \n         distance_miles                      age                     hour  \n                0.06526                 -0.26818                 -0.82812  \n                weekday                  amt_log     category_food_dining  \n               -0.12721                 -1.87149                 -0.00929  \n category_gas_transport     category_grocery_net     category_grocery_pos  \n               -0.62772                 -0.29571                 -0.67063  \ncategory_health_fitness            category_home       category_kids_pets  \n                0.06286                  0.10517                  0.01683  \n      category_misc_net        category_misc_pos   category_personal_care  \n               -0.42138                 -0.13380                 -0.05152  \n  category_shopping_net    category_shopping_pos          category_travel  \n               -0.38932                 -0.16399                  0.18122  \n\nDegrees of Freedom: 254704 Total (i.e. Null);  254684 Residual\nNull Deviance:      16570 \nResidual Deviance: 11910    AIC: 11950\n\n\nWhile you can use standard R operations for interacting with lists and nested data to extract the desired information from wflow_fit, it is much easier to use the broom package. Broom is part of the core tidymodels installation, so it does not need to be installed separately. To get the model coefficients and p-values in tibble form, use tidy(). For high-level statistics about the model, use glance(). Just remember that the information you extract from the output of fit() relates to the model as applied to the training data. For information about the model performance as applied to the test data, you need to use the output of predict(). Since this output is only a vector of predictions, you need to bind it to the test dataframe and then perform analysis on the new object.\nSo it is pretty straightforward to get our model coefficients:\n\n# Code block 13: Summarize wflow_fit with tidy\nwflow_fit %&gt;% tidy() #summarizes information about model components\n\n# A tibble: 21 × 5\n   term                   estimate std.error statistic   p.value\n   &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)             7.07       0.0703   101.    0        \n 2 lat_trans              -0.102      0.0305    -3.36  7.94e-  4\n 3 long_trans             -0.0141     0.0306    -0.462 6.44e-  1\n 4 distance_miles          0.0653     0.0318     2.05  4.02e-  2\n 5 age                    -0.268      0.0289    -9.27  1.87e- 20\n 6 hour                   -0.828      0.0397   -20.9   1.27e- 96\n 7 weekday                -0.127      0.0288    -4.41  1.03e-  5\n 8 amt_log                -1.87       0.0510   -36.7   2.76e-294\n 9 category_food_dining   -0.00929    0.0599    -0.155 8.77e-  1\n10 category_gas_transport -0.628      0.0593   -10.6   3.62e- 26\n# ℹ 11 more rows\n\n\nOr to get details of the model performance:\n\n# Code block 14: model info from wflow_fit with glance\nwflow_fit %&gt;% glance() #reports information about the entire model\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik    AIC    BIC deviance df.residual   nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt;  &lt;int&gt;\n1        16568.  254704 -5956. 11953. 12173.   11911.      254684 254705"
  },
  {
    "objectID": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#last_fit",
    "href": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#last_fit",
    "title": "A Tidymodels Tutorial: A Structural Approach",
    "section": "last_fit()",
    "text": "last_fit()\nSo, from the tidymodels webpage, last_fit() is described as “last_fit() emulates the process where, after determining the best model, the final fit on the entire training set is needed and is then evaluated on the test set.” (Actually this is from the tune subpage, which is important, though I didn’t realize it.)\nI pass the workflow to last_fit() along with the data split object (with the info about testing and training) and the metrics set. In theory, the result should be the same as from fit()/predict() above.\n\n# Code block 15: Using lastfit() in hard classifier mode\nlast_fit_results &lt;- last_fit(wflow_fit, data_split, metrics = fraud_metrics_hard)\n\nSo, I look at the results just as I did with predict in Code Block 11. And RStudio sometimes locks up. Other times, it produces a high-level overview as expected.\n\n# Code block 16: creating a workflow set\nstart_time_display &lt;- Sys.time()\nhead(last_fit_results) \n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [254705/84902]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\nend_time_display &lt;- Sys.time()\nprint(paste(\"last_fit_results: \", end_time_display - start_time_display))\n\n[1] \"last_fit_results:  0.0788729190826416\"\n\n\nSo how to get the predictions out? According to the manual page for last_fit(), the output is “A single row tibble that emulates the structure of fit_resamples(). However, a list column called .workflow is also attached with the fitted model (and recipe, if any) that used the training set.” I also see that last_fit() is actually from the tune package and not from parsnip as I expected. Nothing I’m doing here involves tuning hyperparameters at all. I expected that is was a parsnip object both thematically and because you interact with last_fit() using extract_fit_parsnip(), see Code Block 23.\nLooking fit_resamples() isn’t very helpful for answering this question. (Oh, but it is. It just took me another few paragraphs of writing to realize it.)\nI did find a Stackoverflow discussion that provided the answer in their code: last_fit1_pred &lt;- last_fit1[[5]][[1]]\nThat’s not very straightforward!\nPull out the predictions from last_fit_pred.\n\n# Code block 17: extracting predictions from last_fit\nlast_fit_pred &lt;- last_fit_results[[5]][[1]]\n\nLook at the head() of this object.\n\n# Code block 18: Examine the outcome of lastfit by head\nhead(last_fit_pred)\n\n# A tibble: 6 × 4\n  .pred_class  .row is_fraud .config             \n  &lt;fct&gt;       &lt;int&gt; &lt;fct&gt;    &lt;chr&gt;               \n1 0               1 0        Preprocessor1_Model1\n2 0               2 0        Preprocessor1_Model1\n3 0               8 0        Preprocessor1_Model1\n4 0              12 0        Preprocessor1_Model1\n5 0              13 0        Preprocessor1_Model1\n6 0              14 0        Preprocessor1_Model1\n\n\nLook at the head() of the object from predict().\n\n# Code block 19: Examine the outcome of predict by head\nhead(wflow_predict)\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 0          \n3 0          \n4 0          \n5 0          \n6 0          \n\n\nUse identical() to compare the two hard predictions and verify they are the same.\n\n# Code block 20: showing that predict and the predictions in last_fit are the same\nidentical(last_fit_pred$.pred_class, wflow_predict$.pred_class)\n\n[1] TRUE\n\n\nNow, let the realization of what all the stuff about the tune package means hit you. We now know the full secrets of last_fit(). It turns out that any of the helper functions for tuning functions from the tune package work on last_fit() because it is a tune function. I don’t find the documentation for either the helper functions or last_fit() make that connection clear. I think that is what the reference to fit_resamples() on the last_fit() page is getting at.\nTidy Modeling with R also contains an example of using collect_predictions with last_fit(), but most examples are with tuning functions, so obviously from the tune family. One of the tutorials on the main tidymodels webpage does as well. But in general, extracting predictions from the test data is not demonstrated, just collecting metrics and analyzing model performance. So it is hard to google your way to the answer. This is the kind of situation I’ve struggled with throughout learning tidymodels and part of what motivated me to write this tutorial.\nSo now I get the predictions the easy way.\n\n# Code block 21: Examine the outcome of lastfit by head\nhead(last_fit_pred)\n\n# A tibble: 6 × 4\n  .pred_class  .row is_fraud .config             \n  &lt;fct&gt;       &lt;int&gt; &lt;fct&gt;    &lt;chr&gt;               \n1 0               1 0        Preprocessor1_Model1\n2 0               2 0        Preprocessor1_Model1\n3 0               8 0        Preprocessor1_Model1\n4 0              12 0        Preprocessor1_Model1\n5 0              13 0        Preprocessor1_Model1\n6 0              14 0        Preprocessor1_Model1\n\nlast_fit_results %&gt;% collect_predictions()\n\n# A tibble: 84,902 × 5\n   id               .pred_class  .row is_fraud .config             \n   &lt;chr&gt;            &lt;fct&gt;       &lt;int&gt; &lt;fct&gt;    &lt;chr&gt;               \n 1 train/test split 0               1 0        Preprocessor1_Model1\n 2 train/test split 0               2 0        Preprocessor1_Model1\n 3 train/test split 0               8 0        Preprocessor1_Model1\n 4 train/test split 0              12 0        Preprocessor1_Model1\n 5 train/test split 0              13 0        Preprocessor1_Model1\n 6 train/test split 0              14 0        Preprocessor1_Model1\n 7 train/test split 0              16 0        Preprocessor1_Model1\n 8 train/test split 0              17 0        Preprocessor1_Model1\n 9 train/test split 0              19 0        Preprocessor1_Model1\n10 train/test split 0              25 0        Preprocessor1_Model1\n# ℹ 84,892 more rows\n\n\nAnd can evaluate the model performance.\n\n# Code block 22: collecting metrics from lastfit collect_metrics()\nlast_fit_results %&gt;% collect_metrics()\n\n# A tibble: 1 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.995 Preprocessor1_Model1\n\n\nAnd extract the fit. This extract_fit_parsnip() result is an identical parsnip object as the workflow_fit object we got from fit() and can be handled the same way (i.e. via broom). You can refer back to Code Block 13 to see the results are the same. This is perhaps the key takeaway; these larger, more complex objects contain the simpler objects (workflows, parsnip objects) and they should be extracted and handled normally. Understanding this will make understanding how to handle a workflow_set() much easier.\n\n# Code block 23: extract model coefficients from last_fit() \nlast_fit_results %&gt;% extract_fit_parsnip() %&gt;% tidy()\n\n# A tibble: 21 × 5\n   term                   estimate std.error statistic   p.value\n   &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)             7.07       0.0703   101.    0        \n 2 lat_trans              -0.102      0.0305    -3.36  7.94e-  4\n 3 long_trans             -0.0141     0.0306    -0.462 6.44e-  1\n 4 distance_miles          0.0653     0.0318     2.05  4.02e-  2\n 5 age                    -0.268      0.0289    -9.27  1.87e- 20\n 6 hour                   -0.828      0.0397   -20.9   1.27e- 96\n 7 weekday                -0.127      0.0288    -4.41  1.03e-  5\n 8 amt_log                -1.87       0.0510   -36.7   2.76e-294\n 9 category_food_dining   -0.00929    0.0599    -0.155 8.77e-  1\n10 category_gas_transport -0.628      0.0593   -10.6   3.62e- 26\n# ℹ 11 more rows"
  },
  {
    "objectID": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#handling-a-model-with-no-hyperparameters",
    "href": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#handling-a-model-with-no-hyperparameters",
    "title": "A Tidymodels Tutorial: A Structural Approach",
    "section": "Handling a model with no hyperparameters",
    "text": "Handling a model with no hyperparameters\nNormally, we’d want to extract the best recipe/model combination from this set. I’ll do that here. Again, I’m using j-index as my metric and from the output of Code Block 25, we see down_logreg is the best performing model. I extract that workflow from the set of results, and pass it to last_fit().\n\n# Code Block 27: Validating the best model with the test data\nvalidation_results &lt;- tune_results %&gt;%\n  extract_workflow(\"down_logreg\") %&gt;%\n  last_fit(split =  data_split, metrics = fraud_metrics)\n\nNow we can use the same helper functions we did when we used last_fit() on the simple workflow, because we are working with a simple workflow! We pulled just the one workflow we wanted out.\nYou can see now that in addition to the hard classification we got from last_fit() before we also get the probabilities. This is driven by the metrics that make up the metrics set (see the yardstick section for more information). I use these predictions to create the ROC curve as well.\n\n# Code Block 28: Metric for best model with the test data\nhead(collect_predictions(validation_results))\n\n# A tibble: 6 × 7\n  id               .pred_1 .pred_0  .row .pred_class is_fraud .config           \n  &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;             \n1 train/test split  0.552    0.448     1 1           0        Preprocessor1_Mod…\n2 train/test split  0.197    0.803     2 0           0        Preprocessor1_Mod…\n3 train/test split  0.0329   0.967     8 0           0        Preprocessor1_Mod…\n4 train/test split  0.472    0.528    12 0           0        Preprocessor1_Mod…\n5 train/test split  0.0254   0.975    13 0           0        Preprocessor1_Mod…\n6 train/test split  0.312    0.688    14 0           0        Preprocessor1_Mod…\n\nvalidation_results %&gt;% \n  collect_predictions() %&gt;% \n  roc_curve(is_fraud, .pred_1) %&gt;% \n  autoplot() + \n  ggtitle(\"ROC Curve\")"
  },
  {
    "objectID": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#handling-a-model-with-hyperparameters",
    "href": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#handling-a-model-with-hyperparameters",
    "title": "A Tidymodels Tutorial: A Structural Approach",
    "section": "Handling a model with hyperparameters",
    "text": "Handling a model with hyperparameters\nSuppose the best model was the elastic net. I tuned the hyperparameters when I did the fitting in workflow_map(). How do I deal with that?\nFirst, I need to extract the best set of hyperparameters. Here we aren’t extracting the workflow, we are extracting the workflow set result, which is our set of hyperparameters. This is a really simple object, so you can view it without fear.\n\n# Code Block 29: getting-hyperparameters\nbest_hyperparam &lt;- tune_results %&gt;% \n    extract_workflow_set_result(\"down_glmnet\") %&gt;%\n    select_best(metric = \"j_index\")\n\nbest_hyperparam\n\n# A tibble: 1 × 3\n      penalty mixture .config             \n        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 0.000000137   0.570 Preprocessor1_Model4\n\n\nOur workflow for the glmnet is incomplete because it has tune() for the two hyperparameters, instead of the values. We know the best values (at least from the limited parameter space we explored.) I first extract_workflow() just as I did for the no hyperparameter case and then call finalize_workflow(best_hyperparam). This updates the workflow hyperparameters with the values we found. Everything is identical to the no hyperparameter case or the simple workflow/ last-fit() case. Realizing how/when to extract or reduce the more complex objects to the simpler objects is key to using tidymodels effectively.\n\n# Code Block 30: last_fit for a workflow with hyperparameter\nvalidation_results &lt;- tune_results %&gt;%\n  extract_workflow(\"down_glmnet\") %&gt;%\n  finalize_workflow(best_hyperparam) %&gt;%\n  last_fit(split =  data_split, metrics = fraud_metrics)\n\nNow we can handle this object exactly as before.\n\n# Code Block 31: Metric for best model with the test data\nhead(collect_predictions(validation_results))\n\n# A tibble: 6 × 7\n  id               .pred_1 .pred_0  .row .pred_class is_fraud .config           \n  &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;             \n1 train/test split  0.551    0.449     1 1           0        Preprocessor1_Mod…\n2 train/test split  0.217    0.783     2 0           0        Preprocessor1_Mod…\n3 train/test split  0.0342   0.966     8 0           0        Preprocessor1_Mod…\n4 train/test split  0.474    0.526    12 0           0        Preprocessor1_Mod…\n5 train/test split  0.0263   0.974    13 0           0        Preprocessor1_Mod…\n6 train/test split  0.316    0.684    14 0           0        Preprocessor1_Mod…\n\nvalidation_results %&gt;% \n  collect_predictions() %&gt;% \n  roc_curve(is_fraud, .pred_1) %&gt;% \n  autoplot() + \n  ggtitle(\"ROC Curve\")\n\n\n\n\nSo that’s it. I hope this clarifies some of the different procedures you can use to fit models in the tidymodels framework."
  },
  {
    "objectID": "posts/2023-04-18-tidytuesday-founder-crops/founder-crops.html",
    "href": "posts/2023-04-18-tidytuesday-founder-crops/founder-crops.html",
    "title": "TidyTuesday Week 16: Neolithic Founder Crops",
    "section": "",
    "text": "Today’s TidyTuesday relates to the use of plants in the Neolithic era and is based on a paper by Arranz-Otaegul and Roe.\nThe authors have made their data and analysis available on GitHub. The methods for generating all the figures and tables are in an RMarkdown document with some explanatory text. Having just recently looked at the code and data for one of my older papers, I now appreciate this markdown approach. Everything needed is linked together through the markdown document and the GitHub repo. My code is only as understandable with the paper notebook that resides in my former lab. Even though the code was commented, it is less clear many years later. I find markdown/ quarto a little difficult to code in. I’m not sure why- maybe the interspersed text and code is distracting. I usually code in R files for more complicated projects and then copy them into markdown/quarto. But I definitely appreciate markdown after working on today’s project.\n\nlibrary(skimr)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(gt)\n\nThe data can be downloaded through the tidytuesday package using tuesdata &lt;- tidytuesdayR::tt_load(2023, week = 16). Or the data and the code can be downloaded from the project’s Github repo. I actually don’t use the data from tidytuesday, but instead work off their analysis.\nTo briefly summarize the work of Arranz-Otaegul and Roe, in the 1980s the concept of “founder crops” was proposed. These founder crops were a set of crops that were generally all cultivated together and were wide spread as the first main agricultural crops. Arranz-Otaegul and Roe propose that with the increased data generated since the 80s, a more refined approach to the beginnings of argriculture can be taken. They break down agriculture into several different processes (selection of plants, domesitcation, plant management, etc.) which occurs over longer periods of time rather than the development agriculture occuring as a single, rapid event. Arranz-Otaegul and Roe examine the plant remains reported from 135 sites over a wide range of periods in the neolithic era. They find that founder crops don’t necessarily come as a “package”, that a variety of other plants were fairly important, and that the increased cultivation of wheat was the dominate change in the types plants used during the neolithic period. I should note, I’m a chemist, not an archeology/ archeobotanist, so this is not the most nuanced summary of their work.\nI was particularly intrigued by their conclusion about wheat, which is graphically represented in Figure 4 of their paper. I found the figure a bit unclear; stacked area plots are not common in chemistry and I’m not expert at reading them. Wheat certainly does increase with time, but it was hard for me to tell what the other crops were doing. So, for tidytuesday, I wanted to explore this specific question- how does the use of wheat change over time.\nThe file you get from the tidytuesday package is the same as the “swasia_neolithic_flora.tsv” from the paper’s repo with some minor clean-up. I don’t want to replicate all there analysis here to get to the point of being able to make the figure I want, so I saved the object flora_ts after it was generated in line 256 of their SI1.RMD file. This has the data partitioned by century and has been reshaped.\n\nflora_ts &lt;- read_rds(\"flora_ts.rds\")\n\nThere are 8 founder crops. The original analysis binned together wheat varieties and I am also binning all the legumes together, just to make the resulting graph less busy. I’m also dropping flax. This code chunk is modified from their code chunk ts-founder-crops, which starts at line 573 and ends with saving figure 4. This chunk calculates the proportion of archaeological sites at which the crop was found at in each century. (This is called an assemblage in the paper.)\n\nfounder_crops_binned &lt;- flora_ts %&gt;%\n  # aggregate the legumes and the wheat\n  mutate(founder_crop = recode(founder_crop,\n                               \"einkorn wheat\" = \"wheat\",\n                               \"emmer wheat\" = \"wheat\",\n                               \"chickpea\" = \"legumes\",\n                               \"bitter vetch\" = \"legumes\",\n                               \"lentil\" = \"legumes\",\n                               \"pea\" = \"legumes\",\n                               \"flax\" = \"flax\",\n                               .default = founder_crop)) %&gt;%\n  filter(founder_crop != \"flax\") %&gt;%\n  # Aggregate by founder crops\n  group_by(century, site_name, phase_code, founder_crop) %&gt;%\n  summarise(prop = sum(prop, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  # Add number of assemblages per century\n  group_by(century) %&gt;%\n  mutate(n_assemb = length(unique(phase_code))) %&gt;%\n  # Calculate average proportion\n  group_by(century, founder_crop) %&gt;%\n  summarise(avg_prop = sum(prop) / first(n_assemb))\n\nDrop the NAs. This corresponds to all the plants which are not founder crops.\n\nfounder_crops_only &lt;- founder_crops_binned %&gt;%\n  drop_na(founder_crop)\n\nFirst, I wanted to see roughly how common each crop was over all time.\n\nfounder_crops_only %&gt;%\n  group_by(founder_crop) %&gt;%\n  summarize(pct = round(mean(avg_prop), 2) * 100) %&gt;%\n  gt() %&gt;%\n  cols_label(founder_crop = \"Founder Crop\", pct = \"% of Sites\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Mean Frequency of Crops over Time\") \n\n\n\n\n\n  \n    \n      Mean Frequency of Crops over Time\n    \n    \n    \n      Founder Crop\n      % of Sites\n    \n  \n  \n    barley\n9\n    legumes\n6\n    wheat\n15\n  \n  \n  \n\n\n\n\nNow, I’m going to create a scatter plot with a trendline as a guide for the eyes. I’m using facet wrap, so each crop is on its own plot.\n\nfounder_crops_only  %&gt;% ggplot(aes(century / 1000, avg_prop, color = founder_crop)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(~ founder_crop, nrow = 3) +\n  scale_x_reverse(\n    breaks = scales::breaks_width(-1),\n    limits = c(11.7, 6.5),\n    expand = expansion(0)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 0.3),\n    breaks = scales::breaks_width(0.1),\n    labels = scales::label_percent(accuracy = 1),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  theme_pander() +\n  theme(legend.position = \"none\") +\n  labs(x = \"ka cal BP\", y = \"Mean proportion of assemblages\", fill = NULL) %&gt;%\n  labs(title = \"Frequency of Crops in SW Asia during the Neolithic Period\", caption = \"data from https://github.com/joeroe/SWAsiaNeolithicFounderCrops\")\n\n\n\n\nSo, it looks like barley and legumes are fairly constant with time, but wheat does increase consistently. A note about the x-axis- the times are listed as before present (BP), so a larger number is longer ago. Present is defined as January 1, 1950.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {TidyTuesday {Week} 16: {Neolithic} {Founder} {Crops}},\n  date = {2023-04-18},\n  url = {https://lsinks.github.io/posts/2023-04-18-tidytuesday-founder-crops/founder-crops},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “TidyTuesday Week 16: Neolithic Founder\nCrops.” April 18, 2023. https://lsinks.github.io/posts/2023-04-18-tidytuesday-founder-crops/founder-crops."
  },
  {
    "objectID": "posts/2023-05-02-portal-project/portal.html",
    "href": "posts/2023-05-02-portal-project/portal.html",
    "title": "TidyTuesday Week 18: Portal Project",
    "section": "",
    "text": "Today’s TidyTuesday is about the Portal Project, which is a long terms study on the ecology of Arizona. The study explores how ants, rodents, plants respond to climate in the desert near Portal, Az. A subset of the data is provided for this week’s TidyTuesday.\nLoading libraries. Not really using anything fancy today!\n\nlibrary(tidyverse)\nlibrary(ggthemes)\n\nLoading the data.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 18)\n\n--- Compiling #TidyTuesday Information for 2023-05-02 ----\n\n\n--- There are 3 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `plots.csv`\n    Downloading file 2 of 3: `species.csv`\n    Downloading file 3 of 3: `surveys.csv`\n\n\n--- Download complete ---\n\nplots &lt;- tuesdata$plots\nspecies &lt;- tuesdata$species\nsurveys &lt;- tuesdata$surveys\n\nThis dataset focuses on rodents. The portal project also studies plants and ants, but they are ommitted from these datasets.\n\nglimpse(plots)\n\nRows: 8\nColumns: 2\n$ plot      &lt;dbl&gt; 3, 4, 11, 14, 15, 17, 19, 21\n$ treatment &lt;chr&gt; \"exclosure\", \"control\", \"control\", \"control\", \"exclosure\", \"…\n\n\nWe have information about 8 plots in the plots dataframe, and they are coded as exclosure or control. It isn’t clear what exclosure or control means. The portal website says this about treatments of the plots:\n“Rodents are manipulated using gates in the fencing of each plot. Rodent removals contain no gates and any rodents captured on those plots are removed. All other plots contain 16 gates (4 per plot side); gates consist of holes cut through the hardware cloth of the fencing. Gate size is used to exclude subsets of the rodent community (larger gates allow all rodents access, smaller gates exclude kangaroo rats). Dimensions for gates on kangaroo rat removal plots are 1.9 cm x 1.9 cm, D. spectabilis removals were 2.6 cm x 3.0 cm, and control plots are 3.7 cm x 5.7 cm. In 2005, Dipodomys spectabilis removals were converted to controls – a state these plots had effectively been in with the local extinction of Dipodomys spectabilis in the late 1990s. Species caught on plots from which they are supposed to be excluded are removed from the site and the access point to the plot is located and eliminated. Plots affected by these treatments are listed in Portal_plot_treatments.”\nPresumably, the exclosures are those plots designed with gates of specific sizes, designed to exclude specific size rodents. It is less clear what a control is. The text says “Dipodomys spectabilis removals were converted to controls”, which is also unclear because it also says removals contain no rodents, so the species part is confusing. This page suggests controls are “unmanipulated controls” so maybe there is no fence at all?\nLooking at the history of the plots, we see that the exclosure plots all have had kangaroo rats removed from them at certain times.\nSo, do we find fewer Kangaroo rates on those plots? The survey data includes the rodent type by code; the key is found in the species data. Let’s pull out the kangaroo rat code(s).\n\nrats &lt;- species[(str_detect(species$commonname, \"kangaroo\")), ]\nrats\n\n# A tibble: 3 × 15\n  species scientificname       taxa  commonname censustarget unidentified rodent\n  &lt;chr&gt;   &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 DM      Dipodomys merriami   Rode… Merriam's…            1            0      1\n2 DO      Dipodomys ordii      Rode… Ord's kan…            1            0      1\n3 DS      Dipodomys spectabil… Rode… Banner-ta…            1            0      1\n# ℹ 8 more variables: granivore &lt;dbl&gt;, minhfl &lt;dbl&gt;, meanhfl &lt;dbl&gt;,\n#   maxhfl &lt;dbl&gt;, minwgt &lt;dbl&gt;, meanwgt &lt;dbl&gt;, maxwgt &lt;dbl&gt;, juvwgt &lt;dbl&gt;\n\n\nWe have three types of Kangaroo rate, coded DM, DO, and DS.\nLet’s make sure the survey data only includes the plots we know about from the plots dataframe.\n\nsurveys %&gt;% group_by(plot) %&gt;% count(plot) \n\n# A tibble: 11 × 2\n# Groups:   plot [11]\n    plot     n\n   &lt;dbl&gt; &lt;int&gt;\n 1     3  3580\n 2     4  3928\n 3    10   469\n 4    11  3640\n 5    14  3625\n 6    15  2106\n 7    16  1079\n 8    17  4023\n 9    19  2256\n10    21  2681\n11    23   977\n\n\nWe definitely have plots not in our plot dataframe. This is a small enough dataset that you can just look at the two lists and see we have 10, 16, and 23 as extra plots. If we go back to the history of the plots page, we can see that these three plots all had all rodents removed at several times over the course of the project. I’ll come back to that, but first I want to demonstrate how we can find these extra plots using a join. For larger datasets, it might not be possible to determine the extra plots by inspection.\nTo do this, I’m going to use an antijoin from dplyr. The syntax is anti_join(x , y) and returns rows of x that do not have a match in y. So here, we want x to be the plots from the survey data, and y to be the plots from out plots. I’ll just build off the summary pipe from the previous code block.\n\nsurveys %&gt;% group_by(plot) %&gt;% count(plot) %&gt;%\n  anti_join(plots)\n\nJoining with `by = join_by(plot)`\n\n\n# A tibble: 3 × 2\n# Groups:   plot [3]\n   plot     n\n  &lt;dbl&gt; &lt;int&gt;\n1    10   469\n2    16  1079\n3    23   977\n\n\nDoing this type of basic check is really important when you start working with a new dataset. It looked like we had two treatments, but there are actually 3. If you had missed this, you could have lumped the third treatment in with one of the other two while analyzing, and obtained incorrect results.\nI’m going to store these other plots numbers, both as an array with counts and as a 1D object of just the plot numbers. Depending on what I decide to do, I might annotate the plots dataframe to include this data.\n\nextra_plots_array &lt;- surveys %&gt;% group_by(plot) %&gt;% count(plot) %&gt;%\n  anti_join(plots)\n\nJoining with `by = join_by(plot)`\n\nextra_plots_array\n\n# A tibble: 3 × 2\n# Groups:   plot [3]\n   plot     n\n  &lt;dbl&gt; &lt;int&gt;\n1    10   469\n2    16  1079\n3    23   977\n\nextra_plots &lt;- extra_plots_array$plot\n\nThere are a couple of different things that could be explored. The treatments were not applied over every time block. We can look at how persistent a given treatment is. Plot 3 had the kangaroo rats removed in three of the five time blocks (1977-1985, 1988-2004, 2010-2015). Does the survey data reflect this?\nI’m going to pull out the plot 3 survey data.\n\nplot3_survey &lt;- surveys %&gt;%\n  filter(plot == 3)\n\nThere are a few different ways I could go. I’m going to create two groups of rodents: kangaroo rats and others. “NAs” are going in other. These are rodents that were caught and not identified. Perhaps using the size data, it might be possible to impute the species of rodent, but that requires more subject matter knowledge than I have.\nNAs are annoying, so I’ll handle them first. Then I’ll recode everything else to be kangaroo or other. I’m just hard coding from the species name, but I could use the species labels I saved earlier (rats$species).\n\nplot3_survey_recode &lt;- plot3_survey %&gt;%\n    mutate(species = ifelse(is.na(species) == TRUE, \"OTHER\", species)) %&gt;%\n    mutate(species = recode(species,\n                               \"DM\" = \"KAN\",\n                               \"DO\" = \"KAN\",\n                               \"DS\" = \"KAN\",\n                               .default = \"OTHER\")) \n\nSo, first, did the treatment work?\n\n  plot3_survey_recode %&gt;%\n    group_by(species) %&gt;%\n    count(species)\n\n# A tibble: 2 × 2\n# Groups:   species [2]\n  species     n\n  &lt;chr&gt;   &lt;int&gt;\n1 KAN       207\n2 OTHER    3373\n\n\nLooks like it did. But we should really compare to a control plot. Plot 4 is a control plot.\n\nplot4_survey &lt;- surveys %&gt;%\n  filter(plot == 4) %&gt;%\n    mutate(species = ifelse(is.na(species) == TRUE, \"OTHER\", species)) %&gt;%\n    mutate(species = recode(species,\n                               \"DM\" = \"KAN\",\n                               \"DO\" = \"KAN\",\n                               \"DS\" = \"KAN\",\n                               .default = \"OTHER\")) %&gt;%\n    group_by(species) %&gt;%\n    count(species)\n\nplot4_survey\n\n# A tibble: 2 × 2\n# Groups:   species [2]\n  species     n\n  &lt;chr&gt;   &lt;int&gt;\n1 KAN      2162\n2 OTHER    1766\n\n\nIn this plot, 55% of the captured rodents are kangaroo rats, compared to about 6% in plot 3. Again, to be completely rigorous, we should probably compare data aggregated over all plots with the same treatment types, rather than a single plot from treatment and single plot from control. These weren’t even randomly picked; I chose the first one of each type on the list.\nDid the kangaroo rat populations increase in years without treatment?\n\nplot3_survey_recode %&gt;%\n  filter (species == \"KAN\") %&gt;% group_by(year) %&gt;% count(species) %&gt;%\n  ggplot(aes(year, n)) +\n  geom_point() +\n  annotate(\n    \"rect\",\n    xmin = 1977,\n    xmax = 1985,\n    ymin = 0,\n    ymax = 40,\n    alpha = .1,\n    fill = \"red\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = 1988,\n    xmax = 2004,\n    ymin = 0,\n    ymax = 40,\n    alpha = .1,\n    fill = \"red\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = 2010,\n    xmax = 2015,\n    ymin = 0,\n    ymax = 40,\n    alpha = .1,\n    fill = \"red\"\n  ) +\n  ylab(\"# of Kangaroo rats\") +\n  xlab(\"Year\") +\n  labs(title = \"Persistance of treatment on Plot 3\",\n       subtitle = \"Kangaroo Rats removed in red periods\",\n       caption = \"Data from https://portal.weecology.org/\") +\n  theme_pander()\n\n\n\n\nSo that’s pretty interesting. The treatment did seem to be persistent. The years with the highest numbers of kangaroo rats were in times when they were actively being removed. (Perhaps the researchers were more diligent about identifying the rat species in removal period. This might be reflected in have fewer or proportionally fewer “NAs” during removal times.)\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {TidyTuesday {Week} 18: {Portal} {Project}},\n  date = {2023-05-02},\n  url = {https://lsinks.github.io/posts/2023-05-02-tidytuesday-portal-project/portal},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “TidyTuesday Week 18: Portal\nProject.” May 2, 2023. https://lsinks.github.io/posts/2023-05-02-tidytuesday-portal-project/portal."
  },
  {
    "objectID": "posts/2023-06-19-tidytuesday-UFOs/UFOs.html",
    "href": "posts/2023-06-19-tidytuesday-UFOs/UFOs.html",
    "title": "TidyTuesday Week 25: UFO Sightings Redux",
    "section": "",
    "text": "I haven’t been TidyTuesdaying because I’ve been learning Tableau. I’ll write more about that later; but it has been an interesting experience and provides different perspectives on data compared to what you might get from R. (I’m sure you could reproduce everything in Tableau in R and vice versa, but it is certainly easier to perform certain actions in one program over the other.)\nToday’s TidyTuesday is based on a dataset about the UFO sightings. This is an updated version of a tidytuesday challenge from 2019. The readme suggests that differences between the two datasets might be especially interesting.\n\nlibrary(skimr)\nlibrary(tidyverse)\nlibrary(ggthemes)\n\nLoad the data with the TidyTuesday package in the usual way.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 25)\n\n--- Compiling #TidyTuesday Information for 2023-06-20 ----\n\n\n--- There are 3 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `ufo_sightings.csv`\n    Downloading file 2 of 3: `places.csv`\n    Downloading file 3 of 3: `day_parts_map.csv`\n\n\n--- Download complete ---\n\nufo_sightings &lt;- tuesdata$`ufo_sightings`\nplaces &lt;- tuesdata$`places`\nday_parts_map &lt;- tuesdata$`day_parts_map`\n\nThis dataset combines information about reported UFO sightings with information about the lighting conditions at that time from sunrise-sunset. That is, was it day time, night time, twilight, etc. when the sighting occurred. This is an augmentation of the original dataset.\n\nskim(ufo_sightings)\n\n\nData summary\n\n\nName\nufo_sightings\n\n\nNumber of rows\n96429\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\nDate\n1\n\n\nlogical\n1\n\n\nnumeric\n1\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncity\n0\n1.00\n3\n26\n0\n10721\n0\n\n\nstate\n85\n1.00\n2\n31\n0\n684\n0\n\n\ncountry_code\n0\n1.00\n2\n2\n0\n152\n0\n\n\nshape\n2039\n0.98\n3\n9\n0\n24\n0\n\n\nreported_duration\n0\n1.00\n2\n25\n0\n4956\n0\n\n\nsummary\n31\n1.00\n1\n135\n0\n95898\n0\n\n\nday_part\n2563\n0.97\n5\n17\n0\n9\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nposted_date\n0\n1\n1998-03-07\n2023-05-19\n2012-08-19\n619\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nhas_images\n0\n1\n0\nFAL: 96429\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nduration_seconds\n0\n1\n31613.25\n6399774\n0\n30\n180\n600\n1987200000\n▇▁▁▁▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreported_date_time\n0\n1\n1925-12-29\n2023-05-18 19:27:00\n2012-02-05 03:00:00\n86201\n\n\nreported_date_time_utc\n0\n1\n1925-12-29\n2023-05-18 19:27:00\n2012-02-05 03:00:00\n86201\n\n\n\n\n\nThe data is fairly complete. The daypart is the least complete with only 97% completion. This variable is currently a string; it would be better as a factor.\n\nufo_sightings$day_part &lt;- as.factor(ufo_sightings$day_part)\n\nNow we can see when it is most common to see UFOs.\n\nggplot(ufo_sightings, aes(day_part)) +\n  geom_bar()\n\n\n\n\nThis is a pretty ugly plot. First, I’m going to clean up the formatting. The day-parts should be tilted so they are not overlapping, the axis labels are not clear, and I don’t like the default ggplot theme. You need to change the plot theme before you tilt the axis labels, otherwise the theme’s defaults will reset the axis label orientation.\n\nggplot(ufo_sightings, aes(day_part)) +\n  geom_bar() +\n  ylab(\"Number of Sighting\") +\n  xlab(\"\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1)) \n\n\n\n\nNow, there are two ways to order the bars. Usually, I’d use either ascending or descending order by count. But here, the day parts do have an intrinsic order- most light to least light (or the reverse) or something along the cycle. So, I need to order the factor day_part. I used the explanation here get the order and decide on the color scale. I decided that afternoon was lighter than morning.\nFirst, I’ll code the NAs as “unknown”. It is a small percentage of the total data, so dropping them is also a defensible choice. It doesn’t really impact the visualization to leave them in, and it does reveal some information about how detailed the reports are.\nI love the POSIT cheatsheets for checking syntax, especially the printed pdfs because I annotate them with my own notes. However, they can end up out of date. Recently POSIT has created HTML versions and updated most of the pdfs. So if you have a printed forcats cheatsheet branded RStudio, go get the new POSIT’s one, since some of the functions on the old cheatsheet are deprecated.\nTo convert NAs to a named level, the current function is fct_na_value_to_level rather than fct_explicit_na and the level is specified with level = \"blah\" rather than na_level = \"blah\". Both ways are shown in the code block and both will work.\n\n# old way of converting NAs to a specific level\n#ufo_sightings$day_part &lt;- fct_explicit_na(ufo_sightings$day_part, na_level = #\"unknown\")\n\n# new way of converting NAs to a specific level\nufo_sightings$day_part &lt;- fct_na_value_to_level(ufo_sightings$day_part, level = \"unknown\")\n\nNow, I need to relevel the day_parts factor. I started with morning, and then progressed through the day_parts in the order they occur. I put the unknown level last.\n\nufo_sightings$day_part &lt;- fct_relevel(ufo_sightings$day_part,c(\"morning\",\n                                      \"afternoon\", \"civil dusk\", \"nautical dusk\",\n                                      \"astronomical dusk\", \"night\",\n                                      \"astronomical dawn\", \"nautical dawn\",\n                                      \"civil dawn\", \"unknown\"))\n\nNow, I’m going to use the aesthetic fill = day_part to color my bars by time of day. I’m also going to define a manual color scale in blues, grays, and blacks to reflect the state of the sky- light blues for daylight and darker blues/blacks for night times. I made the unknown green (for little green men). I played around with both colors and values to get a gradient that I liked. Note that both gray and grey work. I kept the legend on while I was adjusting the colors because it was easier to see the gradient as compared to looking at the bar chart, but I turned it off for the final graph.\n\nggplot(ufo_sightings, aes(day_part, fill = day_part)) +\n  geom_bar() +\n  ylab(\"Number of Sighting\") +\n  xlab(\"\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1)) +\n  scale_fill_manual(values = c(\"lightskyblue2\", \"lightskyblue1\", \"skyblue2\",\n \"skyblue3\",\"lightskyblue4\", \"grey23\",\"lightskyblue4\",\"skyblue3\",\"skyblue2\",\n \"green\")) +\n  ggtitle(\"When do UFO Sightings Occur?\") +\n  theme(legend.position = \"none\")\n\n\n\n\nSo most sightings take place at night and very few take place in the early morning.\nThe sightings database also contains pictures of UFOs. Not all reports include pictures. It seems like it would be hard to get a good picture at night, so I’m wondering if most pictures are taken during daylight hours, despite having fewer over all sightings.\nhas_images is a boolean and the dataset is complete, but it seems that everything is coded FALSE? The original dataset from 2019 doesn’t include has_images, so I can’t use that to repair most of the entries.\n\nufo_sightings %&gt;%\n  group_by(day_part) %&gt;%\n  summarise(mean(has_images))\n\n# A tibble: 10 × 2\n   day_part          `mean(has_images)`\n   &lt;fct&gt;                          &lt;dbl&gt;\n 1 morning                            0\n 2 afternoon                          0\n 3 civil dusk                         0\n 4 nautical dusk                      0\n 5 astronomical dusk                  0\n 6 night                              0\n 7 astronomical dawn                  0\n 8 nautical dawn                      0\n 9 civil dawn                         0\n10 unknown                            0\n\n\nLooking at how the current dataset was prepared doesn’t help much either. The data was rescraped from National UFO Reporting Center as shown here and does not seem to include the original tidytuesday dataset at all. Searching for has_image, I find it first mentioned in line 890.\ndata_ufo_reports_clean &lt;- data_ufo_reports_durations |&gt;\ndplyr::mutate(\nstate = dplyr::coalesce(state, ascii_state),\n# Recode \"images\" to TRUE/FALSE, and replace NAs while we're at it.\nhas_images = isTRUE(images == \"Yes\"),\nLooking at the saved scraped data, found here, there is an images column that contains “Yes” or NA. It is very incomplete, with a completion rate of 1%. I also looked at the final cleaned data (post encoding to TRUE/FALSE) and it only has FALSES. So, it looks like the re-coding might have gone wrong.\nHere is the original data from the scraping.\n\nurl &lt;- \"https://github.com/jonthegeek/apis/raw/main/data/data_ufo_reports.rds\"\nufo_path &lt;- withr::local_tempfile(fileext = \".rds\")\ndownload.file(url, ufo_path)\ndata_ufo_reports_1 &lt;- readRDS(ufo_path)\n\nSee how many images are in this dataset.\n\ndata_ufo_reports_1 %&gt;%\n  filter(images == \"Yes\") %&gt;%\n  nrow()\n\n[1] 2342\n\n\nThere are only 2342 images in the database. The raw scrapped data has 144451 rows, while our cleaned processed data has 96429 rows. So a fair number of data points were dropped. It is possible that all of the records with images were dropped during the cleaning and processing.\nI’m going to join the UFO dataset with the scrapped data. I’m letting R decide what columns to join on, but the join does need to include summary, since that is one column that certainly was not cleaned/ processed.\nThe place names were cleaned (see for example lines 277 and beyond in the cleaning code). When I do an inner_join, I only get 1881 records. I’d expect to get the 96429 records in the ufo_sightings data file, since they should be contained in the original larger dataset.\n\ncombined &lt;- ufo_sightings %&gt;%\n  inner_join(data_ufo_reports_1) \n\nJoining with `by = join_by(city, state, shape, summary)`\n\ncombined %&gt;%\n  nrow()\n\n[1] 1881\n\n\nIf I were working on this for something more mission critical, I’d dig through the cleaning code more carefully, and make sure it was all ok. For now, I’m just going to check the Boolean encoding step, using my much smaller combined dataframe.\n\n# counting how many images\ncombined %&gt;%\n  filter(images == \"Yes\") %&gt;%\n  nrow()\n\n[1] 18\n\n#sthis is the re-coding step from the tidytuesday data\ncombined2 &lt;- combined %&gt;%\n  dplyr::mutate(\n       # Recode \"images\" to TRUE/FALSE, and replace NAs while we're at it.\n    has_images_tester = isTRUE(images == \"Yes\"))\n\n# counting how many images again\ncombined2 %&gt;%\n  filter(images == \"Yes\") %&gt;%\n  nrow()\n\n[1] 18\n\n# counting how many images after recode\ncombined2 %&gt;%\n  filter(has_images_tester == TRUE) %&gt;%\n  nrow()\n\n[1] 0\n\n\nSo, the specified code clearly doesn’t work. This is how I’d do it. I’d explicitly code both TRUE and FALSE using an ifelse clause in the mutate. Since the images contains a bunch of NAs, you need to be more careful about your test condition: images == \"Yes\" does give you the correct TRUEs, but it gives no FALSEs and retains all the NAs.\n\ncombined3 &lt;- combined2 %&gt;%\n  dplyr::mutate(\n       # Recode \"images\" to TRUE/FALSE, and replace NAs while we're at it.\n    has_images_correct1 = ifelse(images == \"Yes\", TRUE, FALSE))\n\ncombined3 %&gt;%\n  filter(has_images_correct1 == TRUE) %&gt;%\n  nrow()\n\n[1] 18\n\ncombined3 %&gt;%\n  filter(has_images_correct1 == FALSE) %&gt;%\n  nrow()\n\n[1] 0\n\n\nThe better test condition for this dataset is to use is.na(images) == FALSE (or the reverse, it doesn’t matter) and code NAs as FALSE and everything else as TRUE. This works because we have only have two values in the column (NA/ Yes). If you had other correct values, say Yes/ No/ NA, then this would not work. You’d need more complicated logic or two sets of ifelse.\n\ncombined4 &lt;- combined3 %&gt;%\n  dplyr::mutate(\n       # Recode \"images\" to TRUE/FALSE, and replace NAs while we're at it.\n    has_images_correct2 = ifelse(is.na(images) == FALSE, TRUE, FALSE))\n\ncombined4 %&gt;%\n  filter(has_images_correct2 == TRUE) %&gt;%\n  nrow()\n\n[1] 18\n\ncombined4 %&gt;%\n  filter(has_images_correct2 == FALSE) %&gt;%\n  nrow()\n\n[1] 1863\n\n\nSo this method of recoding works. My approach would be to first copy the entire processing code from line 140 and update the coding of images at 890 and re-run the whole thing and see what was produced. But more detailed checking of other steps might be required, depending on the application.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {TidyTuesday {Week} 25: {UFO} {Sightings} {Redux}},\n  date = {2023-06-20},\n  url = {https://lsinks.github.io/posts/2023-06-19-tidytuesday-UFOs/UFOs},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “TidyTuesday Week 25: UFO Sightings\nRedux.” June 20, 2023. https://lsinks.github.io/posts/2023-06-19-tidytuesday-UFOs/UFOs."
  },
  {
    "objectID": "posts/2023-06-29-tidytuesday-populated-places-leaflet/arlington-neighborhoods-leaflet.html",
    "href": "posts/2023-06-29-tidytuesday-populated-places-leaflet/arlington-neighborhoods-leaflet.html",
    "title": "Tidy Tuesday Revisited: Interactive Map of Arlington Historic Neighborhoods",
    "section": "",
    "text": "This week’s TidyTuesday is about place names as recorded by the US Board on Geographic Names. The dataset has been cleaned to include only populated places. I ended up augmenting the dataset with information about Arlington Historic neighborhoods and current neighborhood boundaries. My post with code on this project is here.\nI wanted to create an interactive map with leaflet, but I encountered two problems:\n1- I couldn’t figure out how to add my civic association map.\n2- The map that I did make worked fine when I ran it from a code chunk, but failed when I rendered the quarto document.\nI’ve solved both problems and I really enjoyed working with leaflet.\nHere are the libraries:\n\nlibrary(tidyverse) # who doesn't want to be tidy?\nlibrary(sf) # for handling geo data\nlibrary(leaflet) # interacting mapping\n\nI saved the two datasets from my previous work: historic_4269 and arlington_polygons_sf. I saved them using:\nst_write(historic_4269, \"points.shp\")\nst_write(arlington_polygons_sf, \"polygons.shp\")\nfrom the sf package.\nHere, I’m reading them in. The process does change some of the variable names. The dataset from the National Register of Historic Places had non-standard names such as Property.Name, which gets converted to a shorter name, Prprt_N, with _ instead of period.\n\nhistoric_4269 &lt;- st_read(\"points.shp\")\narlington_polygons_sf &lt;- st_read(\"polygons.shp\")\n\nI mentioned that I found tutorials here and here to make the pop-up URL using leaflet. So, following them I add the HTML anchor tag.\n\n# turn the url to HTML anchor tag\nhistoric_4269 &lt;- historic_4269 %&gt;% \n  mutate(tag = paste0(\"More Info: &lt;a href=\", Extrn_L,\"&gt;\", Extrn_L, \"&lt;/a&gt;\"))\n\nLeaflet uses background map tiles as the canvas for the map. As with all mapping, the coordinate reference system (CRS) of all your component layers needs to be the same. The two datasets I have used the CRS= 4269 projection, but this isn’t the usual CRS. The background map I chose uses the 4326 CRS, so I need to transform my data to that projection. Leaflet will give you a warning if you add layers with unexpected CRSs, so make sure to read the messages carefully and correct them.\n\nhistoric_4326 &lt;- sf::st_transform(historic_4269, crs = 4326)\narlington_polygons_sf_4326 &lt;- sf::st_transform(arlington_polygons_sf, crs = 4326) \n\nFor the issue of adding the polygon data, I was just not really thinking about things. Leaflet uses tidyverse piping, so you either need to have the dataset at the start of the pipe chain or you need to explicitly pass it as data = blah. The error message wasn’t super help to me either : addPolygons must be called with both lng and lat, or with neither. I thought that meant I needed to transform the polygons into some other type geometry format.\nSo this doesn’t work:\nleaflet_map &lt;- leaflet() %&gt;%\naddPolygons(arlington_polygons_sf_4326)\nleaflet_map\nBut this does:\n\nleaflet_map &lt;- leaflet(arlington_polygons_sf_4326) %&gt;% \n  addPolygons() \n\nleaflet_map\n\n\n\n\n\nOr this:\n\nleaflet_map &lt;- arlington_polygons_sf_4326 %&gt;% \n  leaflet() %&gt;%\n  addPolygons() \n\nleaflet_map\n\n\n\n\n\nOr this:\n\nleaflet_map &lt;- leaflet() %&gt;% \n  addPolygons(data = arlington_polygons_sf_4326) \n\nleaflet_map\n\n\n\n\n\nI chose to use the last method, since I was adding data from different sources and I thought it would be more understandable to have the data source explicitly stated in each layer call.\nTo make things a bit clearer, I set a color palette for the Arlington neighborhoods. There are 62 of them, so I used viridis, which is more suited for numerical data, but creates a pleasing effect here. There is information encoded in the colors, the purples correspond to neighborhoods starting with “A” and the yellows correspond to those at the end of the alphabet, but that isn’t really important. The choice was purely an aesthetic one.\n\npal &lt;- colorFactor(palette = \"viridis\", domain = arlington_polygons_sf_4326$CIVIC)\n\nThe final leaflet map has three layers:\n\nthe underlying map created using addProviderTiles()\nthe current Arlington neighborhoods created using addPolygons()\nthe point markers for the historic districts created using addCircleMarkers()\n\nThe neighborhood names appear when you hover over the polygon, while the name of the historic district and the link to the application submitted to be added to the National Register of Historic Places appears as a pop-up when you click on it.\nLeaflet uses ~ notation to reference variables in the data, which you can see in code below.\n\nleaflet_map &lt;- leaflet() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(\n    data = arlington_polygons_sf_4326,\n    weight = 1,\n    label = ~ CIVIC,\n    color = ~ pal(CIVIC)\n  ) %&gt;%\n  addCircleMarkers(\n    data = historic_4326,\n    popup = ~ paste0(\"&lt;b&gt;\", Prprt_N, \"&lt;/b&gt;\", \"&lt;br&gt;\", tag),\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"black\",\n    stroke = NA\n  )\nleaflet_map\n\n\n\n\n\nDatacamp has a really nice starter course on leaflet that I found very helpful for understanding leaflet conceptually as well as learning about the basic formatting options. There is also a nice set of documentation here.\nSo why was my leaflet map causing the quarto document to fail to render? Apparently, there was a issue with knitr and quarto that popped up after some updates in May 2023. It applies to packages other than leaflet as well. If you get an error message along the lines of :\nError in `add_html_caption()`: ! unused argument (xfun::grep_sub(\"^[^&lt;]*&lt;[^&gt;]+aria-labelledby[ ]*=[ ]*\\\"([^\\\"]+)\\\".*$\", \"\\\\1\", x))\nBacktrace:\n1. global .main()\n2. execute(...)\n3. rmarkdown::render(...)\n4. knitr::knit(knit_input, knit_output, envir = envir, quiet = quiet)\n5. knitr:::process_file(text, output) ...\n14. sew(res, options)\n15. knitr:::sew.list(x, options, ...)\n16. base::lapply(x, sew, options, ...)\n17. FUN(X[[i]], ...)\n18. knitr:::sew.knit_asis(x, options, ...) Execution halted.\nthen you probably have this issue. Quarto has already fixed the issue with stable release 1.3.433. The version of Quarto bundled with RStudio RStudio 2023.06.0+421 “Mountain Hydrangea” for Windows was 1.3.353 and has the problem. If you use the bundled version with RStudio, close RStudio, install the latest Quarto as a standalone program. When you open RStudio, it should automatically detect the new version and switch to that.\nTo check what version of Quarto you have, go to the terminal (not console) and type quarto check.\nLeaflet is pretty amazing. I’ve always found mapping in R to be unpleasant, but leaflet makes it easy and produces beautiful maps.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {Tidy {Tuesday} {Revisited:} {Interactive} {Map} of\n    {Arlington} {Historic} {Neighborhoods}},\n  date = {2023-06-29},\n  url = {https://lsinks.github.io/posts/2023-06-29-tidytuesday-US-populated-places-leaflet/arlington-neighborhoods-leaflet},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “Tidy Tuesday Revisited: Interactive Map\nof Arlington Historic Neighborhoods.” June 29, 2023. https://lsinks.github.io/posts/2023-06-29-tidytuesday-US-populated-places-leaflet/arlington-neighborhoods-leaflet."
  }
]