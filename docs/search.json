[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Entries",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nThoughts on Building a Quarto Website\n\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nturtle\n\n\n\n\nSome tips and tricks for using Quarto and RStudio to create a portfolio website and blog.\n\n\n\n\n\n\nNov 1, 2023\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to Web Scraping Using rvest\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\nData-Viz\n\n\ntombstone\n\n\nregex\n\n\nweb scraping\n\n\nrvest\n\n\ndata cleaning\n\n\nTidyTuesday\n\n\n\n\nWeb scraping data with rvest to enhance the information in the Tombstone Project.\n\n\n\n\n\n\nSep 8, 2023\n\n\n30 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday 36: Visualizing Worker Demographic Information with Treemaps\n\n\n\n\n\n\n\nData-Viz\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\ntreemap\n\n\nTidyTuesday\n\n\nd3treeR\n\n\ninteractive\n\n\n\n\nUsing treemap and d3treeR to create static and dynamic treemaps\n\n\n\n\n\n\nSep 5, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday 35: Exploring Fair Use Cases\n\n\n\n\n\n\n\nData-Viz\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\nTidyTuesday\n\n\nstringr\n\n\ndata cleaning\n\n\ndual axis plot\n\n\nggplot2\n\n\ncowplot\n\n\n\n\nUsing stringr::str_detect() functions to explore why variables from different datasets don’t exactly match.\n\n\n\n\n\n\nAug 29, 2023\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday: Exploring Refugee Flow with A Sankey Diagram\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\nData-Viz\n\n\nSankey\n\n\nnetworkD3\n\n\nggsankey\n\n\nhtmlwidgets\n\n\n\n\nTidyTuesday 34: Looking at the United Nations High Commissioner for Refugees data with a Sankey Diagram. Diagram created with networkD3 in R.\n\n\n\n\n\n\nAug 28, 2023\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nMapping for the Tombstone Project\n\n\n\n\n\n\n\nData-Viz\n\n\nR\n\n\nR-code\n\n\nleaflet\n\n\nquarto\n\n\nproblem solving\n\n\nmapping\n\n\nCode-Along\n\n\nspatial join\n\n\nWordPress\n\n\nleafpop\n\n\nst_jitter\n\n\n\n\nMethods for dealing with overlapping points and then styling a leaflet map with various labels and pop-ups.\n\n\n\n\n\n\nAug 15, 2023\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday Twofer (32 and 33)\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\nTidyTuesday\n\n\ncorrplot\n\n\n\n\nLooking at how heat levels increase on the show The Hot Ones. Then doing some EDA on a data set on spam emails.\n\n\n\n\n\n\nAug 15, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nData Cleaning for the Tombstone Project\n\n\n\n\n\n\n\nData-Viz\n\n\nR\n\n\nR-code\n\n\nfuzzyjoin\n\n\nquarto\n\n\nleaflet\n\n\nregex\n\n\nstringr\n\n\ndata cleaning\n\n\nproblem solving\n\n\nmapping\n\n\nsf\n\n\nCode-Along\n\n\n\n\nUsing StringR to clean a human created excel sheet full of typos and formatting inconsistencies. Then matching excel data to photo names.\n\n\n\n\n\n\nAug 4, 2023\n\n\n51 min\n\n\n\n\n\n\n  \n\n\n\n\nStates\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\nTidyTuesday\n\n\ntidy\n\n\nGWalkr\n\n\nTableau Alternative\n\n\nexploratory data analysis\n\n\ninteractive\n\n\n\n\nUsing GWalkR to create an interactive exploratory data analysis window similar to Tableau.\n\n\n\n\n\n\nAug 1, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nA Heatmap of James Lind’s Scurvy Study\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\nTidyTuesday\n\n\ntidy\n\n\nheatmap\n\n\nggplot\n\n\ntidyverse\n\n\nggthemes\n\n\n\n\nMaking a Heatmap of James Lind’s Scurvy Study using ggplot. Data cleaning and reformating handled through tidyverse packages.\n\n\n\n\n\n\nJul 25, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 28: Global Surface Temperature\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\nTidyTuesday\n\n\ntidy\n\n\ngganimate\n\n\n\n\nMaking an animated graph of global temperature change over time with gganimate.\n\n\n\n\n\n\nJul 11, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 27: Historical Markers\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nData-Viz\n\n\nsf\n\n\nleaflet\n\n\n\n\nTidyTuesday: Interactive Map of Arlington Historic Markers\n\n\n\n\n\n\nJul 4, 2023\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday Revisited: Interactive Map of Arlington Historic Neighborhoods\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nData-Viz\n\n\nsf\n\n\nleaflet\n\n\n\n\nTidyTuesday: Interactive Map of Historic Neighborhoods of Arlington Virginia\n\n\n\n\n\n\nJun 29, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: US Populated Places\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nData-Viz\n\n\nopenxlsx\n\n\nstringr\n\n\nfuzzyjoin\n\n\nmapview\n\n\nsf\n\n\n\n\nTidyTuesday: Historic Neighborhoods of Arlington Virginia\n\n\n\n\n\n\nJun 27, 2023\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 25: UFO Sightings Redux\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\nData-Viz\n\n\n\n\nTidyTuesday: UFO Sightings\n\n\n\n\n\n\nJun 20, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 18: Portal Project\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\nData-Viz\n\n\ndata validation\n\n\nexploratory data analysis\n\n\n\n\nTidyTuesday: Rodents of Portal Arizona\n\n\n\n\n\n\nMay 2, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 17: London Marathon\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nCode-Along\n\n\nData-Viz\n\n\n\n\nTidyTuesday: Exploring the London Marathon\n\n\n\n\n\n\nApr 25, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 16: Neolithic Founder Crops\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\nData-Viz\n\n\n\n\nTidyTuesday: Exploring early agriculture in SW Asia\n\n\n\n\n\n\nApr 18, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCredit Card Fraud: A Tidymodels Tutorial\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ntidymodels\n\n\nMachine Learning\n\n\nclassifiers\n\n\n\n\nAn Imbalanced Class Problem\n\n\n\n\n\n\nApr 11, 2023\n\n\n39 min\n\n\n\n\n\n\n  \n\n\n\n\nA Tidymodels Tutorial: A Structural Approach\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ntidymodels\n\n\nMachine Learning\n\n\n\n\nExploring the different steps for modeling\n\n\n\n\n\n\nApr 10, 2023\n\n\n22 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge- Endangered Species\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\n30DayChartChallenge\n\n\nwaffle\n\n\n\n\nHow many species have been delisted?\n\n\n\n\n\n\nApr 4, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge- Flora and Fauna\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\nturtle\n\n\n30DayChartChallenge\n\n\n\n\nHow Large are Different Types of Turtles?\n\n\n\n\n\n\nApr 3, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge -Arlington Parks\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\nCode-Along\n\n\nWaffle\n\n\n30DayChartChallenge\n\n\n\n\nWho Owns the Parks in Arlington Virgina?\n\n\n\n\n\n\nApr 2, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nSelf-Guided Learning through a Wordle Guess Generator: Part 1\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\n\n\nSkills improvement with a Wordle Guess Generator\n\n\n\n\n\n\nApr 1, 2023\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nSelf-Guided Learning through a Wordle Guess Generator: Part 2\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\n\n\nCurrent version of a Wordle Guess Generator\n\n\n\n\n\n\nApr 1, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nOne Class SVM\n\n\n\n\n\n\n\nR\n\n\nR-code\n\n\ncode-along\n\n\nmachine learning\n\n\ncaret\n\n\nsvm\n\n\nclassifiers\n\n\nsupport vector machines\n\n\n\n\nOne Class SVM for Imbalanced Classes\n\n\n\n\n\n\nMar 30, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Daylight Savings Time\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\nData-Viz\n\n\nggimage\n\n\nmaps\n\n\n\n\nTidyTuesday: Exploring Time Zones and Daylight Savings Time\n\n\n\n\n\n\nMar 28, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 12: Programming Languages Revisited\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\ndata visualization\n\n\nskimr\n\n\n\n\nTidyTuesday: Polishing\n\n\n\n\n\n\nMar 24, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTwitter Cards\n\n\n\n\n\n\n\nquarto\n\n\nturtle\n\n\n\n\nMaking Twitter Cards\n\n\n\n\n\n\nMar 24, 2023\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday Week 12: Programming Languages\n\n\n\n\n\n\n\nR\n\n\nTidyTuesday\n\n\nR-code\n\n\ncode-along\n\n\n\n\nTidyTuesday: How to comment in Various Programming Languages\n\n\n\n\n\n\nMar 21, 2023\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Blog\n\n\n\n\n\n\n\nquarto\n\n\nR\n\n\nturtle\n\n\n\n\nTrying to create a Quarto Blog\n\n\n\n\n\n\nMar 14, 2023\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "",
    "text": "I’m still behind on TidyTuesday, so here is TidyTuesday #34, which deals with refugee data. This data is collected by the United Nations High Commissioner for Refugees and the R package is updated twice a year."
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#loading-data-and-libraries",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#loading-data-and-libraries",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "Loading Data and Libraries",
    "text": "Loading Data and Libraries\nLibraries.\n\nlibrary(tidyverse) # who doesn't want to be tidy\nlibrary(networkD3) # for Sankey plots\nlibrary(ggsankey) # another sankey package \nlibrary(htmlwidgets) # html widgets helps to handle the networkD3 objects\nlibrary(htmltools) # for formatting html code\n\nLoading data.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 34)\npopulation &lt;- tuesdata$population"
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#exploratory-data-analysis",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#exploratory-data-analysis",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThis dataset tracks the number of refugees by year. It tracks how many people request entry at each country and breaks this information down by country of origin.\n\nhead(population)\n\n# A tibble: 6 × 16\n   year coo_name    coo   coo_iso coa_name coa   coa_iso refugees asylum_seekers\n  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n1  2010 Afghanistan AFG   AFG     Afghani… AFG   AFG            0              0\n2  2010 Iran (Isla… IRN   IRN     Afghani… AFG   AFG           30             21\n3  2010 Iraq        IRQ   IRQ     Afghani… AFG   AFG            6              0\n4  2010 Pakistan    PAK   PAK     Afghani… AFG   AFG         6398              9\n5  2010 Egypt       ARE   EGY     Albania  ALB   ALB            5              0\n6  2010 China       CHI   CHN     Albania  ALB   ALB            6              0\n# ℹ 7 more variables: returned_refugees &lt;dbl&gt;, idps &lt;dbl&gt;, returned_idps &lt;dbl&gt;,\n#   stateless &lt;dbl&gt;, ooc &lt;dbl&gt;, oip &lt;dbl&gt;, hst &lt;dbl&gt;\n\n\nWhat years have the most data?\n\npopulation %&gt;%\n  group_by(year) %&gt;%\n  count() %&gt;%\n  ggplot(aes(year, n)) +\n  geom_col()\n\n\n\n\nI expected to see spikes corresponding to specific events, but instead, it looks like the data is gradually increasing. This upward trend might be due to better record-keeping/ data collection over the years.\nI’m going to look at last year’s data.\n\npop_2022 &lt;- population %&gt;%\n  filter(year == 2022)"
  },
  {
    "objectID": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#a-sankey-diagram-to-explore-flow",
    "href": "posts/2023-08-28-TidyTuesday-Refugees/TidyTuesday34-refugees.html#a-sankey-diagram-to-explore-flow",
    "title": "TidyTuesday: Exploring Refugee Flow with A Sankey Diagram",
    "section": "A Sankey Diagram to Explore Flow",
    "text": "A Sankey Diagram to Explore Flow\nNow, I will look at where most refugees come from.\nThere is a technical and legal difference between refugees and asylum seekers, but I will sum them up. I also will be using refugees and asylum seekers interchangeably here.\n\npop_2022 &lt;- pop_2022 %&gt;%\n  mutate(total = refugees + asylum_seekers) %&gt;%\n  select(coo_name, coa_name, total)\n\nI’ve been interested in learning how to make Sankey diagrams in R. A Sankey diagram shows the flow between nodes; there is a great gallery of examples here. This tidytuesday data does lend itself to a Sankey diagram. There are two nodes- country of orgin and country of arrival and the flow is the number of refugees between each pair.\nI do want to highlight that the UN Refugee Agency has a separate dataset that tracks flow. The dataset we are working with is the net number of refugees. As the UNHCR webpage says:\n\nOften the net increase or decrease in stock figures between years has been used in lieu of accurate flow figures. But these may underrepresent the true magnitude of population movements if, for example, refugee arrivals and departures balance each other out.\n\nStill, for TidyTuesday, the net population is fine.\nI’m going to pull out the top 3 sources of refugees. I’m grouping by the country of origin (coo_name) and then summing the refugees going to all countries. I then take the top 3 using dpylr’s slice_max(). Note that this function replaces top_n().\n\ntop_3_source &lt;- pop_2022 %&gt;%\n  group_by(coo_name) %&gt;%\n  summarize(num_by_coo = sum(total)) %&gt;%\n  slice_max(order_by = num_by_coo, n = 3)\n\ntop_3_source_names &lt;- top_3_source$coo_name\n\ntop_3_source_names\n\n[1] \"Syrian Arab Rep.\" \"Afghanistan\"      \"Ukraine\"         \n\n\nNow, I’m going to do a related analysis and see where the top destinations are.\n\ntop_3_dest &lt;- pop_2022 %&gt;%\n  group_by(coa_name) %&gt;%\n  summarize(num_by_coa = sum(total)) %&gt;% slice_max(order_by = num_by_coa, n = 3)\n\ntop_3_dest_names &lt;- (top_3_dest$coa_name)\ntop_3_dest_names\n\n[1] \"Türkiye\"                \"Iran (Islamic Rep. of)\" \"Germany\"               \n\n\nNow I’m making a category “other” for both source and destination countries that aren’t in the top 3. I use the forcats function fct_other().\nFirst country of arrival:\n\npop_2022$coa_name = factor(pop_2022$coa_name)\n\npop_2022$coa_name &lt;- pop_2022$coa_name %&gt;%\n  fct_other(keep = top_3_dest_names, other_level = \"other\")\n\nThen, country of origin:\n\npop_2022$coo_name = factor(pop_2022$coo_name)\n\npop_2022$coo_name &lt;- pop_2022$coo_name %&gt;%\n  fct_other(keep = top_3_source_names, other_level = \"other\")\n\nFor now, I will remove the “other” category of country of origin.\n\npop_2022_no_other &lt;- pop_2022 %&gt;%\n  filter((coo_name != \"other\"))\n\nThere are 3 popular ways to make Sankey diagrams in R: ggsankey, networkD3, and plotly. There is a nice tutorial on these methods here.\nI’m going to focus on ggsankey and networkD3.\n\nggsankey method for Sankey Plots in R\nThis package is combined with ggplot to make nice static Sankey plots.\nThe package documentation does a great job explaining the parts of the Sankey chart. It does not do a great job of explaining what format the data should be in. I spent most of my time last Tuesday trying to figure this out.\nThe package includes a function make_long(), which is used to prepare the data for plotting. It labels the columns appropriately and creates the needed null/ NA entries. These entries are used to signify the end of the flow.\nThe UN refugee data is already long, so I spent a fair bit of time trying to figure out how to either label it myself or make it wide so I could use make_long().\nIt turns out that ggsankey can only be used on disaggregated data and not aggregated data such as I have here. I never found that explicitly stated in the documentation (it could be I missed that), but I only discovered it as I was trying to wrangle my data into the correct format and comparing it to the mtcars data in the example.\nWhat do I mean by aggregated vs. disaggregated?\nLet’s look at the mtcars dataframe:\n\nmtcars2 &lt;- mtcars %&gt;%\n  slice_sample(n = 10) %&gt;%\n  select(cyl, vs)\n\nmtcars2\n\n                 cyl vs\nPontiac Firebird   8  0\nFerrari Dino       6  0\nPorsche 914-2      4  0\nMerc 450SLC        8  0\nMerc 280           6  1\nMerc 230           4  1\nDatsun 710         4  1\nCamaro Z28         8  0\nMazda RX4          6  0\nMerc 450SE         8  0\n\n\nThis is disaggregated. It reports the properties of each car. The equivalent in our dataset would be if the database listed every refugee by name and specified where they were from and where they were going.\nWhat the population dataset has instead is the aggregated data. This is the mtcars equivalent:\n\nmtcars2 %&gt;% group_by(cyl, vs) %&gt;% count()\n\n# A tibble: 5 × 3\n# Groups:   cyl, vs [5]\n    cyl    vs     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     4     0     1\n2     4     1     2\n3     6     0     2\n4     6     1     1\n5     8     0     4\n\n\nSo we know we have two cars with cyl = 4 and vs = 1, but we don’t know that they are specifically the Merc 240D and the Fiat 128.\nIf all you have is the aggregated data, you can’t disaggregate it. The names of the cars are not contained in that dataframe. In this case, if you absolutely had to make a Sankey diagram using ggsankey, you could write some code to make a dummy ID to represent the missing dimension. That is, you could split the entry cyl = 4 and vs = 1 into\ncyl = 4 and vs = 1 and id = 1\ncyl = 4 and vs = 1 and id = 2\nAnd so on for each group.\nThen you’d have to dpylr::pivot_wider() and then ggsankey::make_longer(). I wouldn’t recommend this, but it is useful sometimes to think of how you’d approach a problem, even if there are easier solutions. [This :: formatting forces the function to load from the specific library on the left side of the double colon. This is useful if you have conflicting function names in different packages; this notation assure the proper one is called. It is also useful to clarify where specific functions are coming from, even if there would be no conflict.]\n\n\nnetworkD3 method for Sankey Plots in R\nI love the R Graph Gallery when I’m looking for inspiration for TidyTuesday. R Graph Gallery recommends the networkD3 package for Sankey plots, but it is an htmlwidget. I’ve been doing a lot of interactive stuff recently, and I originally wanted to get back to some good old ggplot. But the networkD3 package does create lovely Sankey plots.\nThe networkD3 method seems slightly confusing because you need to transform and label the data yourself. Like ggsankey, you need information about the nodes and how they are connected with each other. One other note- this is based on a JavaScript package (with 0 indexing). So, a few “-1” to convert the index from R (which starts at 1).\nThere is a nice post on Sankey diagrams in networkD3 on the Data to Viz blog. The code is hidden, so you need to toggle it on with the code button if you want to see it.\nThe function call is (from the manual found at CRAN):\nsankeyNetwork(Links, Nodes, Source, Target, Value, NodeID, NodeGroup = NodeID, LinkGroup = NULL, units = \"\", colourScale = JS(\"d3.scaleOrdinal(d3.schemeCategory20);\"), fontSize = 7, fontFamily = NULL, nodeWidth = 15, nodePadding = 10, margin = NULL, height = NULL, width = NULL, iterations = 32, sinksRight = TRUE)\nI need Links, Nodes, Source, Target, Value, NodeID.\nNodes is the list of countries (both origin and arrival). It needs to be a unique list of the countries stored as a dataframe.\nI converted the countries to factors earlier, but that isn’t a problem.\n\nnodes &lt;- data.frame(name =\n                      c(pop_2022_no_other$coo_name, pop_2022_no_other$coa_name) %&gt;%\n                      unique()\n                    )\n\nNext, we need the IDs for source and target, zero-indexed. The ID is generated by position in the nodes dataframe I just created. The match() function is a base R function that returns a vector of the positions of the first argument (pop_2022_no_other$coo_name) as found in the second argument (nodes$name). Note that it returns a vector of all matches. I know that nodes is a unique list, so it is only going to return a single index. This may not be true for other use cases. Again, subtract 1 for the difference in indexing.\n\npop_2022_no_other$IDsource = match(pop_2022_no_other$coo_name, nodes$name) -\n  1\npop_2022_no_other$IDtarget = match(pop_2022_no_other$coa_name,  nodes$name) -\n  1\n\nNext, create the color scales. Details about the options for the color scales are found in the D3 API documentation.\nI am going to change the colors a bit. The other category will be gray. The colors are assigned matches using the order in the nodes df. Note that this does NOT match the order in the diagram. Most Sankey plotting programs reorder the nodes to minimize crossing and to create a cleaner diagram.\n\nColourScal = 'd3.scaleOrdinal([`#946943`, `#b63e36`,`#F5B041`, `#909497`,`#383867`, `#584c77`, `#33431e`, `#a36629`, `#92462f`])'\n\nThe colors are specified in hex code here. You can find color pickers that give you the codes on the web, such as this one.\nNow I can make the diagram. I’m also specifying height and width so I can add a title and a caption using htmlwidgets. If you don’t specify height and width the figure might be truncated when the titles are applied. (This method might work for leaflet maps also. The leaflet package also lacks a method for titles.) I am using the html header 2 styling (h2) for the title, while the caption is just a normal paragraph (p).\nThe sinksRight parameter is used to put the label either outside the flows (FALSE) as I have done here. TRUE puts it inside the flow lines. Unfortunately, there is not a matching sinksLeft, so those labels will always be inside the flow.\n\n# Make the Network\nsankey &lt;- sankeyNetwork(\n  Links = pop_2022_no_other,\n  Nodes = nodes,\n  Source = \"IDsource\",\n  Target = \"IDtarget\",\n  Value = \"total\",\n  NodeID = \"name\",\n  sinksRight = FALSE,\n  colourScale = ColourScal,\n  nodeWidth = 40,\n  fontSize = 13,\n  nodePadding = 20,\n  width = 600,\n  height = 400\n)\n\nLinks is a tbl_df. Converting to a plain data frame.\n\nsankey &lt;-\n  htmlwidgets::prependContent(sankey, htmltools::tags$h2(\"Refugee Flow in 2022\"))\nsankey &lt;-\n  htmlwidgets::appendContent(sankey, htmltools::tags$p(\"from UNHCR’s refugees R package\"))\n\nsankey\n\nRefugee Flow in 2022\n\nfrom UNHCR’s refugees R package\n\n\n\nSo this is pretty clear. Because the other category is disaggregated, we get individual lines for each country of arrival (but we don’t know what countries they are.) If you mouse over the traces, you can see how many refugees are involved in each path. For example, if you mouse over the large gray band on the Afghanistan to other, you see 1.8 million refugees have gone to a single color. Some of the other lines represent only a handful of refugees. I could also aggregate the other data, creating a single band from each source to other. But I think it is neat the way it is.\n\n\nSaving an htmlwidget object\nSaving the interactive figures to a thumbnail is always tricky. Various methods are suggested- using webshot seems to be the most popular, but I haven’t had it work. (First, you save the diagram as an HTML file, and then webshot converts that to a static image). While the HTML saves fine, the webshotted png is the title and the caption with a vast white block of no figure in between. The only consistent way I’ve found is to export from RStudio’s viewer. You do have to fiddle with the height and width of your object; otherwise, the exported file will either be truncated or have non-functional scroll bars in the image."
  },
  {
    "objectID": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html",
    "href": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html",
    "title": "Mapping for the Tombstone Project",
    "section": "",
    "text": "I’m working on a project for my father that will culminate in a website for his genealogy research. There are a couple of different parts that I’m working on independently. In a previous part of the project, I matched an excel sheet with GPS coordinates and some biographic data with photos of various tombstones. This part involves making a leaflet map of various family grave sites.\nIn this part, I’m going to show you how to create a nicely styled leaflet map. I will use the R packages leaflet and leafpop. While the package leaflet.extras has some nice features, the contributor stopped maintaining it in 2018 and recommends against using it for security reasons. There is a leaflet.extras2 package, but it doesn’t have functionality that I want."
  },
  {
    "objectID": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#loading-libraries",
    "href": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#loading-libraries",
    "title": "Mapping for the Tombstone Project",
    "section": "Loading Libraries",
    "text": "Loading Libraries\n\nlibrary(tidyverse) # who doesn't want to be tidy?\nlibrary(gt) # for nice tables\nlibrary(sf) # for handling geo data\nlibrary(here) # reproducible file paths\nlibrary(htmltools) # making html labels\nlibrary(htmlwidgets) # saving final map\nlibrary(leafpop) # pop-up with images\nlibrary(leaflet) # mapping"
  },
  {
    "objectID": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#file-folder-names-and-loading-data",
    "href": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#file-folder-names-and-loading-data",
    "title": "Mapping for the Tombstone Project",
    "section": "File Folder Names and Loading Data",
    "text": "File Folder Names and Loading Data\nHere set-up some variables that I use for the file/ folder structure and I read in the spreadsheet.\n\n# folder names\nblog_folder &lt;- \"posts/2023-08-14-mapping-tombstone\"\nphoto_folder &lt;- \"Map\"\n\nReading in the data. This is data created by the previous portion of the project. There are also a bunch of photos in the Map folder. The specific photo that is associated with a given entry is listed in the photo_list column.\n\ntombstones_geo &lt;- readRDS(here(blog_folder, \"tombstones_geo.RDS\"))\n\nhead(tombstones_geo) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      photos_Matched_Round1\n      Surname\n      First.Name\n      Middle.Name\n      Maiden/Title\n      Cemetery\n      City\n      State\n      DOB\n      DOD\n      Age\n      N\n      W\n      X13\n      X14\n      X15\n      N_degree\n      N_minute\n      N_second\n      W_degree\n      W_minute\n      W_second\n      DOB_date\n      DOD_date\n      Extra_First_Name\n      Extra_Middle_Name1\n      Extra_Middle_Name2\n      full_name_MI\n      full_name\n      full_name_MI_no_space\n      photos_Matched_Round2\n      photos_Matched_Round3\n      photos_Matched_Round4\n      photo_list\n      complete_name\n      cemetery_name\n      geometry\n    \n  \n  \n    Anderson E Francis.JPG\nAnderson\nE\nFrancis\nNA\nLebanon Cumberland Presbyterian Church\nSaline Co.\nIL\n23 Sep 1877\n24 Oct 1899\n22y 1m 1d\n37  52.853\n88  39.164\nWife of AA Anderson\nNA\nNA\n37\n52\n853\n88\n39\n164\n1877-09-23\n1899-10-24\nNA\nNA\nNA\nAnderson E Francis\nAnderson E\nNA\nNA\nNA\nNA\nAnderson E Francis.JPG\nE Francis Anderson\nLebanon Cumberland Presbyterian Church Cemetery\nc(-88.6955555555556, 38.1036111111111)\n    Brown Elizabeth Minnich.jpg\nBrown\nElizabeth\nMinnich\nNA\n\nEgypt, Lehigh Co., PA\nPA\n1 July 1807\n2 Feb 1888\nNA\n40  40.760 \n75  31.705 \nOne stone for husband & wife\nNA\nNA\n40\n40\n760\n75\n31\n705\n1807-07-01\n1888-02-02\nNA\nNA\nNA\nBrown Elizabeth Minnich\nBrown Elizabeth\nNA\nNA\nNA\nNA\nBrown Elizabeth Minnich.jpg\nElizabeth Minnich Brown\n\nc(-75.7125, 40.8777777777778)\n    Doley L Earl.JPG\nDoley\nL\nEarl\nNA\nWebber Campground\nGalatia\nIL\n17 Apr 1889\n19 Nov 1960\nNA\n37  49.907\n88  35.306\nOne headstone\nNA\nNA\n37\n49\n907\n88\n35\n306\n1889-04-17\n1960-11-19\n[eaman]\nNA\nNA\nDoley L Earl\nDoley L\nNA\nNA\nNA\nNA\nDoley L Earl.JPG\nL Earl Doley\nWebber Campground Cemetery\nc(-88.6683333333333, 38.0686111111111)\n    Doley G Clyde and D Hilda.JPG\nDoley\nG\nClyde\nNA\nMasonic & Odd Fellows\nBenton\nIL\n5 Aug 1894\n1 June 1960\nNA\n37  58.810\n88  55.084\nOne stone for husband & wife; footstone--USNavy WWI\nNA\nNA\n37\n58\n810\n88\n55\n84\n1894-08-05\n1960-06-01\n[uilford]\nNA\nNA\nDoley G Clyde\nDoley G\nNA\nNA\nNA\nNA\nDoley G Clyde and D Hilda.JPG\nG Clyde Doley\nMasonic & Odd Fellows Cemetery\nc(-88.94, 38.1916666666667)\n    Hess Samuel Jackson Chapman.JPG\nHess\nSamuel\nJackson Chapman\nNA\nVienna Fraternal\nJohnson Co\nIL\n1854\n1949\nNA\n37  25.693\n88 53.949\nNA\nNA\nNA\n37\n25\n693\n88\n53\n949\nNA\nNA\nNA\nNA\nNA\nHess Samuel Jackson Chapman\nHess Samuel\nNA\nNA\nNA\nNA\nHess Samuel Jackson Chapman.JPG\nSamuel Jackson Chapman Hess\nVienna Fraternal Cemetery\nc(-89.1469444444445, 37.6091666666667)\n    Hess Catherine West.JPG\nHess\nCatherine\nWest\nNA\nVienna Fraternal\nJohnson Co\nIL\n1856\n1906\nNA\n37  25.693\n88 53.949\nNA\nNA\nNA\n37\n25\n693\n88\n53\n949\nNA\nNA\nNA\nNA\nNA\nHess Catherine West\nHess Catherine\nNA\nNA\nNA\nNA\nHess Catherine West.JPG\nCatherine West Hess\nVienna Fraternal Cemetery\nc(-89.1469444444445, 37.6091666666667)"
  },
  {
    "objectID": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#basic-map",
    "href": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#basic-map",
    "title": "Mapping for the Tombstone Project",
    "section": "Basic Map",
    "text": "Basic Map\n\nsimple_map &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%\n  addScaleBar() %&gt;%\n  addCircleMarkers(data = test,\n                   label = ~ (paste(complete_name, cemetery_name, sep = \" \")))\n\nsimple_map\n\n\n\n\n\nAs you can see, no matter how far you zoom, the points never separate. And for what it is worth, the last entry in the test dataframe is shown, indicating the points are added sequentially."
  },
  {
    "objectID": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#markercluster-map",
    "href": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#markercluster-map",
    "title": "Mapping for the Tombstone Project",
    "section": "markerCluster Map",
    "text": "markerCluster Map\nMany maps you see on line use a technique called “spiderify” to separate overlapping points. Spiderifying disperses the points as you zoom in.\nThere are two ways to spiderify a leaflet map. There is a javascript module for spiderifying in can be found here. There is another plug-in called markerCluster, which both clusters and spiderifies points, depending on the zoom level. There is access to this package through the R leaflet package. To access it, use clusterOptions = markerClusterOptions(). For details about the options that can be passed to markerClusterOptions() see the js plug-in documentation.\nmarkerCluster clusters/ groups the data points into a single dot color coded by the number of points it contains (orange, yellow, green) and also displays the number of points in the dot. Single points (not clusters) are displayed in blue or whatever color you set them in the leaflet options. The grouping is set by the number of pixels at the current zoom level (set with maxClusterRadius and defaults to 80 pixels. As you click on a cluster, it zooms in and breaks it up into smaller clusters. (This functionality can be turned off with the parameter zoomToBoundsOnClick.) At the highest zoom, the points are spiderified. This too can be turned off with the parameter spiderfyOnMaxZoom. Both of these features are turned on by default.\nI’m going to go back to the 55 data points set just to illustrate this functionality more clearly. (Click on a cluster. It will zoom and split the point as you keep clicking until you end up with just the points with the same coordinates.)\n\ncluster_map &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%\n  addScaleBar() %&gt;%\n  addCircleMarkers(\n    data = test,\n    label = ~ (paste(complete_name, cemetery_name, sep = \" \")), \n    clusterOptions = markerClusterOptions(),\n    )\n\ncluster_map\n\n\n\n\n\nIt does separate the points, but it is very hard to click on correct portion of the dot to get the label or pop-up. I think you need to hover on the part of the circle marker that does not overlap with either the cluster marker or the or the other circle markers. This can be solved by making the circle markers very large, so there is more non-overlapping area, but I find this to be ugly.\nLuckily, the option spiderfyDistanceMultiplier lets you tailor the distance. I set it to 2, which does put the points km apart (exact distance depends on the zoom). But since they are connected back to the original location with a line, I think it is clear where the points are actually located. When I had other people test the map, the larger displacement was much easier for them to operate the map.\n\ncluster_map_separated &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addScaleBar() %&gt;%\n  addCircleMarkers(\n    data = test,\n    label = ~ (paste(complete_name, cemetery_name, sep = \" \")), \n    clusterOptions = markerClusterOptions(spiderfyDistanceMultiplier = 2)\n  ) \n\ncluster_map_separated"
  },
  {
    "objectID": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#using-sf_jitter",
    "href": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#using-sf_jitter",
    "title": "Mapping for the Tombstone Project",
    "section": "Using sf_jitter",
    "text": "Using sf_jitter\nThe other option for handling this problem (and my original approach) is to jitter the points using st_jitter(). I think this approach wasn’t as bad as I originally thought. I was guessing which points should have been originally the same location based on cemetery name, but I think probably many of them were in the same cemetery, but with different coordinates to start with.\nI found the documentation for this package unclear. Things didn’t behave as I expected. I’m going to go through my work in detail, in case it helps someone else struggling with sf_jitter().\nThe function st_jitter() takes two parameters, amount and factor. Most examples I’ve seen just use factor, perhaps because everyone found amount as confusing as I did.\nHere are the definitions of the parameters.\n\namount\n\nnumeric; amount of jittering applied; if missing, the amount is set to factor * the bounding box diagonal; units of coordinates.\n\nfactor\n\nnumeric; fractional amount of jittering to be applied\n\n\nFirst, it isn’t that hard to find cases where the default value for amount doesn’t work. This can be really disconcerting when recycling working code to other dataframes, and suddenly things are not as expected.\nMy dataframe test with 3 points at the same location reports a bounding box, but xmin = xmax and ymin = ymax. (The bounding box can be extracted with st_bbox().)\nApplying st_jitter() to this dataframe doesn’t produce any sort of errors (and probably should, since then the diagonal of the bounding box is 0.) Instead, the original coordinates are returned with no error or message.\n\nst_bbox(test)\n\n     xmin      ymin      xmax      ymax \n-88.86500  37.98889 -88.86500  37.98889 \n\n\nAnd here’s the jittered dataframe. You can see it didn’t jitter it at all.\n\ntest_jittered_no_amt &lt;- test %&gt;%\n  st_jitter(factor = 1)\n\ntest_jittered_no_amt %&gt;% select(geometry)\n\nSimple feature collection with 3 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -88.865 ymin: 37.98889 xmax: -88.865 ymax: 37.98889\nGeodetic CRS:  WGS 84\n                  geometry\n1 POINT (-88.865 37.98889)\n2 POINT (-88.865 37.98889)\n3 POINT (-88.865 37.98889)\n\n\nPerhaps it is always better to specify amount, so the code will be more robust. This seems reasonable, but I cannot figure out what units amount is in! I think it should be in degrees. The documentation says it is in units of coordinates. The units of the the sf object can be extracted using st_crs().\n\nst_crs(test_for_jitter_inner, parameters = TRUE)$ud_unit\n\n1 [°]\n\n\nThen, I’d expect amount = 1 and factor = 1 to produce coordinates jittered by about a degree. (The documentation says “For longlat data, a latitude correction is made such that jittering in East and North directions are identical in distance in the center of the bounding box of x.”, so maybe not exactly 1 degree based on the correction.)\n\ntest_jittered &lt;- test %&gt;%\n  st_jitter(amount = 1,  factor = 1)\n\ntest_jittered %&gt;% select(geometry)\n\nSimple feature collection with 3 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -88.99865 ymin: 37.08968 xmax: -88.45622 ymax: 38.86458\nGeodetic CRS:  WGS 84\n                    geometry\n1 POINT (-88.99865 38.56987)\n2 POINT (-88.45622 37.08968)\n3  POINT (-88.5941 38.86458)\n\n\nMaybe it is the diagonal that is supposed to be 1? This can be calculated with the Pythagorean theorem.\n\n((-89.63298 - -89.32716)^2 + ( 37.39874 - 38.41781)^2)^0.5\n\n[1] 1.063969\n\n\nSo that it, and not at all what I understood from reading the description. The second point of confusion is that I thought that you could specify both factor and amount. (Clearly! As you can see from the example above.) But if you specify amount, then factor doesn’t get used.\nIn this case, I’d like the points to be jittered by a few meters or so. It is a bit of math to figure out what that is relative to the bounding box diagonal in degrees or some trial and error. On the plus side, the displacement of the dots is independent of the zoom of the map or any other map parameter since it is generated outside the mapping.\nSo here is the jittered map.\n\ntest_jittered &lt;- test %&gt;%\n  st_jitter(amount = 0.0001)\n\nmap_sf_jitter_all &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%\n  addScaleBar() %&gt;%\n  addCircleMarkers(\n    data = test_jittered,\n    label = ~ (paste(complete_name, cemetery_name, sep = \" \"))\n  ) \n\nmap_sf_jitter_all\n\n\n\n\n\nThere isn’t any indication that a dot represents multiple points at the more zoomed out scales. Additionally, the zooming experience is very unpleasant. With the clustered option, the map is zoomed and centered as you click through the clusters. There is also a very clear indication (the number in the cluster) that there are multiple data points in the dot. Here, more data points make a darker circle, but this isn’t crystal clear."
  },
  {
    "objectID": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#make-a-nice-label-formatted-with-html",
    "href": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#make-a-nice-label-formatted-with-html",
    "title": "Mapping for the Tombstone Project",
    "section": "Make a nice label formatted with HTML",
    "text": "Make a nice label formatted with HTML\nI’d like a pop-up with biographical information along with the picture, but there is a lot of missing data, so I need to construct the info box carefully or it will be full of NAs.\n\ntombstones_geo %&gt;% select(DOB, DOD)\n\nSimple feature collection with 194 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -89.2775 ymin: 35.97833 xmax: -73.48583 ymax: 44.75056\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           DOB          DOD                   geometry\n1  23 Sep 1877  24 Oct 1899 POINT (-88.69556 38.10361)\n2  1 July 1807   2 Feb 1888  POINT (-75.7125 40.87778)\n3  17 Apr 1889  19 Nov 1960 POINT (-88.66833 38.06861)\n4   5 Aug 1894  1 June 1960    POINT (-88.94 38.19167)\n5         1854         1949 POINT (-89.14694 37.60917)\n6         1856         1906 POINT (-89.14694 37.60917)\n7  27 Feb 1864   3 Dec 1942  POINT (-89.1475 37.60833)\n8  29 Sep 1748 19 June 1817 POINT (-75.65028 40.78278)\n9  15 Feb 1746  3 June 1823  POINT (-76.0425 41.22528)\n10        &lt;NA&gt;         &lt;NA&gt;  POINT (-89.00722 38.1875)\n\ntombstones_geo &lt;- tombstones_geo %&gt;%\n  mutate(dob_label = ifelse(is.na(DOB) == TRUE, \"\", paste0(\"Born: \", DOB)))\n\ntombstones_geo &lt;- tombstones_geo %&gt;%\n  mutate(dod_label = ifelse(is.na(DOD) == TRUE, \"\", paste0(\"Died: \", DOD)))\n\nPlaying with the map, I see that two entries have the state in both the city and the state column, so I’m going to fix that.\n\ntombstones_geo %&gt;% filter(Surname == \"Brown\") %&gt;% select(City, State)\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.7125 ymin: 40.87778 xmax: -75.7125 ymax: 40.87778\nGeodetic CRS:  WGS 84\n                   City State                  geometry\n1 Egypt, Lehigh Co., PA    PA POINT (-75.7125 40.87778)\n2 Egypt, Lehigh Co., PA    PA POINT (-75.7125 40.87778)\n\ntombstones_geo &lt;-\n  tombstones_geo %&gt;%\n  mutate(City = ifelse(City == \"Egypt, Lehigh Co., PA\", \"Egypt, Lehigh Co.\", City)\n)"
  },
  {
    "objectID": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#cleaning-the-city-field",
    "href": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#cleaning-the-city-field",
    "title": "Mapping for the Tombstone Project",
    "section": "Cleaning the City Field",
    "text": "Cleaning the City Field\nThe formatting of the city column is pretty inconsistent. I’m going to clean it up also.\n\ntombstones_geo &lt;-\n  tombstones_geo %&gt;%\n  mutate(City = str_replace_all(City, \"Co\\\\.\", \"County\")) %&gt;%\n  mutate(City = str_replace_all(City, \"Co$\", \"County\"))         \n\nIn leaflet, it seems that labels require that the html be generated with htmltools::HTML, while popups understand html tags already and can just be passed something like paste(\"&lt;strong&gt;\", complete_name, \"&lt;/strong&gt;\"). Since I was playing around with what info was displayed in label vs. popup, I just rendered everything with htmltools::HTML so I could switch things around without having to modify the text. There is a nice demonstration of the difference between labels and pop-ups on the Dr.Data.King blog.\n\ntombstones_geo &lt;- tombstones_geo %&gt;%\n  mutate(\n    boxinfo = paste0(\n      \"&lt;strong&gt;\",\n      complete_name ,\n      \"&lt;/strong&gt;\",\n      \"&lt;br/&gt;&lt;hr&gt;\",\n      dob_label,\n      \" \",\n      dod_label,\n      \"&lt;br/&gt;&lt;hr&gt;\",\n      cemetery_name ,\n      \" in \",\n      City,\n      \" , \",\n      State,\n      \"&lt;br/&gt;\"\n    ) %&gt;%\n      lapply(htmltools::HTML)\n  )"
  },
  {
    "objectID": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#final-map",
    "href": "posts/2023-08-14-mapping-tombstone/tombstone_mapping_leaflet.html#final-map",
    "title": "Mapping for the Tombstone Project",
    "section": "Final Map",
    "text": "Final Map\nI’m adding a specific provider tile rather than the default. The label for the point appears when you hover over it and has name and location. There is a pop-up generated in the addCircleMarkers with the biographical info. When addPopupImages() is called, it just appends the photos to that text. The marker pop-up and the pop-up with the picture need to be linked by group.\nI couldn’t find a way to directly add text to the addPopupImages() call.\nI don’t like the outline around the markers, so I turned it off with stroke = NA and I made the radius = 10.\nNext, I call leafpop to add the popup with the images. Documentation for leafpop can be found at CRAN and on the leafpop website. You can add tables, charts, graphs, and images with leafpop. While leafpop can be called within addCircleMarker via popup = popupImage() it will not embed the image within the map. If you intend to save it, then you need to use the separate call to addPopupImages(). Unlike leaflet, the package leafpop does not use the formula notation and requires a “character vector of file path(s) or web-URL(s) to any sort of image file(s).” This is extracted from my dataframe using image_list &lt;- tombstones_geo$photo_list. (Here I use the suffix _list to mean list in the plain English sense of the word, not a list type object.)\nI found that specifying the width and max width of the popup was critical. If it were left to the defaults or “too large” then I just got the broken picture icon.\nThere is also a really weird issue that I can’t figure out that I want to highlight. Sometimes, when I run my quarto document in R, the leaflet map displays with broken pic icons. BUT, rendering the quarto doc does create working pop-ups with the right pics. Other times, both running and rendering produces a working map with pic popups. And even if the map is broken when I “run” instead of “render”, it saves a perfectly working map. I can’t reliably produce either state, and I don’t have any warnings. I couldn’t find anything about this when I googled, so if your map doesn’t work when you are working in quarto (or probably R markdown) try rendering it or saving it.\n\nimage_list &lt;- tombstones_geo$photo_list\n\nfinal_map &lt;- leaflet() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addScaleBar() %&gt;%\n  addCircleMarkers(\n    data = tombstones_geo,\n    label = ~ lapply(\n      paste(\"&lt;strong&gt;\", complete_name, \"&lt;/strong&gt;\"),\n      htmltools::HTML\n    ),\n    popup = ~ boxinfo,\n    clusterOptions = markerClusterOptions(\n      spiderfyDistanceMultiplier = 2,\n      maxClusterRadius = 50\n    ),\n    radius = 10,\n    stroke = NA,\n    group = \"group1\"\n  ) %&gt;%\n  leafpop::addPopupImages(\n    image = paste0(here(blog_folder, photo_folder), \"/\", image_list),\n    src = local,\n    group = \"group1\",\n    width = 400,\n    maxHeight = 300,\n    maxWidth = 400\n  )\n\n\nfinal_map\n\n\n\n\n\nThe map can be saved with saveWidget() from htmlwidgets.\n\n# fancy interactive map to upload\nsaveWidget(final_map, file = \"map_to_upload.html\")"
  },
  {
    "objectID": "posts/2023-08-01-tidytuesday-US-states/states.html",
    "href": "posts/2023-08-01-tidytuesday-US-states/states.html",
    "title": "States",
    "section": "",
    "text": "Today’s TidyTuesday takes some data about US states from Wikipedia.\nLast week I saw this post on Mastodon and was really intrigued by the package mentioned, GWalkr. I thought this package looked very cool, so I put it on my to try list.\n\nEmbedding Social Media Posts in Quarto\nAs a side note, I used the Quarto social embeds extension to show the Mastodon toot. I use RStudio to compose my quarto documents, so to install it for this project, I run the code given at the terminal (not console) window in RStudio. I did see that the toot overflowed the text below it, so you may need to adjust the spacing with some hard returns. The specific code to render the toot is found here.\nWhile it worked(ish) I discovered that it overflowed the following text, even with lots of space and hard returns entered after.\nI also found that the actual link to the toot kept disappearing and defaulting back to the server name only (e.g. https://fosstodon.org/) which then refused the connection.\nTo uninstall a quarto extension, use quarto remove extension from the terminal and select the one you want from the list or use quarto remove extension - NAME_OF_EXTENSION.\nSomething also broke the formatting of this post. Headers, spacing, and justification are all different. Removing the extension did not restore this, so I will dig more later into this. (I suppose it could be the vis config for GWalkR, but removing that also did not fix the problem.)\n\n\nInstalling GWalkR\nThe GWalkR package isn’t available on CRAN yet, so you’ll need to install it via the devtools package using the following code: devtools::install_url(\"https://kanaries-app.s3.ap-northeast-1.amazonaws.com/oss/gwalkr/GWalkR_latest.tar.gz\")\n\nlibrary(tidyverse)\nlibrary(GWalkR)\n\nLoading the tidytuesday data.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 31)\n\nstates &lt;- tuesdata$states\nstate_name_etymology &lt;- tuesdata$state_name_etymology\n\nGWalkR is a interactive EDA tool styled similarly to Tableau. It is a lighter weight version of Graphics Walker and is designed to work in R. To use it, you just call it on the dataframe.\nI’ve created 3 simple visualizations that I will walk you through. You can interact with any of them and change the data as you wish. This is designed to be an exploratory interactive tool. It isn’t the best option to create static, polished figures.\nAnother side note, if you are doing this in quarto, you may wish to use the #| column: page option to give the gwalkr panel more room.\n\ngwalkr(states)\n\n\n\n\n\n\n\nMaking your Viz “Stick”\nThis part is only necessary if you are publishing or sharing your visualizations. When you run gwalkr you first get the empty explorer/builder window. Once you have created the visualizations, click on the export configuration button (like &lt;&gt;) and copy the text into a code block. This initializes GWalkr to the way you created it. Without this, you end up with an empty viewer every time you run the code. I’m including the entire configuration here, since this is a semi-tutorial format, but you’d probably want to hide this in your markdown/ quarto document. This code chunk also works find when I am rendering my Quarto document, but fails when I am executing code normally.\n\nvisConfig &lt;- '[{\"visId\":\"gw_Gtm3\",\"name\":\"Bar Chart of State Pop.\",\"encodings\":{\"dimensions\":[{\"dragId\":\"gw_jx5q\",\"fid\":\"c3RhdGU=\",\"name\":\"state\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_3nGt\",\"fid\":\"cG9zdGFsX2FiYnJldmlhdGlvbg==\",\"name\":\"postal_abbreviation\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_FNnC\",\"fid\":\"Y2FwaXRhbF9jaXR5\",\"name\":\"capital_city\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_-YgP\",\"fid\":\"bGFyZ2VzdF9jaXR5\",\"name\":\"largest_city\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_QC9G\",\"fid\":\"YWRtaXNzaW9u\",\"name\":\"admission\",\"semanticType\":\"temporal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_hm9z\",\"fid\":\"ZGVtb255bQ==\",\"name\":\"demonym\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"}],\"measures\":[{\"dragId\":\"gw_FnuC\",\"fid\":\"cG9wdWxhdGlvbl8yMDIw\",\"name\":\"population_2020\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_hRWB\",\"fid\":\"dG90YWxfYXJlYV9taTI=\",\"name\":\"total_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_cUQK\",\"fid\":\"dG90YWxfYXJlYV9rbTI=\",\"name\":\"total_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_QH84\",\"fid\":\"bGFuZF9hcmVhX21pMg==\",\"name\":\"land_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_tzCn\",\"fid\":\"bGFuZF9hcmVhX2ttMg==\",\"name\":\"land_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_1ggm\",\"fid\":\"d2F0ZXJfYXJlYV9taTI=\",\"name\":\"water_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_0lnU\",\"fid\":\"d2F0ZXJfYXJlYV9rbTI=\",\"name\":\"water_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_eiwY\",\"fid\":\"bl9yZXByZXNlbnRhdGl2ZXM=\",\"name\":\"n_representatives\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_count_fid\",\"fid\":\"gw_count_fid\",\"name\":\"Row count\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\",\"computed\":true,\"expression\":{\"op\":\"one\",\"params\":[],\"as\":\"gw_count_fid\"}}],\"rows\":[{\"dragId\":\"gw_Tlb_\",\"fid\":\"c3RhdGU=\",\"name\":\"state\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"}],\"columns\":[{\"dragId\":\"gw_tHvs\",\"fid\":\"cG9wdWxhdGlvbl8yMDIw\",\"name\":\"population_2020\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"}],\"color\":[],\"opacity\":[],\"size\":[],\"shape\":[],\"radius\":[],\"theta\":[],\"details\":[],\"filters\":[],\"text\":[]},\"config\":{\"defaultAggregated\":true,\"geoms\":[\"auto\"],\"stack\":\"stack\",\"showActions\":false,\"interactiveScale\":false,\"sorted\":\"none\",\"zeroScale\":true,\"size\":{\"mode\":\"auto\",\"width\":320,\"height\":200},\"format\":{}}},{\"visId\":\"gw_uHPW\",\"name\":\"Area vs. Pop (Agg)\",\"encodings\":{\"dimensions\":[{\"dragId\":\"gw_fHm7\",\"fid\":\"c3RhdGU=\",\"name\":\"state\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_GIbq\",\"fid\":\"cG9zdGFsX2FiYnJldmlhdGlvbg==\",\"name\":\"postal_abbreviation\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_j9gr\",\"fid\":\"Y2FwaXRhbF9jaXR5\",\"name\":\"capital_city\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_UlJ3\",\"fid\":\"bGFyZ2VzdF9jaXR5\",\"name\":\"largest_city\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_7t-q\",\"fid\":\"YWRtaXNzaW9u\",\"name\":\"admission\",\"semanticType\":\"temporal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw__0zS\",\"fid\":\"ZGVtb255bQ==\",\"name\":\"demonym\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"}],\"measures\":[{\"dragId\":\"gw_5ToL\",\"fid\":\"cG9wdWxhdGlvbl8yMDIw\",\"name\":\"population_2020\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_hIf6\",\"fid\":\"dG90YWxfYXJlYV9taTI=\",\"name\":\"total_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_MxWn\",\"fid\":\"dG90YWxfYXJlYV9rbTI=\",\"name\":\"total_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_YbvT\",\"fid\":\"bGFuZF9hcmVhX21pMg==\",\"name\":\"land_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_nnL-\",\"fid\":\"bGFuZF9hcmVhX2ttMg==\",\"name\":\"land_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_zJz3\",\"fid\":\"d2F0ZXJfYXJlYV9taTI=\",\"name\":\"water_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_67yP\",\"fid\":\"d2F0ZXJfYXJlYV9rbTI=\",\"name\":\"water_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_ta24\",\"fid\":\"bl9yZXByZXNlbnRhdGl2ZXM=\",\"name\":\"n_representatives\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_count_fid\",\"fid\":\"gw_count_fid\",\"name\":\"Row count\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\",\"computed\":true,\"expression\":{\"op\":\"one\",\"params\":[],\"as\":\"gw_count_fid\"}}],\"rows\":[{\"dragId\":\"gw_YKoG\",\"fid\":\"dG90YWxfYXJlYV9taTI=\",\"name\":\"total_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"}],\"columns\":[{\"dragId\":\"gw_3HaQ\",\"fid\":\"cG9wdWxhdGlvbl8yMDIw\",\"name\":\"population_2020\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"}],\"color\":[],\"opacity\":[],\"size\":[],\"shape\":[],\"radius\":[],\"theta\":[],\"details\":[],\"filters\":[],\"text\":[]},\"config\":{\"defaultAggregated\":true,\"geoms\":[\"auto\"],\"stack\":\"stack\",\"showActions\":false,\"interactiveScale\":false,\"sorted\":\"none\",\"zeroScale\":true,\"size\":{\"mode\":\"auto\",\"width\":320,\"height\":200},\"format\":{}}},{\"visId\":\"gw_RuCs\",\"name\":\"Area vs. Pop (dis-agg)\",\"encodings\":{\"dimensions\":[{\"dragId\":\"gw_f-aN\",\"fid\":\"c3RhdGU=\",\"name\":\"state\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_T0HW\",\"fid\":\"cG9zdGFsX2FiYnJldmlhdGlvbg==\",\"name\":\"postal_abbreviation\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_Uga-\",\"fid\":\"Y2FwaXRhbF9jaXR5\",\"name\":\"capital_city\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_CZOt\",\"fid\":\"bGFyZ2VzdF9jaXR5\",\"name\":\"largest_city\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_NhIi\",\"fid\":\"YWRtaXNzaW9u\",\"name\":\"admission\",\"semanticType\":\"temporal\",\"analyticType\":\"dimension\"},{\"dragId\":\"gw_kD6Y\",\"fid\":\"ZGVtb255bQ==\",\"name\":\"demonym\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"}],\"measures\":[{\"dragId\":\"gw_80B1\",\"fid\":\"cG9wdWxhdGlvbl8yMDIw\",\"name\":\"population_2020\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_oncj\",\"fid\":\"dG90YWxfYXJlYV9taTI=\",\"name\":\"total_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_p-Ch\",\"fid\":\"dG90YWxfYXJlYV9rbTI=\",\"name\":\"total_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_MtXt\",\"fid\":\"bGFuZF9hcmVhX21pMg==\",\"name\":\"land_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_fGM4\",\"fid\":\"bGFuZF9hcmVhX2ttMg==\",\"name\":\"land_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_s6UR\",\"fid\":\"d2F0ZXJfYXJlYV9taTI=\",\"name\":\"water_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_D1e8\",\"fid\":\"d2F0ZXJfYXJlYV9rbTI=\",\"name\":\"water_area_km2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_4hWE\",\"fid\":\"bl9yZXByZXNlbnRhdGl2ZXM=\",\"name\":\"n_representatives\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"},{\"dragId\":\"gw_count_fid\",\"fid\":\"gw_count_fid\",\"name\":\"Row count\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\",\"computed\":true,\"expression\":{\"op\":\"one\",\"params\":[],\"as\":\"gw_count_fid\"}}],\"rows\":[{\"dragId\":\"gw_g_M-\",\"fid\":\"dG90YWxfYXJlYV9taTI=\",\"name\":\"total_area_mi2\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"}],\"columns\":[{\"dragId\":\"gw_A9lF\",\"fid\":\"cG9wdWxhdGlvbl8yMDIw\",\"name\":\"population_2020\",\"analyticType\":\"measure\",\"semanticType\":\"quantitative\",\"aggName\":\"sum\"}],\"color\":[],\"opacity\":[],\"size\":[],\"shape\":[],\"radius\":[],\"theta\":[],\"details\":[{\"dragId\":\"gw_Fv7k\",\"fid\":\"c3RhdGU=\",\"name\":\"state\",\"semanticType\":\"nominal\",\"analyticType\":\"dimension\"}],\"filters\":[],\"text\":[]},\"config\":{\"defaultAggregated\":true,\"geoms\":[\"auto\"],\"stack\":\"stack\",\"showActions\":false,\"interactiveScale\":false,\"sorted\":\"none\",\"zeroScale\":true,\"size\":{\"mode\":\"auto\",\"width\":320,\"height\":200},\"format\":{}}}]'\ngwalkr(data=states, visConfig=visConfig)\n\n\n\n\n\n\n\nBar Chart of State Population\nGWalkr produces a Tableau style interface. To graph something, just drag variables to X-axis and Y-axis. I did population (x) and state (y). The default setting is to autogenerate the best possible visualization, so you end up with the bar chart. You can transpose the axes using the circular arrows button at the top of the screen- probably not the best choice for this dataset though.\nIt doesn’t look like you can join data in GWalkR yet, so I’m going to do that now in R.\n\nstates_joined &lt;- states %&gt;% left_join(state_name_etymology)\n\nJoining with `by = join_by(state)`\n\n\nNow explore by starting gwalkr the same way.\n\ngwalkr(states_joined)\n\n\n\n\n\nNotice that it starts up with the visualizations I created before. This is because of the config code. It applies to all instances of GWalkr. It is a bit odd since this uses a different dataframe, but in this case it doesn’t matter.\n\n\nAggregated and Dis-Aggregated Data\nThe second and third visualization explore the idea of aggregated and dis-aggregated data. Visualize total_area vs. population- you get the very Tableau result of a single point. Tableau and similar BI platforms almost always summarize/ group/ aggregate the data automatically. You can see that in the GWalkR interface too- the X-axis and Y-Axis now have sum next to the variables. To get the scatter plot you might be expecting, drag states to “Details”, which is found in the column between the variables and the visualization. Providing a unique identifier disaggregates the data. If you hove your mouse over a point, it tells you the state, the population and the area associated with the point.\n\n\nConclusions\nI think some people think in a way where this type of interactive exploratory data analysis really works with their workflow. It is certainly faster to drag and drop a bunch of different combos to see what visualizations are most informative, especially compared to creating 15 ggplots and then deleting most of them. I also think this type of tool will probably frustrate other folks.\n\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {States},\n  date = {2023-08-01},\n  url = {https://lsinks.github.io/posts/2023-08-01-tidytuesday-US-States/states},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “States.” August 1, 2023. https://lsinks.github.io/posts/2023-08-01-tidytuesday-US-States/states."
  },
  {
    "objectID": "posts/2023-07-04-tidytuesday-historic-markers/markers.html",
    "href": "posts/2023-07-04-tidytuesday-historic-markers/markers.html",
    "title": "TidyTuesday Week 27: Historical Markers",
    "section": "",
    "text": "Today’s TidyTuesday is about historical markers with the data coming from the Historical Marker Database. I’m going to add to the map that I made last week with information about Historic Districts in Arlington, VA. I’m going to make an interactive leaflet map with the new information added to the old map.\nLoading libraries.\n\nlibrary(tidyverse) # who doesn't want to be tidy\nlibrary(leaflet) # interactive mapping\nlibrary(mapview) # simple interactive mapping\nlibrary(sf) # geocoding objects\nlibrary(openxlsx) # importing excel files from a URL\n\nI’m not going to loading the no markers data, because I know I’m not going to use it.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 27)\n\n--- Compiling #TidyTuesday Information for 2023-07-04 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `historical_markers.csv`\n    Downloading file 2 of 2: `no_markers.csv`\n\n\n--- Download complete ---\n\nhistorical_markers &lt;- tuesdata$`historical_markers`\n#no_markers &lt;- tuesdata$`no_markers`\n\nThe data isn’t very clean. The website might want to consider drop-down menus for some of the bigger groups. Here’s an illustration look at some of many ways people rendered “Kentucky Historical Society and Kentucky Department of Highways”.\n\nhistorical_markers %&gt;% filter(state_or_prov == \"Kentucky\") %&gt;% \n  group_by(erected_by) %&gt;% count(sort = TRUE) %&gt;% filter(n &gt; 50)\n\n# A tibble: 10 × 2\n# Groups:   erected_by [10]\n   erected_by                                                                n\n   &lt;chr&gt;                                                                 &lt;int&gt;\n 1 Kentucky Historical Society and Kentucky Department of Highways         634\n 2 Kentucky Historical Society, Kentucky Department of Highways            165\n 3 Kentucky Historical Society & Kentucky Department of Highways           159\n 4 Kentucky Historical Society, Kentucky Department of Highways.           141\n 5 the Kentucky Historical Society, Kentucky Department of Highways.        81\n 6 Kentucky Historical Society-Kentucky Department of Highways              79\n 7 Kentucky Department of Highways                                          75\n 8 Kentucky Historical Society • Kentucky Department of Highways            65\n 9 Kentucky Historical Society and Kentucky Department of Transportation    57\n10 James Harrod Trust                                                       53\n\n\nFiltering for Virginia only. I could filter by county == \"Arlington County\" also, but I actually want to get some of the adjacent markers, because I know there are some right on the county line and I’m not sure which jurisdiction they will fall in.\n\nvirginia_markers &lt;- historical_markers %&gt;% filter(state_or_prov == \"Virginia\") \n\nI’m going to load in my data from the previous visualization. The blog post on how I created these objects is here.\n\nhistoric_4269 &lt;- st_read(\"points.shp\")\narlington_polygons_sf &lt;- st_read(\"polygons.shp\")\n\nNow I’m adding html tags and transforming the coordinate system. More about that here.\n\n# turn the url to HTML anchor tag\nhistoric_4269 &lt;- historic_4269 %&gt;% \n  mutate(tag = paste0(\"More Info: &lt;a href=\", Extrn_L,\"&gt;\", Extrn_L, \"&lt;/a&gt;\"))\n\n#transforming crs\nhistoric_4326 &lt;- sf::st_transform(historic_4269, crs = 4326)\narlington_polygons_sf_4326 &lt;- sf::st_transform(arlington_polygons_sf, crs = 4326) \n\nNow I’m roughly sub-setting to Arlington based on latitude and longitude.\n\nva_markers_nova &lt;-\n  virginia_markers %&gt;% filter(longitude_minus_w &lt; -76.5 &\n                                longitude_minus_w &gt; -77.25) %&gt;%\n  filter(latitude_minus_s &gt; 38.8 &\n           latitude_minus_s &lt; 39.4)\n\nI know there is a lot of variation in the erected_by data (as seen for KY), so I’m going to check that out for this sub-set.\n\nva_markers_nova %&gt;% group_by(erected_by) %&gt;% count(sort = TRUE)\n\n# A tibble: 13 × 2\n# Groups:   erected_by [13]\n   erected_by                                                                  n\n   &lt;chr&gt;                                                                   &lt;int&gt;\n 1 Department of Historic Resources                                           34\n 2 Arlington County, Virginia                                                 22\n 3 Arlington County Virginia                                                   4\n 4 Arlington County                                                            3\n 5 Conservation & Development Commission                                       3\n 6 Virginia Historic Landmarks Commission                                      3\n 7 City of Alexandria                                                          1\n 8 Continental Chapter, Daughters of the American Revolution                   1\n 9 The Washington Society of Alexandria, U.S. Dept. of the Interior            1\n10 Virginia Conservation Commission                                            1\n11 Virginia Department of Historic Resources                                   1\n12 Washington-Lee Society, Children of the American Revolution; Thomas Ne…     1\n13 William G. Pomeroy Foundation                                               1\n\n\nChange all the Arlington stuff to Arlington County. The Conservation & Development Comission and the Virginia Conservation Commission are the same entity- the name changed over the years. I’m adding the years to those entries. I suspect they and the Virginia Landmarks Commission are all now replaced by the Virginia Department of Historic Resources, but I couldn’t find a source for that.\n\nva_markers_nova &lt;- va_markers_nova %&gt;%\n  mutate(erected_by_clean = ifelse(\n    str_detect(erected_by, \"Arlington County\"),\n    \"Arlington County\",\n    erected_by\n  )) %&gt;%\n  mutate(\n    erected_by_clean = ifelse(\n      str_detect(erected_by_clean, \"Historic Resources\"),\n      \"Virginia Dept. of Historic Resources\",\n      erected_by_clean\n    )\n  ) %&gt;%\n  mutate(\n    erected_by_clean = ifelse(\n      str_detect(erected_by_clean, \"Conservation &\"),\n      \"Virginia Conservation & Development Commission (1926- 1938)\",\n      erected_by_clean\n    )) %&gt;%\n  mutate(\n    erected_by_clean = ifelse(\n      str_detect(erected_by_clean, \"Virginia Conservation Commission\"),\n      \"Virginia Conservation Commission (1938-1948)\",\n      erected_by_clean\n    ))\n\nChecking our cleaned list.\n\nva_markers_nova %&gt;% group_by(erected_by_clean) %&gt;% count(sort = TRUE)\n\n# A tibble: 10 × 2\n# Groups:   erected_by_clean [10]\n   erected_by_clean                                                            n\n   &lt;chr&gt;                                                                   &lt;int&gt;\n 1 Virginia Dept. of Historic Resources                                       35\n 2 Arlington County                                                           29\n 3 Virginia Conservation & Development Commission (1926- 1938)                 3\n 4 Virginia Historic Landmarks Commission                                      3\n 5 City of Alexandria                                                          1\n 6 Continental Chapter, Daughters of the American Revolution                   1\n 7 The Washington Society of Alexandria, U.S. Dept. of the Interior            1\n 8 Virginia Conservation Commission (1938-1948)                                1\n 9 Washington-Lee Society, Children of the American Revolution; Thomas Ne…     1\n10 William G. Pomeroy Foundation                                               1\n\n\nConverting this to a sf object. For more information about that, see my last week’s TidyTuesday. Just take a quick look to make sure I’m happy. The mapview package is great for quick and dirty maps; I’ll use leaflet to make the fancy one.\n\nva_markers_nova_geo &lt;- st_as_sf(va_markers_nova, coords = c(9, 8), crs = 4326)\n\nmapview(va_markers_nova_geo) + mapview(historic_4326) + mapview(arlington_polygons_sf_4326)\n\n\n\n\n\n\nMaking the HTML anchor tag for my pop-up with a live link. This is covered in my leaflet revision to last week’s TidyTuesday.\n\n# turn the url to HTML anchor tag\nva_markers_nova_geo &lt;- va_markers_nova_geo %&gt;% \n  mutate(tag = paste0(\"More Info: &lt;a href=\", link,\"&gt;\", link, \"&lt;/a&gt;\"))\n\nA first view of the map.\n\npal &lt;- colorFactor(palette = \"viridis\", domain = arlington_polygons_sf_4326$CIVIC)\n\nleaflet_map &lt;- leaflet() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(\n    data = arlington_polygons_sf_4326,\n    weight = 1,\n    label = ~ CIVIC,\n    color = ~ pal(CIVIC), \n    group = \"Arlington Neighborhoods\"\n  ) %&gt;%\n  addCircleMarkers(\n    data = historic_4326,\n    popup = ~ paste0(\"&lt;b&gt;\", Prprt_N, \"&lt;/b&gt;\", \"&lt;br&gt;\", tag),\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"black\",\n    stroke = NA,\n    group = \"Historic Neighborhoods\"\n  ) %&gt;% \n    addCircleMarkers(\n    data = va_markers_nova_geo,\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"red\",\n    stroke = NA,\n    group = \"Historic Markers\")\n  \nleaflet_map\n\n\n\n\n\nNow I’m going to subset the point data into data that is within the Arlington polygons and that which is outside the boundaries. I found a nice explanation on the GIS Stack Exchange on how to use st_intersects() for this.\nBasically, I find the interections between the two geometry datasets. If there isn’t one, the length of the list will be 0. Using sapply, we can create a TRUE/FALSE vector when we test against the condition of length == 0.\n\nintersection &lt;-\n  st_intersects(va_markers_nova_geo, arlington_polygons_sf_4326)\nb &lt;-\n  sapply(st_intersects(va_markers_nova_geo, arlington_polygons_sf_4326), function(x) {\n    length(x) == 0\n  })\n\nSo, now I subset the original marker data into Arlington and not Arlington.\n\nmarkers_arlington &lt;- va_markers_nova_geo[!b, ]\nmarkers_not_arlington &lt;- va_markers_nova_geo[b, ]\n\nAnd then map this. I put in a layer control so you can remove layers.\n\nleaflet_map &lt;- leaflet() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(\n    data = arlington_polygons_sf_4326,\n    weight = 1,\n    label = ~ CIVIC,\n    color = ~ pal(CIVIC),\n    group = \"Arlington Neighborhoods\"\n  ) %&gt;%\n  addCircleMarkers(\n    data = historic_4326,\n    label = ~ Prprt_N,\n    popup = ~ paste0(\"&lt;b&gt;\", Prprt_N, \"&lt;/b&gt;\", \"&lt;br&gt;\", tag),\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"black\",\n    group = \"Historic Neighborhoods\",\n    stroke = NA\n  ) %&gt;%\n  addCircleMarkers(\n    data = markers_arlington,\n    label = ~ title,\n    popup = ~ paste0(\"&lt;b&gt;\", title, \"&lt;/b&gt;\", \"&lt;br&gt;\", tag),\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"red\",\n    stroke = NA,\n    group = \"Historic Markers in Arlington\"\n  ) %&gt;%\n  addCircleMarkers(\n    data = markers_not_arlington,\n    label = ~ title,\n    popup = ~ paste0(\"&lt;b&gt;\", title, \"&lt;/b&gt;\", \"&lt;br&gt;\", tag),\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"blue\",\n    stroke = NA,\n    group = \"Historic Markers not in Arlington\"\n  ) %&gt;%\n  addLayersControl(\n    #baseGroups = c(\"CartoDB.Positron\"),\n    overlayGroups = c(\n      \"Arlington Neighborhoods\",\n      \"Historic Neighborhoods\",\n      \"Historic Markers\",\n      \"Historic Markers not in Arlington\"\n    ),\n    options = layersControlOptions(collapsed = FALSE)\n  )\nleaflet_map\n\n\n\n\n\nSeveral markers are on the county boundary and some do fall outside the county, such as those at Washington Reagan Airport.\nNone of Arlington’s historic neighborhood districts show up. According to the markers page, “A second exception in these guidelines are National Register of Historic Places and other officially sanctioned brass tablets that simply name the historic building or site. They can only anchor a marker page when there is no other qualifying marker nearby. When there is one, it should be used to further illustrate that other marker’s page.” So, as I understand it, there should be entries.\nMany of the Historic Marker’s are related to the defense of Washington, DC during the American Civil War. I made a list manually from Wikipedia, and added a column labeling these “civil war defense”.\n\nfort &lt;- read.xlsx(here::here(\"posts/2023-07-04-tidytuesday-historic-markers\", \"forts2.xlsx\"), sheet = 1)\n\nI’m going to use a join to determine if a marker is a civil war defense fort. If it matches, there will be an entry in the column Defense_Label that says “Civil War Defense”. If there isn’t a match, there will be an NA. Using a left join with markers_arlington keeps all of the entries.\n\nmarkers_arlington &lt;-\n  markers_arlington %&gt;% left_join(fort, by = c(\"title\" = \"Fort_Name\"))\n\nNow making a TRUE/FALSE column to subset on. If the Defense_Label is NA, then we know it does not match with a fort, so I set the label to FALSE. Otherwise, TRUE.\n\nCivil_war &lt;- markers_arlington %&gt;%\n  mutate(Defense_Label = ifelse(is.na(Defense_Label), FALSE, TRUE))\n\nNow create the two groups by using this TRUE/FALSE column to index on.\n\nmarkers_arlington_cw &lt;- Civil_war[Civil_war$Defense_Label , ]\nmarkers_arlington_other &lt;- Civil_war[!Civil_war$Defense_Label , ]\n\nNow add these new groups to the map and remove the old historic markers group.\n\nleaflet_map &lt;- leaflet() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(\n    data = arlington_polygons_sf_4326,\n    weight = 1,\n    label = ~ CIVIC,\n    color = ~ pal(CIVIC),\n    group = \"Arlington Neighborhoods\"\n  ) %&gt;%\n  addCircleMarkers(\n    data = historic_4326,\n    label = ~ Prprt_N,\n    popup = ~ paste0(\"&lt;b&gt;\", Prprt_N, \"&lt;/b&gt;\", \"&lt;br&gt;\", tag),\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"black\",\n    group = \"Arlington Historic Neighborhoods\",\n    stroke = NA\n  ) %&gt;%\n  addCircleMarkers(\n    data = markers_arlington_cw,\n    label = ~ title,\n    popup = ~ paste0(\"&lt;b&gt;\", title, \"&lt;/b&gt;\", \"&lt;br&gt;\", tag),\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"red\",\n    stroke = NA,\n    group = \"Civil War Federal Defense Historic Markers\"\n  ) %&gt;%\n  \n  addCircleMarkers(\n    data = markers_arlington_other,\n    label = ~ title,\n    popup = ~ paste0(\"&lt;b&gt;\", title, \"&lt;/b&gt;\", \"&lt;br&gt;\", tag),\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"green\",\n    stroke = NA,\n    group = \"Arlington Historic Markers\"\n  ) %&gt;%\n  addCircleMarkers(\n    data = markers_not_arlington,\n    label = ~ title,\n    popup = ~ paste0(\"&lt;b&gt;\", title, \"&lt;/b&gt;\", \"&lt;br&gt;\", tag),\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"blue\",\n    stroke = NA,\n    group = \"Historic Markers not in Arlington\"\n  ) %&gt;%\n  addLayersControl(\n    #baseGroups = c(\"CartoDB.Positron\"),\n    overlayGroups = c(\n      \"Arlington Neighborhoods\",\n      \"Arlington Historic Neighborhoods\",\n      \"Arlington Historic Markers\",\n      \"Civil War Federal Defense Historic Markers\" ,\n      \"Historic Markers not in Arlington\"\n    ),\n    options = layersControlOptions(collapsed = FALSE)\n  )\nleaflet_map\n\n\n\n\n\nAnd there we have it. A lovely map with different types of local historic features.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {TidyTuesday {Week} 27: {Historical} {Markers}},\n  date = {2023-07-04},\n  url = {https://lsinks.github.io/posts/2023-07-04-historic-markers/markers},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “TidyTuesday Week 27: Historical\nMarkers.” July 4, 2023. https://lsinks.github.io/posts/2023-07-04-historic-markers/markers."
  },
  {
    "objectID": "posts/2023-06-27-tidytuesday-US-populated-places/arlington-neighborhoods.html",
    "href": "posts/2023-06-27-tidytuesday-US-populated-places/arlington-neighborhoods.html",
    "title": "Tidy Tuesday: US Populated Places",
    "section": "",
    "text": "Today’s TidyTuesday is about place names as recorded by the US Board on Geographic Names. The dataset has been cleaned to include only populated places.\nThis week will involve more libraries than normal, since I am going to play with mapping.\n\nlibrary(tidyverse) # who doesn't want to be tidy?\nlibrary(ggthemes) # more themes for ggplot\nlibrary(gt) # For nice tables\nlibrary(ggrepel) # to help position labels in ggplot graphs\nlibrary(openxlsx) # importing excel files from a URL\nlibrary(fuzzyjoin) # for joining on inexact matches\nlibrary(sf) # for handling geo data\nlibrary(mapview) # quick interactive mapping\nlibrary(leaflet) # more mapping\n\nLoad dataset in the usual way.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 26) \nus_place_names &lt;- tuesdata$`us_place_names` \nus_place_history &lt;- tuesdata$`us_place_history`\n\nI’d like to look at the places local to me. The dataset contains two dataframes- one with geographic details about the location and the other with some commentary like description and history.\n\nva &lt;- us_place_names %&gt;% filter(state_name == \"Virginia\")\nva &lt;- va %&gt;% filter(county_name == \"Arlington\")\nva_joined &lt;- va %&gt;% left_join(us_place_history, by = join_by(feature_id))\n\nI don’t need city, state, and county number since I am dealing with a single city/county. So I am removing them from the dataset and then viewing what I have.\n\nva_joined %&gt;% select(-state_name,-county_name,-county_numeric) %&gt;%\n  gt()\n\n\n\n\n\n  \n    \n    \n      feature_id\n      feature_name\n      date_created\n      date_edited\n      prim_lat_dec\n      prim_long_dec\n      description\n      history\n    \n  \n  \n    1471986\nOverlee Knolls\n1979-09-28\n2022-06-07\n38.88956\n-77.14776\nNA\nNA\n    1492448\nAddison Heights\n1979-09-28\n2022-06-07\n38.85567\n-77.06026\nNA\nNA\n    1492455\nAlcova Heights\n1979-09-28\n2022-06-07\n38.86456\n-77.09720\nNA\nNA\n    1492483\nArlington Forest\n1979-09-28\n2022-06-07\n38.86872\n-77.11303\nNA\nNA\n    1492484\nArlington Heights\n1979-09-28\n2022-06-07\n38.86956\n-77.09220\nNA\nNA\n    1492485\nArlington Village\n1979-09-28\n2022-06-07\n38.86178\n-77.08526\nNA\nNA\n    1492487\nArna Valley\n1979-09-28\n2022-06-07\n38.84428\n-77.07637\nNA\nNA\n    1492496\nAurora Hills\n1979-09-28\n2022-06-07\n38.85150\n-77.06414\nNA\nNA\n    1492512\nBarcroft\n1979-09-28\n2022-06-07\n38.85595\n-77.10387\nNA\nNA\n    1492597\nBluemont Junction\n1979-09-28\n2022-06-07\n38.87483\n-77.13331\nNA\nNA\n    1492606\nBon Air\n1979-09-28\n2022-06-07\n38.87317\n-77.12665\nNA\nNA\n    1492659\nBuckingham\n1979-09-28\n2022-06-07\n38.87345\n-77.10665\nNA\nNA\n    1492771\nClaremont\n1979-09-28\n2022-06-07\n38.84317\n-77.10470\nNA\nNA\n    1492797\nColumbia Forest\n1979-09-28\n2022-06-07\n38.85400\n-77.11026\nNA\nNA\n    1492798\nColumbia Heights\n1979-09-28\n2022-06-07\n38.85761\n-77.12109\nNA\nNA\n    1492877\nDouglass Park\n1979-09-28\n2022-06-07\n38.84983\n-77.09303\nNA\nNA\n    1492958\nFort Barnard Heights\n1979-09-28\n2022-06-07\n38.84650\n-77.08942\nNA\nNA\n    1493006\nGlencarlyn\n1979-09-28\n2011-05-11\n38.86178\n-77.12915\nNA\nNA\n    1493353\nNorth Fairlington\n1979-09-28\n2022-06-07\n38.83650\n-77.09720\nNA\nNA\n    1493397\nParkglen\n1979-09-28\n2022-06-07\n38.85595\n-77.11637\nNA\nNA\n    1493586\nShirlington\n1979-09-28\n2022-06-07\n38.84178\n-77.08831\nNA\nNA\n    1493630\nSouth Fairlington\n1979-09-28\n2022-06-07\n38.83261\n-77.08970\nNA\nNA\n    1493744\nVirginia Heights\n1979-09-28\n2022-06-07\n38.85095\n-77.11637\nNA\nNA\n    1493745\nVirginia Highlands\n1979-09-28\n2022-06-07\n38.85845\n-77.06470\nNA\nNA\n    1493784\nWestmont\n1979-09-28\n2022-06-07\n38.86261\n-77.09192\nNA\nNA\n    1495188\nAllencrest\n1979-09-28\n2022-06-07\n38.89344\n-77.15026\nNA\nNA\n    1495260\nBerkshire\n1979-09-28\n2022-06-07\n38.89789\n-77.15137\nNA\nNA\n    1495429\nCountry Club Hills\n1979-09-28\n2022-06-07\n38.91400\n-77.13081\nNA\nNA\n    1495430\nCountry Club Manor\n1979-09-28\n2022-06-07\n38.91372\n-77.13776\nNA\nNA\n    1495438\nCrescent Hills\n1979-09-28\n2022-06-07\n38.90483\n-77.14581\nNA\nNA\n    1495472\nDominion Hills\n1979-09-28\n2022-06-07\n38.87595\n-77.14109\nNA\nNA\n    1495490\nEast Falls Church\n1979-09-28\n2022-06-07\n38.88733\n-77.15442\nNA\nNA\n    1495579\nGarden City\n1979-09-28\n2022-06-07\n38.90011\n-77.13526\nNA\nNA\n    1495641\nHalls Hill\n1979-09-28\n2022-06-07\n38.89761\n-77.12859\nNA\nNA\n    1495692\nHighview Park\n1979-09-28\n2022-06-07\n38.89372\n-77.12748\nNA\nNA\n    1495804\nLacey Forest\n1979-09-28\n2022-06-07\n38.88289\n-77.12915\nNA\nNA\n    1495821\nLarchmont\n1979-09-28\n2022-06-07\n38.88650\n-77.12776\nNA\nNA\n    1495887\nMadison Manor\n1979-09-28\n2022-06-07\n38.88039\n-77.14720\nNA\nNA\n    1496037\nOakwood\n1979-09-28\n2022-06-07\n38.89733\n-77.16248\nNA\nNA\n    1496271\nStratford Hills\n1979-09-28\n2022-06-07\n38.90872\n-77.14053\nNA\nNA\n    1496293\nTara\n1979-09-28\n2022-06-07\n38.89039\n-77.13498\nNA\nNA\n    1496368\nWalker Chapel\n1979-09-28\n2022-06-07\n38.92150\n-77.12942\nNA\nNA\n    1496386\nWest Arlington\n1979-09-28\n2022-06-07\n38.89400\n-77.16831\nNA\nNA\n    1496394\nWestover\n1979-09-28\n2022-06-07\n38.88706\n-77.13942\nNA\nNA\n    1496421\nWilliamsburg Village\n1979-09-28\n2022-06-07\n38.90511\n-77.15498\nNA\nNA\n    1496434\nWoodland Acres\n1979-09-28\n2022-06-07\n38.91261\n-77.14526\nNA\nNA\n    1499060\nArlingwood\n1979-09-28\n2022-06-07\n38.92761\n-77.12192\nNA\nNA\n    1499086\nBallston\n1979-09-28\n2011-05-11\n38.88011\n-77.11387\nNA\nNA\n    1499108\nBeechwood Hills\n1979-09-28\n2022-06-07\n38.90900\n-77.10998\nNA\nNA\n    1499116\nBellevue Forest\n1979-09-28\n2022-06-07\n38.91428\n-77.11359\nNA\nNA\n    1499157\nBrandon Village\n1979-09-28\n2022-06-07\n38.87567\n-77.11581\nNA\nNA\n    1499172\nBroyhill Forest\n1979-09-28\n2022-06-07\n38.91539\n-77.12248\nNA\nNA\n    1499245\nCherrydale\n1979-09-28\n2022-06-07\n38.89706\n-77.10831\nNA\nNA\n    1499266\nClarendon\n1979-09-28\n2022-06-07\n38.88595\n-77.09692\nNA\nNA\n    1499290\nColonial Village\n1979-09-28\n2022-06-07\n38.89317\n-77.08609\nNA\nNA\n    1499313\nCrystal Spring Knolls\n1979-09-28\n2022-06-07\n38.90344\n-77.10498\nNA\nNA\n    1499349\nDominion Heights\n1979-09-28\n2022-06-07\n38.89289\n-77.10776\nNA\nNA\n    1499354\nDover\n1979-09-28\n2022-06-07\n38.90678\n-77.10581\nNA\nNA\n    1499439\nFort Myer Heights\n1979-09-28\n2022-06-07\n38.89206\n-77.07942\nNA\nNA\n    1499560\nHighlands\n1979-09-28\n2022-06-07\n38.89817\n-77.08303\nNA\nNA\n    1499652\nLee Heights\n1979-09-28\n2022-06-07\n38.90206\n-77.11720\nNA\nNA\n    1499696\nLyon Park\n1979-09-28\n2022-06-07\n38.88067\n-77.09026\nNA\nNA\n    1499697\nLyon Village\n1979-09-28\n2022-06-07\n38.89483\n-77.09498\nNA\nNA\n    1499930\nRadnor Heights\n1979-09-28\n2022-06-07\n38.88900\n-77.07303\nNA\nNA\n    1499964\nRivercrest\n1979-09-28\n2022-06-07\n38.92206\n-77.11915\nNA\nNA\n    1499969\nRiverwood\n1979-09-28\n2022-06-07\n38.90539\n-77.10248\nNA\nNA\n    1499990\nRosslyn\n1979-09-28\n2022-06-07\n38.89678\n-77.07248\nNA\nNA\n    1500349\nWoodmont\n1979-09-28\n2022-06-07\n38.90067\n-77.09498\nNA\nNA\n    1779110\nBrockwood\n1998-02-05\n2022-06-07\n38.87761\n-77.12887\nNA\nNA\n    1779112\nCountry Club Grove\n1998-02-05\n2022-06-07\n38.91956\n-77.12942\nNA\nNA\n    1779118\nEast Arlington (historical)\n1998-02-05\n2022-06-07\n38.87345\n-77.06220\nNA\nNA\n    1779119\nGreen Valley\n1998-02-05\n2022-06-07\n38.85511\n-77.08859\nNA\nNA\n    1779147\nMillburn Terrace\n1998-02-05\n2022-06-07\n38.90067\n-77.13831\nNA\nNA\n    1783506\nArlington\n1998-03-02\n2022-06-07\n38.89039\n-77.08414\nNA\nNA\n    2646878\nCrystal City\n2010-08-26\n2018-11-14\n38.85535\n-77.05090\nNA\nNA\n  \n  \n  \n\n\n\n\nThere is no historical or descriptive data for any of the features in Arlington. Many of these are historical sites or are otherwise of interest. I’d like to augment this data with some context. Arlington has 23 neighborhoods that are on the National Register of Historic Places. The National Register does have scanned applications available for post 2012 applications, but most of the historic neighborhoods were designated prior to that. The National Register does also have a spreadsheet with links to the National archives, which contains the pre-2012 applications.\nI normally like to use tidyverse packages, but read_excel won’t work with URLs. There are workarounds, but it is easier just to use the openxlsx package. The read.xlsx function works as you’d expect but you do need to specify the sheet to read in.\n\nnational_historic &lt;-\n  read.xlsx(\n    'https://www.nps.gov/subjects/nationalregister/upload/national-register-listed-20230119.xlsx' ,\n    sheet = 1\n  )\n\nTaking only my local historic sites. This dataset is annoying because some entries are in all CAPS (like state), but others are in titlecase (like City/County). Some, like building category are in both. To use the entire dataset some string cleaning and formating might be necessary, but for this case, I don’t need to do this.\n\narlington_historic &lt;- national_historic %&gt;%\n  filter(State == \"VIRGINIA\" & County == \"Arlington\")\n\nLooking at the data, it neighborhoods seem to be encoded as districts.\n\narlington_historic_districts &lt;- arlington_historic %&gt;%\n  filter(Category.of.Property == \"DISTRICT\")\n\nArlington County has a website listing historic neighborhoods, and I know there should be 23. The National Register has 29 local entries. I should also note that only 17 of the Arlington neighborhoods appeared in our place names dataset.\nOn to figure out what the extra 3 historic places are. Apparently forts are also districts. There are also applications for boundary increases. To do this I am going to use the stringr function str_detect to find “Boundary Increase” and “Fort” and use the negate = TRUE flag to return everything that doesn’t match.\n\narlington_historic_districts2 &lt;- arlington_historic_districts %&gt;%\n  filter(str_detect(Property.Name, \"Boundary Increase\", negate = TRUE)) %&gt;%\n  filter(str_detect(Property.Name, \"Fort\", negate = TRUE))  \n\n\narlington_historic_districts2 %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      Reference.number\n      Property.Name\n      Status\n      Request.Type\n      Restricted.Address\n      Category.of.Property\n      State\n      County\n      City\n      Street.&.Number\n      External.Link\n      Federal.Agencies\n      Level.of.Significance.-.International\n      Level.of.Significance.-.Local\n      Level.of.Significance.-.National\n      Level.of.Significance.-.Not.Indicated\n      Level.of.Significance.-.State\n      Listed.Date\n      Name.of.Multiple.Property.Listing\n      NHL.Designated.Date\n      Other.Names\n      Park.Name\n      Status.Date\n      Area.of.Significance\n    \n  \n  \n    _05001344\nArlington Forest Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by Carlin Springs Rd., George Mason Dr., Henderson Rd., Aberdeen St., Columbus St., Granada, Galveston and 2nd\nhttps://catalog.archives.gov/id/77834749\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38688\nNA\nNA\n VDHR File No.000-7808\nNA\n38688\n ARCHITECTURE; COMMUNICATIONS\n    _08000063\nArlington Heights Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by Arlington Blvd., S. Fillmore St., S. Walter Reed Dr., columbia Pk., & S. Glebe Rd.\nhttps://catalog.archives.gov/id/41678540\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n39499\n Garden Apartments, Apartment Houses and Apartment Complexes in Arlington County, Virginia MPS\nNA\n 000-3383\nNA\n39499\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _14000146\nArlington National Cemetery Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\n1 Memorial Ave.\nNA\n DEPARTMENT OF THE ARMY\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\n41740\nNA\nNA\n Arlington National Cemetery; DHR #000-0042\nNA\n41740\n MILITARY; LANDSCAPE ARCHITECTURE; POLITICS/GOVERNMENT; ARCHITECTURE\n    _03000215\nArlington Village Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nS 13th St., S 13 Rd., S 16th St., S Barton S., S. Cleveland St. and Edgewood St.\nhttps://catalog.archives.gov/id/41679618\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37722\nNA\nNA\n 000-0024\nNA\n37722\n COMMUNITY PLANNING AND DEVELOPMENT\n    _03000561\nAshton Heights Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by Wilson Bvd., N. Irving St., Arlington Bvd., N. Oxford St., N. Piedmont & N. Oakland Sts.\nhttps://catalog.archives.gov/id/41679598\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37795\nNA\nNA\n 000-7819\nNA\n37795\n ARCHITECTURE; COMMERCE\n    _08001018\nAurora Highlands Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by 16th St. S., S. Eads St., 26th St. S., and S. Joyce St.\nhttps://catalog.archives.gov/id/77834759\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n39743\nNA\nNA\n 000-9706\nNA\n39743\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _98001649\nBuckingham Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by N. 5th, N. Oxford, and N. 2nd Sts., and N. Glebe Rd.\nhttps://catalog.archives.gov/id/41679602\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n36181\nNA\nNA\n DHR File # 00-0025\nNA\n36181\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT; LANDSCAPE ARCHITECTURE\n    _03000461\nCherrydale Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by Lorcom Ln., N. Utah and N. Taylor Sts., and I-66\nhttps://catalog.archives.gov/id/41679592\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37763\nNA\nNA\n VDHR File Number 000-7821\nNA\n37763\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _06000751\nClaremont Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by S. Dinwiddie St., S. Chesterfield Rd., S. Buchanan St., 25th St. S, 24th St. S, 23rd St. S and 22nd St. S\nhttps://catalog.archives.gov/id/77834757\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38960\nNA\nNA\n 000-9700\nNA\n38960\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _04000047\nColumbia Forest Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by 11th, S. Edison, S. Dinwiddie, S. Columbus, S. George Mason, and S. Frederick St.\nhttps://catalog.archives.gov/id/41679620\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38028\nNA\nNA\n VDHR # 000-9416\nNA\n38028\n COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n    _12000239\nDominion Hills Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by N. Four Mile Run Dr., N. McKinley Rd., N. Larrimore, N. Madison, N. Montana Sts., & 9th St. N.\nhttps://catalog.archives.gov/id/77834753\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n41023\n Historic Residential Suburbs in the United States, 1830-1960 MPS\nNA\n VDHR FILE NUMBER: 000-4212\nNA\n41023\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _99000368\nFairlington Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by Quaker Lane, King St., I-395, S. Walter Reed Dr., and S. Abingdon St.\nhttps://catalog.archives.gov/id/41679636\nNA\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\n36248\nNA\nNA\n DHR File No. 000-5772\nNA\n36248\n MILITARY; COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n    _04000049\nGlebewood Village Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nN. Brandywine St. Bet. Lee Hwy and 10th Place N, 21St Rd. bet. N. Brandywine St. and N. Glebe Rd.\nhttps://catalog.archives.gov/id/41679622\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38028\nNA\nNA\n 000-9414\nNA\n38028\n COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n    _08000910\nGlencarlyn Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by S. Carlin Springs Rd., Arlington Blvd., 5th Rd. S., Glencarlyn Park\nhttps://catalog.archives.gov/id/77834761\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n39709\nNA\nNA\n 000-9704\nNA\n39709\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _11000548\nHighland Park-Overlee Knolls\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by 22nd St. N., N. Lexington St., 16th St. N., N. Longfellow St., McKinley Rd., I-66 & N. Quantico St.\nhttps://catalog.archives.gov/id/77834763\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n40773\n Historic Residential Suburbs in the United States, 1830-1960 MPS\nNA\n Fostoria/VDHR File Number OOO-9703\nNA\n40773\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _04000109\nLee Gardens North Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\n2300-2341 N. 11th St.\nhttps://catalog.archives.gov/id/41678536\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38043\n Garden Apartments, Apartment Houses and Apartment Complexes in Arlington County, Virginia MPS\nNA\n 000-9411; Woodbury Park Apartments\nNA\n38043\n COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n    _03000437\nLyon Park Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by 10th St. N, Arlington Blvd., and N. Irving St.\nhttps://catalog.archives.gov/id/41679594\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37937\nNA\nNA\n 000-7820\nNA\n37937\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _02000512\nLyon Village Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by Lee Hwy, N. Veitch St., N. Franklin Rd., N. Highland St., N. Fillmore St., and N. Kirkwood Rd.\nhttps://catalog.archives.gov/id/41679590\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37386\nNA\nNA\n VDHR File No. 000-7822\nNA\n37386\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _03000460\nMaywood Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by Lorcom Ln., Spout Run Parkway, I-66, Lee Highway, N. Oakland St., N. Nelson St., and N. Lincoln St.\nhttps://catalog.archives.gov/id/41679596\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37763\nNA\nNA\n VDHR File Number 000-5056\nNA\n37763\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _08000064\nMonroe Courts Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\n1041-1067 N. Nelson and 1036-1062 & 1033-1055 N. Monroe Sts.\nhttps://catalog.archives.gov/id/77834751\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n39499\nNA\nNA\n 000-4105\nNA\n39499\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _04000112\nPenrose Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by Arlington Blvd., S. Courthouse Rd., S. Fillmore St., S. Barton St. S, and Columbia Pike\nhttps://catalog.archives.gov/id/41679600\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38306\nNA\nNA\n VDHR File Number 000-8823\nNA\n38306\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT; BLACK\n    _08000065\nVirginia Heights Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by 10th Pl. S., S. Frederick St. & S. George Mason Dr.\nhttps://catalog.archives.gov/id/77834755\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n39499\nNA\nNA\n 000-9701\nNA\n39499\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n    _03000451\nWalter Reed Gardens Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\n2900-2906 13th St. S, 2900-2914 13th Rd S, 1301-1319 S. Walter Reed Dr.\nhttps://catalog.archives.gov/id/41678548\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n37763\n Garden Apartments, Apartment Houses and Apartment Complexes in Arlington County, Virginia MPS\nNA\n Commons of Arlington; 000-8824\nNA\n37763\n COMMUNITY PLANNING AND DEVELOPMENT\n    _04000111\nWaverly Hills Historic District\nListed\nSingle\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by 20th Rd. N, N. Utah St, I-66, N. Glebe Rd. and N. Vermont St.\nhttps://catalog.archives.gov/id/41679624\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38043\nNA\nNA\n VDHR File Number 000-9413\nNA\n38043\n COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n    _06000345\nWestover Historic District\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nBounded by McKinley Rd., N. Washington Blvd., N. 16th St., N. Jefferson St., N. 11th St. and N. Fairfax Dr.\nhttps://catalog.archives.gov/id/41678538\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n38839\n Garden Apartments, Apartment Houses and Apartment Complexes in Arlington County, Virginia MPS\nNA\n 000-0032\nNA\n38839\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n  \n  \n  \n\n\n\n\nI still have too many entries. It turns out that Arlington National Cemetary is also encoded as a DISTRICT. There is also an entry for Walter Reed Gardens Historic District. Arlington County has this listed as a building on their site (and the other entries like Calvert Manor are noted as buildings in the National Register.)\nI could remove these two items manually, but they will be removed when I join it to the place names dataset, since neither one appears in the populated place names.\nJoining the two datasets will require some sort of string manipulation since the place names are not the same. The place names dataset contains just the place names (“Addison Heights”), while the historic sites data contains the phrase “Historic District” appended to the end. In addition, some place names don’t exactly match the historic district names (“Overlee Knolls” and “Highland Park/ Overlee Knolls Historic district”).\nSo I want to do some fuzzy matching and luckily (of course!) there is an R package for that.\nHowever, the populated place names data contains “Arlington” which will match to a ton of different neighborhoods (Arlington Forest, Arlington Heights, etc.) I’m going to change Arlington to Arlington County.\n\nva_joined2 &lt;- va_joined %&gt;%\n  mutate(feature_name = ifelse(feature_name == \"Arlington\", \"Arlington County\", feature_name))\n\nI also know that North and South Fairlington, while separate places in the populated place names, are a single historic district called Fairlington. I’m going to make both North and South Fairlington entry in the historical sites dataframe. I’m not removing the original Fairlington entry because I know I’m going to filter it out with my joins later. But this is the kind of thing that could lead to errors/ extraneous entries later on, so if you do something like this, just make sure you do clean it up later.\n\nsouth_fairlington &lt;- arlington_historic_districts2 %&gt;% \n  filter(Property.Name == \"Fairlington Historic District\") %&gt;%\n  mutate(Property.Name = \"South Fairlington\")\n\nnorth_fairlington &lt;- arlington_historic_districts2 %&gt;%\n  filter(Property.Name == \"Fairlington Historic District\") %&gt;%\n  mutate(Property.Name = \"North Fairlington\")\n\narlington_historic_districts3 &lt;- arlington_historic_districts2 %&gt;%\n  rbind(south_fairlington) %&gt;%\n  rbind(north_fairlington)\n\nOkay, on to fuzzyjoining. The name from the populated places names dataset should be a subset of the name from the historic district dataset. I’m going to illustrate this in a very simply way using str_detect(). “Overlee Knolls” is the first entry in the populated places dataset. I’m going to use this as the pattern to search for in the Historic places dataset. The expected returned neighborhood is “Highland Park/ Overlee Knolls Historic district”.\n\nva_joined2$feature_name[1]\n\n[1] \"Overlee Knolls\"\n\narlington_historic_districts %&gt;%\n  filter(str_detect(Property.Name, va_joined2$feature_name[1])) %&gt;%\n  gt()\n\n\n\n\n\n  \n    \n    \n      Reference.number\n      Property.Name\n      Status\n      Request.Type\n      Restricted.Address\n      Category.of.Property\n      State\n      County\n      City\n      Street.&.Number\n      External.Link\n      Federal.Agencies\n      Level.of.Significance.-.International\n      Level.of.Significance.-.Local\n      Level.of.Significance.-.National\n      Level.of.Significance.-.Not.Indicated\n      Level.of.Significance.-.State\n      Listed.Date\n      Name.of.Multiple.Property.Listing\n      NHL.Designated.Date\n      Other.Names\n      Park.Name\n      Status.Date\n      Area.of.Significance\n    \n  \n  \n    _11000548\nHighland Park-Overlee Knolls\nListed\nMultiple\nFALSE\nDISTRICT\nVIRGINIA\nArlington\nArlington\nRoughly bounded by 22nd St. N., N. Lexington St., 16th St. N., N. Longfellow St., McKinley Rd., I-66 & N. Quantico St.\nhttps://catalog.archives.gov/id/77834763\nNA\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\n40773\n Historic Residential Suburbs in the United States, 1830-1960 MPS\nNA\n Fostoria/VDHR File Number OOO-9703\nNA\n40773\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n  \n  \n  \n\n\n\n\nI’ve decided I only want to look at the historic areas in the populated place names. I’m choosing an inner join so I will only get entries that exist in BOTH the populated places and the historic register. This is 17 items (from manually comparing the populated places to the Arlington County website). I’m going to map these places on top of current Arlington County neighborhood groups/civic associates. I’m interested in how current neighborhood compare to the historic districts. (Note that I could have done this without the populated places dataset at all, but this is the Tidytuesday dataset and it is what lead me to my question.)\nThere are a few different ways to use fuzzyjoins. I found this discussion on stackoverflow to be a good starting point. I chose to use the match_fun version, since I had already prototyped with str_detect. The only thing that wasn’t clear to me is which dataframe would be sent to str_detect as the pattern and which was the string. That is, for\nfuzzy_inner_join(x, y, by = c(x$name1 = y$name2), match_fun = str_detect) would I get\nstr_detect(string = x$name1,  pattern = y$name2)\nor\nstr_detect(string = y$name2,  pattern = x$name1)\n?\nMaybe it is clear to others from the stackoverflow example or the fuzzyjoin manual, but it wasn’t clear to me, so I ended up trying it both ways. It turns out that the dataframes are passed to str_detect in the order they are listed, which makes sense (and is probably the convention, but I had never seen it explicitly stated). [To be absolutely clear, what happens is the first case (str_detect(string = x$name1,  pattern = y$name2))]\n\nhistoric_pop_places &lt;-\n  arlington_historic_districts3 %&gt;% fuzzy_inner_join(va_joined2,\n                                                     by = c(\"Property.Name\" = \"feature_name\"),\n                                                     match_fun = str_detect)\n\nFor what I plan to do, I need the place name and the location. I want the reason the place is important and a link to the historic registry application. I started this project wanting to know why these places were important! I’m leaving in both sets of place names, just so I can visually check that my dataset is correct.\n\nhistoric_pop_places &lt;- historic_pop_places %&gt;%\n  select(\n    Property.Name,\n    feature_name,\n    Area.of.Significance,\n    prim_lat_dec,\n    prim_long_dec,\n    External.Link\n  )\ngt(historic_pop_places)\n\n\n\n\n\n  \n    \n    \n      Property.Name\n      feature_name\n      Area.of.Significance\n      prim_lat_dec\n      prim_long_dec\n      External.Link\n    \n  \n  \n    Arlington Forest Historic District\nArlington Forest\n ARCHITECTURE; COMMUNICATIONS\n38.86872\n-77.11303\nhttps://catalog.archives.gov/id/77834749\n    Arlington Heights Historic District\nArlington Heights\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.86956\n-77.09220\nhttps://catalog.archives.gov/id/41678540\n    Arlington Village Historic District\nArlington Village\n COMMUNITY PLANNING AND DEVELOPMENT\n38.86178\n-77.08526\nhttps://catalog.archives.gov/id/41679618\n    Aurora Highlands Historic District\nHighlands\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.89817\n-77.08303\nhttps://catalog.archives.gov/id/77834759\n    Buckingham Historic District\nBuckingham\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT; LANDSCAPE ARCHITECTURE\n38.87345\n-77.10665\nhttps://catalog.archives.gov/id/41679602\n    Cherrydale Historic District\nCherrydale\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.89706\n-77.10831\nhttps://catalog.archives.gov/id/41679592\n    Claremont Historic District\nClaremont\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.84317\n-77.10470\nhttps://catalog.archives.gov/id/77834757\n    Columbia Forest Historic District\nColumbia Forest\n COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n38.85400\n-77.11026\nhttps://catalog.archives.gov/id/41679620\n    Dominion Hills Historic District\nDominion Hills\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.87595\n-77.14109\nhttps://catalog.archives.gov/id/77834753\n    Glencarlyn Historic District\nGlencarlyn\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.86178\n-77.12915\nhttps://catalog.archives.gov/id/77834761\n    Highland Park-Overlee Knolls\nOverlee Knolls\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.88956\n-77.14776\nhttps://catalog.archives.gov/id/77834763\n    Lyon Park Historic District\nLyon Park\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.88067\n-77.09026\nhttps://catalog.archives.gov/id/41679594\n    Lyon Village Historic District\nLyon Village\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.89483\n-77.09498\nhttps://catalog.archives.gov/id/41679590\n    Virginia Heights Historic District\nVirginia Heights\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.85095\n-77.11637\nhttps://catalog.archives.gov/id/77834755\n    Westover Historic District\nWestover\n ARCHITECTURE; COMMUNITY PLANNING AND DEVELOPMENT\n38.88706\n-77.13942\nhttps://catalog.archives.gov/id/41678538\n    South Fairlington\nSouth Fairlington\n MILITARY; COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n38.83261\n-77.08970\nhttps://catalog.archives.gov/id/41679636\n    North Fairlington\nNorth Fairlington\n MILITARY; COMMUNITY PLANNING AND DEVELOPMENT; ARCHITECTURE\n38.83650\n-77.09720\nhttps://catalog.archives.gov/id/41679636\n  \n  \n  \n\n\n\n\nAurora Highlands and Highlands are the same place- the description of Aurora Highlands from Wikipedia matches the description in the application to be entered on the National Historic Register.\nNow, I found a map of all the Civic associations in Arlington on the county’s open data page. Data can be downloaded in a variety of formats, including shape files or geoJSON. I chose to download the shapefile and extracted the zip to my project directory (not shown).\nThe R Graph Gallery (which is a great resource and source of inspiration) has a great section on mapping, but unfortunately one of the needed packages is being retired. The code below still works but you will get a very long message telling you to migrate away from rgal.\n\n# library(sp)\n# library(rgdal)\n# my_spdf &lt;- readOGR( \n#  dsn = \"Civic_poly.shp\" , \n#  verbose=FALSE\n#)\n\nSo, here is another way to read in the shape file using the sf package. This contains the polygons that define the boundaries of modern neighborhoods in Arlington. There are a lot of neighborhoods!\n\narlington_polygons &lt;- st_read(dsn = \"Civic_poly.shp\")\n\nMapping points (which is what we have in our TidyTuesday dataset- we have the lat/long of the “official feature location”) and polygons from the Arlington County dataset involved a few steps. Shape files can be encoded using different coordinate reference systems (CRS) and care needs to be taken that all the map layers are using the same CRS. I found the mapview package invaluable during this process, as it is simple to create an interactive map. This made trouble shooting incredibly easy.\nGenerally, the first step for handling shape files in R is to convert them to simple features objects. Here, I’m using the sf_package. With a shape file, you generally don’t need to pass the coordinates or CRS, since that data is encoded in the shape file in a way that is easily detectable by the function.\n\narlington_polygons_sf &lt;- st_as_sf(arlington_polygons)\nmapview(arlington_polygons_sf)\n\n\n\n\n\n\nThe generated map looks perfect. Arlington is in the right place in the world mapview(arlington_polygons_sf)and the civic association map looks as it should.\nFor the point data, the conversion does require additional parameters (description of parameters here). Specifically, the coordinates for point data need to be specified. The order for this is longitude, latitude, which I did not do properly the first name, since in spoken English, you usually say latitude/longitude. The mapview map made that very easy to troubleshoot when I saw my points were all in Antarctica. The pop up made it clear that latitude and longitude were flipped. I also need the CRS for this dataset if I’m going to map it with the polygon data. (You also need a CRS for mapview to place your points on a map- without a CRS you get the pattern, but not the geolocation.)\nThe CRS is not specified in the data dictionary for TidyTuesday. There are two likely choices, 4326 and 4269. In this application, there isn’t actually a significant difference. With the mapview data you can select and deselect the layers and see both sets of points are in the same place on this map.\n\nhistoric_4269 &lt;- st_as_sf(historic_pop_places, coords = c(5:4), crs = 4269)\nhistoric_4326 &lt;- st_as_sf(historic_pop_places, coords = c(5:4), crs = 4326)\n\nmapview(historic_4269) + mapview(historic_4326) + mapview(arlington_polygons_sf)\n\n\n\n\n\n\nGoing back to the original data source, it notes that “Datum is NAD83”. This means that the CRS = 4269 as found at the EPGS registry.\nThe Arlington polygons dataset is also NAD83/ 4269, so you can go directly to plotting. If they were different CRSs then you would need to transform them to the same projection, such as with:\npoints_transformed &lt;- sf::st_transform(points_wrong, crs = sf::st_crs(arlington_polygons_sf))\n\nmapview(arlington_polygons_sf ,\n  col.regions = \"purple\") + mapview(historic_4269, col.regions = \"blue\")\n\n\n\n\n\n\nSo many of these historic neighborhoods don’t appear to correspond strongly to modern day neighborhoods. Several of them appear the borders of multiple neighborhood groups. And for example, the location of the Westover feature from the populated places names is actually in the Tara-Leeway Heights neighborhood according to the county’s description of civic association boundaries.\nNow I’m going to make a static ggplot map. Mapview is great for exploratory data analysis, but it isn’t as highly customizable as other graphing packages. I’m displaying the populated place names/ historic district using geom_sf_text(). This needs to be passed both the data and the label, and despite being passed the data it still needs the full variable name (historic_4269$feature_name not feature_name). The units for nudging the text are depend on what crs is used? I just played around until I got the label to move and 800 was not the kind of number I was expecting. (I was thinking 0 to 1 like for hjust.)\n\nggplot() +\n  geom_sf(data = arlington_polygons_sf) +\n  geom_sf(data = historic_4269, alpha = 0.5) +\n  theme_void() +\n  geom_sf_text(\n    data = historic_4269,\n    label = historic_4269$feature_name,\n    size = 2.0,\n    nudge_y = 850\n  ) +\n  labs(title = \"Historic Districs in Arlington compared to modern neighborhoods\") +\n  labs(caption = \"Data from: US Board of Geographic Names,  \\nArlington County, VA - Official GIS Open Data Portal,  \\nand the National Register of Historic Places\") +\n  theme(plot.title = element_text(size = 10),\n        plot.caption = element_text(size = 6, hjust = 0))\n\n\n\n\nAs a further project, I’d like to make an interactive map with a pop-up giving a clickable link to the National Archives page on the historic district application. The mapview version does have the pop-up, but the link isn’t live.\nI found tutorial here to make the pop-up URL using leaflet, but I can’t figure out how to add my polygons. I add the points just fine. It also fails causes the quarto document to fail during render, though it works just fine as regular code.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {Tidy {Tuesday:} {US} {Populated} {Places}},\n  date = {2023-06-27},\n  url = {https://lsinks.github.io/posts/2023-06-27-tidytuesday-US-populated-places/arlington-neighborhoods},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “Tidy Tuesday: US Populated\nPlaces.” June 27, 2023. https://lsinks.github.io/posts/2023-06-27-tidytuesday-US-populated-places/arlington-neighborhoods."
  },
  {
    "objectID": "posts/2023-04-25-tidytuesday-marathon/marathon.html",
    "href": "posts/2023-04-25-tidytuesday-marathon/marathon.html",
    "title": "TidyTuesday Week 17: London Marathon",
    "section": "",
    "text": "Today’s TidyTuesday is based on a dataset about the London Marathon. The data is via a package by Nicola Rennie and there is an accompanying tutorial about how to scrape data from the web.\n\nlibrary(skimr)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(skimr)\n\nLoad the data with the TidyTuesday package in the usual way.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 17)\n\n--- Compiling #TidyTuesday Information for 2023-04-25 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `winners.csv`\n    Downloading file 2 of 2: `london_marathon.csv`\n\n\n--- Download complete ---\n\nwinners &lt;- tuesdata$winners\nlondon_marathon &lt;- tuesdata$london_marathon\n\nThere are two dataframes today: a winner’s dataframe and info about the marathon in general. Looking at the winner’s first.\n\nwinners %&gt;% skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n163\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\ndifftime\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nCategory\n0\n1\n3\n16\n0\n4\n0\n\n\nAthlete\n0\n1\n9\n26\n0\n99\n0\n\n\nNationality\n0\n1\n5\n14\n0\n24\n0\n\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nTime\n0\n1\n5187 secs\n16143 secs\n02:07:55\n158\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYear\n0\n1\n2001.61\n11.83\n1981\n1991.5\n2002\n2012\n2022\n▇▇▇▇▇\n\n\n\n\n\nAll the data is complete.\nCategory and Nationality should probably be factors rather than characters.\n\nwinners &lt;- winners %&gt;%\n  mutate(Category = as.factor(Category), Nationality = as.factor(Nationality))\n\nglimpse(winners)\n\nRows: 163\nColumns: 5\n$ Category    &lt;fct&gt; Men, Men, Men, Men, Men, Men, Men, Men, Men, Men, Men, Men…\n$ Year        &lt;dbl&gt; 1981, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989…\n$ Athlete     &lt;chr&gt; \"Dick Beardsley (Tie)\", \"Inge Simonsen (Tie)\", \"Hugh Jones…\n$ Nationality &lt;fct&gt; United States, Norway, United Kingdom, United Kingdom, Uni…\n$ Time        &lt;time&gt; 02:11:48, 02:11:48, 02:09:24, 02:09:43, 02:09:57, 02:08:1…\n\n\nWhen I check to make sure everything is correct after making the factors, I see that some athletes have (Tie) after their name. If I do something with the runner’s names later (or if I care about ties) then I need to handle this.\nThere are four categories of races in our dataset- Men, Wheelchair Men, Wheelchair Women, Women.\nI’m going to do some simple comparisons first. I’m looking for something interesting to focus on.\n\nwinners %&gt;%\n  filter(Category == \"Men\" | Category == \"Women\") %&gt;%\n  ggplot(aes(Category, Time)) +\n  geom_boxplot()\n\n\n\n\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\" | Category == \"Wheelchair Women\") %&gt;%\n  ggplot(aes(Category, Time)) +\n  geom_boxplot()\n\n\n\n\nMen are faster than women, in both classes of races.\n\nwinners %&gt;%\n  ggplot(aes(Category, Time)) +\n  geom_boxplot()\n\n\n\n\nWheelchair races are faster than the running races. The Men/Women’s races are much closer than the Wheelchair races, and don’t have outliers.\nI’m interested in ties. How many ties have there been? I’m going to make a column called “Tied” and remove (Tied) from the names. There are a few different ways to do this, but I’m going to use tidyr::separate(). I’m going to take the space too, when I separate out the name and (tie), so my name column is cleanly formatted. This is going to generate a bunch of NAs in the Tied column, but I’ll handle that in the next code chunk. I’m using \" (\" as my separator, so the Name column will not have the trailing space. The Tied column will have “Tie)” and a bunch of NAs, but I’ll clean this up in the next step. I’ve left warnings on for this code chunk, so you can see that the NAs are flagged for your notice.\nRemember that special characters like ( need to be escaped out, so the appropriate regular expression for the separator is \" \\\\(\".\nAs a side note, as of winter 2022, separate has been superseded by a family of functions separate_wider_*.\n\nwinners_tied &lt;- winners %&gt;%\n  separate(Athlete, into = c(\"Name\", \"Tied\"), \" \\\\(\")\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 161 rows [3, 4, 5, 6, 7,\n8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, ...].\n\n\nNow I’m going to clean up the Tied column.\n\nwinners_tied &lt;- winners_tied %&gt;%\n  mutate(Tied = ifelse(is.na(Tied) == TRUE, FALSE, TRUE))\n\nSo how many ties are there?\n\nwinners_tied %&gt;% filter(Tied == TRUE)\n\n# A tibble: 2 × 6\n  Category  Year Name           Tied  Nationality   Time    \n  &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;lgl&gt; &lt;fct&gt;         &lt;time&gt;  \n1 Men       1981 Dick Beardsley TRUE  United States 02:11:48\n2 Men       1981 Inge Simonsen  TRUE  Norway        02:11:48\n\n\nJust those first two, from the very first race. According to wikipedia, they crossed the finish line holding hands!\nAnyone win more than once?\n\nwinners_tied %&gt;%\n  group_by(Name) %&gt;%\n  count(Name, sort = TRUE) %&gt;%\n  filter(n &gt; 1)\n\n# A tibble: 34 × 2\n# Groups:   Name [34]\n   Name                     n\n   &lt;chr&gt;                &lt;int&gt;\n 1 David Weir               8\n 2 Tanni Grey-Thompson      6\n 3 David Holding            4\n 4 Eliud Kipchoge           4\n 5 Francesca Porcellato     4\n 6 Ingrid Kristiansen       4\n 7 Tatyana McFadden         4\n 8 António Pinto            3\n 9 Dionicio Cerón           3\n10 Heinz Frei               3\n# ℹ 24 more rows\n\n\nMore than I expected! David Weir has won the most London Marathons, with 8 wins in the Men’s Wheelchair race category. How has his race time changed over the years?\n\nwinners_tied %&gt;%\n  filter(Name == \"David Weir\") %&gt;%\n  ggplot(aes(Year, Time)) +\n  geom_point()\n\n\n\n\nIt looks like his first race was much slower than the other times he has won. It turns out he has competed in the London Marathon 23 times, and placed 5th in the 2023 Marathon which happened this weekend. His time on Saturday was 01:32:45. This is interesting, because it is quite similar to his more recent races.\nHas the field for this race gotten faster?\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\") %&gt;%\n  ggplot(aes(Year, Time)) +\n  geom_point()\n\n\n\n\nYes, and quite clearly too! How many years would he have won with this year’s time?\n\ntime_2023 &lt;- as.difftime(\"01:32:45\", format = \"%H:%M:%S\", units = \"auto\")\nslow_wins &lt;- winners %&gt;%\n  filter(Category == \"Wheelchair Men\" & Time &gt;= time_2023) \nfast_wins &lt;- winners %&gt;%\n  filter(Category == \"Wheelchair Men\" & Time &lt; time_2023) \n\nSo 12 years the wins were faster than his time this year, but 27 years were slower. And just to note, 6 of those 12 faster wins are held by David Weir…\nLet’s throw together a visual for this and make it out TidyTuesday viz. Should be simple, right?\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\") %&gt;%\n  ggplot(aes(Year, Time)) +\n  geom_point() +\n  geom_hline(yintercept = time_2023)\n\n\n\n\nHmm. ggplot is not playing well with our difftimes.\n\nstr(time_2023)\n\n 'difftime' num 1.54583333333333\n - attr(*, \"units\")= chr \"hours\"\n\nstr(winners$Time[1])\n\n 'hms' num 02:11:48\n - attr(*, \"units\")= chr \"secs\"\n\n\nApparently, our race time from winners is actually hms and not difftime. Skim reported it was a difftime. Our difftime has units of hours, while Time has units of seconds. This is probably due to be setting units to “auto” when I did the conversion. Interesting that dplyr filtering handles this smoothly, but ggplot doesn’t.\n\ntime_2023 &lt;- as.difftime(\"01:32:45\", format = \"%H:%M:%S\", units = \"sec\")\nstr(time_2023)\n\n 'difftime' num 5565\n - attr(*, \"units\")= chr \"secs\"\n\n\nI’m going to create a TRUE/FALSE column for if David Weir won that I will color code the win by.\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\") %&gt;%\n  mutate(Weir = ifelse(Athlete == \"David Weir\", TRUE, FALSE)) %&gt;%\n  ggplot(aes(Year, Time, color = Weir)) +\n  geom_point() +\n  geom_hline(yintercept = time_2023)\n\n\n\n\nOkay, now lets clean up the formatting. I need to:\n\nApply a theme. I like theme_classic() or theme_pander() as a clean base for my graphs.\nRemove the legend. This needs to go after the theme_classic/pander() call or the legend will reappear.\nAdd title, subtitle and data source\nFix the y-axis units\nChange the colors for the David Weir points.\nLabel my horizontal line.\n\n#4 is apparently quite challenging. Apparently, the time axis should be displayed as hms, so it isn’t clear to me why fractional seconds are shown. I tried a bunch of different suggestion from the web, but the top answer to this question is what worked.\nIt actually doesn’t matter if the aesthetic is difftime or hms. The key is that the label section of scale_y_time needs a formatted string generated from strftime.\n\nwinners %&gt;%\n  filter(Category == \"Wheelchair Men\") %&gt;%\n  mutate(Weir = ifelse(Athlete == \"David Weir\", TRUE, FALSE)) %&gt;%\n\n  ggplot(aes(Year, Time, color = Weir)) +\n  geom_point() +\n  scale_color_manual(values = c(\"black\", \"cyan2\")) +\n  geom_hline(yintercept = (time_2023),\n             color = \"cyan4\") +\n  annotate(\n    \"text\",\n    x = 1990,\n    y = time_2023 - 200,\n    label = \"David Weir's 2023 time\",\n    color = \"cyan4\"\n  ) +\n  scale_y_time(name = \"Winning Time\", labels = function(l) strftime(l, '%H:%M:%S')) +\n  labs(title = \"Race times for the London Marathon: Men's Wheelchair Race\",\n       subtitle = \"compared to David Weir's wins\",\n       caption = \"data from https://github.com/nrennie/LondonMarathon\") +\n  theme_classic(12) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {TidyTuesday {Week} 17: {London} {Marathon}},\n  date = {2023-04-25},\n  url = {https://lsinks.github.io/posts/2023-04-25-tidytuesday-marathon/marathon},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “TidyTuesday Week 17: London\nMarathon.” April 25, 2023. https://lsinks.github.io/posts/2023-04-25-tidytuesday-marathon/marathon."
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "",
    "text": "I will walk through a classification problem from importing the data, cleaning, exploring, fitting, choosing a model, and finalizing the model.\nI wanted to create a project that could serve as a template for other two-class classification problems. I also wanted to fully use the tidymodels framework, particularly more advanced functionalities like workflowsets. There are some great tutorials on tidymodels, in particular Olivier Gimenez’s tutorial on Kaggle’s Titanic competition. This tutorial steps through each model individually, while I wanted to use the more streamlined approach offered by workflowsets. I also found myself confused as I started doing more advanced procedures in tidymodels, despite having read the book Tidy Modeling with R multiple times and working through several tutorials on Julia Silge’s excellent blog. I ended up writing my own tutorial on tidymodels objects that goes through the differences in the various ways to perform fitting and the various objects produced.\nIn addition to providing a template for the machine learning portion, I wanted to create nice figures and tables that could also be re-used.\nI will also have a different version of this code on Datacamp. I’ve numbered the code chunks manually to aid in comparison between the two versions. I start numbering at 2, because Code Block 1 will be installing libraries at the Datacamp workspace. There are some important differences between the RStudio environment and online notebooks/workspaces.\nPlease feel free to copy and use any of my code in your work. I’d appreciate an acknowledgment or link back if you find this tutorial useful."
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-strings",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-strings",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "5.1. Looking at the strings",
    "text": "5.1. Looking at the strings\nStrings are usually not a useful format for classification problems. The strings should be converted to factors, dropped, or otherwise transformed.\n5.1.1. Strings to Factors (Code Block 6 - 8)\n\ncategory, Category of Merchant\njob, Job of Credit Card Holder\n\n5.1.2. Strings as Strings (Code Block 9)\n\nmerchant, Merchant Name\ntrans_num, Transaction Number\n\nI’m not going to retain these, as they are either unlikely to have predictive power (trans_num) or are highly correlated with other predictors (merchant with merch_lat/merch_long.)\n5.2. Strings to Geospatial Data (Code Block 13)\nWe have plenty of geospatial data as lat/long pairs, so I want to convert city/state to lat/long so I can compare to the other geospatial variables. This will also make it easier to compute new variables like the distance the transaction is from the home location. I will transform and explore this when I handle the other geospatial data.\n\ncity, City of Credit Card Holder\nstate, State of Credit Card Holder\n\nThings to consider as we walk through the data:\n\nDo we have typos that lead to duplicate entries : VA/ Va. / Virginia?\nDo we have excessive # of categories? Do we want to combine some?\nShould they be ordered?\n\n\n5.1.1. Exploring the factors: how is the compactness of categories?\nThe predictors category and job are transformed into factors.\n\n# Code Block 6: Converting Strings to Factors\nfraud$category &lt;- factor(fraud$category)\nfraud$job &lt;- factor(fraud$job)\n\nFrom the skim output, I see that category has 14 unique values, and job has 163 unique values. The dataset is quite large, with 339,607 records, so these variables don’t have an excessive number of levels at first glance. However, it is worth seeing if I can compact the levels to a smaller number.\n\nWhy do we care about the number of categories and whether they are “excessive”?\nConsider the extreme case where a dataset had categories that only contained one record each. There is simply insufficient data to make correct predictions using category as a predictor on new data with that category label. Additionally, if your modeling uses dummy variables, having an extremely large number of categories will lead to the production of a huge number of predictors, which can slow down the fitting. This is fine if all the predictors are useful, but if they aren’t useful (as in the case of having only one record for a category), trimming them will improve the speed and quality of the data fitting.\nIf I had subject matter expertise, I could manually combine categories. For example, in this dataset, the three largest categories in job are surveying-related and perhaps could be combined. If you don’t have subject matter expertise, or if performing this task would be too labor intensive, then you can use cutoffs based on the amount of data in a category. If the majority of the data exists in only a few categories, then it might be reasonable to keep those categories and lump everything else in an “other” category or perhaps even drop the data points in smaller categories. As a side note, the forcats package has a variety of tools to handle consolidating and dropping levels based on different cutoffs if this is the approach you decide to take.\nOne way to evaluate the compactness of a factor is to group the data by category and look at a table of counts. I like the gt package for making attractive tables in R. (Uncomment the line in Code Block 7 #gt:::as.tags.gt_tbl(table_3a) to see the table.) The tabular data also shows that there aren’t typos leading to duplicate categories.\nAnother way to evaluate the compactness is to make a cumulative plot. This looks at the proportion of data that is described as you add categories. I’m using the cowplot package to make multipanel figures. I want to look at both factors at once; this is fine for exploratory data analysis, but I wouldn’t recommend it for a report or presentation, since there is no connection between the two variables.\n\n# Code Block 7: Exploring the Compactness of the Categories\n\n# Exploring the jobs factor\n# bin and count the data and return sorted\ntable_3a_data &lt;- fraud %&gt;% count(job, sort = TRUE) \n\n# creating a table to go with this, but not displaying it\ntable_3a &lt;- table_3a_data %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Jobs of Card Holders\") %&gt;%\n  cols_label(job = \"Jobs\", n = \"Count\") %&gt;%\n  opt_stylize(style = 1,\n              color = \"green\",\n              add_row_striping = TRUE)\n#gt:::as.tags.gt_tbl(table_3a)  #displays the table \n\nfig_1a &lt;- ggplot(table_3a_data, aes(\n  x = 1:nlevels(fraud$job),\n  y = (cumsum(n) * 100 / nrow(fraud))\n)) +\n  geom_point(color = \"darkcyan\") +\n  geom_hline(yintercept = 80) +  #marker for 80% of the data\n  xlab(\"jobs index\") +\n  ylab(\"% of Total\") +\n  ylim(0, 100) # +\n  #ggtitle(\"Jobs of Card Holder\")  #use if standalone graph\n                       \n\n# same as above, but just for the category variable\ntable_3b_data &lt;- fraud %&gt;% count(category, sort = TRUE)\ntable_3b &lt;- table_3b_data %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Transaction Category in Credit Card Fraud\") %&gt;%\n  cols_label(category = \"Category\", n = \"Count\") %&gt;%\n  opt_stylize(style = 1,\n              color = \"blue\",\n              add_row_striping = TRUE) #%&gt;%\n#gt:::as.tags.gt_tbl(table_3b)\n\nfig_1b &lt;- ggplot(table_3b_data, aes(\n  x = 1:nlevels(fraud$category),\n  y = (cumsum(n) * 100 / nrow(fraud))\n)) +\n  geom_point(color = \"darkcyan\") +\n  geom_hline(yintercept = 80) +\n  xlab(\"category index\") +\n  ylab(\"% of Total\") +\n  ylim(0, 100) #+\n#ggtitle(\"Jobs of Card Holder\") #use if standalone graph\n\n\n#this makes the panel grid and labels it\nplot_fig_1 &lt;-\n  plot_grid(fig_1a,\n            fig_1b,\n            labels = c('A', 'B'),\n            label_size = 14)\n\n#This creates the figure title\ntitle_1 &lt;- ggdraw() +\n  draw_label(\n    \"Figure 1: Exploring Categorical Variables\",\n    fontface = 'bold',\n    x = 0,\n    hjust = 0,\n    size = 14\n  ) +\n  theme(# add margin on the left of the drawing canvas,\n    # so title is aligned with left edge of first plot\n    plot.margin = margin(0, 0, 0, 7))\n\n#this combines the panel grid, title, and displays both\nplot_grid(title_1,\n          plot_fig_1,\n          ncol = 1,\n          # rel_heights values control vertical title margins\n          rel_heights = c(0.1, 1))\n\n\n\n\nIf you look at Figure 1A, roughly 75-80 categories have to be included to capture 80% of the data. For Figure 1B, roughly ten categories have to be included. Ideally, you’d like a very steep curve initially (where a “small number” of categories cover the “majority” of the data) and then a long, shallow tail approaching 100% that corresponds to the data to be binned in “other” or dropped. There aren’t hard and fast rules on making these decisions. I decided to use 80% as my threshold. Both of these curves look relatively shallow to me, so I decided not to do any binning, grouping, or dropping of levels.\nI decided to look at all the categories of transactions just to see which ones were the most common.\n\n# Code Block 8: Exploring the Category factor\nggplot(fraud, aes(fct_infreq(category))) +\n  geom_bar(color = \"darkcyan\", fill = \"darkcyan\") +\n  ggtitle(\"Figure 2: Types of Transactions\") +\n  coord_flip() +\n  ylab(\"Count\") +\n  xlab(\"Merchant Type\")\n\n\n\n\nGas/transport was the most common category, and grocery was the second most common, both of which make sense. The least common category was travel. Nothing seemed unusual in the ranking.\n\n\n\n5.1.2. Looking at our character strings\nMerchant name (merchant) and transaction number(trans_num) are both strings. Transaction number should not influence fraud rate as it is a number assigned to the transaction when processed. I will drop it from our dataset. Merchant name could be correlated with fraud, for example, if a company’s employee was involved. However, this data is also represented by the location and category. If a location/category is found to have higher levels of fraud, then a more detailed examination of those transactions can be performed, including the merchant name. Here, I also remove it from the dataset.\n\n# Code Block 9: Removing Character/ String Variables\nfraud &lt;- fraud %&gt;%\n  select(-merchant,-trans_num)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-geographic-data",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-geographic-data",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "5.2. Looking at the geographic data",
    "text": "5.2. Looking at the geographic data\nThis data is coded as numeric (latitude and longitude) or character (city/state), but we can recognize it as geographic data and treat it appropriately.\nFirst, there are two sets of geographic data related to the merchant. The location of the merchant and where the transaction occurred. I create scatter plots of latitude and longitude separately, because I want to check the correlation between the two sources of data (merchant and transaction). I create a shared legend following the article here.\n\n# Code Block 10: Comparing Merchant and Transaction Locations\n\n# calculate correlations\ncor_lat &lt;- round(cor(fraud$lat, fraud$merch_lat), 3)\ncor_long &lt;- round(cor(fraud$long, fraud$merch_long), 3)\n\n# make figure\nfig_3a &lt;-\n  ggplot(fraud, aes(lat, merch_lat, fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5\n  ) +\n  ggtitle(\"Latitude\") +\n  ylab(\"Merchant Latitude\") +\n  xlab(\"Transaction Latitude\") +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  geom_abline(slope = 1, intercept = 0) \n\nfig_3b &lt;-\n  ggplot(fraud, aes(long, merch_long, fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5\n  ) +\n  ggtitle(\"Longitude\") +\n  ylab(\"Merchant Longitude\") +\n  xlab(\"Transaction Longitude\") +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  geom_abline(slope = 1, intercept = 0) \n\n# create the plot with the two figs on a grid, no legend\nprow_fig_3 &lt;- plot_grid(\n  fig_3a + theme(legend.position = \"none\"),\n  fig_3b + theme(legend.position = \"none\"),\n  align = 'vh',\n  labels = c(\"A\", \"B\"),\n  label_size = 12,\n  hjust = -1,\n  nrow = 1\n)\n\n# extract the legend from one of the figures\nlegend &lt;- get_legend(\n  fig_3a + \n    guides(color = guide_legend(nrow = 1)) +\n    theme(legend.position = \"bottom\")\n)\n\n# add the legend to the row of figures, prow_fig_3\nplot_fig_3 &lt;- plot_grid(prow_fig_3, legend, ncol = 1, rel_heights = c(1, .1))\n\n# title\ntitle_3 &lt;- ggdraw() +\n  draw_label(\n    \"Figure 3. Are Merchant and Transaction Coordinates Correlated?\",\n    fontface = 'bold',\n    size = 14,\n    x = 0,\n    hjust = 0\n  ) +\n  theme(plot.margin = margin(0, 0, 0, 7))\n\n# graph everything\nplot_grid(title_3,\n          plot_fig_3,\n          ncol = 1,\n          rel_heights = c(0.1, 1))\n\n\n\n\nThese two sets of data are highly correlated (for latitude = 0.994 and for longitude = 0.999) and thus are redundant. So I remove merch_lat and merch_long from the dataset.\n\n# Code Block 11: Removing merch_lat and merch_long\nfraud &lt;- fraud %&gt;%\n  select(-merch_lat,-merch_long) %&gt;%\n  rename(lat_trans = lat, long_trans = long)\n\nNext, I will look and see if some locations are more prone to fraud.\n\n# Code Block 12: Looking at Fraud by Location\nggplot(fraud, aes(long_trans, lat_trans, fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5,\n    position = \"jitter\"\n  ) +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  ggtitle(\"Figure 4: Where does fraud occur? \") +\n  ylab(\"Latitude\") +\n  xlab(\"Longitude\") \n\n\n\n\nIt looks like there are some locations which only have fraudulent transactions.\nNext, I’m going to convert city/state into latitude and longitude using the tidygeocoder package. Also included code to save this output and then re-import it. You likely do not want to be pulling the data from the internet every time you run the code, so this gives you the option to work from a local copy. For many services, it is against terms of service to repeatedly make the same calls rather than working from a local version. I did find that I could originally pull all data from ‘osm’, but while double checking this code, I found that the service is now imposing some rate limit and denies some requests, leading to some NA entries. So do check your results.\n\n# Code Block 13: Converting city/state data lat/long\n\n# need to pass an address to geo to convert to lat/long\nfraud &lt;- fraud %&gt;%\n  mutate(address = str_c(city, state, sep = \" , \"))\n\n# generate a list of distinct addresses to look up\n# the dataset is large, so it is better to only look up unique address rather that the address\n# for every record\naddress_list &lt;- fraud %&gt;%\n  distinct(address)\n\n# this has one more than number in the cities, so there must be a city with the same name in more than one state.\n\n#I don't want to run this api call everytime I open the notebook, so I downloaded the data and will reimport it and load it\n# Below is the code to run the call.  Uncomment it.\n# gets coordinates for city,states\n#home_coords &lt;-\n#  geo(address_list$address,\n#      method = \"osm\",\n#      full_results = FALSE)\n#write.csv(\"home_coords.csv\", home_coords)\n#home_coords &lt;- home_coords %&gt;%\n#  rename(lat_home = lat, long_home = long)\n\n# I downloaded it using the gui interface provided by datacamp when you view the object. This adds an extra set of \"\" compared to write.csv.\n\n# Reimport the data and load it\nhome_coords &lt;-\n  read_csv('datacamp_workspace/downloaded_coords.csv', show_col_types = FALSE)\n\n\n# imported home coords has an extra set of quotation marks\nhome_coords &lt;- home_coords %&gt;%\n  mutate(address = str_replace_all(address, \"\\\"\", \"\")) %&gt;%\n  rename(lat_home = lat, long_home = long)\n\n# use a left join on fraud and home_coords to assign the coord to every address in fraud\nfraud &lt;- fraud %&gt;%\n  left_join(home_coords, by = \"address\")\n\nNow I’m going to calculate the distance between the card holder’s home and the location of the transaction. I think distance might be a feature that is related to fraud. I followed the tutorial here for calculating distance\n\n# Code Block 14: Distance Between Home and Transaction\n\n# I believe this assuming a spherical Earth\n\n# convert to radians\nfraud &lt;- fraud %&gt;%\n  mutate(\n    lat1_radians = lat_home / 57.29577951,\n    lat2_radians = lat_trans / 57.29577951,\n    long1_radians = long_home / 57.29577951,\n    long2_radians = long_trans / 57.29577951\n  )\n\n# calculating distance\nfraud &lt;-\n  fraud %&gt;% mutate(distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)\n  ))\n\n# calculating the correlation\nfraud_distance &lt;- round(cor(fraud$distance_miles, fraud$is_fraud), 3) \n\nDespite my assumption that distance would be correlated with fraud, the correlation value is quite low, -0.003.\nI’m going to visualize it anyway.\n\n# Code Block 15: Distance from Home and Fraud\nggplot(fraud, aes(distance_miles, is_fraud , fill = factor(is_fraud))) +\n  geom_point(\n    alpha = 1,\n    shape = 21,\n    colour = \"black\",\n    size = 5,\n    position = \"jitter\"\n  ) +\n  scale_fill_viridis(\n    discrete = TRUE,\n    labels = c('Not Fraud', 'Fraud'),\n    name = \"\"\n  ) +\n  ggtitle(\"Figure 5: How far from home does fraud occur?\") +\n  xlab(\"Distance from Home (miles)\") +\n  ylab(\"Is Fraud?\") \n\n\n\n\nSome distances only have fraudulent transactions. This might be related to the locations that are only fraud, Figure 4.\nThis new feature distances_miles is retained, and the original variables (city, state) and the intermediate variables (address, variables used to calculate distance) are removed in Code Block 16.\n\n# Code Block 16: Remove Extraneous/Temp Variables\n\n# created to calculate distance\nfraud &lt;- fraud %&gt;%\n  select(-lat1_radians,-lat2_radians,-long1_radians,-long2_radians)\n\n#remove city and state and address, replaced by lat/long\nfraud &lt;- fraud %&gt;%\n  select(-city, -state, -address)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-dates",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-dates",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "5.3. Looking at the dates",
    "text": "5.3. Looking at the dates\nDate\ndob, Date of Birth of Credit Card Holder\nQuestions:\n\nWhat is the date range, and does it make sense?\nDo we have improbably old or young people?\nDo we have historic or futuristic transaction dates?\n\nI calculate the age from the dob and visualize them both.\n\n# Code Block 17: Looking at dob\n\n#summary(fraud$dob) #if you wanted a printed summary stats\n\nfig_6a &lt;- ggplot(fraud, aes(dob)) +\n  geom_histogram(color = \"darkcyan\",\n                 fill = \"darkcyan\" ,\n                 bins = 10) +\n  #ggtitle(\"How old are card Holders?\") +\n  ylab(\"Count\") +\n  xlab(\"Date of Birth\") \n\nfraud &lt;- fraud %&gt;%\n  #mutate (age = trunc((dob %--% today()) / years(1))) #if you wanted to calculate age relative to today\n  mutate(age = trunc((\n    dob %--% min(fraud$trans_date_trans_time)\n  ) / years(1)))\n#summary(fraud$age) #if you wanted a printed summary stats\n\n\nfig_6b &lt;- ggplot(fraud, aes(age)) +\n  geom_histogram(color = \"darkcyan\",\n                 fill = \"darkcyan\",\n                 bins = 10) +\n  #ggtitle(\"How old are card holders?\") +\n  ylab(\"Count\") +\n  xlab(\"Age\") \nplot_fig_6 &lt;- plot_grid(fig_6a, fig_6b, labels = c('A', 'B'))\n\ntitle_6 &lt;- ggdraw() +\n  draw_label(\n    \"Figure 6. How old are the card holders?\",\n    fontface = 'bold',\n    x = 0,\n    hjust = 0\n  ) +\n  theme(# add margin on the left of the drawing canvas,\n    # so title is aligned with left edge of first plot\n    plot.margin = margin(0, 0, 0, 7))\nplot_grid(title_6,\n          plot_fig_6,\n          ncol = 1,\n          # rel_heights values control vertical title margins\n          rel_heights = c(0.1, 1))\n\n\n\ntable_4_data &lt;- fraud %&gt;% count(age)\n\ntable_4 &lt;- table_4_data %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Ages of Card Holders\") %&gt;%\n  cols_label(age = \"Ages\", n = \"Count\") %&gt;%\n  opt_stylize(style = 1,\n              color = \"green\",\n              add_row_striping = TRUE)\ngt:::as.tags.gt_tbl(table_4)\n\n\n\n\n\n  \n    \n      Ages of Card Holders\n    \n    \n    \n      Ages\n      Count\n    \n  \n  \n    17\n2190\n    18\n2198\n    19\n6592\n    20\n741\n    21\n2207\n    22\n31\n    23\n3663\n    25\n3659\n    26\n6587\n    27\n4378\n    28\n2926\n    29\n8772\n    30\n17525\n    31\n16093\n    32\n2940\n    33\n10220\n    34\n11675\n    35\n2204\n    36\n10218\n    37\n9489\n    38\n8049\n    39\n5120\n    40\n8017\n    41\n1467\n    42\n6592\n    43\n5847\n    44\n14619\n    45\n14608\n    46\n13888\n    47\n8015\n    48\n1500\n    49\n5135\n    50\n1482\n    51\n12449\n    52\n7330\n    53\n5855\n    54\n2922\n    55\n3684\n    56\n735\n    57\n7340\n    58\n2201\n    59\n7318\n    60\n2194\n    61\n2951\n    62\n5878\n    63\n5143\n    64\n5115\n    65\n3657\n    66\n737\n    67\n4391\n    68\n6582\n    70\n1461\n    72\n8\n    73\n2939\n    75\n1463\n    76\n1472\n    77\n753\n    78\n2951\n    79\n5118\n    80\n11\n    81\n1465\n    82\n2926\n    83\n4393\n    86\n744\n    89\n2929\n    90\n3652\n    91\n2193\n  \n  \n  \n\n\n\n\nThe ages seem reasonable (calculated relative to the earliest date of transactions). There are a few thousand 17-year-olds, which is too young to have their own credit card, but it is plausible that they would be an authorized user on their parents’ card. age seems a more reasonable variable than dob, so dob is also dropped from the dataset. For example, scammers might be more likely to target 90-year-olds. The age is the feature that leads to them being targeted, not the birth year. The birth year is related to age through the current date- in 10 years, a new cohort of birth years would be targeted if age is the important feature. So the age feature is more robust to passing time than dob.\n\n# Code Block 18: Removing dob\n\nfraud &lt;- fraud %&gt;%\n  select(-dob)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-date-times",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-date-times",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "5.4. Looking at the date-times",
    "text": "5.4. Looking at the date-times\ndate-time\ntrans_date_trans_time, Transaction DateTime\nQuestions\nWould processing the date-times yield more useful predictors?\nFirst, I want to look at variation in the number of transactions with date-time. I chose to use a histogram with bins corresponding to one month widths.\n\n# Code Block 19: Looking at Transaction Date/ Times\n\nggplot(fraud, aes(trans_date_trans_time)) +\n  geom_histogram(color = \"darkcyan\",\n                 fill = \"darkcyan\",\n                 bins = 24) + #24 months in dataset\n  ggtitle(\"Figure 7: When do Transactions occur\") +\n  ylab(\"Count\") +\n  xlab(\"Date/ Time\")\n\n\n\n\nNext, I will break the transaction date-time into day of the week, hour, and the date only. I’m doing this here with lubridate functions, but I could also do this in the model building section, when I create recipes by using step_date(). I will also graph transactions by day of the week.\n\n# Code Block 20: \n\nfraud &lt;- fraud %&gt;%\n  mutate(\n    date_only = date(trans_date_trans_time),\n    hour = hour(trans_date_trans_time),\n    weekday = wday(trans_date_trans_time)\n  )\n\nggplot(fraud, aes(weekday)) +\n  geom_histogram(\n    color = \"darkcyan\",\n    fill = \"darkcyan\",\n    binwidth = 1,\n    center = 0.5\n  ) +\n  ggtitle(\"Figure 7: On what days do transactions occur?\") +\n  ylab(\"Count\") +\n  xlab(\"Weekday\")\n\n\n\n\nMonday has the highest number of transactions; this could be due to businesses processing orders that came in over the weekend. By default, lubridate codes the day of the week as a number where 1 means Monday, 7 means Sunday.\nNow, I look at what time of day do most transactions occur?\n\n# Code Block 21: What time do transactions occur\nfig_8a &lt;- ggplot(fraud, aes(hour)) +\n  geom_boxplot(color = \"darkcyan\") +\n  #ggtitle(\"What hour do transactions occur\") +\n  ylab(\"Count\") +\n  xlab(\"Hour\")\n\n\nfig_8b &lt;- ggplot(fraud, aes(hour)) +\n  geom_bar(fill = \"darkcyan\") +\n  #ggtitle(\"What hour do transactions occur\") +\n  ylab(\"Count\") +\n  xlab(\"Hour\") \n\nplot_fig_8 &lt;- plot_grid(fig_8a, fig_8b, labels = c('A', 'B'))\n\ntitle_8 &lt;- ggdraw() +\n  draw_label(\n    \"Figure 8. When do transactions occur?\",\n    fontface = 'bold',\n    x = 0,\n    hjust = 0\n  ) +\n  theme(\n    plot.margin = margin(0, 0, 0, 7))\nplot_grid(title_8,\n          plot_fig_8,\n          ncol = 1,\n          # rel_heights values control vertical title margins\n          rel_heights = c(0.1, 1))\n\n\n\n\nThis data honestly looks funny to me. I might expect that most transactions would occur during normal business hours (~9-5) or more occur during lunch or after work, but what we see is a lower number of transactions from midnight to ~ 2 pm and then a higher number of transactions from 2 pm until midnight. The odd pattern could be a sign that something is wrong with the data (perhaps timezones aren’t being encoded properly?), or it could be simply a lack of subject matter knowledge (for example, transactions are pre-authorized at the time of sale and processed later, and the transaction time is the processing time, not the sale time.) Of course, this is also a synthetic dataset, so this pattern may be simply the result of user input choices when the set was generated. If this were a real dataset, I’d chase this down.\nAnd I made a table too, just to look at this data in another way.\n\n# Code Block 22:\ntable_5_data &lt;- fraud %&gt;% count(hour)\n\ntable_5 &lt;- table_5_data %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Transactions by Time of Day\") %&gt;%\n  cols_label(hour = \"Hour\", n = \"Count\") %&gt;%\n  opt_stylize(style = 1,\n              color = \"green\",\n              add_row_striping = TRUE)\ngt:::as.tags.gt_tbl(table_5)\n\n\n\n\n\n  \n    \n      Transactions by Time of Day\n    \n    \n    \n      Hour\n      Count\n    \n  \n  \n    0\n11039\n    1\n11241\n    2\n11019\n    3\n11227\n    4\n10904\n    5\n11023\n    6\n11145\n    7\n11094\n    8\n11123\n    9\n10997\n    10\n11123\n    11\n11016\n    12\n17168\n    13\n17125\n    14\n16879\n    15\n17169\n    16\n17465\n    17\n17011\n    18\n17021\n    19\n17277\n    20\n17298\n    21\n17267\n    22\n17460\n    23\n17516\n  \n  \n  \n\n\n\n\nStill weird.\nI discard the original variable and keep the new variables.\n\n# Code Block 23:\n#removing the original variable and keeping the component variables.\nfraud &lt;- fraud %&gt;%\n  select(-trans_date_trans_time)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-numerical-variables",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#looking-at-the-numerical-variables",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "5.5. Looking at the numerical variables",
    "text": "5.5. Looking at the numerical variables\nNumerical\namt, transaction amount\nQuestions\nWould transforming this data produce a more normal distribution?\nGenerally, more normal or at least more symmetric data tends to be fitted better, especially when using model-fitting algorithms that arise from statistics rather than pure machine learning.\nI compare the original data with the log-transformed data.\n\n# Code Block 24:\nfig_9a &lt;- ggplot(fraud, aes(amt)) +\n  geom_histogram(color = \"darkcyan\", fill = \"darkcyan\", bins = 50) +\n  #ggtitle(\"Amount of Transaction\") +\n  ylab(\"Count\") +\n  xlab(\"purchase amount ($)\")\n\nfig_9b &lt;- ggplot(fraud, aes(log(amt))) +\n  geom_histogram(color = \"darkcyan\", fill = \"darkcyan\", bins = 50) +\n  #ggtitle(\"log(Amount) of Transaction\") +\n  ylab(\"Count\") +\n  xlab(\"log(purchase amount) ($)\")\n\nplot_fig_9 &lt;-\n  plot_grid(fig_9a, fig_9b, labels = c('A', 'B'), label_size = 12)\ntitle_9 &lt;- ggdraw() +\n  draw_label(\n    \"Figure 9. Distribution of amount and log(amount)\",\n    fontface = 'bold',\n    x = 0,\n    hjust = 0\n  ) +\n  theme(\n    plot.margin = margin(0, 0, 0, 7))\nplot_grid(title_9,\n          plot_fig_9,\n          ncol = 1,\n          rel_heights = c(0.1, 1))\n\n\n\n\nThe transformed data is more symmetric so that the transformed variable will be retained.\n\n# Code Block 25:\nfraud &lt;- fraud %&gt;%\n  mutate(amt_log = log(amt))\n\nI do a final clean-up of variables next. I remove some variables that I don’t think will impact fraud- the population of the home city and the location of the home. I don’t think the home should have an impact on fraud; it is where the card is used, not where it is billed, that should matter. I suppose you could have a neighborhood where all the mail was being stolen, and cards were compromised that way, but I think most cards get compromised at the point of sale.\nI also removed the date. The date itself is unlikely to be related to fraud. It is possible that special dates are correlated with fraud, like a holiday or a big sports match. Engineering a holiday feature could be a future improvement.\nThere is a possibility that job type could have an impact on fraud; for example, a trucker might be more likely to have his/her card stolen just because they are always on the road and visiting a wide variety of places where they would use the card. Or this could come in as an interaction term with distance; distance from home and the occupation trucker might have no correlation, but the distance from home and the occupation teacher might have because it would be a more unusual event for that job. However, some model fitting fails to converge when job is included, and it takes a long time for the models that it does work for. So I remove it too.\n\n# Code Block 26:\n# removed in related clusters, so easy to comment out if you want to add back a group\n\n# remove amt and keep log transformed version\nfraud &lt;- fraud %&gt;%\n  select(-amt)\n\n# home location and home city pop shouldn't impact fraud\nfraud &lt;- fraud %&gt;%\n  select(-city_pop,-lat_home,-long_home)\n\n# remove date\nfraud &lt;- fraud %&gt;% select(-date_only)\n\n# remove jobs\nfraud &lt;- fraud %&gt;%\n  select(-job)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#splitting-the-data",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#splitting-the-data",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.1. Splitting the data",
    "text": "7.1. Splitting the data\nFirst, preparation work. Here, I split the data into a testing and training set. I also create folds for cross-validation from the training set.\n\n# Code Block 30 : Train/Test Splits & CV Folds \n\n# Split the data into a test and training set\nset.seed(222)\ndata_split &lt;-\n  initial_split(fraud, prop = 0.75, strata = is_fraud)\n\n# Create data frames for the two sets:\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\nstart_time &lt;- Sys.time()\n\nset.seed(123)\nfraud_folds &lt;- vfold_cv(train_data, v = 3, strata = is_fraud)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#creating-recipes",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#creating-recipes",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.2. Creating recipes",
    "text": "7.2. Creating recipes\nNext, I create recipes that do preprocessing of the data- making dummy variables, normalizing, and removing variables that only contain one value (step_zv(all_predictors())). The processing will be applied to both the training and testing data as you move through the workflow.\nI used the chart found in Appendix A of the Tidy Modeling with R by Max Kuhn and Julia Silge to choose the preprocessing of data. Some models require specific types of preprocessing, others don’t require it, but it can produce better or faster fitting, and in other cases, the preprocessing isn’t required and probably doesn’t help. The chart breaks this down for each category of preprocessing model by model. The same preprocessing steps were required or recommended for the models I chose, so I used them across the board. You can create recipes for different models and build a workflow manually to match the models to the proper recipe. This process is covered extensively in Chapter 15 of Tidy Modeling with R.\nI use the selector functions (all_nominal_predictors(), all_numerical_predictors(), etc.) available in the tidymodels framework. A listing of all selector functions usable in tidymodels can be found here. Using selector functions when handling groups of features reduces the chance of mistakes and typos.\nI then modify this recipe to handle the imbalanced class problem. I use SMOTE and ROSE hybrid methods to balance the classes. These methods create synthetic data for the minority class and downsample the majority class to balance the classes. I also use downsample, which throws away majority class records to balance the two classes. A good overview is here, and it also provides a tutorial for handling this type of problem with caret, rather than tidymodels. These recipe steps require the themis package.\n\n# Code Block 31: creating recipes\n\nrecipe_plain &lt;-\n  recipe(is_fraud ~ ., data = train_data) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nrecipe_rose &lt;-\n  recipe_plain %&gt;%\n  step_rose(is_fraud)\n\nrecipe_smote &lt;-\n  recipe_plain %&gt;%\n  step_smote(is_fraud)\n\nrecipe_down &lt;-\n  recipe_plain %&gt;%\n  step_downsample(is_fraud)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#setting-the-model-engines",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#setting-the-model-engines",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.3. Setting the model engines",
    "text": "7.3. Setting the model engines\nNext, I set the engines for the models. I tune the hyperparameters of the elastic net logistic regression and the lightgbm. Random Forest also has tuning parameters, but the random forest model is pretty slow to fit, and adding tuning parameters makes it even slower. If none of the other models worked well, then tuning RF would be a good idea.\n\n# Code Block 32: Setting engines\n\n#this is the standard logistic regression\nlogreg_spec &lt;-\n  logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\n#elastic net regularization of logistic regression\n#this has 2 hyperparameters that we will tune\nglmnet_spec &lt;-\n  logistic_reg(penalty = tune(),\n               mixture = tune()) %&gt;%\n  set_engine(\"glmnet\")\n\n#random forest also has tunable hyperparameters, but we won't\nrf_spec &lt;-\n  rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n#This is a boosted gradient method with 6 tuning parameters\nlightgbm_spec &lt;-\n  boost_tree(\n    mtry = tune(),\n    trees = tune(),\n    tree_depth = tune(),\n    learn_rate = tune(),\n    min_n = tune(),\n    loss_reduction = tune()\n  ) %&gt;%\n  set_engine(engine = \"lightgbm\") %&gt;%\n  set_mode(mode = \"classification\")"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#creating-a-metrics-set",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#creating-a-metrics-set",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.4. Creating a metrics set",
    "text": "7.4. Creating a metrics set\nLastly, I create a metrics set in Code Block 33. Accuracy is generally a terrible metric for highly imbalanced problems; the model can achieve high accuracy by assigning everything to the majority class. Alternate metrics like sensitivity or j-index are better choices for the imbalanced class situation.\n\n# Code Block 33: Setting Metrics\n\nfraud_metrics &lt;-\n  metric_set(roc_auc, accuracy, sensitivity, specificity, j_index)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#creating-the-workflow_set",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#creating-the-workflow_set",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.5. Creating the workflow_set",
    "text": "7.5. Creating the workflow_set\nNext, I create the workflow_set. This is where tidymodels shines. I feed it the 4 recipes and the 4 engines, and it makes all the permutations to fit. (As I mentioned earlier, you can manually create a workflow_set where you assign specific recipes to specific models, but here all recipes work with all models.)\n\n# Code block 34:\nwf_set_tune &lt;-\n  workflow_set(\n    list(plain = recipe_plain,\n         rose = recipe_rose,\n         smote = recipe_smote,\n         down = recipe_down),\n    list(glmnet = glmnet_spec,\n      lightgmb = lightgbm_spec,\n      rf = rf_spec,\n      logreg = logreg_spec\n     )\n  )"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#fitting-all-the-models",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#fitting-all-the-models",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.6. Fitting all the models",
    "text": "7.6. Fitting all the models\nI now run these 16 models. I pass workflow_map() the workflow_set from Code Block 34. The next parameter is what type of fitting you want to do. Here, I used tune_grid and had it generate 6 grid points. For the models that don’t require hyperparameter tuning, the function defaults to fit_resamples instead. The acceptable types of fitting functions are found here. It is important to note that you can only use fitting methods that operate on folds; you cannot pass workflow_map() the entire train or test set and have it work.\nI’m using the verbose option when fitting. This shows how long each model takes. When I first started, I had no idea how long various models would take. I’m running this on an older, low-end laptop (Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz 2.71 GHz, 32 GB RAM).\nI would recommend 10 folds rather than 3 if you have the time. Similarly, 6 grids points is a very low number.\n\n# Code block 35: \nset.seed(345)\ntune_results &lt;-\n  workflow_map(\n    wf_set_tune,\n    \"tune_grid\",\n    resamples = fraud_folds,\n    grid = 6,\n    metrics = fraud_metrics,\n    verbose = TRUE\n    )\n\ni  1 of 16 tuning:     plain_glmnet\n\n\n✔  1 of 16 tuning:     plain_glmnet (2m 28.3s)\n\n\ni  2 of 16 tuning:     plain_lightgmb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔  2 of 16 tuning:     plain_lightgmb (4m 58.1s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni  3 of 16 resampling: plain_rf\n\n\n✔  3 of 16 resampling: plain_rf (51.7s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni  4 of 16 resampling: plain_logreg\n\n\n✔  4 of 16 resampling: plain_logreg (9.9s)\n\n\ni  5 of 16 tuning:     rose_glmnet\n\n\n✔  5 of 16 tuning:     rose_glmnet (4m 1.2s)\n\n\ni  6 of 16 tuning:     rose_lightgmb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔  6 of 16 tuning:     rose_lightgmb (11m 46s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni  7 of 16 resampling: rose_rf\n\n\n✔  7 of 16 resampling: rose_rf (24m 23.1s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni  8 of 16 resampling: rose_logreg\n\n\n✔  8 of 16 resampling: rose_logreg (18.1s)\n\n\ni  9 of 16 tuning:     smote_glmnet\n\n\n✔  9 of 16 tuning:     smote_glmnet (5m 45.1s)\n\n\ni 10 of 16 tuning:     smote_lightgmb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 10 of 16 tuning:     smote_lightgmb (7m 24.4s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 11 of 16 resampling: smote_rf\n\n\n✔ 11 of 16 resampling: smote_rf (2m 41.2s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 12 of 16 resampling: smote_logreg\n\n\n✔ 12 of 16 resampling: smote_logreg (12.4s)\n\n\ni 13 of 16 tuning:     down_glmnet\n\n\n✔ 13 of 16 tuning:     down_glmnet (9.9s)\n\n\ni 14 of 16 tuning:     down_lightgmb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 14 of 16 tuning:     down_lightgmb (2m 25.6s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 15 of 16 resampling: down_rf\n\n\n✔ 15 of 16 resampling: down_rf (8.4s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 16 of 16 resampling: down_logreg\n\n\n✔ 16 of 16 resampling: down_logreg (2.7s)"
  },
  {
    "objectID": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#evaluating-the-models",
    "href": "posts/2023-04-11-credit-card-fraud/fraud_tutorial.html#evaluating-the-models",
    "title": "Credit Card Fraud: A Tidymodels Tutorial",
    "section": "7.7. Evaluating the models",
    "text": "7.7. Evaluating the models\nI viewed the results of the fitting as both a table and graphically using autoplot(). The default autoplot legend is unclear, so you’ll want to do both, as I did. The legend doesn’t label by recipe (only that a recipe was used for preprocessing) and folds related categories into one. Here you see that elastic net logistic regression and logistic regression are both labeled log_reg.\nThe object we have now, tune_results, is incredibly large and complicated. Using View() on it has crashed RStudio for me. This object should be interacted with through helper functions. For more information about this, please see my other tutorial on tidymodels.\n\n#|label: rank-results-table\n# Code Block 35\nrank_results(tune_results, rank_metric = \"j_index\")\n\n# A tibble: 280 × 9\n   wflow_id       .config   .metric  mean std_err     n preprocessor model  rank\n   &lt;chr&gt;          &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n 1 down_lightgmb  Preproce… accura… 0.957 0.00240     3 recipe       boos…     1\n 2 down_lightgmb  Preproce… j_index 0.915 0.00483     3 recipe       boos…     1\n 3 down_lightgmb  Preproce… roc_auc 0.992 0.00100     3 recipe       boos…     1\n 4 down_lightgmb  Preproce… sensit… 0.959 0.00407     3 recipe       boos…     1\n 5 down_lightgmb  Preproce… specif… 0.956 0.00241     3 recipe       boos…     1\n 6 smote_lightgmb Preproce… accura… 0.967 0.00170     3 recipe       boos…     2\n 7 smote_lightgmb Preproce… j_index 0.915 0.00310     3 recipe       boos…     2\n 8 smote_lightgmb Preproce… roc_auc 0.990 0.00102     3 recipe       boos…     2\n 9 smote_lightgmb Preproce… sensit… 0.948 0.00399     3 recipe       boos…     2\n10 smote_lightgmb Preproce… specif… 0.967 0.00172     3 recipe       boos…     2\n# ℹ 270 more rows\n\n\n\n# Code Block 36\nautoplot(tune_results, rank_metric = \"j_index\", select_best = TRUE) +\n  ggtitle(\"Figure 11: Performance of various models\")\n\n\n\n\nThe best performing model / recipe pair by j-index is the downsampled lightgmb (down_lightgmb).\nTo see how this model/recipe performs across tuning parameters, we can use extract_workflow_set_result and autoplot. If you wanted to refine the hyperparameters more, you could use these results to narrow the search parameters to areas with the best performance.\n\n# Code block 37: \n\nresults_down_gmb &lt;- tune_results %&gt;%\n  extract_workflow_set_result(\"down_lightgmb\")\nautoplot(results_down_gmb) +\n  theme_pander(8) +\n  ggtitle(\"Figure 12: Perfomance of different hyperparameters\")\n\n\n\n\nIn this case, I’m just going to extract the best set of hyperparameters and move on. This is done using the extract_workflow_set_result and select_best(metric = \"j_index\"). There are other ways to select the best hyperparameters. The list of selectors is found here.\n\n# Code block 38: \nbest_hyperparameters &lt;- tune_results %&gt;%\n  extract_workflow_set_result(\"down_lightgmb\") %&gt;%\n  select_best(metric = \"j_index\")\n\nAnd here I look at the selected hyperparameters.\n\n# Code block 39: \nprint(best_hyperparameters)\n\n# A tibble: 1 × 7\n   mtry trees min_n tree_depth learn_rate loss_reduction .config             \n  &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1    19  1679    35         15     0.0279        0.00249 Preprocessor1_Model6\n\n\nNow, I am going to use the convenience functions finalize_workflow() and last_fit() to add the best hyperparameters to the workflow, train the model/recipe on the entire training set, and then predict on the entire test set. There is a lot of stuff going on here at once (Code Block 40)!\n\n# Code Block 40: Validating the model with the test data\nvalidation_results &lt;- tune_results %&gt;%\n  extract_workflow(\"down_lightgmb\") %&gt;%\n  finalize_workflow(best_hyperparameters) %&gt;%\n  last_fit(split =  data_split, metrics = fraud_metrics)\n\nLastly, I look at the metrics and ROC curve for the test data.\n\n# Code Block 41: Looking at the validation metrics from the test data.\ncollect_metrics(validation_results)\n\n# A tibble: 5 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.962 Preprocessor1_Model1\n2 sensitivity binary         0.950 Preprocessor1_Model1\n3 specificity binary         0.963 Preprocessor1_Model1\n4 j_index     binary         0.912 Preprocessor1_Model1\n5 roc_auc     binary         0.992 Preprocessor1_Model1\n\nvalidation_results %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(is_fraud, .pred_1) %&gt;%\n  autoplot() +\n  ggtitle(\"Figure 13: ROC Curve\")\n\n\n\n\nJust for fun, let’s see how much money this model would have save our credit card company. I’m going to assume the cost of fraud is the cost of the transaction. I calculate the total cost of all the fraudulent transactions in the test dataset. I then calculate the cost based on the model predictions. Any truly fraudulent transactions that were not caught, cost the value of the transaction. Legitimate transactions that were marked as fraud were assigned $0 cost. This likely isn’t true. There is the cost of having to deal with customers calling because the transaction was declined or the cost sending out texts for suspicious transactions, but this cost is very small relative to the cost of a fraudulent transaction. I got the idea from this paper: Zhang, D. , Bhandari, B. and Black, D. (2020) Credit Card Fraud Detection Using Weighted Support Vector Machine. Applied Mathematics, 11, 1275-1291. doi: 10.4236/am.2020.1112087.\nI’m using the list method to access predictions, but you could also use collect_predictions().\n\n#code block 42: Calculating how much fraud cost the company\n\nval &lt;- validation_results[[5]][[1]]\n\nval %&gt;% conf_mat(truth = is_fraud, estimate = .pred_class)\n\n          Truth\nPrediction     1     0\n         1   435  3161\n         0    23 81283\n\nval &lt;-\n  #I'm going to bind this to the test data and I want unique names\n  val %&gt;% rename(is_fraud2  = is_fraud) \n\ncost &lt;- test_data %&gt;%\n  cbind(val)\n\ncost &lt;- cost %&gt;%\n  select(is_fraud, amt_log, pred = .pred_class, is_fraud2) \n\ncost &lt;- cost %&gt;%\n  #cost for missing fraud in prediction\n  mutate(cost_act = ifelse((is_fraud == 1 &\n                              pred == 0), amt_log, 0)) %&gt;%\n  #cost of all fraud\n  mutate(cost_potential = ifelse((is_fraud == 1), amt_log, 0))\n\nmissed_fraud_cost &lt;- round(sum(exp(cost$cost_act)), 2)\nall_fraud_cost &lt;- round(sum(exp(cost$cost_potential)), 2)\n\nsavings &lt;- 100 * round((sum(exp(cost$cost_act)) / sum(exp(cost$cost_potential))), 2)\n\nMy model had dramatic costs savings for the imaginary credit card company! The losses from the model were 27 % of the potential losses."
  },
  {
    "objectID": "posts/2023-04-04-chart-challenge-4/day4.html",
    "href": "posts/2023-04-04-chart-challenge-4/day4.html",
    "title": "30 Day Chart Challenge- Endangered Species",
    "section": "",
    "text": "It is Day 4 of the #30DayChartChallenge. More info can be found at the challenge’s Github page. Today’s theme is history. But this is a subtheme of “comparisions”, so I’d like to avoid doing a simple time series.\nI decided to look at the endangered species list the US Fish and Wildlife Service maintains. They have a bunch of data spread over multiple tables. I decided to look at the 5 year review data. A 5 year review is the assessment to decide if a species remains list or delisted. The dataset also contains the year the species was first listed. So I’d like to compare how many species have been listed vs. delisted.\nThe key to the different listing types is found here.\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(skimr)\nlibrary(waffle)\n\nToday, I’m was going to load the data directly from the website. I’ve been downloading it and reading it in from a local folder, but I thought it would be nice to download directly. However, the data uses a “blob:” url, which is not donwloadable directly. There is a way around this but then you have to process some JSON data. I”ll come back to this later, but for now, I’m just going to use a csv.\n\nendangered_df &lt;- read_csv(\"five_year.csv\", show_col_types = FALSE)\n\n\nendangered_df_sub &lt;- endangered_df %&gt;%\n  select(name = `Common Name`, \n         status = `ESA Listing Status`, \n         date = `Listing Date`,\n         rec = `5YSR Recommendation`)\n\nLet’s see what kind of categories we have.\n\nendangered_df_sub &lt;- endangered_df_sub %&gt;%\n  mutate(status = factor(status), rec = factor(rec))\n\nSkim this bad boy.\n\nskim(endangered_df_sub) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      skim_type\n      skim_variable\n      n_missing\n      complete_rate\n      character.min\n      character.max\n      character.empty\n      character.n_unique\n      character.whitespace\n      factor.ordered\n      factor.n_unique\n      factor.top_counts\n      numeric.mean\n      numeric.sd\n      numeric.p0\n      numeric.p25\n      numeric.p50\n      numeric.p75\n      numeric.p100\n      numeric.hist\n    \n  \n  \n    character\nname\n0\n1\n3\n51\n0\n1159\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n    factor\nstatus\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n8\nE: 1173, T: 316, DM: 35, DNS: 3\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n    factor\nrec\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n7\nNo : 1389, Del: 49, Dow: 40, Del: 27\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n    numeric\ndate\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1993.5\n12.4735\n1967\n1987\n1993\n1999\n2017\n▂▃▇▂▃\n  \n  \n  \n\n\n\n\nrec\n\nsummary(endangered_df_sub$rec)\n\n                    Delist: The listed entity does not meet the statutory definition of a species \n                                                                                                8 \nDelist: The species does not meet the definition of an endangered species or a threatened species \n                                                                                               49 \n                                                                   Delist: The species is extinct \n                                                                                               27 \n                                                                                    Downlist to T \n                                                                                               40 \n                                                                              No change in Status \n                                                                                             1389 \n                                                                        Revision of listed entity \n                                                                                                2 \n                                                                                      Uplist to E \n                                                                                               18 \n\n\nThe recommendations don’t always match the current status. I’m assuming the recommendations will be enacted/adopted eventually, so I am using them as the correct current status.\nWe have 7 levels in recommendations. We need to consolidate them. I’m going to combine “Delist: The listed entity does not meet the statutory definition of a species” and “Delist: The species does not meet the definition of an endangered species or a threatened species” into a level called delisted. The delisting because the species is extinct will be made into a level called extinct later.\n\nendangered_df_sub &lt;- endangered_df_sub %&gt;%\n  mutate(condensed = fct_collapse(rec, delisted = c(\"Delist: The listed entity does not meet the statutory definition of a species\",\n    \"Delist: The species does not meet the definition of an endangered species or a threatened species\")\n  ))\n\nI’m going to count both “Downlist to threatened” and “uplist to Endangered” as endangered. I don’t know the original listing level, so it doesn’t make too much difference to me.\n\nendangered_df_sub &lt;- endangered_df_sub %&gt;%\n  mutate(condensed = fct_collapse(condensed, endangered = c(\"Downlist to T\",\n    \"Uplist to E\")  ))\n\nNow, I’m pulling in the status for the entries that have “No change in Status” as the recommendation. I’m using a case_when and listing every combination. I could get this done if fewer lines if I used or statements (E or T is endangered), but I left it more granular in case I wanted to come back and change the levels. Maybe later I do care about the different between threatened and endangered and want to break them out separately.\n\nendangered_df_sub &lt;- endangered_df_sub %&gt;%\n  mutate(condensed = case_when(\n    condensed == \"No change in Status\" & status == \"E\" ~ \"endangered\",\n    condensed == \"No change in Status\" & status == \"T\" ~ \"endangered\",\n    condensed == \"No change in Status\" & status == \"RT\" ~ \"delisted\",\n    condensed == \"No change in Status\" & status == \"D3A\" ~ \"extinct\",\n    condensed == \"No change in Status\" & status == \"DM\" ~ \"delisted\",\n    condensed == \"No change in Status\" & status == \"DP\" ~ \"delisted\",\n    condensed == \"No change in Status\" & status == \"DR\" ~ \"delisted\",\n    condensed == \"No change in Status\" & status == \"DNS\" ~ \"delisted\",\n    condensed != \"No change in Status\" ~ condensed)\n    )\n\nNow I’m going to group my extincts.\n\nendangered_df_sub &lt;- endangered_df_sub %&gt;%\n  mutate(condensed = \n           fct_collapse(condensed, extinct = \n                          c(\"Delist: The species is extinct\", \"extinct\")))\n\nI’m not sure what : Revision of listed entity means. I’m going to see if there are comments back in the full dataset.\n\nendangered_df %&gt;% \n  filter(`5YSR Recommendation` == \"Revision of listed entity\") %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      Scientific Name\n      Common Name\n      Where Listed\n      ESA Listing Status\n      Lead Region\n      Listing Date\n      Most Recently Completed 5YSR\n      5YSR Recommendation\n      Notice of In Progress 5YSR\n      Notice Date of In Progress 5YSR\n      Group\n    \n  \n  \n    Rangifer tarandus ssp. caribou\nCaribou DPS, Southern Mountain\n&lt;div&gt;Southern Mountain DPS&lt;/div&gt;\nE\n1\n1983\n2019-10-02\nRevision of listed entity\nNo Five Year Review In Progress\nNA\nMammals\n    Cereus eriophorus var. fragrans\nPrickly-apple, fragrant\n&lt;div&gt;&lt;/div&gt;\nE\n4\n1985\n2021-10-19\nRevision of listed entity\nNo Five Year Review In Progress\nNA\nFlowering Plants\n  \n  \n  \n\n\n\n\nI’m not seeing any explanation. There is not an entry in the code key either.\nOkay, now for a visualization. This actually seems perfect for a waffle. I’ve had bad luck with the waffle package, but know how to make it output something now. So, I will try waffling again. I did try a different package (ggwaffle) that also doesn’t work. It does let you use a dataframe, but it also doesn’t handle large numbers well. It soes let you downsample the data if the numbers are too large, but I’d rather just process the data myself to make it waffle.\nSo, first I need to summarize the data to get the counts per class.\n\nprogress &lt;- endangered_df_sub %&gt;%\n  count(condensed)\n\nprogress %&gt;% \n  gt() %&gt;%\n  cols_label(condensed = \"Status\", n = \"Number of species\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Progess of Endangered/Threatened species\")\n\n\n\n\n\n  \n    \n      Progess of Endangered/Threatened species\n    \n    \n    \n      Status\n      Number of species\n    \n  \n  \n    extinct\n27\n    delisted\n64\n    endangered\n1440\n    Revision of listed entity\n2\n  \n  \n  \n\n\n\n\nNow let’s change to percentages for optimal waffling\n\nnum_species &lt;- nrow(endangered_df_sub)\nprogress_percent &lt;- progress %&gt;%\n  mutate(n = ( n/num_species) * 100)\n\nprogress_percent &lt;- progress_percent %&gt;%\n  mutate(n = round(n,1))\n\ngt(progress_percent) %&gt;%\ncols_label(condensed = \"Status\", n = \"% of species\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Progess of Endangered/Threatened species\") \n\n\n\n\n\n  \n    \n      Progess of Endangered/Threatened species\n    \n    \n    \n      Status\n      % of species\n    \n  \n  \n    extinct\n1.8\n    delisted\n4.2\n    endangered\n93.9\n    Revision of listed entity\n0.1\n  \n  \n  \n\n\n\n#Values below 1 won't show in a waffle graph anyway, so remove them.\nprogress_percent &lt;- progress_percent %&gt;%\n  filter(n &gt;= 1)\n\nThe waffle package won’t work with dataframes for me, so make it a vector.\n\nprogress_vec = deframe(progress_percent)\n\n\nwaffle::waffle(progress_vec, colors = c(\"black\", \"darkgreen\", \"darkred\"),\n               title = \"How has the US done with our Endangered species?\",\n               xlab = \"1 square = 1%\") \n\n\n\n\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {30 {Day} {Chart} {Challenge-} {Endangered} {Species}},\n  date = {2023-04-04},\n  url = {https://lsinks.github.io/posts/2023-04-04-chart-challenge-4/day4},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “30 Day Chart Challenge- Endangered\nSpecies.” April 4, 2023. https://lsinks.github.io/posts/2023-04-04-chart-challenge-4/day4."
  },
  {
    "objectID": "posts/2023-04-03-chart-challenge-3/day3.html",
    "href": "posts/2023-04-03-chart-challenge-3/day3.html",
    "title": "30 Day Chart Challenge- Flora and Fauna",
    "section": "",
    "text": "It is Day 3 of the #30DayChartChallenge. More info can be found at the challenge’s Github page. Today’s theme is flora and fauna. I found a trove of fascinating data at Global Assessment of Reptile Distributions. I chose the dataset on body size/ mass from the paper: “Different solutions lead to similar life history traits across the great divides of the amniote tree of life.” by Shai Meiri, Gopal Murali, Anna Zimin, Lior Shak, Yuval Itescu, Gabriel Caetano, and Uri Roll (Journal of Biological Research-Thessaloniki), 202128: 3.\nOkay, let’s go. I’m going to keep the libraries to a minimum. That’s always my goal, but yet I ended up with 6!\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(gt)\nlibrary(ggthemes)\nlibrary(cowplot)\nlibrary(magick)\n\n\nfauna &lt;- read_csv(\"animals.csv\", show_col_types = FALSE)\n\nI’m going to change the class, clade, order, family and bionomial_2020 to factors. I’m keeping those columns and the mass and discarding the rest.\n\nfauna_cleaned &lt;- fauna %&gt;%\n  select(Class:`body mass (g)`) %&gt;%\n  select(-`binomial_(original files)`) %&gt;%\n  rename(mass_g = `body mass (g)`, name = binomial_2020) %&gt;%\n  mutate(Class = factor(Class),\n         Clade = factor(Clade),\n         order = factor(order),\n         family = factor(family),\n         name = factor(name))\n\nSo, let’s see what kind of data we have.\n\ntable1 &lt;- fauna_cleaned %&gt;%\n  count(Class)\n\ngt(table1)\n\n\n\n\n\n  \n    \n    \n      Class\n      n\n    \n  \n  \n    Aves\n9534\n    Mammalia\n5840\n    Reptilia\n11240\n  \n  \n  \n\n\n\n\nWe have data on more than just reptiles, the dataset includes information about birds and mammals as well. But I’m only interested in reptiles.\n\nreptiles &lt;- fauna_cleaned %&gt;%\n  filter(Class == \"Reptilia\")\n\n\ntable2 &lt;- reptiles %&gt;%\n  count(Clade, order)\n\ngt(table2)\n\n\n\n\n\n  \n    \n    \n      Clade\n      order\n      n\n    \n  \n  \n    Crocodylia\nCrocodylia\n24\n    Rhynchocephalia\nRhynchocephalia\n1\n    Squamata\nSquamata (Amphisbaenia)\n195\n    Squamata\nSquamata (Sauria)\n6868\n    Squamata\nSquamata (Serpentes)\n3837\n    Testudines\nTestudines\n315\n  \n  \n  \n\n\n\n\nEveryone knows that turtles are the best type of reptile, so let’s filter even further.\n\nturtles &lt;- reptiles %&gt;% \n  filter(Clade == \"Testudines\")\n\ntable3 &lt;- turtles %&gt;%\n  count(order, family)\n\ngt(table3)\n\n\n\n\n\n  \n    \n    \n      order\n      family\n      n\n    \n  \n  \n    Testudines\nCarettochelyidae\n1\n    Testudines\nChelidae\n53\n    Testudines\nCheloniidae\n6\n    Testudines\nChelydridae\n3\n    Testudines\nDermatemydidae\n1\n    Testudines\nDermochelyidae\n1\n    Testudines\nEmydidae\n47\n    Testudines\nGeoemydidae\n69\n    Testudines\nKinosternidae\n24\n    Testudines\nPelomedusidae\n18\n    Testudines\nPodocnemididae\n8\n    Testudines\nTestudinidae\n54\n    Testudines\nTrionychidae\n30\n  \n  \n  \n\n\n\n\nLet’s take a look at how big (or mighty, as some might say) the different families of turtles are. There is a very large range of masses so I’m using a log scale.\n\nggplot(turtles, aes(x = family, y = mass_g, color = family)) +\n  scale_y_log10() +\n  geom_boxplot() +\n  ggtitle(\"Mightiness of Different Families of Turtle and Tortoise\") +\n  ylab(\"mass (g)\") +\n  theme(legend.position = \"none\" , \n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\nThe heaviest turtles are the giantic leatherback turtles, a species of SEA TURTLE that weigh hundred of kg (around 1,000 lbs). The smallest turtle is the African dwarf mud turtle (Pelusios nanus), family Pelomedusidae, which full grown weighs under 100 grams.\nNow let’s look at everyone’s favorite family of turtles, Emydidae. This family is often known as pond or marsh turtles. Filter them out, and add a category for Eastern Box Turtles. I called the variable box_turtle, but I’m only marking Eastern. There are other types of box turtles though not all of them are in Emydidae. Asian box turtle species were reassigned to the family Geoemydidae.\nThe common name box turtle, arises from the fact that these species have a hinge on the bottom shell, and can close up/ box up completely. Other turtles and tortoises can only pull their bits within their shell.\n\n\n\nA closed Eastern Box Turtle\n\n\n\npond_turtles &lt;- turtles %&gt;%\n  filter(family == 'Emydidae') %&gt;%\n  mutate(box_turtle = ifelse(name == \"Terrapene carolina\", TRUE, FALSE)) \n\nOkay, let’s look at mightiness of the turtles in this family.\n\nturtle_plot &lt;- pond_turtles %&gt;%\n  ggplot(aes(x = fct_reorder(name, mass_g), y = mass_g, fill = box_turtle)) +\n  scale_fill_manual(values=c(\"#999999\", \"#E69F00\")) +\n  geom_col(width = 0.7, position = position_dodge(10)) +\n  coord_flip() +\n  ylab(\"mass (g)\") +\n  xlab(\"\") +\n  ggtitle(\"Mightiness of Different Turtles in family Emydidae\") +\n  labs(caption = \"Data from https://doi.org/10.1186/s40709-021-00134-9\") +\n  theme_classic() +\n  theme(axis.text = element_text(size = 6)) +\n  theme(legend.position = \"none\")\n\n\n#found how to add an image to my graph on stack overflow\n#https://stackoverflow.com/questions/63442933/how-can-i-add-a-logo-to-a-ggplot-visualisation\n\nimg &lt;- image_read(\"pqtk5r.jpg\")\n\n# Set the canvas where you are going to draw the plot and the image\nggdraw() +\n# Draw the plot in the canvas setting the x and y positions, which go from 0,0\n# (lower left corner) to 1,1 (upper right corner) and set the width and height of\n# the plot. It's advisable that x + width = 1 and y + height = 1, to avoid clipping # the plot\ndraw_plot(turtle_plot,x = 0, y = 0.15, width = 1, height = 0.85) +\n# Draw image in the canvas using the same concept as for the plot. Might need to \n# play with the x, y, width and height values to obtain the desired result\ndraw_image(img, x = 0.6, y = 0.35, width = 0.45, height = 0.45) \n\n\n\n\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {30 {Day} {Chart} {Challenge-} {Flora} and {Fauna}},\n  date = {2023-04-03},\n  url = {https://lsinks.github.io/posts/2023-04-03-chart-challenge-3/day3},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “30 Day Chart Challenge- Flora and\nFauna.” April 3, 2023. https://lsinks.github.io/posts/2023-04-03-chart-challenge-3/day3."
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-2/current_wordle.html",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-2/current_wordle.html",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 2",
    "section": "",
    "text": "This is the current version of the word game guesser. I discussed how I used this project to hone my coding skills in the companion blog post to this one.\nI’m not going to walk through this in much detail, but I’m going to point out some of the major lessons I learned as I revised the code. Again, both the initial version and this version are on GitHub in all their messiness.\nI learned how to put functions in a separate file and call them from my many script. This can make long code much easier to read. Here, I’ve included the helper functions in-line and commented out the source(\"code/helper-functions.R\") in the main code. I’ve also set up switchable troubleshooting help with verbose and debug_detail parameters in my functions. Setting them to TRUE provide more info as the functions are executed.\n\n#|label: helper-functions\nConstruct_Freq_Table &lt;- function(word_list) {\n\n#scoring code uses the counting code from\n\n#https://www.r-bloggers.com/2018/12/rrrrs-in-r-letter-frequency-in-r-package-names/  \n# making the frequency table ----\n\nletters &lt;- unlist(strsplit(word_list[,1], split = \"\"))\nchar_frequencies &lt;- as.data.frame(table(letters))\n\n#normalized\ncommon &lt;- max(char_frequencies[,2])\ny = (char_frequencies[,2]/common)\nchar_frequencies$normalized &lt;- y\nreturn(char_frequencies)\n}\n\nScoring_Word &lt;- function(word, freqs = char_frequencies,\n                         verbose = FALSE, debug_detail = FALSE){\n  letter_vec &lt;-  unlist(strsplit(word, split = \"\"))\n    if (verbose == TRUE)\n    {message(\"I'm in Scoring_words message and scoring: \", word)}\n  \n  value &lt;- 0\n  for (i in 1:length(letter_vec)) {\n    position &lt;- letter_vec[i]== freqs$letters\n    value[i] &lt;- freqs$normalized[position]\n    if (debug_detail == TRUE)\n    {\n      print(\"I am in the scoring loop calculating value: \")\n      print(i)\n      print(sum(value))\n      \n    }\n    \n    if (i == length(letter_vec)) {\n      \n      return(total &lt;- sum(value))\n    }\n    \n  }\n  }\n  \n\nScoring_Word_Unique &lt;- function(word, freqs = char_frequencies, \n                                verbose = FALSE, debug_detail = FALSE){\n  # This does only score on unique letters\n  letter_vec &lt;-  unlist(strsplit(word, split = \"\"))\n  unique_letter_vec &lt;- unique(letter_vec)\n  #unique_letter_vec &lt;- letter_vec\n  if (verbose == TRUE)\n  {message(\"I'm in Scoring_words_Unique and scoring: \", word)}\n  \n  value &lt;- 0\n  if (length(unique_letter_vec) == 0) {\n    return(value)\n  } else{\n    for (i in 1:length(unique_letter_vec)) {\n           position &lt;- unique_letter_vec[i] == freqs$letters\n          value[i] &lt;- freqs$normalized[position]\n      if (debug_detail == TRUE)\n      {\n        print(\"I am in the unique scoring loop calculating value: \")\n        print(i)\n        print(sum(value))\n      }\n      \n      if (i == length(unique_letter_vec)) {\n        \n        return(total &lt;- sum(value))\n      }\n      \n    }\n  }\n}\n\nRemoving_Letters &lt;- function(word, chosen_word, \n                              verbose = FALSE, debug_detail = FALSE) {\n  lvec &lt;- gsub(paste0(\"[\", chosen_word, \"]\"), \"\", word)  \n  return(lvec)}\n\nI finally did figure out how to make variables the types I wanted. I also replaced several loops with map functions from purrr. I also made a reshaped version of my dataframe using pivot_longer from tidyr. Reshaping data is a really useful skill, but might be a bit confusing at first. So I certainly wanted to make sure I could do it correctly. The reshaped data is used to make a nice density plot later.\n\n# Loading libraries and data ----\nlibrary(\"tidyverse\")\n\n\n#from https://www-cs-faculty.stanford.edu/~knuth/sgb-words.txt\nword_list &lt;- \n  read.table(\"C:/Users/drsin/OneDrive/Documents/R Projects/Word-Games/input/sgb-words.txt\") \n\n# Functions ----\n#source(\"code/helper-functions.R\")\n\n# calculate letter frequencies from word list\nchar_frequencies &lt;- Construct_Freq_Table(word_list)\n\n# Initialize the word_scores dataframe ----\nnum_words &lt;- nrow(word_list)\n#num_words &lt;- 5\nword_scores &lt;- data.frame(word_name = word_list[1:num_words,1],\n                    word_length = rep(0, times = num_words),\n                    word_guess1 = rep(0, times = num_words),\n                    word_guess2 = rep(0, times = num_words),\n                    word_guess3 = rep(0, times = num_words),\n                    word_guess4 = rep(0, times = num_words),\n                    score = rep(0, times = num_words), \n                    score_guess1 = rep(0, times = num_words),\n                    score_guess2 = rep(0, times = num_words),\n                    score_guess3 = rep(0, times = num_words),\n                    score_guess4 = rep(0, times = num_words)\n                                                )\n#fill in word lengths.  This is so code can be expended to longer words\nword_scores$word_length &lt;-  str_length(word_scores$word_name)\n\n# Calculates the initial scores for all words -----\n\nword_scores &lt;- word_scores %&gt;% \n  mutate(score = map_dbl(word_name, Scoring_Word))\n\nword_scores &lt;- word_scores %&gt;%\n  mutate(score_guess1 = map_dbl(word_name, Scoring_Word_Unique))\n\n\n# Finding the best first word\ntop_words &lt;- word_scores %&gt;%\n arrange(desc(score_guess1))\nword_1 &lt;- top_words$word_name[1]\n\n# Scoring for second guess\nword_scores &lt;- word_scores %&gt;%\n  mutate(word_guess2 = \n           map_chr(word_name, Removing_Letters, chosen_word = word_1))\nword_scores &lt;- word_scores %&gt;%\n  mutate(score_guess2 = map_dbl(word_guess2, Scoring_Word_Unique))\n\ntop_words &lt;- word_scores %&gt;%\n  arrange(desc(score_guess2))\n\nword_2 &lt;- top_words$word_name[1]\n\n# Scoring for third guess\nword_scores &lt;- word_scores %&gt;% \n  mutate(word_guess3 =\n           map_chr(word_guess2, Removing_Letters, chosen_word = word_2))\nword_scores &lt;- word_scores %&gt;%\n  mutate(score_guess3 = map_dbl(word_guess3, Scoring_Word_Unique))\n\n\ntop_words &lt;- word_scores %&gt;%\n  arrange(desc(score_guess3))\nword_3 &lt;- top_words$word_name[1]\n\n# Scoring for fourth guess\nword_scores &lt;- word_scores %&gt;%\n  mutate(word_guess4 = \n           map_chr(word_guess3, Removing_Letters, chosen_word = word_3))\nword_scores &lt;- word_scores %&gt;%\n  mutate(score_guess4 = map_dbl(word_guess4, Scoring_Word_Unique))\n\n\ntop_words &lt;- word_scores %&gt;%\n  arrange(desc(score_guess4))\n\nword_4 &lt;- top_words$word_name[1]\n\n# subsetting this dataframe and reshaping it.\n# This is used to make a density plot later.\nword_scores2 &lt;- word_scores %&gt;%\n   select(word_name, score_guess1, score_guess2, score_guess3, score_guess4)\nword_scores_reshaped &lt;- \n  pivot_longer(word_scores2, cols = 2:5, \n               names_to = \"score_type\", values_to = \"score\")\n\nNice visualizations were definitely not part of the initial code. In the next chunk, I make some prettier visualizations.\n\n### This is now just visualizing what we've done. ------\n\n#plotting the frequency of the letters in our word_set\nggplot(char_frequencies, \n       aes(x = fct_rev(fct_reorder(letters,  normalized)), y = normalized )) +\n  geom_col() +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Frequencies of Letters\", caption = \"from 5 letter words\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  xlab(\"Letter\") +\n  ylab(\"Frequency\") +\n    scale_y_continuous(expand = c(0, 0))\n\n\n\n## This looks at the distribution of scores as guessing occurs.  Initially, you have a\n\nword_scores_reshaped$score_type &lt;- as.factor(word_scores_reshaped$score_type)\n\nggplot(word_scores_reshaped, aes(score, fill = score_type)) +\n  geom_density(alpha = 0.5) +\n  theme_classic() +\n  labs(title = \"Evolution of Word Scores as Guessing Progresses\",\n       caption = \"for 5 letter words\") +\n  xlab(\"Score\") +\n  ylab(\"Density\") +\n  labs(fill = \"\") +\n  theme(legend.position = c(0.7, 0.8)) +\n  scale_x_continuous( expand = c(0, 0)) +\n  scale_y_continuous( expand = c(0, 0)) \n\n\n\n## Now we are visualizing what letters are picked in each guess\nguess &lt;- rep(\"not guessed\", times = 26)\nchar_frequencies &lt;- cbind(char_frequencies, guess)\n\n# this is done in reverse order because some letters are guessed in more than\n# one word and I'd like them marked at the earliest guess.\nletter_vec &lt;-  unlist(strsplit(word_4, split = \"\"))\nprint(letter_vec)\n\n[1] \"w\" \"h\" \"a\" \"c\" \"k\"\n\nfor (i in 1:length(letter_vec)) {\n  position &lt;- letter_vec[i] == char_frequencies$letters\n  char_frequencies$guess[position] &lt;- \"Guess 4\"\n}\n\nletter_vec &lt;-  unlist(strsplit(word_3, split = \"\"))\nprint(letter_vec)\n\n[1] \"d\" \"u\" \"m\" \"p\" \"y\"\n\nfor (i in 1:length(letter_vec)) {\n  position &lt;- letter_vec[i] == char_frequencies$letters\n  char_frequencies$guess[position] &lt;- \"Guess 3\"\n}\n\nletter_vec &lt;-  unlist(strsplit(word_2, split = \"\"))\nprint(letter_vec)\n\n[1] \"u\" \"n\" \"t\" \"i\" \"l\"\n\nfor (i in 1:length(letter_vec)) {\n  position &lt;- letter_vec[i] == char_frequencies$letters\n  char_frequencies$guess[position] &lt;- \"Guess 2\"\n}\n\n\nletter_vec &lt;-  unlist(strsplit(word_1, split = \"\"))\nprint(letter_vec)\n\n[1] \"a\" \"r\" \"o\" \"s\" \"e\"\n\nfor (i in 1:length(letter_vec)) {\n  position &lt;- letter_vec[i] == char_frequencies$letters\n  char_frequencies$guess[position] &lt;- \"Guess 1\"\n}\n\n\nggplot(char_frequencies, aes(\n  x = fct_rev(fct_reorder(letters,  normalized)),\n  y = normalized,\n  fill = guess)) +\n  geom_col() +\n  ggtitle(\"When Letters are Guessed\") +\n  ylab(\"Normalized Counts\") +\n  xlab(\"Letter\") +\n  theme_classic() +\n  theme(legend.position = c(0.6, 0.6)) +\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\nSo that’s the current state of this project. This was a really useful project for me and it really strengthened by R and Tidyverse skills.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {Self-Guided {Learning} Through a {Wordle} {Guess}\n    {Generator:} {Part} 2},\n  date = {2023-04-01},\n  url = {https://lsinks.github.io/posts/2023-04-01-self-guided-learning-wordle-guesser-part-2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “Self-Guided Learning Through a Wordle\nGuess Generator: Part 2.” April 1, 2023. https://lsinks.github.io/posts/2023-04-01-self-guided-learning-wordle-guesser-part-2."
  },
  {
    "objectID": "posts/2023-03-30-One-Class-SVM/one-class-svm.html",
    "href": "posts/2023-03-30-One-Class-SVM/one-class-svm.html",
    "title": "One Class SVM",
    "section": "",
    "text": "I’ve recently been playing around with classification models, specifically on data sets with a skewed class distribution. In imbalanced classification problems, one class occurs infrequently. The minority class is often the class of interest (think fraudulent transaction, positive disease diagnosis, or intruder detection). Sometimes these applications are framed as a two-class classification problem, but other times they are called anomaly, outlier, or novelty detection.\nImbalanced classification problems are tricky for a couple of reasons. Models can achieve high accuracy by classifying everything as the dominant class. You can somewhat mitigate this problem by choosing models based on other metrics, such as sensitivity. You can also downsample the data to balance the classes (which throws out a lot of data) or upsample the infrequent class using a technique like SMOTE or ROSE to create synthetic data points.\nCollecting enough labeled data can also be expensive in highly imbalanced classes. Techniques like SMOTE won’t help if you only have 2 of a class in the dataset; the model needs “sufficient” data to learn from.\nAnother way to handle a minority class is to use a one-class classifier. One-class classifiers are one of the most widely used methods in anomaly detection because it does not require extensive labeled data for training. This method can either be semi-supervised, where only the normal (major) class is used for training, or unsupervised, where the method can handle anomalies in the training class. The one-class SVM is a popular implementation of one-class classifiers.\nHere I’m going to use a toy dataset from Datacamp. They have told me that all datasets used in their courses can be used outside Datacamp.\nI’m using some specialty packages here, specifically e1071 and caret for the machine learning.\nlibrary(tidyverse)\nlibrary(skimr) # for EDA\nlibrary(corrplot) # for cool correlation graph\nlibrary(gt) # for tables\nlibrary(e1071) # for svm\nlibrary(caret) # for data split\nthyroid &lt;- read.csv(\"thyroid.csv\", header = TRUE)"
  },
  {
    "objectID": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#exploratory-data-analysis",
    "href": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#exploratory-data-analysis",
    "title": "One Class SVM",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThe dataset explores thyroid disease as a function of thyroid hormone levels. I’m using a custom skim function to tailor the output. More info on that can be found here.\n\nmy_skim &lt;- skim_with(numeric = sfl(p25 = NULL, p50 = NULL, p75 = NULL)) \nthyroid_skim &lt;- my_skim(thyroid)\n\nthyroid_skim %&gt;%\n  select(-skim_type)   %&gt;% \n  gt() %&gt;%\n  cols_label(n_missing = \"# Missing\", complete_rate = \"Completeness\", \n             numeric.mean = \"Mean\", numeric.sd = \"Standard Deviation\",\n             numeric.p0 = \"Min\", numeric.p100 = \"Max\",\n             numeric.hist = \"Histogram\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Summary of Variables in Thyroid\") \n\n\n\n\n\n  \n    \n      Summary of Variables in Thyroid\n    \n    \n    \n      skim_variable\n      # Missing\n      Completeness\n      Mean\n      Standard Deviation\n      Min\n      Max\n      Histogram\n    \n  \n  \n    label\n0\n1\n0.0220000\n0.1467567\n0.000000\n1.00000000\n▇▁▁▁▁\n    TSH\n0\n1\n-0.6881938\n0.4455654\n-4.532599\n-0.02173999\n▁▁▁▃▇\n    T3\n0\n1\n-6.5046015\n1.3994315\n-9.268609\n-1.43659510\n▅▇▇▁▁\n    TT4\n0\n1\n-1.7235631\n0.4421667\n-5.350910\n-0.37417607\n▁▁▁▇▁\n    T4U\n0\n1\n-1.4666057\n0.4495771\n-6.164484\n0.00000000\n▁▁▁▇▂\n    FTI\n0\n1\n-1.0093125\n0.2522809\n-3.569533\n-0.17950862\n▁▁▁▇▂\n    TBG\n0\n1\n-1.7932517\n0.4318577\n-6.636603\n0.00000000\n▁▁▁▇▁\n  \n  \n  \n\n\n\n\nWe see that the dataset is complete with no missing values. All data types are numeric. About 2% of the patients are diagnosed with thyroid disease.\nI like to look at a correlation plot to get an overview of how the predictors relate to each other and the outcome. The correlation plot created by corrplot() has the title truncated in a lot of notebook/ markdown environments. The solution, which I found here, is to add a margin.\n\n# examining correlation between variables categories\n# moving the outcome to the first column to start\n# will be sorted by decreasing correlation with outcome\nthyroid %&gt;%\n    dplyr::select(label, everything()) %&gt;%\n    cor %&gt;%\n        {.[order(abs(.[, 1]), decreasing = TRUE), \n       order(abs(.[, 1]), decreasing = TRUE)]} %&gt;% \n    corrplot( type = 'lower', tl.col = 'black', \n            addCoef.col = 'black', cl.ratio = 0.2, tl.srt = 45, \n            col = COL2('PuOr', 10), diag = FALSE , mar = c(0,0,2,0),\n            title = \" Correlations between Thyroid Disease and hormone levels\")\n\n\n\n\nMany of the features are strongly correlated with the outcome. So, we can expect to get reasonably decent results from our model."
  },
  {
    "objectID": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#setting-up-for-ml-with-caret",
    "href": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#setting-up-for-ml-with-caret",
    "title": "One Class SVM",
    "section": "Setting up for ML with caret",
    "text": "Setting up for ML with caret\nI’m using the e1071 package for SVM, which is not supported by tidymodels, so I will use caret as the wrapper for a lot of the machine modeling workflow. First, I’m going to make a train and test split. createDataPartition will stratify the sampling over the two classes if you pass it the vector of labels. Stratification is usually critical with an imbalanced dataset; you don’t want a scenario where the train or test dataset has most of the minority class observations.\n\n# Relabel the classes to TRUE if it is normal data and FALSE if it is\n# an anomaly.  (That is, it is false that the outlier data is normal).  \n# makes it easier to compare with the output of the SVM model.  \nthyroid &lt;- thyroid %&gt;%\n  mutate(label = ifelse(label == 0, TRUE, FALSE))\n\n# create data split for test and training\n# will be split among strata\nset.seed(2346)\ninTrain &lt;- createDataPartition(thyroid$label, p = 0.6, list = FALSE) \n\n# formatting the data as required for svm()\ntrain_predictors &lt;- thyroid[inTrain, 2:7]\ntrain_labels &lt;- thyroid[inTrain, 1]\n\n# Creating the test set\ntest &lt;- thyroid[-inTrain,]\n\n# formatting the data as required for svm()\ntest_predictors &lt;- test[,2:7]\ntest_labels &lt;- test[,1]\n\n#double checking that the test and train sets do contain ~2% disease\n# or rather 98% normal.\nmean(train_labels)\n\n[1] 0.9767055\n\nmean(test_labels)\n\n[1] 0.9799499"
  },
  {
    "objectID": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#two-class-svm",
    "href": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#two-class-svm",
    "title": "One Class SVM",
    "section": "Two-class SVM",
    "text": "Two-class SVM\nFirst, I’m going to fit the data with a traditional 2 class classifier. I’m using SVM for the classification. The option type ='C-classification' performs normal classification. I’m not going to get into the details of SVM here, but for more information check out this tutorial. I’m also not going to tune any hyper-parameters.\n\n# fitting SVM on training data \ntwo_class_svm_model &lt;- svm(train_predictors, y = train_labels,\n               type = 'C-classification',\n               scale = TRUE,\n               kernel = \"radial\")\n\n# now predicting both classes on train and test data\ntwo_class_svm_predtrain &lt;- predict(two_class_svm_model,train_predictors)\ntwo_class_svm_predtest &lt;- predict(two_class_svm_model,test_predictors)\n\n\n# code below here will be provided\n# seeing how well the model did\ntwo_class_confTrain &lt;- table(Predicted = two_class_svm_predtrain, Reference = train_labels)\ntwo_class_confTest &lt;- table(Predicted = two_class_svm_predtest, Reference = test_labels)\n\n# printing out the results\nprint(\"These are the predictions on the training data:\")\n\n[1] \"These are the predictions on the training data:\"\n\nprint(two_class_confTrain)\n\n         Reference\nPredicted FALSE TRUE\n    FALSE    12    0\n    TRUE      2  587\n\nprint(\"These are the predictions on the test data:\")\n\n[1] \"These are the predictions on the test data:\"\n\nprint(two_class_confTest)\n\n         Reference\nPredicted FALSE TRUE\n    FALSE     6    0\n    TRUE      2  391\n\n\nWe see that the two-class classifier does very well! In the test data set, it correctly predicts 397/ 399 data points. However, it misidentified a quarter of the disease patients as having normal thyroid. This is as I mentioned above- models can generally achieve good accuracy, but by over predicting the majority class. This result could potentially be unacceptable for a healthcare application."
  },
  {
    "objectID": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#one-class-svm",
    "href": "posts/2023-03-30-One-Class-SVM/one-class-svm.html#one-class-svm",
    "title": "One Class SVM",
    "section": "One-class SVM",
    "text": "One-class SVM\nNow, let’s compare this to the one-class classifier. I will use the one-class classifier in supervised mode; that is, I will pass it labeled data, but only for the normal class. Then I will predict and calculate metrics based on both classes. There are a few different ways we can prepare this data. For ease of comparison with the regular classifier, I will use the same splits but filter out the anomalies from the training data. You might instead filter out all the outliers from the training set and add them to the test set, so you can get a better idea of how the model works for outlier detection. However, I want an apples-to-apples comparison, so I’m not doing that here. The regular and one class SVM will be predicting on the same test data set.\n\n# subset the labeled data into the two classes\n# the normal class should be called \"train_normal\" and the anomaly\n# class should be called \"test_outlier\"\n\ntrain_normal_class &lt;- subset(thyroid[inTrain, ], label == TRUE)\n\n\ntrain_normal_class_pred &lt;- train_normal_class[,2:7]\ntrain_normal_class_label &lt;- train_normal_class[,1]\n\n\n# fitting one class SVM on training data- no labels needed! \none_class_svm_model &lt;- svm(train_normal_class_pred, y = NULL,\n               type = 'one-classification',\n               nu = 0.10,\n               scale = TRUE,\n               kernel = \"radial\")\n\n# now predicting both classes on train and test data\none_class_svm_predtrain &lt;- predict(one_class_svm_model,train_normal_class_pred)\none_class_svm_predtest &lt;- predict(one_class_svm_model,test_predictors)\n\n\n# code below here will be provided\n# seeing how well the model did\none_class_confTrain &lt;- table(Predicted = one_class_svm_predtrain,\n                             Reference = train_normal_class_label)\none_class_confTest &lt;- table(Predicted = one_class_svm_predtest,\n                            Reference = test_labels)\n\n# printing out the results\nprint(\"These are the predictions on the normal class training data only:\")\n\n[1] \"These are the predictions on the normal class training data only:\"\n\nprint(one_class_confTrain)\n\n         Reference\nPredicted TRUE\n    FALSE   61\n    TRUE   526\n\nprint(\"These are the predictions on the test data with both classes:\")\n\n[1] \"These are the predictions on the test data with both classes:\"\n\nprint(one_class_confTest)\n\n         Reference\nPredicted FALSE TRUE\n    FALSE     8   40\n    TRUE      0  351\n\n\nThis model doesn’t do quite as well, but it is pretty impressive given that it only learned on normal data. It correctly predicted 359/399 data points in the test set. It incorrectly classified 44 cases as abnormal when they were normal, but correctly found all 8 disease cases.\nSo now I’ve showed you how to use a one-class SVM to predict outliers. This is an incredible useful tool to keep in mind for classification tasks."
  },
  {
    "objectID": "posts/2023-03-25-twitter-cards/twitter-cards.html",
    "href": "posts/2023-03-25-twitter-cards/twitter-cards.html",
    "title": "Twitter Cards",
    "section": "",
    "text": "Trying to get the picture to show in a twitter card. Apparently you need to specify the image on every document, not just in the main yml doc, which is what I understood from the instructions.\nNow this works for new posts, but not old posts.\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E.},\n  title = {Twitter {Cards}},\n  date = {2023-03-24},\n  url = {https://lsinks.github.io/posts/2023-03-25-twitter-cards/twitter-cards.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E. 2023. “Twitter Cards.” March 24, 2023. https://lsinks.github.io/posts/2023-03-25-twitter-cards/twitter-cards.html."
  },
  {
    "objectID": "posts/2023-03-21-tidytuesday-programming-languages/index.html",
    "href": "posts/2023-03-21-tidytuesday-programming-languages/index.html",
    "title": "TidyTuesday Week 12: Programming Languages",
    "section": "",
    "text": "This is my first attempt at Tidy Tuesday. The dataset today is about Programming Languages. The sample visualizations are about the comment codes.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(ggrepel)\n\nLoad the data first. There has been some cleaning done as outlined on the TidyTuesday github page.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 12)\n\nlanguages &lt;- tuesdata$languages\n\nFirst, let’s look at how complete the data is. The skimr package produces nice summary information about the variables and their completeness.\n\nskim(languages)\n\n\nData summary\n\n\nName\nlanguages\n\n\nNumber of rows\n4303\n\n\nNumber of columns\n49\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n21\n\n\nlogical\n4\n\n\nnumeric\n24\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n4303\n0\n\n\ntitle\n0\n1.00\n1\n56\n0\n4267\n0\n\n\ndescription\n3480\n0.19\n4\n2273\n0\n811\n0\n\n\ntype\n0\n1.00\n2\n27\n0\n40\n0\n\n\ncreators\n3203\n0.26\n2\n253\n0\n985\n0\n\n\nwebsite\n2928\n0.32\n13\n131\n0\n1368\n0\n\n\ndomain_name\n3588\n0.17\n6\n32\n0\n700\n0\n\n\nreference\n2314\n0.46\n15\n251\n0\n1955\n0\n\n\ngithub_repo\n3402\n0.21\n25\n73\n0\n897\n0\n\n\ngithub_repo_description\n3438\n0.20\n4\n419\n0\n853\n0\n\n\ngithub_language\n3829\n0.11\n1\n30\n0\n474\n0\n\n\ngithub_language_tm_scope\n3837\n0.11\n4\n34\n0\n361\n0\n\n\ngithub_language_type\n3837\n0.11\n4\n11\n0\n4\n0\n\n\ngithub_language_ace_mode\n3838\n0.11\n1\n16\n0\n96\n0\n\n\ngithub_language_file_extensions\n3833\n0.11\n1\n606\n0\n466\n0\n\n\nwikipedia\n2731\n0.37\n32\n104\n0\n1566\n0\n\n\nwikipedia_summary\n2884\n0.33\n17\n6741\n0\n1407\n0\n\n\nwikipedia_related\n3145\n0.27\n1\n1761\n0\n1059\n0\n\n\nline_comment_token\n3831\n0.11\n1\n7\n0\n23\n0\n\n\norigin_community\n1190\n0.72\n3\n305\n0\n2232\n0\n\n\nfile_type\n3213\n0.25\n2\n6\n0\n4\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n3683\n0.14\n1.00\nTRU: 617, FAL: 3\n\n\nfeatures_has_semantic_indentation\n3722\n0.14\n0.11\nFAL: 516, TRU: 65\n\n\nfeatures_has_line_comments\n3765\n0.13\n0.96\nTRU: 517, FAL: 21\n\n\nis_open_source\n3792\n0.12\n0.89\nTRU: 453, FAL: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1991.11\n111.44\n-2000\n1984.00\n1997.0\n2012.00\n2023\n▁▁▁▁▇\n\n\ndomain_name_registered\n3801\n0.12\n2011.33\n7.02\n1990\n2007.00\n2013.0\n2017.00\n2023\n▁▃▃▇▆\n\n\nisbndb\n3217\n0.25\n7.71\n33.16\n0\n0.00\n0.0\n2.00\n400\n▇▁▁▁▁\n\n\nbook_count\n0\n1.00\n2.08\n17.34\n0\n0.00\n0.0\n0.00\n401\n▇▁▁▁▁\n\n\nsemantic_scholar\n3545\n0.18\n3.79\n8.32\n0\n0.00\n0.0\n3.00\n52\n▇▁▁▁▁\n\n\nlanguage_rank\n0\n1.00\n2151.00\n1242.31\n0\n1075.50\n2151.0\n3226.50\n4302\n▇▇▇▇▇\n\n\ngithub_repo_stars\n3414\n0.21\n2127.40\n7554.02\n0\n29.00\n194.0\n1071.00\n88526\n▇▁▁▁▁\n\n\ngithub_repo_forks\n3417\n0.21\n261.29\n1203.00\n0\n2.25\n16.0\n91.50\n23732\n▇▁▁▁▁\n\n\ngithub_repo_updated\n3418\n0.21\n2021.39\n1.76\n2012\n2022.00\n2022.0\n2022.00\n2023\n▁▁▁▁▇\n\n\ngithub_repo_subscribers\n3418\n0.21\n62.34\n200.88\n0\n4.00\n13.0\n44.00\n2910\n▇▁▁▁▁\n\n\ngithub_repo_created\n3425\n0.20\n2015.84\n3.48\n2006\n2013.00\n2016.0\n2019.00\n2022\n▁▅▇▇▇\n\n\ngithub_repo_issues\n3518\n0.18\n123.03\n546.26\n0\n1.00\n9.0\n61.00\n9522\n▇▁▁▁▁\n\n\ngithub_repo_first_commit\n3567\n0.17\n2014.74\n4.99\n1987\n2012.00\n2015.0\n2018.00\n2022\n▁▁▁▆▇\n\n\ngithub_language_repos\n3833\n0.11\n197134.67\n1226900.57\n0\n91.25\n725.5\n7900.25\n16046489\n▇▁▁▁▁\n\n\nwikipedia_daily_page_views\n2837\n0.34\n227.13\n783.55\n-1\n9.00\n24.0\n99.00\n13394\n▇▁▁▁▁\n\n\nwikipedia_backlinks_count\n2877\n0.33\n318.55\n1635.29\n1\n13.00\n39.0\n126.00\n34348\n▇▁▁▁▁\n\n\nwikipedia_page_id\n2893\n0.33\n9167847.21\n13506832.90\n928\n375153.75\n2114700.5\n12321223.00\n63063548\n▇▁▁▁▁\n\n\nwikipedia_appeared\n2958\n0.31\n1991.14\n17.03\n1830\n1980.00\n1994.0\n2005.00\n2019\n▁▁▁▃▇\n\n\nwikipedia_created\n3040\n0.29\n2005.75\n3.77\n2001\n2003.00\n2005.0\n2007.00\n2020\n▇▇▂▁▁\n\n\nwikipedia_revision_count\n3130\n0.27\n330.43\n813.26\n1\n35.00\n84.0\n242.00\n10104\n▇▁▁▁▁\n\n\nlast_activity\n0\n1.00\n2000.62\n84.60\n-900\n1992.00\n2006.0\n2021.00\n2023\n▁▁▁▁▇\n\n\nnumber_of_users\n0\n1.00\n13771.26\n227712.95\n0\n0.00\n20.0\n230.00\n7179119\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n422.18\n12572.99\n0\n0.00\n0.0\n0.00\n771996\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n1482\n0.66\n0.00\n0.00\n0\n0.00\n0.0\n0.00\n0\n▁▁▇▁▁\n\n\n\n\n\nThe data is pretty incomplete. Only 9 of the 49 variables are fully complete. The line comment token is only 0.110 complete and the has comments is only 0.144 complete. This variable has only 3 false values; it is likely that the missing data is skewed towards false. It is more likely that you’d complete this entry if there were a comment, than if there weren’t. It is also possible that the cleaning and prep done to prepare the #TidyTuesday dataset removed some entries which did have FALSE values for the comments.\nThere are some funny entries that appeared in the skim report, like -2000 as the year the earliest language appeared. It turns out this is Babylonian numerals, so it probably correct. This does show there is a lot more than computer languages in this dataset though.\nLooking through the variables, I see there is a “type” in the data dictionary, and it appears that “pl” means programming language. So let’s filter for that. (I couldn’t find an explanation of this variable on https://pldb.com/) It is used on various pages, but I couldn’t find the definition of the types.\nAlso, rank starts at 0, and I’d like it to start at 1.\n\nprogramming_lang &lt;- languages %&gt;%\n  filter(type == 'pl') %&gt;%\n  select(-starts_with(\"github\"), -starts_with(\"wikipedia\"),\n         -description, -creators, -(website:semantic_scholar)) %&gt;%\n  mutate(language_rank = language_rank + 1)\n\nskim(programming_lang)\n\n\nData summary\n\n\nName\nprogramming_lang\n\n\nNumber of rows\n3368\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlogical\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n3368\n0\n\n\ntitle\n0\n1.00\n1\n54\n0\n3347\n0\n\n\ntype\n0\n1.00\n2\n2\n0\n1\n0\n\n\nline_comment_token\n3002\n0.11\n1\n3\n0\n18\n0\n\n\norigin_community\n883\n0.74\n3\n176\n0\n1825\n0\n\n\nfile_type\n2609\n0.23\n4\n4\n0\n1\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n2886\n0.14\n1.00\nTRU: 482\n\n\nfeatures_has_semantic_indentation\n2917\n0.13\n0.09\nFAL: 410, TRU: 41\n\n\nfeatures_has_line_comments\n2954\n0.12\n0.97\nTRU: 401, FAL: 13\n\n\nis_open_source\n2984\n0.11\n0.85\nTRU: 328, FAL: 56\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1994.16\n17.34\n1948\n1982.0\n1994.0\n2010.0\n2022\n▁▅▇▇▇\n\n\nlanguage_rank\n0\n1.00\n2296.75\n1249.08\n1\n1243.5\n2334.5\n3423.5\n4303\n▆▆▆▆▇\n\n\nlast_activity\n0\n1.00\n2002.04\n17.91\n1951\n1989.0\n2005.0\n2019.0\n2023\n▁▂▃▆▇\n\n\nnumber_of_users\n0\n1.00\n10793.85\n190197.19\n0\n0.0\n15.0\n165.0\n5962666\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n160.22\n2692.65\n0\n0.0\n0.0\n0.0\n85206\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n939\n0.72\n0.00\n0.00\n0\n0.0\n0.0\n0.0\n0\n▁▁▇▁▁\n\n\n\n\n\nThis now produces a dataset with 0.143 completeness for features_has_comments. All non-missing entries are TRUE, which again suggests that FALSE is over represented in the missing data.\nLet’s only look at the programming languages that have data for comments.\n\nprogramming_lang &lt;- programming_lang %&gt;%\n  filter(features_has_comments == TRUE)\n\nskim(programming_lang)\n\n\nData summary\n\n\nName\nprogramming_lang\n\n\nNumber of rows\n482\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlogical\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n35\n0\n482\n0\n\n\ntitle\n0\n1.00\n1\n45\n0\n481\n0\n\n\ntype\n0\n1.00\n2\n2\n0\n1\n0\n\n\nline_comment_token\n120\n0.75\n1\n3\n0\n18\n0\n\n\norigin_community\n112\n0.77\n3\n105\n0\n311\n0\n\n\nfile_type\n146\n0.70\n4\n4\n0\n1\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n0\n1.00\n1.00\nTRU: 482\n\n\nfeatures_has_semantic_indentation\n57\n0.88\n0.05\nFAL: 405, TRU: 20\n\n\nfeatures_has_line_comments\n71\n0.85\n0.97\nTRU: 400, FAL: 11\n\n\nis_open_source\n305\n0.37\n0.91\nTRU: 161, FAL: 16\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n2000.17\n14.07\n1957\n1991.00\n2003.0\n2011.00\n2022\n▁▂▆▇▇\n\n\nlanguage_rank\n0\n1.00\n656.10\n559.75\n1\n201.25\n515.5\n997.25\n2994\n▇▃▂▁▁\n\n\nlast_activity\n0\n1.00\n2016.20\n8.27\n1967\n2011.00\n2022.0\n2022.00\n2023\n▁▁▁▂▇\n\n\nnumber_of_users\n0\n1.00\n62892.08\n462314.18\n0\n112.00\n437.5\n1615.25\n5962666\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n971.30\n6489.83\n0\n0.00\n0.0\n0.00\n85206\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n136\n0.72\n0.00\n0.00\n0\n0.00\n0.0\n0.00\n0\n▁▁▇▁▁\n\n\n\n\n\nThis subset is still moderately incomplete for information about comments. Only 75% of the data has the type of comment entered (#, //, etc). 86% of the entries are completed for “feature_has_line_comments” which indicates if comments must occupy a single line or if they can be made inline.\n\nprogramming_lang %&gt;% filter(is.na(line_comment_token) == FALSE) %&gt;%\n  group_by(line_comment_token) %&gt;%\n  count(line_comment_token) %&gt;%\n  ggplot(aes(fct_rev(fct_reorder(line_comment_token, n)), n)) +\n  geom_col(fill = \"dodgerblue2\") +\n  ylab(\"Count\") +\n  xlab(\"Comment Token\") +\n  ggtitle(\"Popularity of different comment tokens\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,  vjust = 0.25, hjust = 0.25))\n\n\n\n\nLet’s make a nice table of the popular comment types.\n\n# | label: table-tokens\nprogramming_lang2 &lt;- programming_lang %&gt;%\n  filter(is.na(line_comment_token) == FALSE) %&gt;%\n  count(line_comment_token, sort = TRUE) \n\nprogramming_lang2 %&gt;%\ngt() %&gt;%\ntab_header(title = \"Most Common Comment Tokens\") %&gt;%\ncols_label(line_comment_token = \"Token\", n = \"# of Languages that use token\")\n\n\n\n\n\n  \n    \n      Most Common Comment Tokens\n    \n    \n    \n      Token\n      # of Languages that use token\n    \n  \n  \n    //\n161\n    #\n70\n    ;\n49\n    --\n31\n    '\n16\n    %\n12\n    !\n7\n    *\n5\n    REM\n2\n    *&gt;\n1\n    ---\n1\n    /\n1\n    NB.\n1\n    \\\n1\n    \\*\n1\n    __\n1\n    ~\n1\n    ⍝\n1\n  \n  \n  \n\n\n\n\nThere is a language rank, which measures the popularity of the language based on signals such as number of users and number of jobs. Let’s see the average rank of languages for each token.\n\nprogramming_lang %&gt;% filter(is.na(line_comment_token) == FALSE) %&gt;%\n  group_by(line_comment_token) %&gt;%\n  summarize(avg_rank = mean(language_rank)) %&gt;%\n  ggplot(aes((fct_reorder(line_comment_token, avg_rank)), avg_rank)) +\n  geom_col(fill = \"dodgerblue2\") +\n  ylab(\"Average Rank of Language\") +\n  xlab(\"Comment Token\") +\n  ggtitle(\"Average rank of languages using different comment tokens\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,  vjust = 0.25, hjust = 0.25))\n\n\n\n\nThe highest (average) ranked token is “*&gt;”. What languages use this?\n\nprogramming_lang %&gt;% filter(line_comment_token == \"*&gt;\") %&gt;%\n  select(title, language_rank, line_comment_token)\n\n# A tibble: 1 × 3\n  title language_rank line_comment_token\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;             \n1 COBOL            19 *&gt;                \n\n\nOnly COBOL does, so the rank of this token isn’t diluted by many less popular languages. We can view the distribution of the language ranks for all the tokens.\n\nprogramming_lang %&gt;%\n  filter(is.na(line_comment_token) == FALSE) %&gt;%\n  ggplot(aes(line_comment_token, language_rank)) +\n  geom_boxplot(color = \"dodgerblue2\") +\n  ggtitle(\"The rank of languages by token.\") +\n  xlab(\"Token\") +\n  ylab (\"Language Rank\") +\n  theme_classic()\n\n\n\n\nOkay, let’s clean this up. I’d like it sorted by the median rank. Remeber rank is in reverse numerical order- a low number means a higher rank.\n\nprogramming_lang %&gt;%\n  filter(is.na(line_comment_token) == FALSE) %&gt;%\n  ggplot(aes(fct_reorder(line_comment_token, language_rank,\n                         .fun = median, .desc = FALSE), language_rank)) +\n  geom_boxplot(color = \"dodgerblue2\") +\n  ggtitle(\"The rank of languages by token\") +\n  xlab(\"Token\") +\n  ylab(\"Language Rank\") +\n    theme_classic()\n\n\n\n\nLet’s see the most popular language for each symbol. There might be a way to do this all at once, but I’m going to pull it out with joins to previous tables I’ve created.\n\nprogramming_lang3 &lt;- programming_lang %&gt;%\n  filter(is.na(line_comment_token) == FALSE) %&gt;%\n  group_by(line_comment_token) %&gt;%\n  summarize(highest_rank = min(language_rank)) \n\njoin_madness &lt;- programming_lang2 %&gt;%\n  left_join(programming_lang3, by = \"line_comment_token\") %&gt;% \n  left_join(programming_lang, \n            by = c(\"highest_rank\" = \"language_rank\",\n                   \"line_comment_token\" = \"line_comment_token\")) \n\njoin_madness &lt;- join_madness %&gt;%\n  select(line_comment_token, n, highest_rank, title, appeared, number_of_users,\n         number_of_jobs)\n\nSo now we have a bunch of summarized data in a single dataframe. Here’s a graph. It is saying something, but I’m not sure what. When you can’t come up with a concise title, then you probably don’t know what you are trying to say…\n\njoin_madness %&gt;%\n  ggplot(aes(highest_rank, n, size = log(number_of_users), \n             color = log(number_of_users), label = line_comment_token)) +\n  scale_y_log10() +\n  scale_x_log10() +\n  geom_text_repel(show.legend = FALSE) +\n  ggtitle(\"Popularity of tokens by language rank and usage\") +\n  xlab(\"Highest Rank of language using Token\") +\n  ylab(\"Number of Languages using token\") +\n  theme_classic()\n\n\n\n\nThis is a visualization of the highest ranked languages for each token. The number of users of the dominant language is also encoded in the size and color of the label. Having it ordered makes it difficult to tell if Java or Python is the most popular/ highest ranked language.\n\njoin_madness %&gt;%\n  ggplot(aes(fct_rev(fct_reorder(line_comment_token, highest_rank)), n,\n             size = log(number_of_users), color = log(number_of_users),\n             label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\nHere is the same graph just ordered “alphabetically” by token.\n\njoin_madness %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E.},\n  title = {TidyTuesday {Week} 12: {Programming} {Languages}},\n  date = {2023-03-21},\n  url = {https://lsinks.github.io/posts/2023-03-21-tidytuesday-programming-languages/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E. 2023. “TidyTuesday Week 12: Programming\nLanguages.” March 21, 2023. https://lsinks.github.io/posts/2023-03-21-tidytuesday-programming-languages/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Louise E. Sinks",
    "section": "",
    "text": "Hello! I’m Louise Sinks. Welcome to my website. I’m a chemist by training, and I’ve spent my career designing, running, and analyzing a variety of types of experiments.\nYou’ll find some of my projects for former employers in the portfolio section. Most of my research was on advanced functional materials with novel photophysical properties. In addition to spectroscopy, I employed statistical learning/ fitting methods, as interpretability is of primary importance in the sciences. Of course, being in charge of the experimental design gives you a huge advantage if you are trying to develop an interpretable model, as you can make sure you thoroughly explore the relevant parameter space. Most of my modeling and simulation work was done in Matlab, which seems to be a more popular language in academia than in industry and business.\nYou’ll also find my blog, where I do small projects to master new languages (R and Python currently). These are generally written as tutorials; throughout my career, I’ve benefitted immensely from code chunks and explanations I’ve found online, so this is my way of paying it forward. The blog posts are written in Quarto, and the raw .qmd files can be found in my GitHub repo. These can be downloaded and run in RStudio or another IDE that supports Quarto. The current projects section includes upcoming topics for the blog."
  },
  {
    "objectID": "blogroll.html",
    "href": "blogroll.html",
    "title": "Blog Roll",
    "section": "",
    "text": "Blogs:\nR Bloggers\nTutorials I’ve followed:\nWord Games Project:\nr letter frequency in R packages via R-Bloggers\nSetting Up Webpage:\nCreating Quarto Websites by Sam Csik\nAlbert Rapp: The ultimate guide to starting a Quarto blog"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me/ CV",
    "section": "",
    "text": "More details about my background below if you are interested. Links in the employer name will take you to a more narrative write up."
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me/ CV",
    "section": "TECHNICAL SKILLS",
    "text": "TECHNICAL SKILLS\nR Programming, MATLAB, VBA, Python, SQL, Excel/ Google Sheets, Stat-Ease, Tableau, Data Presentation, Data Cleaning, Exploratory Data Analysis (EDA), Data Visualization, Project Management, Web Scraping (rvest), Mapping (leaflet), Statistical Analysis, Statistical Process Control, Design of Experiments, Fitting, Modeling/ Machine Learning, Simulations"
  },
  {
    "objectID": "about.html#employment",
    "href": "about.html#employment",
    "title": "About Me/ CV",
    "section": "EMPLOYMENT",
    "text": "EMPLOYMENT\nFreelance Technical Consultant (part-time), 2017-present\n\nPrepares technical responses to US and foreign Patent Office inquiries and works with the patent lawyer to get claims approved\nAdvises on the technical and business merit of patent claims and drafts patent claims to improve the strategic value of the patent\n\nUS Nano LLC\nVice President, 2012-2017\nSenior Scientist, 2011-2012\nProject Management\n\nWrote and managed grants to fund research, leading to ~$924,000 of NSF SBIR grants (non-dilutive funding)\nDirected US Nano’s research activities to develop semiconductor nanowire technologies and managed a team of 5-10 technical and support staff, resulting in two US patents for scalable synthesis of nanowires and the reduction of the materials cost by 140-fold\n\nTechnical & Analytical Work\n\nIntroduced applied statistical methods such as DOE (Design of Experiments), leading to such improvements as 3-fold faster development of new materials and a one million-fold improvement in printed photosensor performance\nImplemented Statistical Process Control on manufacturing activities, which allowed detection of issues earlier in the workflow, saving ~30 person-hours and thousands of dollars in materials cost per prevented mis-run\nWrote custom Excel VBA macros to reduce analysis and reporting time for specific experiments from hours to under five minutes\nDesigned Excel dashboards with pivot tables and graphs, allowing the scientific team to data mine all experiments performed by the company, often leading to insights beyond the planned experimental results\nFormulated Nanowire inks using experimental designs based on fractional factor designs and Hansen solubility parameters to create ink-jet inks that were higher performing than commercially available carbon nanotube analogs.\nBuilt a photo/electrical test station, performed device testing, and wrote Matlab code to automatically analyze the data, streamlining the process and reducing analysis errors\n\nMarketing & Business Development efforts\n\nGenerated copy and visualizations for white papers, company website, and other promotional materials\nDeveloped and presented pitches to Angels, VCs, and potential industrial partners\nInterviewed ~50 potential customers through the Michigan I-Corps program to understand needs around nanowire technology\n\nProf. Sergei A. Vinogradov, University of Pennsylvania\nPostdoctoral Research Fellow, 2008-2011\n\nDeveloped a new two-photon lifetime imaging microscopy technique for quantifying oxygen content in cells and produced the first 3D phosphorescence lifetime and intensity images\nWrote Matlab code to simulate the microscope’s point spread function to understand the relationship between resolution and signal-to-noise ratio\nValidated and characterized new fluorescent or phosphorescent oxygen, zinc, and pH sensing probes\nWrote Matlab code to fit data and extract photophysical parameters such as excited state lifetimes, two-photon absorption cross-sections, Stern-Volmer constants, and quantum yields of emission\nPerformed cell culture work to maintain cells (HeLa, macrophages, fibroblasts) used in microscopy\nCollaborated with multiple research groups to explore methods of loading probe molecules into cells, such as microporation, transfection agents, scrape loading, and endocytosis of vesicles\nPerformed various assays to determine the viability of cells after loading with the probe molecules and under experimental conditions such as irradiation with light\n\nCenter for Oxygen Microscopic Imaging University of Aarhus, July-August 2009\nVisiting Researcher\n\nTested cytotoxic effects of fluorescent probes developed in Vinogradov lab\nPerformed singlet oxygen microscopy to study reactive oxygen species\nLearned and performed patch-clamp technique (whole-cell electrophysiological measurements) to study the effect of localized reactive oxygen species generation on ion channel function in mouse neurons\n\nProf. R. Hochstrasser & Prof. M. J. Therien, University of Pennsylvania\nNIH National Research Service Award Postdoctoral Fellow, 2005-2008\nPostdoctoral Research Fellow, 2004-2005\n\nMaintained and improved femtosecond spectrometer, including redesigning modules to reduce temporal chirp 6-fold and building a VIS pump/ IR-probe module\nUtilized IR-pump/ probe, VIS-pump/probe, 2DIR, and VIS-pump/ IR-probe ultra-fast techniques to study the influence of the environment on biologically relevant processes such as proton-coupled electron transfer.\nAssisted in grant writing for group funding and presented research overviews to external review committees during audits and evaluations.\nModeled various photophysical processes using OriginPro and Matlab\n\nFord Motor Company, Summers 1998, 1999\nProfessional/Technical Trainee (Intern)\n\nContributed to developing a more sensitive and durable pressure-sensitive paint (PSP) for aerodynamic applications, resulting in two Ford confidential papers and an invention disclosure.\nResearched appropriate polymer bases, identified chromophores with high sensing potential, and designed and implemented standards to evaluate PSPs.\nAssisted in constructing and automating an emission spectrometer that could apply various gas mixtures to the sample.\nHelped implement Pressure Sensitive Paint measurement technique in a Ford wind tunnel and demonstrated the system to United States Air Force and Boeing collaborators."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me/ CV",
    "section": "EDUCATION",
    "text": "EDUCATION\nNorthwestern University, Evanston, IL\nThesis Advisor: Prof. M. R. Wasielewski\nPhD in Chemistry, Dissertation Title: Role of Environment on Photo-induced Electron Transfer\nMS in Chemistry, Thesis Title: Photo-induced Electron Transfer of Donor-Acceptor Molecules in a Liquid Crystalline Environment\nResearched solvent/media effects on photo-induced electron transfer in organic covalent donor-acceptor systems as well as the role of conformational gating on electronic coupling of donors and acceptors. Studied impact on π- π driven self-assembly on electron and energy transfer within the nanostructure. Designed, constructed, and maintained two femtosecond transient absorption laser systems. Synthesized donor-acceptor molecules.\nUniversity of Virginia, Charlottesville, VA\n\nIntermediate Honors\nEchols Scholar (College of Arts and Sciences honors program)\nMajor: BS in Chemistry with Honors, BA in Physics Minor: Math"
  },
  {
    "objectID": "about.html#grants-and-awards",
    "href": "about.html#grants-and-awards",
    "title": "About Me/ CV",
    "section": "GRANTS AND AWARDS",
    "text": "GRANTS AND AWARDS\n\nNSF SBIR Phase II: Innovations in Nanowire Manufacturing: Large Scale Synthesis of Inorganic Semiconducting Nanowires and Application to Printed Photosensors (April 2014- June 2016)\nNSF SBIR Phase IB: Supplemental Funding to “Innovations in Nanowire Manufacturing: Large Scale Synthesis of Inorganic Semiconducting Nanowires and Application to Printed Electronics.” (January 2013- June 2013)\nNSF SBIR Phase I: Innovations in Nanowire Manufacturing: Large Scale Synthesis of Inorganic Semiconducting Nanowires and Application to Printed Electronics. (June 2012-December 2012)\nNIH National Research Service Award Postdoctoral Fellowship (January 2005- June 2007)"
  },
  {
    "objectID": "about.html#patents",
    "href": "about.html#patents",
    "title": "About Me/ CV",
    "section": "PATENTS",
    "text": "PATENTS\n\nSynthesis of CdSe/ZnS Core/Shell Semiconductor Nanowires, US Patent 9,627,299. Issued Apr 18, 2017\nApparatus and Methods for Continuous Flow Synthesis of Semiconductor Nanowires, US 9,306,110. Issued Apr 5, 2016"
  },
  {
    "objectID": "about.html#certifications",
    "href": "about.html#certifications",
    "title": "About Me/ CV",
    "section": "CERTIFICATIONS",
    "text": "CERTIFICATIONS\nDataCamp Data Scientist Professional Certificate, Google Data Analytics Certificate"
  },
  {
    "objectID": "about.html#portfolio",
    "href": "about.html#portfolio",
    "title": "About Me/ CV",
    "section": "PORTFOLIO",
    "text": "PORTFOLIO\nPeer reviewed papers: Google Scholar\nTableau: https://public.tableau.com/app/profile/louise.sinks\nWebsite: https://lsinks.github.io/"
  },
  {
    "objectID": "about.html#press",
    "href": "about.html#press",
    "title": "About Me/ CV",
    "section": "PRESS",
    "text": "PRESS\nFlorida High Tech Corridor 2016 Faces of Technology\nPrint article, p. 38\nVideo Interview\nBiz(941)’s 2015 Person to Watch List\nAssembling functional nanowire yarns with light, Nanowerk Blog"
  },
  {
    "objectID": "about.html#selected-talks",
    "href": "about.html#selected-talks",
    "title": "About Me/ CV",
    "section": "SELECTED TALKS",
    "text": "SELECTED TALKS\n“Visible Pump/ IR Probe Technique”, 1st Annual Ultrafast Spectroscopy Workshop Sarasota, FL January 2010\n“Oxygen Microscopy with Two-Photon-Enhanced Phosphorescent Nanoprobes”, Optical Society of America Spring Congress: Novel Techniques in Microscopy Vancouver, Canada April 2009\n“Oxygen microscopy with two-photon enhanced phosphorescence nanoprobes”, Chemical Biophysics Mini-Symposium Series: Lasers in Chemistry, Biochemistry, and Medicine Philadelphia, PA March 2009\n“Photophysics of porphyrins in solution and in films”, International Conference on Porphyrins and Phthalocyanines Rome, Italy July 2006\n“Photophysics of Self-Assembled Multichromophoric Arrays”, Argonne National Laboratory Center for Nanoscale Materials Seminar Series Argonne, IL July 2005"
  },
  {
    "objectID": "about.html#poster-presentations",
    "href": "about.html#poster-presentations",
    "title": "About Me/ CV",
    "section": "POSTER PRESENTATIONS",
    "text": "POSTER PRESENTATIONS\nHoward Hughes Medical Institute Janelia Research Campus’s Symposium on Multiphoton Imaging: The Next 6X10^23 Femtoseconds, 2011\nElectron Donor-Acceptor Interactions Gordon Conference in Newport, RI 2004\n30th Reaction Mechanisms Conference in Evanston, IL 2004\n15th Inter-American Photochemical Society Winter Conference in Tempe, AZ 2004\nThe 7th Northwestern University Industrial Associates Conference in Evanston, IL 2003\nElectron Donor-Acceptor Interactions Gordon Conference in Newport, RI 2002\nChicago Section of the Society of Applied Spectroscopy in Evanston, IL 2001"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About Me/ CV",
    "section": "PUBLICATIONS",
    "text": "PUBLICATIONS\n(Collectively have more than 3200 citations, and 10 papers have over 100 citations)\n32. Park, J.; Park, T.-H.; Sinks, L. E.; Deria, P.; Park, J.; Baik, M.-H.; Therien, M. J. Unusual Solvent Polarity Dependent Excitation Relaxation Dynamics of a Bis [p-Ethynyldithiobenzoato] Pd-Linked Bis [(Porphinato) Zinc] Complex. Molecular Systems Design & Engineering 2018, 3 (1), 275–284.\n31. Devor, A., Sakadžić, S; Yaseen, M. A.; Roussakis,E.; Tian, P.; Slovin, H.; Vanzetta, I.; Teng, I.; Saisan, P. A.; Sinks, L. E.; Dale, A. M.; Vinogradov, S.V.; Boas, D.A. Functional imaging of cerebral oxygenation with intrinsic optical contrast and phosphorescent probes, Optical Imaging of Neocortical Dynamics 2014\n30. Fry, H. C.; Lehmann, A.; Sinks, L. E.; Asselberghs, I.; Tronin, A.; Krishnan, V.; Blasie, J. K.; Clays, K.; DeGrado, W. F.; Saven, J. G.; Therien, M. J. Computational de novo design and characterization of a protein that selectively binds a highly hyperpolarizable abiological chromophore. J. Am. Chem. Soc. 2013, 135, 13914–26.\n29. Petchsang, N.; McDonald, M. P.; Sinks, L. E.; Kuno, M. Light induced nanowire assembly: the electrostatic alignment of semiconductor nanowires into functional macroscopic yarns. Advanced materials (Deerfield Beach, Fla.) 2013, 25, 601–5.\n28. Pedersen, B. W.; Sinks, L. E.; Breitenbach, T.; Schack, N. B.; Vinogradov, S. A.; Ogilby, P. R. Single cell responses to spatially controlled photosensitized production of extracellular singlet oxygen. Photochemistry and Photobiology 2011, 87, 1077–1091.\n27. Sinks, L. E.; Roussakis, E.; Sakadžic, S.; Robbins, G. P.; Hammer, D. A.; Devor, A.; Boas, D. A.; Vinogradov, S. A. Two-photon phosphorescence lifetime microscopy (2PLM) for high resolution imaging of oxygen. In Proceedings of SPIE 2011, Vol. 7903, p. 79032A–79032A–8.\n26. Ishizuka, T.; Sinks, L. E.; Song, K.; Hung, S.-T.; Nayak, A.; Clays, K.; Therien, M. J. The roles of molecular structure and effective optical symmetry in evolving dipolar chromophoric building blocks to potent octopolar nonlinear optical chromophores. J. Am. Chem. Soc. 2010, 133, 2884–2896.\n25. Sinks, L. E.; Roussakis, E.; Esipova, T. V.; Vinogradov, S. A. Phosphorescent Dendritic Nanoprobes for Biological Imaging of Oxygen. Abstracts of Papers of the American Chemical, 2010, Vol. 239.\n24. Deria, Pravas; Sinks, Louise; Park, Tae-Hong; Tomezsko, Diana; Brukman, Matthew; Bonnell, Dawn; Therien, Michael, Phase Transfer Catalysts Drive Diverse Organic Solvent Solubility of Single-Walled Carbon Nanotubes Helically Wrapped by Ionic, Semi-Conducting Polymers. Nano Letters, 2010, 10(10), 4192-4199.\n23. Sinks, L. E.; Robbins, G. P.; Roussakis, E.; Troxler, T.; Hammer, D. A.; Vinogradov, S. A., Two-Photon Microscopy of Oxygen: Polymersomes as Probe Carrier Vehicles. J. Phys. Chem. B, 2010, 114(45), 14373-14382.\n22. Sinks, L. E.; Roussakis, E.; Esipova Tatiana, V.; Vinogradov Sergei, A., Synthesis and calibration of phosphorescent nanoprobes for oxygen imaging in biological systems. Journal of visualized experiments : JoVE 2010, (37).\n21. Sinks, L. E.; Finikova, O. S.; Vinogradov, S. A. Oxygen Microscopy with Two-Photon-Enhanced Phosphorescent Nanoprobes. Novel Techniques in Microscopy; Optical Society of America, 2009.\n20. Giaimo, J. M.; Lockard, J. V.; Sinks, L. E.; Scott, A. M.; Wilson, T. M.; Wasielewski, M. R., Excited Singlet States of Covalently Bound, Cofacial Dimers and Trimers of Perylene-3,4:9,10-bis(dicarboximide)s. J. Phys. Chem. A 2008, 112 (11), 2322-30.\n19. Kelley, R. F.; Shin, W. S.; Rybtchinski, B.; Sinks, L. E.; Wasielewski, M. R., Photoinitiated Charge Transport in Supramolecular Assemblies of a 1,7,N,N’-Tetrakis(zinc porphyrin)-perylene-3,4:9,10-bis(dicarboximide). J. Am. Chem. Soc. 2007, 129 (11), 3173-81.\n18. Sinks, L. E.; Frail, P. R.; Therien, M. J. Symposium Lectures-Photophysics of Porphyrins in Solution and in Films. Journal of Porphyrins and Phthalocyanines 2006, 10 (4–6), 399–399.\n17. Kumar, K.; Sinks, L. E.; Wang, J.; Kim, Y. S.; Hochstrasser, R. M., Coupling between C-D and CO motions using dual-frequency two-dimensional IR photon echo spectroscopy. Chem. Phys. Lett. 2006, 432 (1-3), 122-27.\n16. Duncan, T. V.; Susumu, K.; Sinks, L. E.; Therien, M. J., Exceptional Near-Infrared Fluorescence Quantum Yields and Excited-State Absorptivity of Highly Conjugated Porphyrin Arrays. J. Am. Chem. Soc. 2006, 128 (28), 9000-01.\n15. Sinks, L. E.; Weiss, E. A.; Giaimo, J. M.; Wasielewski, M. R., Effect of charge delocalization on radical ion pair electronic coupling. Chem. Phys. Lett. 2005, 404 (4-6), 244-49.\n14. Sinks, L. E.; Rybtchinski, B.; Iimura, M.; Jones, B. A.; Goshe, A. J.; Zuo, X.; Tiede, D. M.; Li, X.; Wasielewski, M. R., Self-Assembly of Photofunctional Cylindrical Nanostructures Based on Perylene-3,4:9,10-bis(dicarboximide). Chem. Mater. 2005, 17 (25), 6295-303.\n13. Sinks, L.; Fuller, M. J.; Liu, W.; Ahrens, M. J.; Wasielewski, M. R., Photoinduced electron transfer in a donor-acceptor dyad oriented by an aligned nematic liquid crystal solvent. Chem. Phys. 2005, 319 (1-3), 226-34.\n12. Lewis, F. D.; Sinks, L. E.; Weigel, W.; Sajimon, M. C.; Crompton, E. M., Ultrafast Proton Transfer Dynamics of Hydroxystilbene Photoacids. J. Phys. Chem. A 2005, 109 (11), 2443-51.\n11. Goldsmith, R. H.; Sinks, L. E.; Kelley, R. F.; Betzen, L. J.; Liu, W.; Weiss, E. A.; Ratner, M. A.; Wasielewski, M. R., Wire-like charge transport at near constant bridge energy through fluorene oligomers. Proc. Natl. Acad. Sci. U. S. A. 2005, 102 (10), 3540-45.\n10. Fuller, M. J.; Sinks, L. E.; Rybtchinski, B.; Giaimo, J. M.; Li, X.; Wasielewski, M. R., Ultrafast Photoinduced Charge Separation Resulting from Self-assembly of a Green Perylene-based Dye into pi-Stacked Arrays. J. Phys. Chem. A 2005, 109 (6), 970-75.\n9. Weiss, E. A.; Sinks, L. E.; Lukas, A. S.; Chernick, E. T.; Ratner, M. A.; Wasielewski, M. R., Influence of Energetics and Electronic Coupling on Through-Bond and Through-Space Electron Transfer within U-Shaped Donor-Bridge-Acceptor Arrays. J. Phys. Chem. B 2004, 108 (29), 10309-16.\n8. Weiss, E. A.; Ahrens, M. J.; Sinks, L. E.; Ratner, M. A.; Wasielewski, M. R., Solvent Control of Spin-Dependent Charge Recombination Mechanisms within Donor-Conjugated Bridge-Acceptor Molecules. J. Am. Chem. Soc. 2004, 126 (31), 9510-11.\n7. Weiss, E. A.; Ahrens, M. J.; Sinks, L. E.; Gusev, A. V.; Ratner, M. A.; Wasielewski, M. R., Making a molecular wire: charge and spin transport through para-phenylene oligomers. J. Am. Chem. Soc. 2004, 126 (17), 5577-84.\n6. Rybtchinski, B.; Sinks, L. E.; Wasielewski, M. R., Photoinduced Electron Transfer in Self-Assembled Dimers of 3-Fold Symmetric Donor-Acceptor Molecules Based on Perylene-3,4:9,10-bis(dicarboximide). J. Phys. Chem. A 2004, 108 (37), 7497-505.\n5. Rybtchinski, B.; Sinks, L. E.; Wasielewski, M. R., Combining Light-Harvesting and Charge Separation in a Self-Assembled Artificial Photosynthetic System Based on Perylenediimide Chromophores. J. Am. Chem. Soc. 2004, 126 (39), 12268-69.\n4. Li, X.; Sinks, L. E.; Rybtchinski, B.; Wasielewski, M. R., Ultrafast Aggregate-to-Aggregate Energy Transfer within Self-assembled Light-Harvesting Columns of Zinc Phthalocyanine Tetrakis(Perylenediimide). J. Am. Chem. Soc. 2004, 126 (35), 10810-11.\n3. Ahrens, M. J.; Sinks, L. E.; Rybtchinski, B.; Liu, W.; Jones, B. A.; Giaimo, J. M.; Gusev, A. V.; Goshe, A. J.; Tiede, D. M.; Wasielewski, M. R., Self-Assembly of Supramolecular Light-Harvesting Arrays from Covalent Multi-Chromophore Perylene-3,4:9,10-bis(dicarboximide) Building Blocks. J. Am. Chem. Soc. 2004, 126 (26), 8284-94.\n2. Sinks, L. E.; Wasielewski, M. R., Effects of solvent and structural dynamics on electron transfer reactions of 4-Aminonaphthalene-1,8-dicarboximide donor-acceptor molecules in nematic liquid crystals and isotropic solvents. J. Phys. Chem. A 2003, 107 (5), 611-20.\n1. Andersson, M.; Sinks, L. E.; Hayes, R. T.; Zhao, Y.; Wasielewski, M. R., Bio-inspired optically controlled ultrafast molecular AND gate. Angew. Chem., Int. Ed. 2003, 42 (27), 3139-43."
  },
  {
    "objectID": "currentprojects.html",
    "href": "currentprojects.html",
    "title": "Current Projects",
    "section": "",
    "text": "I generally have a couple of big projects I’m working on that are “works in progress” and haven’t made it to a blog post yet. Links to the drafts are included, but they are just drafts.\nWhat I’m working on now:"
  },
  {
    "objectID": "currentprojects.html#family-history-project",
    "href": "currentprojects.html#family-history-project",
    "title": "Current Projects",
    "section": "Family History Project",
    "text": "Family History Project\nTombstones (in R)- I’m working on a project for my father that will culminate in a website for his genealogy research. There are a couple of different parts that I’m working on independently.\n\nLinking photos of family gravestones to an Excel sheet that records the GPS location of the tombstones. This combined dataset will be used to generate a leaflet map (and is currently called leaflet_tester). I will probably separate this into two or three portions, though.\n\nthe data cleanup and the actual matching (done and posted)\nleaflet styling (done and posted)\nweb scraping portion to add more information about some of the ancestors (done and posted)\nMy father also gave me several papers he’s written about family members. I want to write code to determine which family members are mentioned in each document and match them to his Excel sheet. (not started)\n\n\nI’ve spent a surprising large amount of time today learning how to handle “large” files. The photos are added to the tombstone repo using git’s lfs if anyone wanted to run my code. There are about 2 GB of photos, compressed to ~300 MB. The map with photos is about 500 MB, which is also too large for github pages. That’s okay, since I plan to host it on a different website devoted entirely to my father’s family history research."
  },
  {
    "objectID": "currentprojects.html#credit-card-fraud",
    "href": "currentprojects.html#credit-card-fraud",
    "title": "Current Projects",
    "section": "Credit Card Fraud",
    "text": "Credit Card Fraud\nI’m also exploring the credit card fraud project in other languages. I previously wrote two R Tidymodels tutorials using this data. They can be found here and here.\nBy using the same dataset, I can focus on learning the new language instead of thinking the higher level aspects. And I’m able to spot errors and mistakes much more easily.\nCredit Card Fraud (Tableau)- I’ve been working on replicating the credit card fraud EDA in Tableau. I have an adequate draft, but I need to go back and fix the tooltips. There were some interesting results concerning the geographic variables. I made some assumptions when I did the modeling in R that weren’t correct. I will write up some lessons I learned while working in Tableau, but I also want to see how I could have caught that while working in R.\nCredit Card Fraud (Python)- I am rebuilding my credit card fraud project in Python to improve my skills. I’m also learning how to use Python within R Studio and the ins and outs of the reticulate package."
  },
  {
    "objectID": "posts/2023-03-14-tester-post/index.html",
    "href": "posts/2023-03-14-tester-post/index.html",
    "title": "Creating a Blog",
    "section": "",
    "text": "This is my first blog entry. I am following the tutorials here:\nhttps://ucsb-meds.github.io/creating-quarto-websites/#where-you-should-start-changing-stuff\nhttps://samanthacsik.github.io/posts/2022-10-24-quarto-blogs/\nGenerally, this process has been a nightmare. The website is being created within RStudio, then pushed to GitHub and published through GitHub pages. As I’ve made changes per the tutorial, I have repeatedly been unable to push changes to GitHub due to a variety of fatal errors and merge conflicts. Since I’m only working in a single place I have no idea where all these merge conflicts are arising from. I don’t understand how I can have everything in sync everywhere, make a local change, commit it, and then be unable to push it. I’ve had to delete the GitHub repository at least half a dozen times and recreate it from my local version because I couldn’t find any way to fix the conflicts and fatal errors. I’m not sure whose fault this is (Quarto, GitHub or RStudio). It could be my fault, but I really don’t understand why things are breaking so spectacularly. I’ve used git/ GitHub for version control of R projects before and I’ve never had an error. (I don’t really see how you can get a merge conflict if you are the only person working on a project and you are only working at a single location, but maybe I’m failing to envision some use case.)\nI decided to go with Quarto because it is now built-in to RStudio and the tutorials by Samantha Csik seemed very clear. (And to be fair, they are! Very easy to follow.) The tutorials I found for R Markdown to make a website seemed a little more involved and a little more kludgey.\nCouldn’t have done it without the best helper turtle in the world. Mac took a lot of executive naps to work on the problem.\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E.},\n  title = {Creating a {Blog}},\n  date = {2023-03-14},\n  url = {https://lsinks.github.io/posts/2023-03-14-tester-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E. 2023. “Creating a Blog.” March 14, 2023.\nhttps://lsinks.github.io/posts/2023-03-14-tester-post/."
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "",
    "text": "This week, I participated in my first #TidyTuesday challenge. My goal was to get something out on the day of the challenge rather than perfection. I did notice that the skimr output wasn’t formatted nicely on the webpage. Today, I’m going to delve into the skimr and Quarto documentation and make a nicer version of the output. Secondly, I’m going to fix up my final figure, which is the one I shared on social media:"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#skimr-to-understand-your-data",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#skimr-to-understand-your-data",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "Skimr to understand your data",
    "text": "Skimr to understand your data\nSkimr is a package that provides statistical summaries of the variables in your dataframe. It also provides information about the missingness of each variable.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(ggrepel)\nlibrary(visdat) # visualizing missing data in dataframe\n\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 12)\n\nlanguages &lt;- tuesdata$languages\n\n\nCustomizing the skim Output\nMy main objection is that the numerical summary is too wide and has a scroll bar. I especially want the histogram to be viewable on the first screen. I also don’t particularly care about all the quartile information; min and max are enough. If I want to delve more into the stats of a variable, I will do it another way, not with skimr.\nFirst, quarto lets you expand the output of the code chunk to fill the page via the option “#| column: page”, so I’ll do that. Next, I’ll create a custom skim function that drops the p25, p50, and p75 output from the summary of the numerical variables.\n\nmy_skim &lt;- skim_with(numeric = sfl(p25 = NULL, p50 = NULL, p75 = NULL)) \n\nmy_skim(languages)\n\n\nData summary\n\n\nName\nlanguages\n\n\nNumber of rows\n4303\n\n\nNumber of columns\n49\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n21\n\n\nlogical\n4\n\n\nnumeric\n24\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npldb_id\n0\n1.00\n1\n52\n0\n4303\n0\n\n\ntitle\n0\n1.00\n1\n56\n0\n4267\n0\n\n\ndescription\n3480\n0.19\n4\n2273\n0\n811\n0\n\n\ntype\n0\n1.00\n2\n27\n0\n40\n0\n\n\ncreators\n3203\n0.26\n2\n253\n0\n985\n0\n\n\nwebsite\n2928\n0.32\n13\n131\n0\n1368\n0\n\n\ndomain_name\n3588\n0.17\n6\n32\n0\n700\n0\n\n\nreference\n2314\n0.46\n15\n251\n0\n1955\n0\n\n\ngithub_repo\n3402\n0.21\n25\n73\n0\n897\n0\n\n\ngithub_repo_description\n3438\n0.20\n4\n419\n0\n853\n0\n\n\ngithub_language\n3829\n0.11\n1\n30\n0\n474\n0\n\n\ngithub_language_tm_scope\n3837\n0.11\n4\n34\n0\n361\n0\n\n\ngithub_language_type\n3837\n0.11\n4\n11\n0\n4\n0\n\n\ngithub_language_ace_mode\n3838\n0.11\n1\n16\n0\n96\n0\n\n\ngithub_language_file_extensions\n3833\n0.11\n1\n606\n0\n466\n0\n\n\nwikipedia\n2731\n0.37\n32\n104\n0\n1566\n0\n\n\nwikipedia_summary\n2884\n0.33\n17\n6741\n0\n1407\n0\n\n\nwikipedia_related\n3145\n0.27\n1\n1761\n0\n1059\n0\n\n\nline_comment_token\n3831\n0.11\n1\n7\n0\n23\n0\n\n\norigin_community\n1190\n0.72\n3\n305\n0\n2232\n0\n\n\nfile_type\n3213\n0.25\n2\n6\n0\n4\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nfeatures_has_comments\n3683\n0.14\n1.00\nTRU: 617, FAL: 3\n\n\nfeatures_has_semantic_indentation\n3722\n0.14\n0.11\nFAL: 516, TRU: 65\n\n\nfeatures_has_line_comments\n3765\n0.13\n0.96\nTRU: 517, FAL: 21\n\n\nis_open_source\n3792\n0.12\n0.89\nTRU: 453, FAL: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np100\nhist\n\n\n\n\nappeared\n0\n1.00\n1991.11\n111.44\n-2000\n2023\n▁▁▁▁▇\n\n\ndomain_name_registered\n3801\n0.12\n2011.33\n7.02\n1990\n2023\n▁▃▃▇▆\n\n\nisbndb\n3217\n0.25\n7.71\n33.16\n0\n400\n▇▁▁▁▁\n\n\nbook_count\n0\n1.00\n2.08\n17.34\n0\n401\n▇▁▁▁▁\n\n\nsemantic_scholar\n3545\n0.18\n3.79\n8.32\n0\n52\n▇▁▁▁▁\n\n\nlanguage_rank\n0\n1.00\n2151.00\n1242.31\n0\n4302\n▇▇▇▇▇\n\n\ngithub_repo_stars\n3414\n0.21\n2127.40\n7554.02\n0\n88526\n▇▁▁▁▁\n\n\ngithub_repo_forks\n3417\n0.21\n261.29\n1203.00\n0\n23732\n▇▁▁▁▁\n\n\ngithub_repo_updated\n3418\n0.21\n2021.39\n1.76\n2012\n2023\n▁▁▁▁▇\n\n\ngithub_repo_subscribers\n3418\n0.21\n62.34\n200.88\n0\n2910\n▇▁▁▁▁\n\n\ngithub_repo_created\n3425\n0.20\n2015.84\n3.48\n2006\n2022\n▁▅▇▇▇\n\n\ngithub_repo_issues\n3518\n0.18\n123.03\n546.26\n0\n9522\n▇▁▁▁▁\n\n\ngithub_repo_first_commit\n3567\n0.17\n2014.74\n4.99\n1987\n2022\n▁▁▁▆▇\n\n\ngithub_language_repos\n3833\n0.11\n197134.67\n1226900.57\n0\n16046489\n▇▁▁▁▁\n\n\nwikipedia_daily_page_views\n2837\n0.34\n227.13\n783.55\n-1\n13394\n▇▁▁▁▁\n\n\nwikipedia_backlinks_count\n2877\n0.33\n318.55\n1635.29\n1\n34348\n▇▁▁▁▁\n\n\nwikipedia_page_id\n2893\n0.33\n9167847.21\n13506832.90\n928\n63063548\n▇▁▁▁▁\n\n\nwikipedia_appeared\n2958\n0.31\n1991.14\n17.03\n1830\n2019\n▁▁▁▃▇\n\n\nwikipedia_created\n3040\n0.29\n2005.75\n3.77\n2001\n2020\n▇▇▂▁▁\n\n\nwikipedia_revision_count\n3130\n0.27\n330.43\n813.26\n1\n10104\n▇▁▁▁▁\n\n\nlast_activity\n0\n1.00\n2000.62\n84.60\n-900\n2023\n▁▁▁▁▇\n\n\nnumber_of_users\n0\n1.00\n13771.26\n227712.95\n0\n7179119\n▇▁▁▁▁\n\n\nnumber_of_jobs\n0\n1.00\n422.18\n12572.99\n0\n771996\n▇▁▁▁▁\n\n\ncentral_package_repository_count\n1482\n0.66\n0.00\n0.00\n0\n0\n▁▁▇▁▁\n\n\n\n\n\nThis output is much nicer. It is a bit wall of text though. I wouldn’t recommend using this in reports, but it is a useful tool when doing your initial dataset analysis. (As a side note, I have noticed skimr doesn’t work well on Kaggle. It performs as expected if you are in interactive mode, but it fails when you try to save the notebook or run non-interactively.)\n\n\nStyling skim output with gt\nIf, for some reason, you did need to include output/ visualizations about missingness in a report, I’d probably recreate visualizations or tables by class of variable, especially if you have many variables, as I do here.\nHere’s an example for numeric variables, of which there are 24 in the dataset. First, we will skim the data and then use the gt package to style the resulting dataframe as a table. I used a built-in style, but each table element can be individually customized.\n\nlanguages_numeric &lt;- languages %&gt;%\n  select_if(is.numeric)\n\nlang_numeric_skim &lt;- my_skim(languages_numeric)\n\nlang_numeric_skim %&gt;%\n  select(-skim_type)   %&gt;% \n  gt() %&gt;%\n  cols_label(n_missing = \"# Missing\", complete_rate = \"Completeness\", \n             numeric.mean = \"Mean\", numeric.sd = \"Standard Deviation\",\n             numeric.p0 = \"Min\", numeric.p100 = \"Max\",\n             numeric.hist = \"Histogram\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Summary of Numerical Variables in Languages\") \n\n\n\n\n\n  \n    \n      Summary of Numerical Variables in Languages\n    \n    \n    \n      skim_variable\n      # Missing\n      Completeness\n      Mean\n      Standard Deviation\n      Min\n      Max\n      Histogram\n    \n  \n  \n    appeared\n0\n1.0000000\n1.991105e+03\n1.114434e+02\n-2000\n2023\n▁▁▁▁▇\n    domain_name_registered\n3801\n0.1166628\n2.011333e+03\n7.021132e+00\n1990\n2023\n▁▃▃▇▆\n    isbndb\n3217\n0.2523821\n7.706262e+00\n3.316421e+01\n0\n400\n▇▁▁▁▁\n    book_count\n0\n1.0000000\n2.079479e+00\n1.734465e+01\n0\n401\n▇▁▁▁▁\n    semantic_scholar\n3545\n0.1761562\n3.794195e+00\n8.316231e+00\n0\n52\n▇▁▁▁▁\n    language_rank\n0\n1.0000000\n2.151000e+03\n1.242313e+03\n0\n4302\n▇▇▇▇▇\n    github_repo_stars\n3414\n0.2066000\n2.127403e+03\n7.554016e+03\n0\n88526\n▇▁▁▁▁\n    github_repo_forks\n3417\n0.2059029\n2.612867e+02\n1.203003e+03\n0\n23732\n▇▁▁▁▁\n    github_repo_updated\n3418\n0.2056705\n2.021390e+03\n1.763285e+00\n2012\n2023\n▁▁▁▁▇\n    github_repo_subscribers\n3418\n0.2056705\n6.234237e+01\n2.008820e+02\n0\n2910\n▇▁▁▁▁\n    github_repo_created\n3425\n0.2040437\n2.015843e+03\n3.479589e+00\n2006\n2022\n▁▅▇▇▇\n    github_repo_issues\n3518\n0.1824309\n1.230344e+02\n5.462553e+02\n0\n9522\n▇▁▁▁▁\n    github_repo_first_commit\n3567\n0.1710435\n2.014739e+03\n4.985409e+00\n1987\n2022\n▁▁▁▆▇\n    github_language_repos\n3833\n0.1092261\n1.971347e+05\n1.226901e+06\n0\n16046489\n▇▁▁▁▁\n    wikipedia_daily_page_views\n2837\n0.3406925\n2.271330e+02\n7.835524e+02\n-1\n13394\n▇▁▁▁▁\n    wikipedia_backlinks_count\n2877\n0.3313967\n3.185484e+02\n1.635289e+03\n1\n34348\n▇▁▁▁▁\n    wikipedia_page_id\n2893\n0.3276784\n9.167847e+06\n1.350683e+07\n928\n63063548\n▇▁▁▁▁\n    wikipedia_appeared\n2958\n0.3125726\n1.991144e+03\n1.702650e+01\n1830\n2019\n▁▁▁▃▇\n    wikipedia_created\n3040\n0.2935162\n2.005748e+03\n3.768240e+00\n2001\n2020\n▇▇▂▁▁\n    wikipedia_revision_count\n3130\n0.2726005\n3.304314e+02\n8.132556e+02\n1\n10104\n▇▁▁▁▁\n    last_activity\n0\n1.0000000\n2.000616e+03\n8.459776e+01\n-900\n2023\n▁▁▁▁▇\n    number_of_users\n0\n1.0000000\n1.377126e+04\n2.277129e+05\n0\n7179119\n▇▁▁▁▁\n    number_of_jobs\n0\n1.0000000\n4.221838e+02\n1.257299e+04\n0\n771996\n▇▁▁▁▁\n    central_package_repository_count\n1482\n0.6555891\n0.000000e+00\n0.000000e+00\n0\n0\n▁▁▇▁▁\n  \n  \n  \n\n\n\n\n\n\nVisualizing Missingness with visdat\nThe visdat package makes ggplot- compatible missingness plots. The cluster = TRUE option groups variables that share missingness. Here we see that usually if some of the GitHub data is missing, then all of the GitHub data is missing. The percent missing is listed for each variable, and the overall missingness of the dataset is shown in the legend.\nNote vis_miss doesn’t work on very large datasets. The documentation suggests keeping the number of records below 1,000. A more extensive package for exploratory visualizations called naniar could also be used.\n\nlanguages_numeric %&gt;%\nvis_miss(cluster = TRUE) +\nggtitle(\"Missing Data in the Languages Dataset\") +\n  #theme_classic() +\n  theme(axis.text.x = element_text(size = 8, angle = 90))"
  },
  {
    "objectID": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#improving-the-most-popular-language-for-each-comment-token-figure",
    "href": "posts/2023-03-24-tidytuesday-figure-polishing/index.html#improving-the-most-popular-language-for-each-comment-token-figure",
    "title": "TidyTuesday Week 12: Programming Languages Revisited",
    "section": "Improving “The Most Popular Language for Each Comment Token” Figure",
    "text": "Improving “The Most Popular Language for Each Comment Token” Figure\n\njoined &lt;- read_csv(\"processed_lang.csv\" , show_col_types = FALSE)\n\nNow the original figure:\n\njoined %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n # geom_point() +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n   ggtitle(\"The Most Popular Language for Each Comment Token\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic()\n\n\n\n\nI thought I had noted this in the previous post, but one of the tokens, ⍝ , is rendered as an empty box in the ggplot figures. I thought fixing this would be easy. First, I thought I could just pass the Unicode value for that symbol. Then, when that didn’t work, I thought I could change the font to one supporting that symbol. Supposedly, changing the font should be easy, yet after 3 hours working on it, I still had blank squares. There is a nice tutorial on changing fonts in ggplot that did not work until I found someone with the same issue. The solution is to add a line of code that doesn’t make much sense to me : windowsFonts(\"Cambria Math\" = windowsFont(\"Cambria Math\"))\nI saw a nice TidyTuesday figure on Twitter:\n\n\nMy submission for #TidyTuesday, Week 12 on programming languages. I explore jobs per users.Code: https://t.co/bV9DUHZmro pic.twitter.com/2D5YLnE5yz\n\n— Mitsuo Shiota (@mitsuoxv) March 21, 2023\n\n\nwith a caption referencing the original dataset. I’d like to add that. I generally want to increase the figure’s legibility and flip the color scale so that darker blue corresponds to more users. I also don’t think what popular means is entirely clear, so I’d like to explain more fully what I’m graphing.\n\nwindowsFonts(\"Cambria Math\" = windowsFont(\"Cambria Math\"))\njoined %&gt;%\n  ggplot(aes(line_comment_token, n, size = log(number_of_users), \n             color = log(number_of_users), label = title)) +\n  scale_y_log10() +\n  geom_text_repel(show.legend = FALSE) +\n    scale_colour_gradient(high = \"#08306b\", low = \"#6baed6\") + \n   labs(title = \"The Most Popular Language for Each Comment Token\",\n       subtitle = \"Based on # Users and Rank\",\n       caption = \"data from https://pldb.com/\") +\n  xlab(\"Token\") +\n  ylab(\"Number of languages using token\") +\n  theme_classic(base_size = 16) +\n  theme(text = element_text( family = \"Cambria Math\")) +\n  theme(axis.text.x = element_text(face = \"bold\"))"
  },
  {
    "objectID": "posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html",
    "href": "posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html",
    "title": "Tidy Tuesday: Daylight Savings Time",
    "section": "",
    "text": "This week’s TidyTuesday is about the timezone data from IANA timezone database.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(lubridate)\nlibrary(skimr)\nlibrary(lutz)\nlibrary(maps)\nlibrary(scales)\nlibrary(sf)\nlibrary(ggimage)\n\nThe history of this database is fascinating. It is used by many computer systems to determine the correct time based on location. To learn more, I recommend reading Daniel Rosehill’s article on the topic. For a drier history, check out the wikipedia article.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 13)\n\ntransitions &lt;- tuesdata$transitions\ntimezones &lt;- tuesdata$timezones\ntimezone_countries &lt;- tuesdata$timezone_countries\ncountries &lt;- tuesdata$countries\n\nIt is suggested that we change the begin and end variables in transitions to datetimes.\n\ntransitions &lt;- transitions %&gt;%\n  mutate(begin = as_datetime(begin), end = as_datetime(end))\n\nI was interested in how many countries had multiple times zones. I know the US has 4 time zones in the continental US.\n\nnum_zones &lt;- timezone_countries %&gt;%\n  count(country_code, sort = TRUE)\n\nnum_zones %&gt;% \n  filter(n &gt; 1) %&gt;%\n  left_join(countries) %&gt;%\n  select(place_name, n) %&gt;%\n  filter(place_name != \"NA\") %&gt;%\n  gt() %&gt;%\n  cols_label(place_name = \"Country\", n = \"Number of TZs\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Countries with Multiple TZs\") \n\n\n\n\n\n  \n    \n      Countries with Multiple TZs\n    \n    \n    \n      Country\n      Number of TZs\n    \n  \n  \n    United States\n29\n    Canada\n28\n    Russia\n27\n    Brazil\n16\n    Argentina\n12\n    Australia\n12\n    Mexico\n11\n    Kazakhstan\n7\n    Greenland\n4\n    Indonesia\n4\n    Ukraine\n4\n    Chile\n3\n    Spain\n3\n    Micronesia\n3\n    Kiribati\n3\n    Mongolia\n3\n    Malaysia\n3\n    French Polynesia\n3\n    Portugal\n3\n    US minor outlying islands\n3\n    Congo (Dem. Rep.)\n2\n    China\n2\n    Cyprus\n2\n    Germany\n2\n    Ecuador\n2\n    Marshall Islands\n2\n    New Zealand\n2\n    Papua New Guinea\n2\n    Palestine\n2\n    French Southern & Antarctic Lands\n2\n    Uzbekistan\n2\n    Vietnam\n2\n  \n  \n  \n\n\n\n\nAnd we find that the United States has 29!! time zones in the database. This was unexpected, so say the least. I thought maybe there were some times zones for territories and perhaps military bases that I did not know about. I also thought there might be some extra time zones arising from some states using daylight savings time, while others in the same area might not. I wanted to visualize where these times zones were.\n\nUS_tz &lt;- timezone_countries %&gt;% \n  filter(country_code == \"US\") %&gt;%\n  left_join(timezones)\n\nJoining with `by = join_by(zone)`\n\n\nI found the lutz package created nice pictograms about when a timezone shifts from DST and back. (This package uses the same underlying database that we are using here to determine when the shifts occur.)\n\n tz_plot(US_tz$zone[21])\n\n\n\n\nI created the plots and saved them as images. I modified a function I found on stack overflow to create the file names.\n\nwd &lt;- getwd()\nfilepath = file.path(wd)\n\n\nmake_filename = function(number){\n  # doing this, putting it all on a single line or using pipe %&gt;%\n  # is just matter of style\n  filename = paste(\"tzplot\", number, sep=\"_\")\n  filename = paste0(filename, \".png\")\n  filename = file.path(filepath, filename)\n  \n  filename\n}\n\n#creating a variable to store the files name\nUS_tz &lt;- US_tz %&gt;%\n  mutate(image_name = \"tbd\")\n\nindex &lt;- 1\nfor (index in seq(1, nrow(US_tz))) {\n  filename = make_filename(index)\n  US_tz[index , \"image_name\"] &lt;- filename\n  # 1. Open jpeg file\n  png(filename, width = 350, height = 350, bg = \"transparent\")\n  # 2. Create the plot\n  # you need to print the plot if you call it inside a loop\n  print(tz_plot(US_tz$zone[index]))\n  # 3. Close the file\n  dev.off()\n  index = index + 1\n}\n\nNext I created a world map, inspired by the one from\n\n\nMy submission for #TidyTuesday, Week 13 on time zones. I plot time zones in the world map.Code: https://t.co/y5Cm4tuaVk pic.twitter.com/BZC3anC5Oa\n\n— Mitsuo Shiota (@mitsuoxv) March 28, 2023\n\n\nI hadn’t previously used the maps package, so I appreciate being introduced to it. The maps package only has a mainland US map, so I used the world map. (Plus, as I mentioned, I thought some of these time zones would be in other parts of the world.) I followed a tutorial on Plotting Points as Images in ggplot and used the hints about aspect ratio to make my tz_plot circles remain circular. However, that did stretch the world a bit.\n\naspect_ratio &lt;- 1.618  \n\nus_tz_map &lt;- map_data(\"world\") %&gt;% \n  ggplot(aes(long, lat)) +\n  geom_polygon(aes(group = group), fill = \"white\", \n               color = \"gray30\", alpha = 0.9) +\n  geom_image(aes(x = longitude, latitude, image = image_name), \n             data = US_tz, size = 0.025, by = \"width\",\n             asp = aspect_ratio) +\n  coord_sf() +\n  labs(title = \"The United States has 29 Timezone- Mostly Redunant\",\n       caption = \"Data from: https://data.iana.org/time-zones/tz-link.html\") +\n  theme_void() +\n  theme(aspect.ratio = 1/aspect_ratio,\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = \"white\")\n    )\n\nggsave(\"thumbnail.png\", us_tz_map, width = 5 * aspect_ratio, height = 5)\nus_tz_map\n\n\n\n\nAnd what we see is there are a bunch of redundant times zone specification, especially in the Midwest.\n\nUS_tz %&gt;%\n  select(zone, latitude, longitude) %&gt;%\n  arrange(longitude) %&gt;%\n  gt() %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Countries with Multiple TZs\") \n\n\n\n\n\n  \n    \n      Countries with Multiple TZs\n    \n    \n    \n      zone\n      latitude\n      longitude\n    \n  \n  \n    America/Adak\n52.66667\n-177.13333\n    America/Nome\n64.56667\n-165.78333\n    Pacific/Honolulu\n21.71667\n-158.35000\n    America/Anchorage\n61.30000\n-149.91667\n    America/Yakutat\n60.35000\n-140.35000\n    America/Sitka\n57.75000\n-135.41667\n    America/Juneau\n58.41667\n-134.60000\n    America/Metlakatla\n55.73333\n-132.15000\n    America/Los_Angeles\n34.18333\n-118.80000\n    America/Boise\n44.41667\n-116.35000\n    America/Phoenix\n34.33333\n-112.46667\n    America/Denver\n40.08333\n-105.03333\n    America/North_Dakota/Beulah\n48.10000\n-102.43333\n    America/North_Dakota/Center\n48.08333\n-102.23333\n    America/North_Dakota/New_Salem\n47.53333\n-102.05000\n    America/Menominee\n45.56667\n-88.45000\n    America/Indiana/Vincennes\n39.30000\n-88.23333\n    America/Indiana/Petersburg\n39.00000\n-87.98333\n    America/Chicago\n41.85000\n-87.65000\n    America/Indiana/Tell_City\n38.13333\n-87.43333\n    America/Indiana/Knox\n42.03333\n-87.11667\n    America/Indiana/Marengo\n38.90000\n-87.01667\n    America/Indiana/Winamac\n41.13333\n-86.78333\n    America/Indiana/Indianapolis\n39.86667\n-86.63333\n    America/Kentucky/Louisville\n38.50000\n-86.31667\n    America/Kentucky/Monticello\n37.60000\n-85.78333\n    America/Indiana/Vevay\n39.60000\n-85.10000\n    America/Detroit\n43.20000\n-83.78333\n    America/New_York\n41.55000\n-74.38333\n  \n  \n  \n\n\n\n\n\n\n\nCitationBibTeX citation:@online{sinks2023,\n  author = {Sinks, Louise E.},\n  title = {Tidy {Tuesday:} {Daylight} {Savings} {Time}},\n  date = {2023-03-28},\n  url = {https://lsinks.github.io/posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSinks, Louise E. 2023. “Tidy Tuesday: Daylight Savings\nTime.” March 28, 2023. https://lsinks.github.io/posts/2023-03-28-tidytuesday-timezones/tidytuesday-timezones.html."
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 1",
    "section": "",
    "text": "The best way to learn a programming language is to code. Obvious, but sometimes it can be daunting to start a project when you are a novice. Influenced by my time at a start-up, I’ve found the best approach is to create a minimum viable product. This is the smallest working part of the project. By continuing to revise and iterate the code, you can rapidly detect weaknesses in your knowledge and take active steps to fix them.\nMy learning project was a Wordle Guess Generator. I will show you how I used this project to create a self-guided learning plan. Motivated partly by my desire to have good guesses for Septle (a seven-letter guessing game), this project has been a crucial part of my journey to learn R. Practicing and learning coding skills were more important to me than devising the optimal strategy; if you are interested in “the best” Wordle strategy, then you will probably find much better answers by Googling.\nFor those who don’t know, Wordle is a word-guessing gamer that gives you five guesses to identify a 5-letter word. Correct letters are shown in green or yellow with each guess, depending on whether they are appropriately placed or not. Incorrect letters are shown in gray.\nI wasn’t interested in coming up with the global “best” answer. I wanted to come up with some guesses that fit my gameplay style. I guess three words in a row without trying to solve the game. Then, with a fair number of letters known, I attempt to solve the word.\nThe code has undergone several significant overhauls as I’ve needed to learn new things and used this project as the test bed. And here it is again, as a vehicle to learn quarto/ markdown. Here I will show you the very first ugly code and how I critiqued and revised it over several months. I’ve put it all on GitHub. The project is unstructured, the code is ugly, and all the versioning was by filename."
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#self-guided-learning-the-ever-evolving-wordle-guess-generator",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#self-guided-learning-the-ever-evolving-wordle-guess-generator",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 1",
    "section": "",
    "text": "The best way to learn a programming language is to code. Obvious, but sometimes it can be daunting to start a project when you are a novice. Influenced by my time at a start-up, I’ve found the best approach is to create a minimum viable product. This is the smallest working part of the project. By continuing to revise and iterate the code, you can rapidly detect weaknesses in your knowledge and take active steps to fix them.\nMy learning project was a Wordle Guess Generator. I will show you how I used this project to create a self-guided learning plan. Motivated partly by my desire to have good guesses for Septle (a seven-letter guessing game), this project has been a crucial part of my journey to learn R. Practicing and learning coding skills were more important to me than devising the optimal strategy; if you are interested in “the best” Wordle strategy, then you will probably find much better answers by Googling.\nFor those who don’t know, Wordle is a word-guessing gamer that gives you five guesses to identify a 5-letter word. Correct letters are shown in green or yellow with each guess, depending on whether they are appropriately placed or not. Incorrect letters are shown in gray.\nI wasn’t interested in coming up with the global “best” answer. I wanted to come up with some guesses that fit my gameplay style. I guess three words in a row without trying to solve the game. Then, with a fair number of letters known, I attempt to solve the word.\nThe code has undergone several significant overhauls as I’ve needed to learn new things and used this project as the test bed. And here it is again, as a vehicle to learn quarto/ markdown. Here I will show you the very first ugly code and how I critiqued and revised it over several months. I’ve put it all on GitHub. The project is unstructured, the code is ugly, and all the versioning was by filename."
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#brainstorming-the-project.",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#brainstorming-the-project.",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 1",
    "section": "1. Brainstorming the project.",
    "text": "1. Brainstorming the project.\nBrainstorm what you want this project to do. Try to have a very clear statement about the project’s goal at the top. If you can think of self-contained modules, mark that. If you can sketch out the scaffold of the code- great! If you have some ideas about the results, even better. Put everything you can think of down. I had been thinking about this project for a while, so this brainstorming sheet is neat. Neatness and organization are not important at this point; getting stuff written down is.\nMy goal was “I want 3 initial guesses for Septle. Ideally, I’ll maximize the number of unique letters, and I want to preferentially pick from the most frequently used letters.”\nI decided my scoring would be based on the frequency letters occur in English; more common letters get a higher score than uncommon letters. To generate the score for the word, I proposed the following process:\n\nRead in letter frequencies\nScoring [the letters, originally I thought it would be based on rank, with the most common letter getting a 26 and the least common getting a 1. That is what the confusing little sketch is attempting to convey. A is not the most common letter, nor is Z the least common.]\nRead in the list of words\nCalculate the scores [of the words]\nPick the best word as a first guess [best meaning highest scoring]\n\nI also talk about how the frequency distribution of letters will likely differ between five and seven-letter words. I suggested looking at the frequency of letters in modern text instead of just frequency lists from linguists. I noted that certain letters are more likely to be in a specific position, and it could be valuable to constrain the guess to typical positions. An example is that “y” is almost always at the end of a word in English, so “sadly” might be a better guess than “yacht” for that reason. You are likelier to lock down a letter with a positionally accurate guess. I also said that I wanted a 4th Wordle guess. There are a lot of ideas here! That’s okay because I winnow them down in the next step.\n\n\n\nThe initial brainstorming session"
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#minimum-viable-product-what-is-the-smallest-program-that-works",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#minimum-viable-product-what-is-the-smallest-program-that-works",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 1",
    "section": "2. Minimum Viable Product: What is the smallest program that works?",
    "text": "2. Minimum Viable Product: What is the smallest program that works?\nPull out the minimum viable product from your brainstorming. What is the smallest result that would satisfy your goals? Is there a way to make things a bit smaller? I would size it so you can get working code to accomplish the goal in a few hours.\nI chose to generate four guesses for Wordle. I also decided to generate my frequency table from the word list itself. I found a five-letter word list that I could download and read in, but all the letter frequency tables I found were on blogs or in articles, and the only way I could see to get them into my program was to type them in and I was too lazy to do that. I decided to implement the process I outlined in the brainstorming session and calculate four guesses."
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#write-some-bad-code.",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#write-some-bad-code.",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 1",
    "section": "3. Write some bad code.",
    "text": "3. Write some bad code.\nWrite some code that does that thing. It will be ugly. If you can’t figure something out, do it the wrong way. Just get something running.\nThis first attempt took me 3-4 hours to write.\nI googled how to calculate the frequency of letters in words (in R) and found this helpful tutorial.\n\nThe Minimal Viable Product: Running Code\nI cleaned up the commenting/formatting of the initial code just for this post. I also added library(tidyverse)- apparently, I was just loading libraries through the gui back then. If you want to see the true initial version, it is on GitHub.\nHere’s what my MVP does:\n\nI found a list of five letter words online and imported it.\nI calculated how often each letter occurred over the word list.\n(This, or the scaled version, was the score assigned to each letter.)\nI looped through the word list and calculated two scores for each word, one using all letters and one using only the unique letters.\nI picked the highest-scoring word as my first guess.\nI then went through the word list and calculated a second score for the words minus the letters already guessed in the first guess (and ignoring duplicated letters).\nI picked the highest-scoring word as my second guess.\nI repeated steps 5 & 6 to pick a third guess.\n\nOkay, so let’s look at some bad code. I will flag a few things as we go through, but I’m certain you can find much more that is not optimal.\nHere’s loading the data and writing two scoring functions. I first wrote these in the code but had time to convert them to functions in this initial work session. It was an opportunity to practice function writing, but it was not critical to the minimum viable product.\nI have lots of print statements commented out; this is a very simple way to debug and see how you are proceeding through the code. There are more sophisticated tools in R Studio, but I didn’t want to figure out how to use them at this moment. I use the global variable char_frequencies for the value of each letter. I create this variable in the next code chunk.\n\nlibrary(tidyverse)\n\nsgb.words &lt;- \n  read.delim(\"C:/Users/drsin/OneDrive/Documents/R Projects/wordle/sgb-words.txt\",\n             sep = \"\")\n\n#probably want this instead because it assumes no headers\n#test6 &lt;- read.table(file.choose())\n\nScoring_Word &lt;- function(word){\n  #i'm not handling duplicate letters at all right now\n  letter_vec &lt;-  unlist(strsplit(word, split = \"\"))\n  value &lt;- 0\n  for (i in 1:5) {\n    position &lt;- letter_vec[i] == char_frequencies$letters\n    value[i] &lt;- y[position]\n   # print(i)\n    if (i == 5) {\n     # print(\"I am here\")\n     # print(sum(value))\n      return(total &lt;- sum(value))\n      }\n    \n  }\n} \n\n\nScoring_Word_Unique &lt;- function(word){\n\n # print(word)\n  letter_vec &lt;-  unlist(strsplit(word, split = \"\"))\n  unique_letter_vec &lt;- unique(letter_vec)\n  #print(unique_letter_vec)\n  #print(length(unique_letter_vec))\n  \n  value &lt;- 0\n  if (length(unique_letter_vec) == 0) {\n    return(value)\n  } else{\n      for (i in 1:length(unique_letter_vec)) {\n      position &lt;- unique_letter_vec[i] == char_frequencies$letters\n      value[i] &lt;- y[position]\n    # print(i)\n    # print(value)\n    if (i == length(unique_letter_vec)) {\n      # print(\"I am here\")\n      # print(sum(value))\n      return(total &lt;- sum(value))\n    }\n    \n  }\n  }\n}\n\nI did run through most of the code with five words initially, and then later the whole word list, when I was more confident that things worked.\nI calculate how often each letter appears in the list and create the scaled version. I created two incredibly ugly graphs: one of the raw counts for each letter and one of the scaled frequencies. This is also a moment to do a quick reality check on the results- are the most and least common letters what you’d expect?\n\nstart_time &lt;- Sys.time()\n\nletters &lt;- unlist(strsplit(sgb.words[,1], split = \"\"))\nchar_frequencies &lt;- as.data.frame(table(letters))\n#char_frequencies\n\nggplot(char_frequencies, \n        aes(x = fct_reorder(char_frequencies[,1], char_frequencies[,2])\n                              , char_frequencies[,2] )) +\n   geom_col() +\n   theme_classic()\n\n\n\ncommon &lt;- max(char_frequencies[,2])\ny = (char_frequencies[,2]/common)\n\nggplot(char_frequencies, \n       aes(x =  fct_reorder(char_frequencies[,1], char_frequencies[,2]), y )) +\n  geom_col() +\n  theme_classic()\n\n\n\n\nNow I calculate the scores for the (hand-picked) words I’ve been playing with. I also hand-calculated these scores using the values from char_frequencies to ensure my scoring functions did what I thought they were.\nI initialized an object to store the words, scores, and guesses. You can also tell that I had no idea what data types my objects were since I called them a list. small_list is a matrix/array of characters, and none of my zeros are numbers. I wanted a dataframe, but I didn’t know how to do that. I didn’t have a strong reason to prefer a dataframe other than it was widely used in the courses I was taking at Datacamp.\nThis chunk also pulls out a single word and sends it to score to check that it works before I loop through the entire object and calculate all the scores.\nYou can also see I hard-coded the number of words (again… I did this in the prior code chunk too.)\n\n#calculate the score for crone\ncrone_score &lt;- Scoring_Word(\"crone\")\n#might_score &lt;- Scoring_Word (\"might\")\n#sadly_score &lt;- Scoring_Word (\"sadly\")\nnum_words &lt;- 5756\n#num_words &lt;- 5\nsmall_list &lt;- cbind(word_name = sgb.words[1:num_words,1], \n                    score =rep(0, times = num_words), \n                    unique_score = rep(0, times = num_words),\n                    post_word_one_unique = rep(0, times = num_words),\n                    post_word_two_unique = rep(0, times = num_words),\n                    post_word_three_unique = rep(0, times = num_words)\n                                                )\nword &lt;- small_list[[1,1]]\nScoring_Word(word)\n\n[1] 3.40422\n\nind2 &lt;- 0\n\nfor (ind2 in 1:num_words){\n  #print(small_list[[ind2,1]])\n  score_ind2 &lt;- Scoring_Word(small_list[[ind2,1]])\n  small_list[[ind2,2]] &lt;- score_ind2\n}\n\n#u_crone_score &lt;- Scoring_Word_Unique(\"crone\")\n#u_there_core &lt;- Scoring_Word_Unique (\"there\")\n#sadly_score &lt;- Scoring_Word (\"sadly\")\n\nind2 &lt;- 0\nfor (ind2 in 1:num_words){\n # print(small_list[[ind2,1]])\n  score_ind2 &lt;- Scoring_Word_Unique(small_list[[ind2,1]])\n # print(score_ind2)\n  small_list[[ind2,3]] &lt;- score_ind2\n}\n\nIn my attempt to sort the word scores and pick out the highest-scoring works, I created an unnecessary number of temporary variables. I forced one of these objects to be a dataframe, but I didn’t check the types of the individual components. Note that all my numbers are still characters. It is funny that things worked even though they were the wrong type.\n\nsmall_list1 &lt;- small_list\nsmall_df &lt;- as.data.frame(small_list1)\ntop_words &lt;- small_df %&gt;%\n arrange(desc(unique_score))\n\nword_1 &lt;- top_words$word_name[1]\n\nNow I calculate the second and third guesses. I wanted to penalize duplicate letters, so I used the unique letter scoring function and removed the letters from the first guess. I couldn’t figure out how to do that automatically, so I hardcoded to remove the letters “a”, “r”, “o”, “s”, and “e” from the words before I sent them to be scored. This is precisely the kind of situation where you can get bogged down doing things “properly” and end up stuck. I quickly attempted to figure it out and then did it incorrectly. You can also see that I have a bunch of stuff commented out that didn’t work and a bunch of print statements for debugging. This is not pretty code.\nThen I loop through the list again and repeat for the last guess. Again, hardcoded in the letters to remove from the first and second guesses.\n\n#now we need a function that sees if a word has the letters of the word_1\n#and removes them and then calculates the word score\n#top word is arose\n# Word 1= arose -----\nind3 &lt;- 1\nfor (ind3 in 1:num_words) {\n # print(top_words$word_name[ind3])\n  test &lt;- small_list[[ind3,1]]\n  lvec &lt;- gsub(\"[a r o s e]\", \"\", test)  #this actually works.  How do I use the string?\n  #lvec &lt;-  unlist(strsplit(word_1, split = \"\"))\n  #lvec&lt;- \"t|h|e|i|r\" #how do I contruct this automatically\n\n  #new_let &lt;- str_remove_all(pattern= lvec, string= test)\n # print(lvec)\n  score_ind3 &lt;- Scoring_Word_Unique(lvec)\n # print(\"writing score\")\n # print(c(ind3, \" \", score_ind3, \"for the word \", test, \"sent as \", lvec))\n  \n  small_list[[ind3,4]] &lt;- score_ind3\n  #print (c(\"output of small list \", top_words[[ind3,4]]))\n}\n\nsmall_df2 &lt;- as.data.frame(small_list)\ntop_words2 &lt;- small_df2 %&gt;%\n  arrange(desc(post_word_one_unique))\n\n\nword_2 &lt;- top_words2$word_name[1]\n\n# top word 2 is until\n\nind4 &lt;- 1\nfor (ind4 in 1:num_words) {\n  # print(top_words$word_name[ind3])\n  test &lt;- small_list[[ind4,1]]\n  lvec &lt;- gsub(\"[u n t i l a r o s e]\", \"\", test)  #this actually works.  How do I use the string?\n  #lvec &lt;-  unlist(strsplit(word_1, split = \"\"))\n  #lvec&lt;- \"t|h|e|i|r\" #how do I contruct this automatically\n  \n  #new_let &lt;- str_remove_all(pattern= lvec, string= test)\n  # print(lvec)\n  score_ind4 &lt;- Scoring_Word_Unique(lvec)\n  # print(\"writing score\")\n  # print(c(ind3, \" \", score_ind3, \"for the word \", test, \"sent as \", lvec))\n  \n  end_time &lt;- Sys.time()\n  end_time - start_time\n  \n  small_list[[ind4,5]] &lt;- score_ind4\n  #print (c(\"output of small list \", top_words[[ind3,4]]))\n}\n\nsmall_df3&lt;- as.data.frame(small_list)\ntop_words2 &lt;- small_df3 %&gt;%\n  arrange(desc(post_word_two_unique))\n\n\nword_3 &lt;- top_words2$word_name[1]\n\nLastly, I calculated the total score of these three words compared to my hand-picked words.\n\na = Scoring_Word_Unique(\"arose\") + \n  Scoring_Word_Unique(\"until\") + \n  Scoring_Word_Unique(\"dumpy\")\na\n\n[1] 8.013518\n\nb = Scoring_Word_Unique(\"crone\") +\n  Scoring_Word_Unique(\"mighty\") +\n  Scoring_Word_Unique(\"sadly\")\nb\n\n[1] 8.081767\n\n\nNote that there is an error here too. By calling Scoring_Words_Unique on individual words, I did not penalize duplicate letters. Thus “u” appears in two words. The correct scoring call should have been:\n\nc = Scoring_Word_Unique(\"aroseuntildumpy\")\nc\n\n[1] 7.654468\n\n\nBut the program works! It generated three reasonable guesses for Wordle that use common letters. (Note that by my scoring rules, the manually chosen set of words is a better choice.)"
  },
  {
    "objectID": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#critique-the-code",
    "href": "posts/2023-04-01-self-guided-learning-wordle-guesser-part-1/Wordle.html#critique-the-code",
    "title": "Self-Guided Learning through a Wordle Guess Generator: Part 1",
    "section": "4. Critique the code",
    "text": "4. Critique the code\nThis step is the critical step to accelerate your learning. You need to review your code and list everything you know is not done properly and everything confusing you. This should be done at the end of every session or at the start of the next. Be honest with yourself. If you don’t understand something, put it on your list, even if your code works. The point of this exercise is to increase your coding proficiency.\nThis is my list from that code we just reviewed. I created this the next day before I started work. Note my versioning by file name.\n\nSave current code as frequency_072222 and work on a new copy. This was functional code last night, so I want to keep it.\nImport is wrong because it takes the first word as a header.\nI need more concise variable names. Also, I create a bunch of temp variables that aren’t needed.\nI manually subtract out (hard-coded) words picked in previous cycles. I need that done on the fly.\nOnce 4 is done, I’d like to write a function to generate however many guesses you ask for.\nI’d like to look at the histogram of the scores as you cycle through the guesses.\nI’m very unclear on when I need lists, dataframes, tibbles, etc., for the different functions.\nGive credit to the website where I took the string split code from.\nSome functions from the other code are out of date, so I should update them. [I got warnings in R Studio about this, which I didn’t understand.]\nUpdate scoring_word to have flexible word length.\n\nAgain, there is a lot more wrong with this code, but this is the list of things I could identify with the knowledge I had at the time.\nThe next day, I created a chart of all my variables, their sizes, and their types. I also proposed new, clearer names for them, though this wasn’t implemented until a few days later. I mostly played around with how long it took to calculate the initial score and unique score using separate or combined loops. I used sys.time() to benchmark, which didn’t seem terribly consistent in its output. However, there didn’t seem to be a huge difference between the methods, so I returned it to two loops for clarity. At the end of this day, I had four items on my task list after critiquing and reviewing. The last one was, “I need to figure out git_hub or come up with a better way of versioning than the date.”\nProving that point, I put the wrong date in all the file names the next time I worked on the code. In that session, I devoted most of the time to figuring out item 4 on my task list. I wrote a function called remove_letters, which could be used instead of hard coding. I also played around with reshaping the data using pivot_longer from tidyr. I created a histogram of word scores as letters are removed from consideration, which required the reshaped data. Reshaping data can be tricky, so this was a good opportunity to work through that process. (Again, versioning by name, this was called “frequency_072422_tidyr”, in case I really messed up.)"
  },
  {
    "objectID": "posts/2023-04-02-chart-challenge-2/parks.html",
    "href": "posts/2023-04-02-chart-challenge-2/parks.html",
    "title": "30 Day Chart Challenge -Arlington Parks",
    "section": "",
    "text": "When I looked at Twitter this morning, my feed was filled with amazing charts. Apparently, April is month for the #30DayChartChallenge. More info can be found at the challenge’s Github page. Each day, the challenge specifies a type of chart, but not a dataset. Today’s challenge is to create a waffle chart. I’d never heard of a waffle chart, but luckily, R has a package for that!\nKeeping it simple and just using tidyverse and waffle packages today. (Spoiler, I had incredible difficulties with this package.)\n\nlibrary(waffle)\nlibrary(tidyverse)\n\nA waffle chart is similar to a pie chart, but since it is constructed from squares, instead of wedges, it is a bit easier to correctly judge the relative areas.\nI discovered that Arlington County has a website with a bunch of open source data, so I poked around there to find a dataset for today’s challenge. I decided to use the dataset on parks and acreage. In addition to having local and federally owned parks, Arlington is park of a consortium of Northern Virginia jurisdictions that also operate regional parks.\n\nparks &lt;- read_csv(\"parks.csv\")\n\nRows: 10 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): Profile Year, County Owned Parkland (Acreage), NOVA Parks (Acreage)...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe dataset contains 3 years worth of data (2019-2021) and the total number of acres owned by the county, NOVA parks, and the National Park Service. It also includes the number of parks own by NOVA parks and the county, but not the number of NPS parks. I did try to find the number of NPS parks and the answer isn’t easily obtainable. The NPS owns one park in Arlington (Arlington House), but there are a couple of multi-state trails that might go through the county, but I wasn’t interested in pulling up maps to check.\nThe dataset comes as an excel sheet and the column names aren’t nice. I’ve been working with so many datasets designed for R or SQL that it was a shock to see long column names with spaces and punctuation. I had to look up how to handle that! The answer is backticks, as below.\n\nparks_2021 &lt;- parks %&gt;%\n  filter(`Profile Year` == 2021)\n\nI’m going to rename the columns and in this filtered dataset. There isn’t much year to year change in this dataset (there is one extra park in 2020), so I’m not going to do anything with the entire dataset. If I were, I’d rename the parent dataset.\n\nparks_2021 &lt;- parks_2021 %&gt;%\n  transmute(year = `Profile Year`,\n         county_acres = `County Owned Parkland (Acreage)`,\n         NOVA_acres = `NOVA Parks (Acreage)`,\n         NPS_acres = `National Parks Service (Acreage)`) \n\nparks_2021 &lt;- parks_2021 %&gt;%\n  select(-year)\n\nNow let’s get waffling!\nAllegedly, you can both pass waffle dataframes or use the provided geom_waffle, but neither was successful for me. With respect to the geom_waffle, I found a suggestion to install waffle from the github repo and restart R Studio, but that didn’t work for me. Others stated that re-installing R Studio fixed the problem, but my commitment to waffle charts is not that great.\nAs to passing it dataframes, waffle constantly complained about 'list' object cannot be coerced to type 'double' even when using code from other folk’s tutorial. Passing waffle a simple vector did work.\n\n# Vector\nx &lt;- c(30, 25, 20, 5)\n\n# Waffle chart\nwaffle(x, rows = 8)\n\n\n\n\nSo, I converted my dataframe to a vector. First, I reshaped it to long rather than wide. (Neither the long nor the wide df waffled.)\n\nparks_2021_long &lt;- pivot_longer(parks_2021, cols = 1:3, names_to = \"owner\", values_to = \"acreage\")\n\nThen I converted it to a vector. This should be a named vector of numbers.\n\nparks_2021_vec = deframe(parks_2021_long)\nclass(parks_2021_vec)\n\n[1] \"numeric\"\n\nprint(parks_2021_vec)\n\ncounty_acres   NOVA_acres    NPS_acres \n         924          136          417 \n\nstr(parks_2021_vec)\n\n Named num [1:3] 924 136 417\n - attr(*, \"names\")= chr [1:3] \"county_acres\" \"NOVA_acres\" \"NPS_acres\"\n\n\nLet’s waffle it. When I first waffled it, I got 4 categories instead of 3. I found an example that said you needed to explicitly pass it 3 colors or else it would fill in the blank space with a 4th color. Then you get the correct labels, but no chart!\n\n#This waffles, it seems like nonsense to me\n\nwaffle(parks_2021_vec, colors = c(\"#FFC0CB\", \"#FFC0AB\", \"green\"))\n\n\n\n\nSo now we reached the sad state of affairs where I type in the values to make this work. And that also does not work.\n\nx &lt;- c(county = 924, nova = 136, nps = 417)\n\n# Waffle chart\nwaffle(x , rows = 10)\n\n\n\n\nSmall numbers work\n\ny &lt;- c(county = 9.24, nova = 1.36, nps = 4.17)\n\n# Waffle chart\nwaffle(y ,  rows = 10)\n\n\n\n\nIf I convert everything to percentages…\n\ntotal = 924 + 136 + 417\ny &lt;- c(county = (924/total)*100, nova = (136/total)*100, nps = (417/total)*100)\n\n# Waffle chart\nwaffle(y ,  rows = 10)\n\n\n\n\nI don’t find any documentation about the size of the numbers. It is not a requirement that the totals must add up to 100 (100%); small numbers adding up to anything works. Waffle charts are not only for proportional data, but can also be used to track progress. There is nothing in the documentation on CRAN that gives a clue about this, nor did I see anything in the tutorials I looked at.\nI’m going to pretty up the chart and call it a day. I thought this would take me about 20 minutes to make a nice chart and instead I’ve spent several hours and I don’t even understand what went wrong. Also, the named vector does work when I adjust the size of the numbers to “smaller” values. I picked nice forest colors since we are talking about parks.\n\nparks_2021_percent = (parks_2021_vec / total) * 100\n\n\nwaffle(parks_2021_percent, colors = c(\"darkgreen\", \"darkseagreen\", \"darkolivegreen\"), title = \"Who owns the parks in Arlington, Virginia?\")\n\n\n\n\nI don’t really understand the waffle package. I don’t find the graphic particularly understandable. I’d like there to be some sort of indication about what each square represents. And I find it very annoying that there are not 100 squares. I know this is a rounding issue, but given that the dataset, by the nature of how it was created, should equal 100%, the chart is just confusing. And for what it is worth, I had to repeatedly restart RStudio, because code chunks would just fail to do anything. They’d run, but there would be no output, not even warnings or errors.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {30 {Day} {Chart} {Challenge} {-Arlington} {Parks}},\n  date = {2023-04-02},\n  url = {https://lsinks.github.io/posts/2023-04-02-chart-challenge-2/parks},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “30 Day Chart Challenge -Arlington\nParks.” April 2, 2023. https://lsinks.github.io/posts/2023-04-02-chart-challenge-2/parks."
  },
  {
    "objectID": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html",
    "href": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html",
    "title": "A Tidymodels Tutorial: A Structural Approach",
    "section": "",
    "text": "As I’ve started working on more complicated machine learning projects, I’ve leaned into the tidymodels approach. Tidymodels is a highly modular approach, and I felt it reduced the number of errors, especially when evaluating many machine models and different preprocessing steps. (This is, in fact, a stated goal of the tidymodels ecosystem.)\nThis tutorial is more about understanding the process of modeling in tidymodels and learning about the various objects created at different steps rather than optimizing a machine learning model.\nThroughout this tutorial, I will use the word “procedure” to describe a set of steps to go from data to final predictions. I’m doing this because tidymodels uses the word workflow for specific types of objects and functions. It would be too confusing to use workflow to also describe the process/procedure.\nBut the tidymodels ecosystem can also be very confusing. There are several component packages in tidymodels. While it is easy to explain what a recipe object (from the recipe package) does, it became increasingly difficult for me to name and describe the objects I was creating as I started building more sophisticated machine-learning procedures. And I found it even more confusing that simple and complex procedures, while going through the same basic steps (preprocess, train, evaluate, predict), created objects with different structures and data within them. I found it confusing that fit, last_fit, fit_resamples, etc., did not all produce objects that contained the same information and could be acted on by the same functions. In my first attempt at using last_fit(), I ended up scrapping the entire ML section and redoing it with fit()/predict() because I couldn’t figure out how to get the predictions out of the object created by last_fit().\nAdding to my woes was the fact that attempting to view/print/ examine these objects, especially in notebook environments, often caused the entire project to time out. RStudio generally handles these objects more gracefully, but I’ve also crashed it hard. It also isn’t consistent whether an object will lock-up RStudio or not. Once RStudio has locked up, restarting the program leads to an increasing number of freezes/locking up, until the computer is restarted.\nI’ve also manually numbered my code blocks and used that for referencing. I believe it is possible to hyperlink code chunks in Quarto, but I plan to replicate this project in an online notebook environment where that isn’t possible. The manual numbering will make it easier to cross-reference the two. I found online notebooks really did not like displaying many tidymodels objects. That’s also why there are timers around many of the display calls.\nSo here I’m going to go through three different procedures for modeling. I will compare and contrast the objects created as we move through the different procedures."
  },
  {
    "objectID": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#fitpredict",
    "href": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#fitpredict",
    "title": "A Tidymodels Tutorial: A Structural Approach",
    "section": "fit()/predict()",
    "text": "fit()/predict()\nFirst, I fit the model on the training data to get the fit and then I pass that fit and the test data to predict() to get the predictions for test.\n\n# Code block 10: Run fit/ predict on workflow\nwflow_fit &lt;- fit(wf_simple, data = train_data)\nwflow_predict &lt;- predict(wflow_fit, new_data = test_data)\nwflow_predict2 &lt;- predict(wflow_fit, new_data = test_data, type = \"prob\" )\n\nWhat comes out of predict is super simple to understand. It is a list of predictions. No complicated nested list objects here. If I want probabilities instead of hard classification, I pass predict() the argument type = \"prob\" to get the probabilities instead.\n\n# Code block 11:  Examine the output of predict\nhead(wflow_predict)\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 0          \n3 0          \n4 0          \n5 0          \n6 0          \n\nhead(wflow_predict2)\n\n# A tibble: 6 × 2\n    .pred_1 .pred_0\n      &lt;dbl&gt;   &lt;dbl&gt;\n1 0.00367     0.996\n2 0.00144     0.999\n3 0.0000262   1.00 \n4 0.00461     0.995\n5 0.0000279   1.00 \n6 0.00138     0.999\n\n\nWhat about our model? Maybe I want model coefficients or to see which features are most important. There is a lot of information here, but it isn’t very well structured. Again, this is a nested list. RStudio is displaying this nicely and the details can be seen using View().\n\n# Code block 12: Examine the outcome of fit \nwflow_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n            (Intercept)                lat_trans               long_trans  \n                7.06537                 -0.10230                 -0.01413  \n         distance_miles                      age                     hour  \n                0.06526                 -0.26818                 -0.82812  \n                weekday                  amt_log     category_food_dining  \n               -0.12721                 -1.87149                 -0.00929  \n category_gas_transport     category_grocery_net     category_grocery_pos  \n               -0.62772                 -0.29571                 -0.67063  \ncategory_health_fitness            category_home       category_kids_pets  \n                0.06286                  0.10517                  0.01683  \n      category_misc_net        category_misc_pos   category_personal_care  \n               -0.42138                 -0.13380                 -0.05152  \n  category_shopping_net    category_shopping_pos          category_travel  \n               -0.38932                 -0.16399                  0.18122  \n\nDegrees of Freedom: 254704 Total (i.e. Null);  254684 Residual\nNull Deviance:      16570 \nResidual Deviance: 11910    AIC: 11950\n\n\nWhile you can use standard R operations for interacting with lists and nested data to extract the desired information from wflow_fit, it is much easier to use the broom package. Broom is part of the core tidymodels installation, so it does not need to be installed separately. To get the model coefficients and p-values in tibble form, use tidy(). For high-level statistics about the model, use glance(). Just remember that the information you extract from the output of fit() relates to the model as applied to the training data. For information about the model performance as applied to the test data, you need to use the output of predict(). Since this output is only a vector of predictions, you need to bind it to the test dataframe and then perform analysis on the new object.\nSo it is pretty straightforward to get our model coefficients:\n\n# Code block 13: Summarize wflow_fit with tidy\nwflow_fit %&gt;% tidy() #summarizes information about model components\n\n# A tibble: 21 × 5\n   term                   estimate std.error statistic   p.value\n   &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)             7.07       0.0703   101.    0        \n 2 lat_trans              -0.102      0.0305    -3.36  7.94e-  4\n 3 long_trans             -0.0141     0.0306    -0.462 6.44e-  1\n 4 distance_miles          0.0653     0.0318     2.05  4.02e-  2\n 5 age                    -0.268      0.0289    -9.27  1.87e- 20\n 6 hour                   -0.828      0.0397   -20.9   1.27e- 96\n 7 weekday                -0.127      0.0288    -4.41  1.03e-  5\n 8 amt_log                -1.87       0.0510   -36.7   2.76e-294\n 9 category_food_dining   -0.00929    0.0599    -0.155 8.77e-  1\n10 category_gas_transport -0.628      0.0593   -10.6   3.62e- 26\n# ℹ 11 more rows\n\n\nOr to get details of the model performance:\n\n# Code block 14: model info from wflow_fit with glance\nwflow_fit %&gt;% glance() #reports information about the entire model\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik    AIC    BIC deviance df.residual   nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt;  &lt;int&gt;\n1        16568.  254704 -5956. 11953. 12173.   11911.      254684 254705"
  },
  {
    "objectID": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#last_fit",
    "href": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#last_fit",
    "title": "A Tidymodels Tutorial: A Structural Approach",
    "section": "last_fit()",
    "text": "last_fit()\nSo, from the tidymodels webpage, last_fit() is described as “last_fit() emulates the process where, after determining the best model, the final fit on the entire training set is needed and is then evaluated on the test set.” (Actually this is from the tune subpage, which is important, though I didn’t realize it.)\nI pass the workflow to last_fit() along with the data split object (with the info about testing and training) and the metrics set. In theory, the result should be the same as from fit()/predict() above.\n\n# Code block 15: Using lastfit() in hard classifier mode\nlast_fit_results &lt;- last_fit(wflow_fit, data_split, metrics = fraud_metrics_hard)\n\nSo, I look at the results just as I did with predict in Code Block 11. And RStudio sometimes locks up. Other times, it produces a high-level overview as expected.\n\n# Code block 16: creating a workflow set\nstart_time_display &lt;- Sys.time()\nhead(last_fit_results) \n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [254705/84902]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\nend_time_display &lt;- Sys.time()\nprint(paste(\"last_fit_results: \", end_time_display - start_time_display))\n\n[1] \"last_fit_results:  0.0788729190826416\"\n\n\nSo how to get the predictions out? According to the manual page for last_fit(), the output is “A single row tibble that emulates the structure of fit_resamples(). However, a list column called .workflow is also attached with the fitted model (and recipe, if any) that used the training set.” I also see that last_fit() is actually from the tune package and not from parsnip as I expected. Nothing I’m doing here involves tuning hyperparameters at all. I expected that is was a parsnip object both thematically and because you interact with last_fit() using extract_fit_parsnip(), see Code Block 23.\nLooking fit_resamples() isn’t very helpful for answering this question. (Oh, but it is. It just took me another few paragraphs of writing to realize it.)\nI did find a Stackoverflow discussion that provided the answer in their code: last_fit1_pred &lt;- last_fit1[[5]][[1]]\nThat’s not very straightforward!\nPull out the predictions from last_fit_pred.\n\n# Code block 17: extracting predictions from last_fit\nlast_fit_pred &lt;- last_fit_results[[5]][[1]]\n\nLook at the head() of this object.\n\n# Code block 18: Examine the outcome of lastfit by head\nhead(last_fit_pred)\n\n# A tibble: 6 × 4\n  .pred_class  .row is_fraud .config             \n  &lt;fct&gt;       &lt;int&gt; &lt;fct&gt;    &lt;chr&gt;               \n1 0               1 0        Preprocessor1_Model1\n2 0               2 0        Preprocessor1_Model1\n3 0               8 0        Preprocessor1_Model1\n4 0              12 0        Preprocessor1_Model1\n5 0              13 0        Preprocessor1_Model1\n6 0              14 0        Preprocessor1_Model1\n\n\nLook at the head() of the object from predict().\n\n# Code block 19: Examine the outcome of predict by head\nhead(wflow_predict)\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 0          \n3 0          \n4 0          \n5 0          \n6 0          \n\n\nUse identical() to compare the two hard predictions and verify they are the same.\n\n# Code block 20: showing that predict and the predictions in last_fit are the same\nidentical(last_fit_pred$.pred_class, wflow_predict$.pred_class)\n\n[1] TRUE\n\n\nNow, let the realization of what all the stuff about the tune package means hit you. We now know the full secrets of last_fit(). It turns out that any of the helper functions for tuning functions from the tune package work on last_fit() because it is a tune function. I don’t find the documentation for either the helper functions or last_fit() make that connection clear. I think that is what the reference to fit_resamples() on the last_fit() page is getting at.\nTidy Modeling with R also contains an example of using collect_predictions with last_fit(), but most examples are with tuning functions, so obviously from the tune family. One of the tutorials on the main tidymodels webpage does as well. But in general, extracting predictions from the test data is not demonstrated, just collecting metrics and analyzing model performance. So it is hard to google your way to the answer. This is the kind of situation I’ve struggled with throughout learning tidymodels and part of what motivated me to write this tutorial.\nSo now I get the predictions the easy way.\n\n# Code block 21: Examine the outcome of lastfit by head\nhead(last_fit_pred)\n\n# A tibble: 6 × 4\n  .pred_class  .row is_fraud .config             \n  &lt;fct&gt;       &lt;int&gt; &lt;fct&gt;    &lt;chr&gt;               \n1 0               1 0        Preprocessor1_Model1\n2 0               2 0        Preprocessor1_Model1\n3 0               8 0        Preprocessor1_Model1\n4 0              12 0        Preprocessor1_Model1\n5 0              13 0        Preprocessor1_Model1\n6 0              14 0        Preprocessor1_Model1\n\nlast_fit_results %&gt;% collect_predictions()\n\n# A tibble: 84,902 × 5\n   id               .pred_class  .row is_fraud .config             \n   &lt;chr&gt;            &lt;fct&gt;       &lt;int&gt; &lt;fct&gt;    &lt;chr&gt;               \n 1 train/test split 0               1 0        Preprocessor1_Model1\n 2 train/test split 0               2 0        Preprocessor1_Model1\n 3 train/test split 0               8 0        Preprocessor1_Model1\n 4 train/test split 0              12 0        Preprocessor1_Model1\n 5 train/test split 0              13 0        Preprocessor1_Model1\n 6 train/test split 0              14 0        Preprocessor1_Model1\n 7 train/test split 0              16 0        Preprocessor1_Model1\n 8 train/test split 0              17 0        Preprocessor1_Model1\n 9 train/test split 0              19 0        Preprocessor1_Model1\n10 train/test split 0              25 0        Preprocessor1_Model1\n# ℹ 84,892 more rows\n\n\nAnd can evaluate the model performance.\n\n# Code block 22: collecting metrics from lastfit collect_metrics()\nlast_fit_results %&gt;% collect_metrics()\n\n# A tibble: 1 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.995 Preprocessor1_Model1\n\n\nAnd extract the fit. This extract_fit_parsnip() result is an identical parsnip object as the workflow_fit object we got from fit() and can be handled the same way (i.e. via broom). You can refer back to Code Block 13 to see the results are the same. This is perhaps the key takeaway; these larger, more complex objects contain the simpler objects (workflows, parsnip objects) and they should be extracted and handled normally. Understanding this will make understanding how to handle a workflow_set() much easier.\n\n# Code block 23: extract model coefficients from last_fit() \nlast_fit_results %&gt;% extract_fit_parsnip() %&gt;% tidy()\n\n# A tibble: 21 × 5\n   term                   estimate std.error statistic   p.value\n   &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)             7.07       0.0703   101.    0        \n 2 lat_trans              -0.102      0.0305    -3.36  7.94e-  4\n 3 long_trans             -0.0141     0.0306    -0.462 6.44e-  1\n 4 distance_miles          0.0653     0.0318     2.05  4.02e-  2\n 5 age                    -0.268      0.0289    -9.27  1.87e- 20\n 6 hour                   -0.828      0.0397   -20.9   1.27e- 96\n 7 weekday                -0.127      0.0288    -4.41  1.03e-  5\n 8 amt_log                -1.87       0.0510   -36.7   2.76e-294\n 9 category_food_dining   -0.00929    0.0599    -0.155 8.77e-  1\n10 category_gas_transport -0.628      0.0593   -10.6   3.62e- 26\n# ℹ 11 more rows"
  },
  {
    "objectID": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#handling-a-model-with-no-hyperparameters",
    "href": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#handling-a-model-with-no-hyperparameters",
    "title": "A Tidymodels Tutorial: A Structural Approach",
    "section": "Handling a model with no hyperparameters",
    "text": "Handling a model with no hyperparameters\nNormally, we’d want to extract the best recipe/model combination from this set. I’ll do that here. Again, I’m using j-index as my metric and from the output of Code Block 25, we see down_logreg is the best performing model. I extract that workflow from the set of results, and pass it to last_fit().\n\n# Code Block 27: Validating the best model with the test data\nvalidation_results &lt;- tune_results %&gt;%\n  extract_workflow(\"down_logreg\") %&gt;%\n  last_fit(split =  data_split, metrics = fraud_metrics)\n\nNow we can use the same helper functions we did when we used last_fit() on the simple workflow, because we are working with a simple workflow! We pulled just the one workflow we wanted out.\nYou can see now that in addition to the hard classification we got from last_fit() before we also get the probabilities. This is driven by the metrics that make up the metrics set (see the yardstick section for more information). I use these predictions to create the ROC curve as well.\n\n# Code Block 28: Metric for best model with the test data\nhead(collect_predictions(validation_results))\n\n# A tibble: 6 × 7\n  id               .pred_1 .pred_0  .row .pred_class is_fraud .config           \n  &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;             \n1 train/test split  0.552    0.448     1 1           0        Preprocessor1_Mod…\n2 train/test split  0.197    0.803     2 0           0        Preprocessor1_Mod…\n3 train/test split  0.0329   0.967     8 0           0        Preprocessor1_Mod…\n4 train/test split  0.472    0.528    12 0           0        Preprocessor1_Mod…\n5 train/test split  0.0254   0.975    13 0           0        Preprocessor1_Mod…\n6 train/test split  0.312    0.688    14 0           0        Preprocessor1_Mod…\n\nvalidation_results %&gt;% \n  collect_predictions() %&gt;% \n  roc_curve(is_fraud, .pred_1) %&gt;% \n  autoplot() + \n  ggtitle(\"ROC Curve\")"
  },
  {
    "objectID": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#handling-a-model-with-hyperparameters",
    "href": "posts/2023-04-10-tidymodels/tidymodels_tutorial.html#handling-a-model-with-hyperparameters",
    "title": "A Tidymodels Tutorial: A Structural Approach",
    "section": "Handling a model with hyperparameters",
    "text": "Handling a model with hyperparameters\nSuppose the best model was the elastic net. I tuned the hyperparameters when I did the fitting in workflow_map(). How do I deal with that?\nFirst, I need to extract the best set of hyperparameters. Here we aren’t extracting the workflow, we are extracting the workflow set result, which is our set of hyperparameters. This is a really simple object, so you can view it without fear.\n\n# Code Block 29: getting-hyperparameters\nbest_hyperparam &lt;- tune_results %&gt;% \n    extract_workflow_set_result(\"down_glmnet\") %&gt;%\n    select_best(metric = \"j_index\")\n\nbest_hyperparam\n\n# A tibble: 1 × 3\n      penalty mixture .config             \n        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 0.000000137   0.570 Preprocessor1_Model4\n\n\nOur workflow for the glmnet is incomplete because it has tune() for the two hyperparameters, instead of the values. We know the best values (at least from the limited parameter space we explored.) I first extract_workflow() just as I did for the no hyperparameter case and then call finalize_workflow(best_hyperparam). This updates the workflow hyperparameters with the values we found. Everything is identical to the no hyperparameter case or the simple workflow/ last-fit() case. Realizing how/when to extract or reduce the more complex objects to the simpler objects is key to using tidymodels effectively.\n\n# Code Block 30: last_fit for a workflow with hyperparameter\nvalidation_results &lt;- tune_results %&gt;%\n  extract_workflow(\"down_glmnet\") %&gt;%\n  finalize_workflow(best_hyperparam) %&gt;%\n  last_fit(split =  data_split, metrics = fraud_metrics)\n\nNow we can handle this object exactly as before.\n\n# Code Block 31: Metric for best model with the test data\nhead(collect_predictions(validation_results))\n\n# A tibble: 6 × 7\n  id               .pred_1 .pred_0  .row .pred_class is_fraud .config           \n  &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;             \n1 train/test split  0.551    0.449     1 1           0        Preprocessor1_Mod…\n2 train/test split  0.217    0.783     2 0           0        Preprocessor1_Mod…\n3 train/test split  0.0342   0.966     8 0           0        Preprocessor1_Mod…\n4 train/test split  0.474    0.526    12 0           0        Preprocessor1_Mod…\n5 train/test split  0.0263   0.974    13 0           0        Preprocessor1_Mod…\n6 train/test split  0.316    0.684    14 0           0        Preprocessor1_Mod…\n\nvalidation_results %&gt;% \n  collect_predictions() %&gt;% \n  roc_curve(is_fraud, .pred_1) %&gt;% \n  autoplot() + \n  ggtitle(\"ROC Curve\")\n\n\n\n\nSo that’s it. I hope this clarifies some of the different procedures you can use to fit models in the tidymodels framework."
  },
  {
    "objectID": "posts/2023-04-18-tidytuesday-founder-crops/founder-crops.html",
    "href": "posts/2023-04-18-tidytuesday-founder-crops/founder-crops.html",
    "title": "TidyTuesday Week 16: Neolithic Founder Crops",
    "section": "",
    "text": "Today’s TidyTuesday relates to the use of plants in the Neolithic era and is based on a paper by Arranz-Otaegul and Roe.\nThe authors have made their data and analysis available on GitHub. The methods for generating all the figures and tables are in an RMarkdown document with some explanatory text. Having just recently looked at the code and data for one of my older papers, I now appreciate this markdown approach. Everything needed is linked together through the markdown document and the GitHub repo. My code is only as understandable with the paper notebook that resides in my former lab. Even though the code was commented, it is less clear many years later. I find markdown/ quarto a little difficult to code in. I’m not sure why- maybe the interspersed text and code is distracting. I usually code in R files for more complicated projects and then copy them into markdown/quarto. But I definitely appreciate markdown after working on today’s project.\n\nlibrary(skimr)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(gt)\n\nThe data can be downloaded through the tidytuesday package using tuesdata &lt;- tidytuesdayR::tt_load(2023, week = 16). Or the data and the code can be downloaded from the project’s Github repo. I actually don’t use the data from tidytuesday, but instead work off their analysis.\nTo briefly summarize the work of Arranz-Otaegul and Roe, in the 1980s the concept of “founder crops” was proposed. These founder crops were a set of crops that were generally all cultivated together and were wide spread as the first main agricultural crops. Arranz-Otaegul and Roe propose that with the increased data generated since the 80s, a more refined approach to the beginnings of argriculture can be taken. They break down agriculture into several different processes (selection of plants, domesitcation, plant management, etc.) which occurs over longer periods of time rather than the development agriculture occuring as a single, rapid event. Arranz-Otaegul and Roe examine the plant remains reported from 135 sites over a wide range of periods in the neolithic era. They find that founder crops don’t necessarily come as a “package”, that a variety of other plants were fairly important, and that the increased cultivation of wheat was the dominate change in the types plants used during the neolithic period. I should note, I’m a chemist, not an archeology/ archeobotanist, so this is not the most nuanced summary of their work.\nI was particularly intrigued by their conclusion about wheat, which is graphically represented in Figure 4 of their paper. I found the figure a bit unclear; stacked area plots are not common in chemistry and I’m not expert at reading them. Wheat certainly does increase with time, but it was hard for me to tell what the other crops were doing. So, for tidytuesday, I wanted to explore this specific question- how does the use of wheat change over time.\nThe file you get from the tidytuesday package is the same as the “swasia_neolithic_flora.tsv” from the paper’s repo with some minor clean-up. I don’t want to replicate all there analysis here to get to the point of being able to make the figure I want, so I saved the object flora_ts after it was generated in line 256 of their SI1.RMD file. This has the data partitioned by century and has been reshaped.\n\nflora_ts &lt;- read_rds(\"flora_ts.rds\")\n\nThere are 8 founder crops. The original analysis binned together wheat varieties and I am also binning all the legumes together, just to make the resulting graph less busy. I’m also dropping flax. This code chunk is modified from their code chunk ts-founder-crops, which starts at line 573 and ends with saving figure 4. This chunk calculates the proportion of archaeological sites at which the crop was found at in each century. (This is called an assemblage in the paper.)\n\nfounder_crops_binned &lt;- flora_ts %&gt;%\n  # aggregate the legumes and the wheat\n  mutate(founder_crop = recode(founder_crop,\n                               \"einkorn wheat\" = \"wheat\",\n                               \"emmer wheat\" = \"wheat\",\n                               \"chickpea\" = \"legumes\",\n                               \"bitter vetch\" = \"legumes\",\n                               \"lentil\" = \"legumes\",\n                               \"pea\" = \"legumes\",\n                               \"flax\" = \"flax\",\n                               .default = founder_crop)) %&gt;%\n  filter(founder_crop != \"flax\") %&gt;%\n  # Aggregate by founder crops\n  group_by(century, site_name, phase_code, founder_crop) %&gt;%\n  summarise(prop = sum(prop, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  # Add number of assemblages per century\n  group_by(century) %&gt;%\n  mutate(n_assemb = length(unique(phase_code))) %&gt;%\n  # Calculate average proportion\n  group_by(century, founder_crop) %&gt;%\n  summarise(avg_prop = sum(prop) / first(n_assemb))\n\nDrop the NAs. This corresponds to all the plants which are not founder crops.\n\nfounder_crops_only &lt;- founder_crops_binned %&gt;%\n  drop_na(founder_crop)\n\nFirst, I wanted to see roughly how common each crop was over all time.\n\nfounder_crops_only %&gt;%\n  group_by(founder_crop) %&gt;%\n  summarize(pct = round(mean(avg_prop), 2) * 100) %&gt;%\n  gt() %&gt;%\n  cols_label(founder_crop = \"Founder Crop\", pct = \"% of Sites\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Mean Frequency of Crops over Time\") \n\n\n\n\n\n  \n    \n      Mean Frequency of Crops over Time\n    \n    \n    \n      Founder Crop\n      % of Sites\n    \n  \n  \n    barley\n9\n    legumes\n6\n    wheat\n15\n  \n  \n  \n\n\n\n\nNow, I’m going to create a scatter plot with a trendline as a guide for the eyes. I’m using facet wrap, so each crop is on its own plot.\n\nfounder_crops_only  %&gt;% ggplot(aes(century / 1000, avg_prop, color = founder_crop)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(~ founder_crop, nrow = 3) +\n  scale_x_reverse(\n    breaks = scales::breaks_width(-1),\n    limits = c(11.7, 6.5),\n    expand = expansion(0)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 0.3),\n    breaks = scales::breaks_width(0.1),\n    labels = scales::label_percent(accuracy = 1),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  theme_pander() +\n  theme(legend.position = \"none\") +\n  labs(x = \"ka cal BP\", y = \"Mean proportion of assemblages\", fill = NULL) %&gt;%\n  labs(title = \"Frequency of Crops in SW Asia during the Neolithic Period\", caption = \"data from https://github.com/joeroe/SWAsiaNeolithicFounderCrops\")\n\n\n\n\nSo, it looks like barley and legumes are fairly constant with time, but wheat does increase consistently. A note about the x-axis- the times are listed as before present (BP), so a larger number is longer ago. Present is defined as January 1, 1950.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {TidyTuesday {Week} 16: {Neolithic} {Founder} {Crops}},\n  date = {2023-04-18},\n  url = {https://lsinks.github.io/posts/2023-04-18-tidytuesday-founder-crops/founder-crops},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “TidyTuesday Week 16: Neolithic Founder\nCrops.” April 18, 2023. https://lsinks.github.io/posts/2023-04-18-tidytuesday-founder-crops/founder-crops."
  },
  {
    "objectID": "posts/2023-05-02-portal-project/portal.html",
    "href": "posts/2023-05-02-portal-project/portal.html",
    "title": "TidyTuesday Week 18: Portal Project",
    "section": "",
    "text": "Today’s TidyTuesday is about the Portal Project, which is a long terms study on the ecology of Arizona. The study explores how ants, rodents, plants respond to climate in the desert near Portal, Az. A subset of the data is provided for this week’s TidyTuesday.\nLoading libraries. Not really using anything fancy today!\n\nlibrary(tidyverse)\nlibrary(ggthemes)\n\nLoading the data.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 18)\n\n--- Compiling #TidyTuesday Information for 2023-05-02 ----\n\n\n--- There are 3 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `plots.csv`\n    Downloading file 2 of 3: `species.csv`\n    Downloading file 3 of 3: `surveys.csv`\n\n\n--- Download complete ---\n\nplots &lt;- tuesdata$plots\nspecies &lt;- tuesdata$species\nsurveys &lt;- tuesdata$surveys\n\nThis dataset focuses on rodents. The portal project also studies plants and ants, but they are ommitted from these datasets.\n\nglimpse(plots)\n\nRows: 8\nColumns: 2\n$ plot      &lt;dbl&gt; 3, 4, 11, 14, 15, 17, 19, 21\n$ treatment &lt;chr&gt; \"exclosure\", \"control\", \"control\", \"control\", \"exclosure\", \"…\n\n\nWe have information about 8 plots in the plots dataframe, and they are coded as exclosure or control. It isn’t clear what exclosure or control means. The portal website says this about treatments of the plots:\n“Rodents are manipulated using gates in the fencing of each plot. Rodent removals contain no gates and any rodents captured on those plots are removed. All other plots contain 16 gates (4 per plot side); gates consist of holes cut through the hardware cloth of the fencing. Gate size is used to exclude subsets of the rodent community (larger gates allow all rodents access, smaller gates exclude kangaroo rats). Dimensions for gates on kangaroo rat removal plots are 1.9 cm x 1.9 cm, D. spectabilis removals were 2.6 cm x 3.0 cm, and control plots are 3.7 cm x 5.7 cm. In 2005, Dipodomys spectabilis removals were converted to controls – a state these plots had effectively been in with the local extinction of Dipodomys spectabilis in the late 1990s. Species caught on plots from which they are supposed to be excluded are removed from the site and the access point to the plot is located and eliminated. Plots affected by these treatments are listed in Portal_plot_treatments.”\nPresumably, the exclosures are those plots designed with gates of specific sizes, designed to exclude specific size rodents. It is less clear what a control is. The text says “Dipodomys spectabilis removals were converted to controls”, which is also unclear because it also says removals contain no rodents, so the species part is confusing. This page suggests controls are “unmanipulated controls” so maybe there is no fence at all?\nLooking at the history of the plots, we see that the exclosure plots all have had kangaroo rats removed from them at certain times.\nSo, do we find fewer Kangaroo rates on those plots? The survey data includes the rodent type by code; the key is found in the species data. Let’s pull out the kangaroo rat code(s).\n\nrats &lt;- species[(str_detect(species$commonname, \"kangaroo\")), ]\nrats\n\n# A tibble: 3 × 15\n  species scientificname       taxa  commonname censustarget unidentified rodent\n  &lt;chr&gt;   &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 DM      Dipodomys merriami   Rode… Merriam's…            1            0      1\n2 DO      Dipodomys ordii      Rode… Ord's kan…            1            0      1\n3 DS      Dipodomys spectabil… Rode… Banner-ta…            1            0      1\n# ℹ 8 more variables: granivore &lt;dbl&gt;, minhfl &lt;dbl&gt;, meanhfl &lt;dbl&gt;,\n#   maxhfl &lt;dbl&gt;, minwgt &lt;dbl&gt;, meanwgt &lt;dbl&gt;, maxwgt &lt;dbl&gt;, juvwgt &lt;dbl&gt;\n\n\nWe have three types of Kangaroo rate, coded DM, DO, and DS.\nLet’s make sure the survey data only includes the plots we know about from the plots dataframe.\n\nsurveys %&gt;% group_by(plot) %&gt;% count(plot) \n\n# A tibble: 11 × 2\n# Groups:   plot [11]\n    plot     n\n   &lt;dbl&gt; &lt;int&gt;\n 1     3  3580\n 2     4  3928\n 3    10   469\n 4    11  3640\n 5    14  3625\n 6    15  2106\n 7    16  1079\n 8    17  4023\n 9    19  2256\n10    21  2681\n11    23   977\n\n\nWe definitely have plots not in our plot dataframe. This is a small enough dataset that you can just look at the two lists and see we have 10, 16, and 23 as extra plots. If we go back to the history of the plots page, we can see that these three plots all had all rodents removed at several times over the course of the project. I’ll come back to that, but first I want to demonstrate how we can find these extra plots using a join. For larger datasets, it might not be possible to determine the extra plots by inspection.\nTo do this, I’m going to use an antijoin from dplyr. The syntax is anti_join(x , y) and returns rows of x that do not have a match in y. So here, we want x to be the plots from the survey data, and y to be the plots from out plots. I’ll just build off the summary pipe from the previous code block.\n\nsurveys %&gt;% group_by(plot) %&gt;% count(plot) %&gt;%\n  anti_join(plots)\n\nJoining with `by = join_by(plot)`\n\n\n# A tibble: 3 × 2\n# Groups:   plot [3]\n   plot     n\n  &lt;dbl&gt; &lt;int&gt;\n1    10   469\n2    16  1079\n3    23   977\n\n\nDoing this type of basic check is really important when you start working with a new dataset. It looked like we had two treatments, but there are actually 3. If you had missed this, you could have lumped the third treatment in with one of the other two while analyzing, and obtained incorrect results.\nI’m going to store these other plots numbers, both as an array with counts and as a 1D object of just the plot numbers. Depending on what I decide to do, I might annotate the plots dataframe to include this data.\n\nextra_plots_array &lt;- surveys %&gt;% group_by(plot) %&gt;% count(plot) %&gt;%\n  anti_join(plots)\n\nJoining with `by = join_by(plot)`\n\nextra_plots_array\n\n# A tibble: 3 × 2\n# Groups:   plot [3]\n   plot     n\n  &lt;dbl&gt; &lt;int&gt;\n1    10   469\n2    16  1079\n3    23   977\n\nextra_plots &lt;- extra_plots_array$plot\n\nThere are a couple of different things that could be explored. The treatments were not applied over every time block. We can look at how persistent a given treatment is. Plot 3 had the kangaroo rats removed in three of the five time blocks (1977-1985, 1988-2004, 2010-2015). Does the survey data reflect this?\nI’m going to pull out the plot 3 survey data.\n\nplot3_survey &lt;- surveys %&gt;%\n  filter(plot == 3)\n\nThere are a few different ways I could go. I’m going to create two groups of rodents: kangaroo rats and others. “NAs” are going in other. These are rodents that were caught and not identified. Perhaps using the size data, it might be possible to impute the species of rodent, but that requires more subject matter knowledge than I have.\nNAs are annoying, so I’ll handle them first. Then I’ll recode everything else to be kangaroo or other. I’m just hard coding from the species name, but I could use the species labels I saved earlier (rats$species).\n\nplot3_survey_recode &lt;- plot3_survey %&gt;%\n    mutate(species = ifelse(is.na(species) == TRUE, \"OTHER\", species)) %&gt;%\n    mutate(species = recode(species,\n                               \"DM\" = \"KAN\",\n                               \"DO\" = \"KAN\",\n                               \"DS\" = \"KAN\",\n                               .default = \"OTHER\")) \n\nSo, first, did the treatment work?\n\n  plot3_survey_recode %&gt;%\n    group_by(species) %&gt;%\n    count(species)\n\n# A tibble: 2 × 2\n# Groups:   species [2]\n  species     n\n  &lt;chr&gt;   &lt;int&gt;\n1 KAN       207\n2 OTHER    3373\n\n\nLooks like it did. But we should really compare to a control plot. Plot 4 is a control plot.\n\nplot4_survey &lt;- surveys %&gt;%\n  filter(plot == 4) %&gt;%\n    mutate(species = ifelse(is.na(species) == TRUE, \"OTHER\", species)) %&gt;%\n    mutate(species = recode(species,\n                               \"DM\" = \"KAN\",\n                               \"DO\" = \"KAN\",\n                               \"DS\" = \"KAN\",\n                               .default = \"OTHER\")) %&gt;%\n    group_by(species) %&gt;%\n    count(species)\n\nplot4_survey\n\n# A tibble: 2 × 2\n# Groups:   species [2]\n  species     n\n  &lt;chr&gt;   &lt;int&gt;\n1 KAN      2162\n2 OTHER    1766\n\n\nIn this plot, 55% of the captured rodents are kangaroo rats, compared to about 6% in plot 3. Again, to be completely rigorous, we should probably compare data aggregated over all plots with the same treatment types, rather than a single plot from treatment and single plot from control. These weren’t even randomly picked; I chose the first one of each type on the list.\nDid the kangaroo rat populations increase in years without treatment?\n\nplot3_survey_recode %&gt;%\n  filter (species == \"KAN\") %&gt;% group_by(year) %&gt;% count(species) %&gt;%\n  ggplot(aes(year, n)) +\n  geom_point() +\n  annotate(\n    \"rect\",\n    xmin = 1977,\n    xmax = 1985,\n    ymin = 0,\n    ymax = 40,\n    alpha = .1,\n    fill = \"red\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = 1988,\n    xmax = 2004,\n    ymin = 0,\n    ymax = 40,\n    alpha = .1,\n    fill = \"red\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = 2010,\n    xmax = 2015,\n    ymin = 0,\n    ymax = 40,\n    alpha = .1,\n    fill = \"red\"\n  ) +\n  ylab(\"# of Kangaroo rats\") +\n  xlab(\"Year\") +\n  labs(title = \"Persistance of treatment on Plot 3\",\n       subtitle = \"Kangaroo Rats removed in red periods\",\n       caption = \"Data from https://portal.weecology.org/\") +\n  theme_pander()\n\n\n\n\nSo that’s pretty interesting. The treatment did seem to be persistent. The years with the highest numbers of kangaroo rats were in times when they were actively being removed. (Perhaps the researchers were more diligent about identifying the rat species in removal period. This might be reflected in have fewer or proportionally fewer “NAs” during removal times.)\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {TidyTuesday {Week} 18: {Portal} {Project}},\n  date = {2023-05-02},\n  url = {https://lsinks.github.io/posts/2023-05-02-tidytuesday-portal-project/portal},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “TidyTuesday Week 18: Portal\nProject.” May 2, 2023. https://lsinks.github.io/posts/2023-05-02-tidytuesday-portal-project/portal."
  },
  {
    "objectID": "posts/2023-06-19-tidytuesday-UFOs/UFOs.html",
    "href": "posts/2023-06-19-tidytuesday-UFOs/UFOs.html",
    "title": "TidyTuesday Week 25: UFO Sightings Redux",
    "section": "",
    "text": "I haven’t been TidyTuesdaying because I’ve been learning Tableau. I’ll write more about that later; but it has been an interesting experience and provides different perspectives on data compared to what you might get from R. (I’m sure you could reproduce everything in Tableau in R and vice versa, but it is certainly easier to perform certain actions in one program over the other.)\nToday’s TidyTuesday is based on a dataset about the UFO sightings. This is an updated version of a tidytuesday challenge from 2019. The readme suggests that differences between the two datasets might be especially interesting.\n\nlibrary(skimr)\nlibrary(tidyverse)\nlibrary(ggthemes)\n\nLoad the data with the TidyTuesday package in the usual way.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 25)\n\n--- Compiling #TidyTuesday Information for 2023-06-20 ----\n\n\n--- There are 3 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `ufo_sightings.csv`\n    Downloading file 2 of 3: `places.csv`\n    Downloading file 3 of 3: `day_parts_map.csv`\n\n\n--- Download complete ---\n\nufo_sightings &lt;- tuesdata$`ufo_sightings`\nplaces &lt;- tuesdata$`places`\nday_parts_map &lt;- tuesdata$`day_parts_map`\n\nThis dataset combines information about reported UFO sightings with information about the lighting conditions at that time from sunrise-sunset. That is, was it day time, night time, twilight, etc. when the sighting occurred. This is an augmentation of the original dataset.\n\nskim(ufo_sightings)\n\n\nData summary\n\n\nName\nufo_sightings\n\n\nNumber of rows\n96429\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\nDate\n1\n\n\nlogical\n1\n\n\nnumeric\n1\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncity\n0\n1.00\n3\n26\n0\n10721\n0\n\n\nstate\n85\n1.00\n2\n31\n0\n684\n0\n\n\ncountry_code\n0\n1.00\n2\n2\n0\n152\n0\n\n\nshape\n2039\n0.98\n3\n9\n0\n24\n0\n\n\nreported_duration\n0\n1.00\n2\n25\n0\n4956\n0\n\n\nsummary\n31\n1.00\n1\n135\n0\n95898\n0\n\n\nday_part\n2563\n0.97\n5\n17\n0\n9\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nposted_date\n0\n1\n1998-03-07\n2023-05-19\n2012-08-19\n619\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nhas_images\n0\n1\n0\nFAL: 96429\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nduration_seconds\n0\n1\n31613.25\n6399774\n0\n30\n180\n600\n1987200000\n▇▁▁▁▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreported_date_time\n0\n1\n1925-12-29\n2023-05-18 19:27:00\n2012-02-05 03:00:00\n86201\n\n\nreported_date_time_utc\n0\n1\n1925-12-29\n2023-05-18 19:27:00\n2012-02-05 03:00:00\n86201\n\n\n\n\n\nThe data is fairly complete. The daypart is the least complete with only 97% completion. This variable is currently a string; it would be better as a factor.\n\nufo_sightings$day_part &lt;- as.factor(ufo_sightings$day_part)\n\nNow we can see when it is most common to see UFOs.\n\nggplot(ufo_sightings, aes(day_part)) +\n  geom_bar()\n\n\n\n\nThis is a pretty ugly plot. First, I’m going to clean up the formatting. The day-parts should be tilted so they are not overlapping, the axis labels are not clear, and I don’t like the default ggplot theme. You need to change the plot theme before you tilt the axis labels, otherwise the theme’s defaults will reset the axis label orientation.\n\nggplot(ufo_sightings, aes(day_part)) +\n  geom_bar() +\n  ylab(\"Number of Sighting\") +\n  xlab(\"\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1)) \n\n\n\n\nNow, there are two ways to order the bars. Usually, I’d use either ascending or descending order by count. But here, the day parts do have an intrinsic order- most light to least light (or the reverse) or something along the cycle. So, I need to order the factor day_part. I used the explanation here get the order and decide on the color scale. I decided that afternoon was lighter than morning.\nFirst, I’ll code the NAs as “unknown”. It is a small percentage of the total data, so dropping them is also a defensible choice. It doesn’t really impact the visualization to leave them in, and it does reveal some information about how detailed the reports are.\nI love the POSIT cheatsheets for checking syntax, especially the printed pdfs because I annotate them with my own notes. However, they can end up out of date. Recently POSIT has created HTML versions and updated most of the pdfs. So if you have a printed forcats cheatsheet branded RStudio, go get the new POSIT’s one, since some of the functions on the old cheatsheet are deprecated.\nTo convert NAs to a named level, the current function is fct_na_value_to_level rather than fct_explicit_na and the level is specified with level = \"blah\" rather than na_level = \"blah\". Both ways are shown in the code block and both will work.\n\n# old way of converting NAs to a specific level\n#ufo_sightings$day_part &lt;- fct_explicit_na(ufo_sightings$day_part, na_level = #\"unknown\")\n\n# new way of converting NAs to a specific level\nufo_sightings$day_part &lt;- fct_na_value_to_level(ufo_sightings$day_part, level = \"unknown\")\n\nNow, I need to relevel the day_parts factor. I started with morning, and then progressed through the day_parts in the order they occur. I put the unknown level last.\n\nufo_sightings$day_part &lt;- fct_relevel(ufo_sightings$day_part,c(\"morning\",\n                                      \"afternoon\", \"civil dusk\", \"nautical dusk\",\n                                      \"astronomical dusk\", \"night\",\n                                      \"astronomical dawn\", \"nautical dawn\",\n                                      \"civil dawn\", \"unknown\"))\n\nNow, I’m going to use the aesthetic fill = day_part to color my bars by time of day. I’m also going to define a manual color scale in blues, grays, and blacks to reflect the state of the sky- light blues for daylight and darker blues/blacks for night times. I made the unknown green (for little green men). I played around with both colors and values to get a gradient that I liked. Note that both gray and grey work. I kept the legend on while I was adjusting the colors because it was easier to see the gradient as compared to looking at the bar chart, but I turned it off for the final graph.\n\nggplot(ufo_sightings, aes(day_part, fill = day_part)) +\n  geom_bar() +\n  ylab(\"Number of Sighting\") +\n  xlab(\"\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1)) +\n  scale_fill_manual(values = c(\"lightskyblue2\", \"lightskyblue1\", \"skyblue2\",\n \"skyblue3\",\"lightskyblue4\", \"grey23\",\"lightskyblue4\",\"skyblue3\",\"skyblue2\",\n \"green\")) +\n  ggtitle(\"When do UFO Sightings Occur?\") +\n  theme(legend.position = \"none\")\n\n\n\n\nSo most sightings take place at night and very few take place in the early morning.\nThe sightings database also contains pictures of UFOs. Not all reports include pictures. It seems like it would be hard to get a good picture at night, so I’m wondering if most pictures are taken during daylight hours, despite having fewer over all sightings.\nhas_images is a boolean and the dataset is complete, but it seems that everything is coded FALSE? The original dataset from 2019 doesn’t include has_images, so I can’t use that to repair most of the entries.\n\nufo_sightings %&gt;%\n  group_by(day_part) %&gt;%\n  summarise(mean(has_images))\n\n# A tibble: 10 × 2\n   day_part          `mean(has_images)`\n   &lt;fct&gt;                          &lt;dbl&gt;\n 1 morning                            0\n 2 afternoon                          0\n 3 civil dusk                         0\n 4 nautical dusk                      0\n 5 astronomical dusk                  0\n 6 night                              0\n 7 astronomical dawn                  0\n 8 nautical dawn                      0\n 9 civil dawn                         0\n10 unknown                            0\n\n\nLooking at how the current dataset was prepared doesn’t help much either. The data was rescraped from National UFO Reporting Center as shown here and does not seem to include the original tidytuesday dataset at all. Searching for has_image, I find it first mentioned in line 890.\ndata_ufo_reports_clean &lt;- data_ufo_reports_durations |&gt;\ndplyr::mutate(\nstate = dplyr::coalesce(state, ascii_state),\n# Recode \"images\" to TRUE/FALSE, and replace NAs while we're at it.\nhas_images = isTRUE(images == \"Yes\"),\nLooking at the saved scraped data, found here, there is an images column that contains “Yes” or NA. It is very incomplete, with a completion rate of 1%. I also looked at the final cleaned data (post encoding to TRUE/FALSE) and it only has FALSES. So, it looks like the re-coding might have gone wrong.\nHere is the original data from the scraping.\n\nurl &lt;- \"https://github.com/jonthegeek/apis/raw/main/data/data_ufo_reports.rds\"\nufo_path &lt;- withr::local_tempfile(fileext = \".rds\")\ndownload.file(url, ufo_path)\ndata_ufo_reports_1 &lt;- readRDS(ufo_path)\n\nSee how many images are in this dataset.\n\ndata_ufo_reports_1 %&gt;%\n  filter(images == \"Yes\") %&gt;%\n  nrow()\n\n[1] 2342\n\n\nThere are only 2342 images in the database. The raw scrapped data has 144451 rows, while our cleaned processed data has 96429 rows. So a fair number of data points were dropped. It is possible that all of the records with images were dropped during the cleaning and processing.\nI’m going to join the UFO dataset with the scrapped data. I’m letting R decide what columns to join on, but the join does need to include summary, since that is one column that certainly was not cleaned/ processed.\nThe place names were cleaned (see for example lines 277 and beyond in the cleaning code). When I do an inner_join, I only get 1881 records. I’d expect to get the 96429 records in the ufo_sightings data file, since they should be contained in the original larger dataset.\n\ncombined &lt;- ufo_sightings %&gt;%\n  inner_join(data_ufo_reports_1) \n\nJoining with `by = join_by(city, state, shape, summary)`\n\ncombined %&gt;%\n  nrow()\n\n[1] 1881\n\n\nIf I were working on this for something more mission critical, I’d dig through the cleaning code more carefully, and make sure it was all ok. For now, I’m just going to check the Boolean encoding step, using my much smaller combined dataframe.\n\n# counting how many images\ncombined %&gt;%\n  filter(images == \"Yes\") %&gt;%\n  nrow()\n\n[1] 18\n\n#sthis is the re-coding step from the tidytuesday data\ncombined2 &lt;- combined %&gt;%\n  dplyr::mutate(\n       # Recode \"images\" to TRUE/FALSE, and replace NAs while we're at it.\n    has_images_tester = isTRUE(images == \"Yes\"))\n\n# counting how many images again\ncombined2 %&gt;%\n  filter(images == \"Yes\") %&gt;%\n  nrow()\n\n[1] 18\n\n# counting how many images after recode\ncombined2 %&gt;%\n  filter(has_images_tester == TRUE) %&gt;%\n  nrow()\n\n[1] 0\n\n\nSo, the specified code clearly doesn’t work. This is how I’d do it. I’d explicitly code both TRUE and FALSE using an ifelse clause in the mutate. Since the images contains a bunch of NAs, you need to be more careful about your test condition: images == \"Yes\" does give you the correct TRUEs, but it gives no FALSEs and retains all the NAs.\n\ncombined3 &lt;- combined2 %&gt;%\n  dplyr::mutate(\n       # Recode \"images\" to TRUE/FALSE, and replace NAs while we're at it.\n    has_images_correct1 = ifelse(images == \"Yes\", TRUE, FALSE))\n\ncombined3 %&gt;%\n  filter(has_images_correct1 == TRUE) %&gt;%\n  nrow()\n\n[1] 18\n\ncombined3 %&gt;%\n  filter(has_images_correct1 == FALSE) %&gt;%\n  nrow()\n\n[1] 0\n\n\nThe better test condition for this dataset is to use is.na(images) == FALSE (or the reverse, it doesn’t matter) and code NAs as FALSE and everything else as TRUE. This works because we have only have two values in the column (NA/ Yes). If you had other correct values, say Yes/ No/ NA, then this would not work. You’d need more complicated logic or two sets of ifelse.\n\ncombined4 &lt;- combined3 %&gt;%\n  dplyr::mutate(\n       # Recode \"images\" to TRUE/FALSE, and replace NAs while we're at it.\n    has_images_correct2 = ifelse(is.na(images) == FALSE, TRUE, FALSE))\n\ncombined4 %&gt;%\n  filter(has_images_correct2 == TRUE) %&gt;%\n  nrow()\n\n[1] 18\n\ncombined4 %&gt;%\n  filter(has_images_correct2 == FALSE) %&gt;%\n  nrow()\n\n[1] 1863\n\n\nSo this method of recoding works. My approach would be to first copy the entire processing code from line 140 and update the coding of images at 890 and re-run the whole thing and see what was produced. But more detailed checking of other steps might be required, depending on the application.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {TidyTuesday {Week} 25: {UFO} {Sightings} {Redux}},\n  date = {2023-06-20},\n  url = {https://lsinks.github.io/posts/2023-06-19-tidytuesday-UFOs/UFOs},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “TidyTuesday Week 25: UFO Sightings\nRedux.” June 20, 2023. https://lsinks.github.io/posts/2023-06-19-tidytuesday-UFOs/UFOs."
  },
  {
    "objectID": "posts/2023-06-29-tidytuesday-populated-places-leaflet/arlington-neighborhoods-leaflet.html",
    "href": "posts/2023-06-29-tidytuesday-populated-places-leaflet/arlington-neighborhoods-leaflet.html",
    "title": "Tidy Tuesday Revisited: Interactive Map of Arlington Historic Neighborhoods",
    "section": "",
    "text": "This week’s TidyTuesday is about place names as recorded by the US Board on Geographic Names. The dataset has been cleaned to include only populated places. I ended up augmenting the dataset with information about Arlington Historic neighborhoods and current neighborhood boundaries. My post with code on this project is here.\nI wanted to create an interactive map with leaflet, but I encountered two problems:\n1- I couldn’t figure out how to add my civic association map.\n2- The map that I did make worked fine when I ran it from a code chunk, but failed when I rendered the quarto document.\nI’ve solved both problems and I really enjoyed working with leaflet.\nHere are the libraries:\n\nlibrary(tidyverse) # who doesn't want to be tidy?\nlibrary(sf) # for handling geo data\nlibrary(leaflet) # interacting mapping\n\nI saved the two datasets from my previous work: historic_4269 and arlington_polygons_sf. I saved them using:\nst_write(historic_4269, \"points.shp\")\nst_write(arlington_polygons_sf, \"polygons.shp\")\nfrom the sf package.\nHere, I’m reading them in. The process does change some of the variable names. The dataset from the National Register of Historic Places had non-standard names such as Property.Name, which gets converted to a shorter name, Prprt_N, with _ instead of period.\n\nhistoric_4269 &lt;- st_read(\"points.shp\")\narlington_polygons_sf &lt;- st_read(\"polygons.shp\")\n\nI mentioned that I found tutorials here and here to make the pop-up URL using leaflet. So, following them I add the HTML anchor tag.\n\n# turn the url to HTML anchor tag\nhistoric_4269 &lt;- historic_4269 %&gt;% \n  mutate(tag = paste0(\"More Info: &lt;a href=\", Extrn_L,\"&gt;\", Extrn_L, \"&lt;/a&gt;\"))\n\nLeaflet uses background map tiles as the canvas for the map. As with all mapping, the coordinate reference system (CRS) of all your component layers needs to be the same. The two datasets I have used the CRS= 4269 projection, but this isn’t the usual CRS. The background map I chose uses the 4326 CRS, so I need to transform my data to that projection. Leaflet will give you a warning if you add layers with unexpected CRSs, so make sure to read the messages carefully and correct them.\n\nhistoric_4326 &lt;- sf::st_transform(historic_4269, crs = 4326)\narlington_polygons_sf_4326 &lt;- sf::st_transform(arlington_polygons_sf, crs = 4326) \n\nFor the issue of adding the polygon data, I was just not really thinking about things. Leaflet uses tidyverse piping, so you either need to have the dataset at the start of the pipe chain or you need to explicitly pass it as data = blah. The error message wasn’t super help to me either : addPolygons must be called with both lng and lat, or with neither. I thought that meant I needed to transform the polygons into some other type geometry format.\nSo this doesn’t work:\nleaflet_map &lt;- leaflet() %&gt;%\naddPolygons(arlington_polygons_sf_4326)\nleaflet_map\nBut this does:\n\nleaflet_map &lt;- leaflet(arlington_polygons_sf_4326) %&gt;% \n  addPolygons() \n\nleaflet_map\n\n\n\n\n\nOr this:\n\nleaflet_map &lt;- arlington_polygons_sf_4326 %&gt;% \n  leaflet() %&gt;%\n  addPolygons() \n\nleaflet_map\n\n\n\n\n\nOr this:\n\nleaflet_map &lt;- leaflet() %&gt;% \n  addPolygons(data = arlington_polygons_sf_4326) \n\nleaflet_map\n\n\n\n\n\nI chose to use the last method, since I was adding data from different sources and I thought it would be more understandable to have the data source explicitly stated in each layer call.\nTo make things a bit clearer, I set a color palette for the Arlington neighborhoods. There are 62 of them, so I used viridis, which is more suited for numerical data, but creates a pleasing effect here. There is information encoded in the colors, the purples correspond to neighborhoods starting with “A” and the yellows correspond to those at the end of the alphabet, but that isn’t really important. The choice was purely an aesthetic one.\n\npal &lt;- colorFactor(palette = \"viridis\", domain = arlington_polygons_sf_4326$CIVIC)\n\nThe final leaflet map has three layers:\n\nthe underlying map created using addProviderTiles()\nthe current Arlington neighborhoods created using addPolygons()\nthe point markers for the historic districts created using addCircleMarkers()\n\nThe neighborhood names appear when you hover over the polygon, while the name of the historic district and the link to the application submitted to be added to the National Register of Historic Places appears as a pop-up when you click on it.\nLeaflet uses ~ notation to reference variables in the data, which you can see in code below.\n\nleaflet_map &lt;- leaflet() %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addPolygons(\n    data = arlington_polygons_sf_4326,\n    weight = 1,\n    label = ~ CIVIC,\n    color = ~ pal(CIVIC)\n  ) %&gt;%\n  addCircleMarkers(\n    data = historic_4326,\n    popup = ~ paste0(\"&lt;b&gt;\", Prprt_N, \"&lt;/b&gt;\", \"&lt;br&gt;\", tag),\n    # note the tilde notation!\n    opacity = 1,\n    radius = 7,\n    color = \"black\",\n    stroke = NA\n  )\nleaflet_map\n\n\n\n\n\nDatacamp has a really nice starter course on leaflet that I found very helpful for understanding leaflet conceptually as well as learning about the basic formatting options. There is also a nice set of documentation here.\nSo why was my leaflet map causing the quarto document to fail to render? Apparently, there was a issue with knitr and quarto that popped up after some updates in May 2023. It applies to packages other than leaflet as well. If you get an error message along the lines of :\nError in `add_html_caption()`: ! unused argument (xfun::grep_sub(\"^[^&lt;]*&lt;[^&gt;]+aria-labelledby[ ]*=[ ]*\\\"([^\\\"]+)\\\".*$\", \"\\\\1\", x))\nBacktrace:\n1. global .main()\n2. execute(...)\n3. rmarkdown::render(...)\n4. knitr::knit(knit_input, knit_output, envir = envir, quiet = quiet)\n5. knitr:::process_file(text, output) ...\n14. sew(res, options)\n15. knitr:::sew.list(x, options, ...)\n16. base::lapply(x, sew, options, ...)\n17. FUN(X[[i]], ...)\n18. knitr:::sew.knit_asis(x, options, ...) Execution halted.\nthen you probably have this issue. Quarto has already fixed the issue with stable release 1.3.433. The version of Quarto bundled with RStudio RStudio 2023.06.0+421 “Mountain Hydrangea” for Windows was 1.3.353 and has the problem. If you use the bundled version with RStudio, close RStudio, install the latest Quarto as a standalone program. When you open RStudio, it should automatically detect the new version and switch to that.\nTo check what version of Quarto you have, go to the terminal (not console) and type quarto check.\nLeaflet is pretty amazing. I’ve always found mapping in R to be unpleasant, but leaflet makes it easy and produces beautiful maps.\n\n\n\nCitationBibTeX citation:@online{e. sinks2023,\n  author = {E. Sinks, Louise},\n  title = {Tidy {Tuesday} {Revisited:} {Interactive} {Map} of\n    {Arlington} {Historic} {Neighborhoods}},\n  date = {2023-06-29},\n  url = {https://lsinks.github.io/posts/2023-06-29-tidytuesday-US-populated-places-leaflet/arlington-neighborhoods-leaflet},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nE. Sinks, Louise. 2023. “Tidy Tuesday Revisited: Interactive Map\nof Arlington Historic Neighborhoods.” June 29, 2023. https://lsinks.github.io/posts/2023-06-29-tidytuesday-US-populated-places-leaflet/arlington-neighborhoods-leaflet."
  },
  {
    "objectID": "posts/2023-07-11-tidytuesday-temps/temperatures.html",
    "href": "posts/2023-07-11-tidytuesday-temps/temperatures.html",
    "title": "TidyTuesday Week 28: Global Surface Temperature",
    "section": "",
    "text": "Today’s TidyTuesday is on global surface temperatures. The source of the data is  NASA GISS Surface Temperature Analysis (GISTEMP v4) and more details about this data set can be found in this published paper: Lenssen, N., G. Schmidt, J. Hansen, M. Menne, A. Persin, R. Ruedy, and D. Zyss, 2019: Improvements in the GISTEMP uncertainty model. J. Geophys. Res. Atmos., 124, no. 12, 6307-6326, doi:10.1029/2018JD029522.\nI saw this lovely animated plot on Twitter and decided that I also wanted to make an aminated plot. I use a somewhat different approach though.\n\n\nThis week’s #TidyTuesday data on global surface temperatures was the perfect excuse to recreate an animated spiral line graph showing how temperatures have changed since 1880 🔥 🔥 🔥 #RStats #R4DS #DataViz pic.twitter.com/sDXWZaQHbz\n\n— Nicola Rennie | @nrennie@fosstodon.org (@nrennie35) July 11, 2023"
  },
  {
    "objectID": "posts/2023-07-11-tidytuesday-temps/temperatures.html#inspecting-the-data",
    "href": "posts/2023-07-11-tidytuesday-temps/temperatures.html#inspecting-the-data",
    "title": "TidyTuesday Week 28: Global Surface Temperature",
    "section": "Inspecting the Data",
    "text": "Inspecting the Data\nFirst, I will check the completeness of the data. All the data is numeric, so I’m going to use a custom skim function that omits some of the quartile data. I go over how to create a custom skim function here. The data comes as 4 separate tibbles.\n\nmy_skim &lt;- skim_with(numeric = sfl(p25 = NULL, p50 = NULL, p75 = NULL))\n\nglobal_temps %&gt;% my_skim() %&gt;% select(-skim_type)   %&gt;% gt() %&gt;%\n  cols_label(n_missing = \"# Missing\", complete_rate = \"Completeness\", \n             numeric.mean = \"Mean\", numeric.sd = \"Standard Deviation\",\n             numeric.p0 = \"Min\", numeric.p100 = \"Max\",\n             numeric.hist = \"Histogram\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Global Temperatures by Year\") \n\n\n\n\n\n  \n    \n      Global Temperatures by Year\n    \n    \n    \n      skim_variable\n      # Missing\n      Completeness\n      Mean\n      Standard Deviation\n      Min\n      Max\n      Histogram\n    \n  \n  \n    Year\n0\n1.0000000\n1.951500e+03\n41.7133072\n1880.00\n2023.00\n▇▇▇▇▇\n    Jan\n0\n1.0000000\n6.333333e-02\n0.4235976\n-0.81\n1.18\n▂▇▆▃▂\n    Feb\n0\n1.0000000\n7.090278e-02\n0.4285133\n-0.63\n1.37\n▅▇▃▂▁\n    Mar\n0\n1.0000000\n8.888889e-02\n0.4337898\n-0.63\n1.36\n▅▇▃▂▁\n    Apr\n0\n1.0000000\n6.368056e-02\n0.3966093\n-0.58\n1.13\n▆▇▅▃▂\n    May\n0\n1.0000000\n5.291667e-02\n0.3778942\n-0.55\n1.02\n▆▇▅▂▂\n    Jun\n1\n0.9930556\n3.314685e-02\n0.3673629\n-0.52\n0.93\n▆▇▃▂▂\n    Jul\n1\n0.9930556\n5.587413e-02\n0.3475312\n-0.51\n0.94\n▅▇▂▂▂\n    Aug\n1\n0.9930556\n5.440559e-02\n0.3633037\n-0.55\n1.02\n▃▇▃▂▂\n    Sep\n1\n0.9930556\n5.818182e-02\n0.3601988\n-0.58\n0.99\n▂▇▃▂▂\n    Oct\n1\n0.9930556\n8.419580e-02\n0.3692902\n-0.58\n1.09\n▃▇▃▂▁\n    Nov\n1\n0.9930556\n7.776224e-02\n0.3761975\n-0.55\n1.11\n▃▇▃▂▁\n    Dec\n1\n0.9930556\n5.181818e-02\n0.3931681\n-0.82\n1.16\n▂▇▃▃▁\n    J-D\n1\n0.9930556\n6.020979e-02\n0.3698448\n-0.48\n1.02\n▆▇▃▂▂\n    D-N\n2\n0.9861111\n6.077465e-02\n0.3707192\n-0.49\n1.04\n▆▇▃▂▂\n    DJF\n1\n0.9930556\n6.356643e-02\n0.4049559\n-0.67\n1.24\n▃▇▃▂▁\n    MAM\n0\n1.0000000\n6.854167e-02\n0.3983760\n-0.58\n1.14\n▆▇▅▂▂\n    JJA\n1\n0.9930556\n4.769231e-02\n0.3555351\n-0.50\n0.94\n▅▇▃▂▂\n    SON\n1\n0.9930556\n7.286713e-02\n0.3630672\n-0.52\n1.00\n▅▇▃▂▂\n  \n  \n  \n\n\n\n\nThere is also northern hemisphere data.\n\nnh_temps %&gt;% my_skim() %&gt;% select(-skim_type)   %&gt;% gt() %&gt;%\n  cols_label(n_missing = \"# Missing\", complete_rate = \"Completeness\", \n             numeric.mean = \"Mean\", numeric.sd = \"Standard Deviation\",\n             numeric.p0 = \"Min\", numeric.p100 = \"Max\",\n             numeric.hist = \"Histogram\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Northern Hemisphere Temperatures by Year\") \n\n\n\n\n\n  \n    \n      Northern Hemisphere Temperatures by Year\n    \n    \n    \n      skim_variable\n      # Missing\n      Completeness\n      Mean\n      Standard Deviation\n      Min\n      Max\n      Histogram\n    \n  \n  \n    Year\n0\n1.0000000\n1.951500e+03\n41.7133072\n1880.00\n2023.00\n▇▇▇▇▇\n    Jan\n0\n1.0000000\n8.576389e-02\n0.5778724\n-1.52\n1.59\n▁▅▇▅▂\n    Feb\n0\n1.0000000\n1.009722e-01\n0.5789589\n-0.98\n1.94\n▅▇▆▂▁\n    Mar\n0\n1.0000000\n1.282639e-01\n0.5701347\n-0.80\n1.91\n▅▇▃▂▁\n    Apr\n0\n1.0000000\n9.756944e-02\n0.4845865\n-0.65\n1.48\n▆▇▃▂▂\n    May\n0\n1.0000000\n9.055556e-02\n0.4203525\n-0.73\n1.28\n▂▇▅▂▂\n    Jun\n1\n0.9930556\n7.951049e-02\n0.4080447\n-0.52\n1.21\n▅▇▂▂▁\n    Jul\n1\n0.9930556\n7.811189e-02\n0.3845263\n-0.59\n1.10\n▃▇▃▂▁\n    Aug\n1\n0.9930556\n6.384615e-02\n0.4130920\n-0.77\n1.17\n▂▇▆▂▂\n    Sep\n1\n0.9930556\n8.013986e-02\n0.4194748\n-0.80\n1.22\n▁▇▅▂▂\n    Oct\n1\n0.9930556\n1.309790e-01\n0.4552020\n-0.84\n1.32\n▂▇▆▂▂\n    Nov\n1\n0.9930556\n1.152448e-01\n0.4939320\n-0.83\n1.61\n▃▇▅▂▁\n    Dec\n1\n0.9930556\n5.692308e-02\n0.5146409\n-1.14\n1.53\n▁▇▇▂▂\n    J-D\n1\n0.9930556\n8.888112e-02\n0.4444241\n-0.57\n1.35\n▆▇▂▂▁\n    D-N\n2\n0.9861111\n9.042254e-02\n0.4448506\n-0.58\n1.37\n▆▇▂▂▁\n    DJF\n1\n0.9930556\n8.377622e-02\n0.5291899\n-1.05\n1.67\n▂▇▆▂▂\n    MAM\n0\n1.0000000\n1.055556e-01\n0.4791598\n-0.71\n1.50\n▅▇▃▂▁\n    JJA\n1\n0.9930556\n7.370629e-02\n0.3969650\n-0.54\n1.12\n▅▇▂▂▂\n    SON\n1\n0.9930556\n1.086713e-01\n0.4451210\n-0.72\n1.34\n▃▇▅▂▁\n  \n  \n  \n\n\n\n\nAnd southern hemisphere data.\n\nsh_temps %&gt;% my_skim() %&gt;% select(-skim_type)   %&gt;% gt() %&gt;%\n  cols_label(n_missing = \"# Missing\", complete_rate = \"Completeness\", \n             numeric.mean = \"Mean\", numeric.sd = \"Standard Deviation\",\n             numeric.p0 = \"Min\", numeric.p100 = \"Max\",\n             numeric.hist = \"Histogram\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Southern Hemisphere Temperatures by Year\") \n\n\n\n\n\n  \n    \n      Southern Hemisphere Temperatures by Year\n    \n    \n    \n      skim_variable\n      # Missing\n      Completeness\n      Mean\n      Standard Deviation\n      Min\n      Max\n      Histogram\n    \n  \n  \n    Year\n0\n1.0000000\n1951.50000000\n41.7133072\n1880.00\n2023.00\n▇▇▇▇▇\n    Jan\n0\n1.0000000\n0.03875000\n0.3165414\n-0.63\n0.80\n▁▇▅▃▂\n    Feb\n0\n1.0000000\n0.03840278\n0.3154565\n-0.59\n0.79\n▂▇▅▃▂\n    Mar\n0\n1.0000000\n0.04750000\n0.3274280\n-0.59\n0.81\n▂▇▃▃▂\n    Apr\n0\n1.0000000\n0.02930556\n0.3421562\n-0.63\n0.98\n▃▇▅▃▁\n    May\n0\n1.0000000\n0.01631944\n0.3613025\n-0.60\n0.91\n▅▇▅▃▂\n    Jun\n1\n0.9930556\n-0.01083916\n0.3535683\n-0.67\n0.81\n▃▇▅▃▂\n    Jul\n1\n0.9930556\n0.03573427\n0.3354343\n-0.47\n0.86\n▇▇▃▃▂\n    Aug\n1\n0.9930556\n0.04594406\n0.3448540\n-0.46\n0.92\n▇▆▃▃▂\n    Sep\n1\n0.9930556\n0.03748252\n0.3357448\n-0.50\n0.91\n▇▇▃▃▂\n    Oct\n1\n0.9930556\n0.03937063\n0.3242656\n-0.51\n0.91\n▆▇▆▃▁\n    Nov\n1\n0.9930556\n0.04160839\n0.3082542\n-0.53\n0.80\n▂▇▃▃▂\n    Dec\n1\n0.9930556\n0.04713287\n0.3133837\n-0.55\n0.80\n▃▇▃▃▂\n    J-D\n1\n0.9930556\n0.03244755\n0.3138072\n-0.48\n0.75\n▅▇▂▃▂\n    D-N\n2\n0.9861111\n0.03218310\n0.3153409\n-0.50\n0.75\n▅▇▃▃▂\n    DJF\n1\n0.9930556\n0.04153846\n0.3125887\n-0.58\n0.80\n▂▇▃▃▂\n    MAM\n0\n1.0000000\n0.03069444\n0.3380438\n-0.60\n0.85\n▃▇▅▅▂\n    JJA\n1\n0.9930556\n0.02440559\n0.3347516\n-0.53\n0.77\n▆▇▃▅▂\n    SON\n1\n0.9930556\n0.03965035\n0.3150909\n-0.47\n0.74\n▅▇▃▃▂\n  \n  \n  \n\n\n\n\nAnd data broken down into finer zones from north to south.\n\nzonann_temps %&gt;% my_skim() %&gt;% select(-skim_type)   %&gt;% gt() %&gt;%\n  cols_label(n_missing = \"# Missing\", complete_rate = \"Completeness\", \n             numeric.mean = \"Mean\", numeric.sd = \"Standard Deviation\",\n             numeric.p0 = \"Min\", numeric.p100 = \"Max\",\n             numeric.hist = \"Histogram\") %&gt;%\n  opt_stylize(style = 6, color = \"blue\", add_row_striping = TRUE) %&gt;%\n  tab_header(title = \"Yearly Temperatures by Zone\") \n\n\n\n\n\n  \n    \n      Yearly Temperatures by Zone\n    \n    \n    \n      skim_variable\n      # Missing\n      Completeness\n      Mean\n      Standard Deviation\n      Min\n      Max\n      Histogram\n    \n  \n  \n    Year\n0\n1\n1.951000e+03\n41.4246304\n1880.00\n2022.00\n▇▇▇▇▇\n    Glob\n0\n1\n6.020979e-02\n0.3698448\n-0.48\n1.02\n▆▇▃▂▂\n    NHem\n0\n1\n8.888112e-02\n0.4444241\n-0.57\n1.35\n▆▇▂▂▁\n    SHem\n0\n1\n3.244755e-02\n0.3138072\n-0.48\n0.75\n▅▇▂▃▂\n    24N-90N\n0\n1\n1.090210e-01\n0.5357058\n-0.67\n1.67\n▆▇▂▂▁\n    24S-24N\n0\n1\n7.041958e-02\n0.3459949\n-0.63\n1.01\n▂▇▅▃▁\n    90S-24S\n0\n1\n-2.517483e-03\n0.3105594\n-0.48\n0.71\n▆▇▃▅▂\n    64N-90N\n0\n1\n2.502098e-01\n1.0028287\n-1.77\n3.24\n▂▇▅▂▁\n    44N-64N\n0\n1\n1.305594e-01\n0.5884089\n-0.79\n1.82\n▆▇▃▃▁\n    24N-44N\n0\n1\n3.643357e-02\n0.4192891\n-0.63\n1.27\n▃▇▂▂▁\n    EQU-24N\n0\n1\n6.237762e-02\n0.3467758\n-0.67\n0.97\n▂▇▆▃▂\n    24S-EQU\n0\n1\n7.818182e-02\n0.3520816\n-0.59\n1.07\n▃▇▅▃▁\n    44S-24S\n0\n1\n4.314685e-02\n0.3300329\n-0.44\n0.80\n▇▇▅▃▃\n    64S-44S\n0\n1\n-5.657343e-02\n0.2739233\n-0.54\n0.44\n▆▇▆▆▆\n    90S-64S\n0\n1\n-8.265734e-02\n0.7609316\n-2.60\n1.22\n▁▂▃▇▅\n  \n  \n  \n\n\n\n\nThe data sets are all over 0.99% complete. Handling missing values by dropping them is reasonable rather than imputing them."
  },
  {
    "objectID": "posts/2023-07-11-tidytuesday-temps/temperatures.html#tidying-the-data",
    "href": "posts/2023-07-11-tidytuesday-temps/temperatures.html#tidying-the-data",
    "title": "TidyTuesday Week 28: Global Surface Temperature",
    "section": "Tidying the Data",
    "text": "Tidying the Data\nI’ll note that the data isn’t tidy. In the tidy paradigm, each row corresponds to an observation. Here, multiple observations are included in each row and information is encoded in the column names. This dataset is in a wide format, rather than a long format, presumably to make it easy to compute the yearly and seasonal quarterly data. The tidyr package’s functions pivot_longer (and the converse, pivot_wider) are useful ways to reshape data.\n\nReshaping Data with pivot_longer\nMy plan is to look at the global temperature by month and then animate across years.\nThis is is original wide data.\n\nhead(global_temps)\n\n# A tibble: 6 × 19\n   Year   Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1880 -0.19 -0.25 -0.09 -0.17 -0.1  -0.21 -0.18 -0.11 -0.15 -0.24 -0.22 -0.18\n2  1881 -0.2  -0.15  0.03  0.05  0.05 -0.19  0    -0.04 -0.16 -0.22 -0.19 -0.08\n3  1882  0.16  0.13  0.04 -0.16 -0.14 -0.22 -0.17 -0.08 -0.15 -0.24 -0.17 -0.36\n4  1883 -0.3  -0.37 -0.13 -0.19 -0.18 -0.08 -0.08 -0.14 -0.23 -0.12 -0.24 -0.11\n5  1884 -0.13 -0.09 -0.37 -0.4  -0.34 -0.35 -0.31 -0.28 -0.28 -0.25 -0.34 -0.31\n6  1885 -0.59 -0.34 -0.27 -0.42 -0.45 -0.44 -0.34 -0.32 -0.29 -0.24 -0.24 -0.11\n# ℹ 6 more variables: `J-D` &lt;dbl&gt;, `D-N` &lt;dbl&gt;, DJF &lt;dbl&gt;, MAM &lt;dbl&gt;,\n#   JJA &lt;dbl&gt;, SON &lt;dbl&gt;\n\n\nI’m dropping the seasonal data and only retaining the monthly data. I then reshape it into three columns: year, month, and change in temperature. I’m dropping the na values; the default is to retain them, so values_drop_na = TRUE should be set explicitly.\n\nglobal_temps_reshaped &lt;- global_temps %&gt;% select(Year:Dec) %&gt;%\n  pivot_longer(Jan:Dec, names_to = \"month\", values_to = \"Delta_temp\", values_drop_na = TRUE)\nhead(global_temps_reshaped)\n\n# A tibble: 6 × 3\n   Year month Delta_temp\n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1  1880 Jan        -0.19\n2  1880 Feb        -0.25\n3  1880 Mar        -0.09\n4  1880 Apr        -0.17\n5  1880 May        -0.1 \n6  1880 Jun        -0.21\n\n\nThis reshaped data has 1721 rows compared to the 144 original rows- definitely longer!\nJust double check the completeness…\n\nskim(global_temps_reshaped) \n\n\nData summary\n\n\nName\nglobal_temps_reshaped\n\n\nNumber of rows\n1721\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nmonth\n0\n1\n3\n3\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYear\n0\n1\n1951.21\n41.41\n1880.00\n1915.00\n1951.00\n1987.00\n2023.00\n▇▇▇▇▇\n\n\nDelta_temp\n0\n1\n0.06\n0.39\n-0.82\n-0.22\n-0.03\n0.29\n1.37\n▁▇▃▂▁\n\n\n\n\n\n\n\nSetting Classes and Factor levels\nNote that our new month column should be a factor. I need to provide the levels otherwise ggplot will plot the months in alphabetical order. It turns out that base R has a few built in constants and abbreviated months is one of them. (English only, I’m sorry.) This saves some typing!\n\nglobal_temps_reshaped$month &lt;-\n  factor(global_temps_reshaped$month, levels = month.abb)\n\nYear should also be an integer. This isn’t super critical for regular ggplot graphs, but for the animation, weird interpolated year values are displayed.\n\nglobal_temps_reshaped$Year &lt;- as.integer(global_temps_reshaped$Year)"
  },
  {
    "objectID": "posts/2023-07-11-tidytuesday-temps/temperatures.html#first-draft",
    "href": "posts/2023-07-11-tidytuesday-temps/temperatures.html#first-draft",
    "title": "TidyTuesday Week 28: Global Surface Temperature",
    "section": "First Draft",
    "text": "First Draft\n\nglobal_temps_reshaped %&gt;%\n  ggplot(aes(month, Delta_temp)) + geom_col() +\n  labs(title = \"Year: {frame_time}\", x = \"Month\", y = \"Delta Temp\") +\n  transition_time(Year)"
  },
  {
    "objectID": "posts/2023-07-11-tidytuesday-temps/temperatures.html#polishing-the-graph",
    "href": "posts/2023-07-11-tidytuesday-temps/temperatures.html#polishing-the-graph",
    "title": "TidyTuesday Week 28: Global Surface Temperature",
    "section": "Polishing the Graph",
    "text": "Polishing the Graph\nNow polish this up.\nI’m going to set the scale to be fixed. This will make year over year comparisons easier. I’m also making it symmetric to illustrate that the deviations are much larger in one direction. I will also add a continuous color scale to make the changes even more obvious. I always use the viridis package whenever possible since it creates scales that easier to read for those with color blindness.\nI’m going to tweak on a static version for speeds sake. The unicode items (e.g. \\U0394) are to add various symbols. You can look them up here; you just need to remove the plus and add it with a \\ in your code.\n\nglobal_temps_reshaped %&gt;%  filter(Year == 1890) %&gt;%\n  ggplot(aes(month, Delta_temp, fill = Delta_temp)) + geom_col() +\n  scale_fill_viridis(option = \"turbo\", name = \"\\U0394 T (\\U00B0 C)\") +\n  ylim(-1.5, 1.5) +\n  labs(x = \"Month\", y = \"\\U0394 Temperature (\\U00B0 C)\") +\n  labs(title = \"Global Deviations in Temperature\") +\n  labs(subtitle = \"Year: 1890\") +\n  labs(caption =  \"Data from: NASA GISTEMP v4 via #TidyTuesday\") +\n  theme_pander(12)"
  },
  {
    "objectID": "posts/2023-07-11-tidytuesday-temps/temperatures.html#final-version",
    "href": "posts/2023-07-11-tidytuesday-temps/temperatures.html#final-version",
    "title": "TidyTuesday Week 28: Global Surface Temperature",
    "section": "Final Version",
    "text": "Final Version\nI went back and forth between the pander and the classic theme for the graph. Both have a white background, but pander has a grid and classic doesn’t. I decided on classic in the end. I ended up moving the caption to the left; when it was right aligned it looked like it was supposed to be aligned with the x axis label, but wasn’t.\n\nglobal_temps_reshaped %&gt;%\n  ggplot(aes(month, Delta_temp, fill = Delta_temp)) + geom_col() +\n  scale_fill_viridis(option = \"turbo\", name = \"\\U0394 T (\\U00B0 C)\") +\n  ylim(-1.4, 1.4) +\n  labs(x = \"Month\", y = \"\\U0394 Temperature (\\U00B0 C)\") +\n  labs(caption =  \"Data from: NASA GISTEMP v4 via #TidyTuesday\") +\n  theme_classic(12) +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(plot.caption = element_text(hjust = 0)) +\n  labs(title = \"Global Deviations in Temperature\", subtitle = \"Year: {frame_time}\") +\n  transition_time(Year)\n\n\n\n\nAnd saving the figure.\n\nanim_save(\"thumbnail.gif\")"
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "",
    "text": "This week’s TidyTuesday concerns what some have called the first randomized clinical trial- a study by James Lind evaluating various treatments for scurvy. This data has been collected into the medicaldata R package from Lind’s book on scurvy."
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#choosing-a-color-palette",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#choosing-a-color-palette",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "Choosing a Color Palette",
    "text": "Choosing a Color Palette\n\npar(cex = 0.5) \ndisplay.brewer.all()"
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#first-draft-of-the-heatmap",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#first-draft-of-the-heatmap",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "First Draft of the Heatmap",
    "text": "First Draft of the Heatmap\nAnd the first draft of the heatmap. I’m coloring the heatmap by numerical severity and I’m labeling it with the descriptive severity.\n\nggplot(scurvy2, aes(treatment, symptom)) +   \n  geom_tile(aes(fill = Numerical_Severity))  +\n  geom_text(aes(label = Descriptive_Severity)) +\n  scale_fill_brewer(palette = \"YlOrRd\")\n\n\n\n\nOh, that is ugly. And ggplot grouped all the treatments, so we have overlapping text labels."
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#plotting-by-study_id",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#plotting-by-study_id",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "Plotting by study_id",
    "text": "Plotting by study_id\nLet’s fix things up. First, plot by study_id instead of treatment group. That will get me back to the 12 patients. And the legend should be removed. This is done using theme(legend.position = \"none\"). I’m also going to flip x and y. Since there are only 4 symptoms, this will give the text labels (mild, moderate, severe) more space since there will only be 4 labels across instead of 12.\n\nscurvy_heatmap &lt;- ggplot(scurvy2, aes(symptom, study_id)) +   \n  geom_tile(aes(fill = Numerical_Severity))  +\n  geom_text(aes(label = Descriptive_Severity)) +\n  scale_fill_brewer(palette = \"YlOrRd\") +\n  theme(legend.position = \"none\") \n\nscurvy_heatmap"
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#mapping-study_id-back-to-treatments",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#mapping-study_id-back-to-treatments",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "Mapping study_id Back to Treatments",
    "text": "Mapping study_id Back to Treatments\nNow I need to get back to the treatment groups. The original data frame, scurvy, has the treatment groups matched to the patient_id. The mapping can be done using scale_x_continuous(breaks = 1:12, labels = scurvy$treatment). This is pretty non-intuitive to me;I found this suggestion on Stackoverflow.\n\nscurvy_heatmap &lt;- scurvy_heatmap +\n  scale_y_continuous(breaks = 1:12, labels = scurvy$treatment)\n\nscurvy_heatmap"
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#modifying-the-style-with-theme",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#modifying-the-style-with-theme",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "Modifying the Style with theme()",
    "text": "Modifying the Style with theme()\nNow tweak the theme to clean things up. First, I’m going to start with a theme that is closest to want I want. In this case, I went with theme_void(), which actually removes all the elements. I then add the few I want back in. There is so much stuff to remove (the axis, the grid marks, the gray background). I want to retain the axis title and text. I’ll add the titles and captions too.\n\nscurvy_heatmap &lt;- scurvy_heatmap +\n  theme_void() +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = 'bold')) +\n  labs(title = \"James Lind's Study of Scurvy Treatments- 1757\", \n       caption = \"data from medicaldata R package\",\n       subtitle = \"Results at Day 6\") +\n  ylab('Treatments') + xlab('Symptoms') +\n  theme(axis.text.x = element_text(angle = 45))\n  \nscurvy_heatmap"
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#theme_void-isnt-completely-empty",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#theme_void-isnt-completely-empty",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "theme_void() Isn’t Completely Empty!",
    "text": "theme_void() Isn’t Completely Empty!\nAnd the legend came back… Apparently, theme_void does include the legend, so it re-spawned. This is a bit surprising, since the description says “A completely empty theme.”\nI’m going to quickly test this.\n\nggplot(scurvy2, aes(treatment, symptom)) + \n  geom_tile(aes(fill = Numerical_Severity)) +\n  theme_void()"
  },
  {
    "objectID": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#final-heatmap",
    "href": "posts/2023-07-25-tidytuesday-scurvy/scurvy.html#final-heatmap",
    "title": "A Heatmap of James Lind’s Scurvy Study",
    "section": "Final Heatmap",
    "text": "Final Heatmap\nOkay, so not a big deal to remove this again. The y axis label also rotated, so that needs to be rotated back.\n\nscurvy_heatmap &lt;- scurvy_heatmap +\ntheme(legend.position = \"none\") +\n  theme(axis.title.y = element_text(angle = 90))\n\nscurvy_heatmap\n\n\n\nggsave(here(\"post\", \"2023-07-tidytuesday-scurvy\", \"thumbnail.png\"))\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "",
    "text": "I’m working on a project for my father that will culminate in a website for his genealogy research. There are a couple of different parts that I’m working on independently. This part involves linking photos of family gravestones to an Excel sheet that records the GPS location of the tombstones. This combined dataset is used to generate a leaflet map. This portion focused on data cleaning and the photo matching. I do generate a leaflet map at the end, but it is not the final map. I’ll do the styling of the map in a separate post.\nThis post is intended both to document what I did for my father so he understands any changes to the data and what results were obtained, but also as a tutorial on how to approach a messy problem. I’ve been solving problems using code for a long time. There are a ton of tutorials that focus on how to solve a specific problem, but fewer that show how to approach an undefined problem. And even fewer tutorials show mistakes and false starts. But these things happen when you are solving real world problems. Constantly checking your results against what you expect to get is critical and then figuring out how you messed up and fixed it is also important. The hard errors to find and fix are the logic errors. Everything runs fine. You get an output that may look right. But you still might not be getting the correct result. You have to approach every output critically and check your work carefully.\nI generally write my posts in a “code-along” style. I include almost everything I do, including dead ends. I could present more polished posts, where I write everything after I achieved the end result. This style of post would only include the steps that directly lead to the end result. I don’t do that because I don’t think the mechanics of getting to an end result is necessarily the hard part. Thinking your way through and self-checking the work is the hard part. If you know what you are trying to do, you can always find some code snippets to achieve that result. If you don’t know what you are trying to do, then all the code snippets in the world won’t help.\nSome sections I do omit mistakes and go to the final product, just so this tutorial doesn’t end up being 5 million pages long. Generally, the first time I do something, I will go into more detail than following times. For the data cleaning portion, the Cleaning Up Cemetery Names section shows the entire process, including mistakes. For the matching part, everything before Round 2 is in detail, including mistakes, and then the other rounds are much less detailed.\nI did also code a simplified version of this project all the way through using only one round of matching and 30 photos, just to make sure the basic elements were working. That isn’t shown here.\nIf for some reason you want to run this yourself, you can get a zipped copy of all the photos from here. I don’t upload the photos in this repo because the files size is too large."
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#loading-libraries",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#loading-libraries",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Loading Libraries",
    "text": "Loading Libraries\nI’ll include more info and reference information about the packages at the code blocks where I use them.\n\nlibrary(tidyverse) # who doesn't want to be tidy?\nlibrary(gt) # For nice tables\nlibrary(openxlsx) # importing excel files from a URL\nlibrary(fuzzyjoin) # for joining on inexact matches\nlibrary(sf) # for handling geo data\nlibrary(leaflet) # mapping\nlibrary(here) # reproducible file paths\nlibrary(magick) # makes panel pictures"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#file-folder-names-and-loading-data",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#file-folder-names-and-loading-data",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "File Folder Names and Loading Data",
    "text": "File Folder Names and Loading Data\nHere set-up some variables that I use for the file/ folder structure and I read in the spreadsheet.\n\n# folder names\nblog_folder &lt;- \"posts/2023-08-04-data-cleaning-tombstone\"\nphoto_folder &lt;- \"Photos\"\narchive_folder &lt;- \"Archived Photos\"\nunmatched_folder &lt;- \"Unmatched Photos\"\nmatch1 &lt;- \"Matched_Round1\"\nmatch2 &lt;- \"Matched_Round2\"\nmatch3 &lt;- \"Matched_Round3\"\nmatch4 &lt;- \"Matched_Round4\"\n\n\n#data_file &lt;- \"Tombstone_Data_small.xlsx\"\ndata_file &lt;- \"Tombstone Data.xlsx\"\n# read in excel sheet\ntombstones_raw &lt;-\n  read.xlsx(here(blog_folder, data_file),\n    sheet = 1\n  )"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#the-here-package-for-reproducible-file-structures",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#the-here-package-for-reproducible-file-structures",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "The here Package for Reproducible File Structures",
    "text": "The here Package for Reproducible File Structures\nI have folder structure that reflected the sequential nature of the matching, so photos get moved into different folders depending on what round they were matched in. I am use here to generate the paths. Quarto documents start the file path in the folder where the document resides, while r files start in the project folder. here always starts in the project folder, so it allows for easy recycling of code between r files and Quarto files and generally prevents you from getting lost in your file structure. It also allows me to easily move between an independent project and the project that is my website without having to recode all the folder names in the code. All I need to do is setup the sub-folder structure and names (as I did above) and then use them to generate file paths relative to here. You can see that usage in the loading of the excel sheet."
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#fixing-the-gps-data",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#fixing-the-gps-data",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Fixing the GPS data",
    "text": "Fixing the GPS data\nThe GPS data is stored as a string representing degrees, minutes, and seconds of latitude and longitude. I’m going to want this as a decimal lat/long (numerical) as I know that is accepted by many mapping programs. Dealing with this data has two parts: cleaning up the typos/ formatting and then converting to the decimal number.\n\nViewing the GPS Data (strings)\nWhen you view the GPS data you can see a couple of issues.\n\ntombstones_raw %&gt;% \n  select(Surname, N, W) %&gt;% \n  gt() %&gt;% \n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Surname\n      N\n      W\n    \n  \n  \n    Anderson\n36o56.472\n86 86.961\n    Anderson\n36 56.472\n86 86.961\n    Anderson\n37  53.396\n88  41.321\n    Anderson\n37  52.856\n88  39.163\n    Anderson\n37  52.856\n88  39.163\n    Anderson\n37  52.855\n88  39.163\n    Anderson\n37  52.853\n88  39.164\n    Anderson\n37  52.853\n88  39.167\n    Anderson\n37  52.852\n88  39.165\n    Appleton\n36  29.552\n86  46.793\n    Baldwin\n38  33.025\n87  06.328\n    Baldwin\n38  33.025\n87  06.328\n    Baggett\n36  29.553\n86  46.793\n    Beasley\n36  35.891\n86  43.204\n    Beasley\n36  36.755\n86  43.145\n    Beasley\n36  36.755\n86  43.145\n    Bell\n36  15.064\n86  11.669\n    Bell\n36  15.064\n86  11.669\n    Brazelton\n35  09.411\n86  03.624\n    Brazelton\n35  09.410\n86  03.624\n    Brown\n40O 40.760’\n75O 31.705'\n    Brown\n40O 40.760’\n75O 31.705'\n    Bundy\n37 45.623\n \n    Bundy\n37 53.380\n88 44.474\n    Bundy\n37 53.380\n88 44.474\n    Bundy\n37 53.380\n88 44.474\n    Bundy\n37 53.379\n88 44.474\n    Bundy\n37  52.875\n88  39.118\n    Bundy\n37  52.875\n88  39.118\n    Bundy\n37  52.873\n88  39.188\n    Bundy\n37  52.873\n88  39.188\n    Burgess\n37  49.224\n88  54.527\n    Burgess\n37  49.224\n88  54.527\n    Clayton\n37  50.788\n88  50.968\n    Clayton\n37  50.788\n88  50.968\n    Clayton\n37  50.795\n88  50.977\n    Clayton\n37  50.795\n88  50.977\n    Chapman\n37  29.894\n88  54.045\n    Chapman\n37  29.894\n88  54.046\n    Chapman\n37  25.692\n88 53.951\n    Chapman\n37  25.691\n88 53.949\n    Chapman\n37  25.691\n88 53.949\n    Chapman\n37  25.692\n88 53.951\n    Chapman\n37  25.692\n88 53.951\n    Chapman\n37  25.694\n88 53.951\n    Chapman\n38  33.026\n87  06.327\n    Crockett\n36 22.801\n86 45.985\n    Crockett\n36 22.804\n86 45.984\n    Davis\n37 44.682\n88 55.994\n    Davis\n37 44.683\n88 55.993\n    Davis\n36  14.260\n86  43.129\n    Dolch\n38  44.563\n82  58.988\n    Dolch\n38  44.584\n82  58.987\n    Dolch\n38  44.564\n82  58.987\n    Doley\n38  44.615\n82  58.882\n    Doley\n38  44.615\n82  58.882\n    Doley\n38  44.615\n82  58.882\n    Doley\n38  44.615\n82  58.882\n    NA\n38  44.618\n82  58.884\n    NA\n38  44.618\n82  58.884\n    NA\n38  44.618\n82  58.885\n    NA\n38  44.618\n82  58.885\n    NA\n38  44.618\n82  58.886\n    Doley\n38  44.615\n82  58.882\n    Doley\n38  44.610\n82  58.923\n    Doley\n38  44.611\n82  58.922\n    Doley\n38  44.611\n82  59.012\n    Doley\n38  44.611\n82  59.013\n    Doley\n37  49.907\n88  35.306\n    Doley\n37  49.907\n88  35.306\n    Doley\n37  49.907\n88  35.306\n    Doley\n37  58.810\n88  55.084\n    Doley\n37  58.810\n88  55.084\n    Doley\n37  58.751\n88  55.161\n    Doley\n37  58.751\n88  55.161\n    Dorris\n36o28.798’\n86o46.011’\n    Dorris\n36  28.811\n86  46.008\n    Dorris\n36  28.812\n86  46.008\n    Dorris\n36  28.812\n86  46.008\n    Dorris\n36  28.813\n86  46.007\n    Dorris\nNA\nNA\n    Dorris\nNA\nNA\n    Dorris\n36  26.485\n86  48.329\n    Dorris\n36  26.484\n86  48.329\n    Dorris\n38  07.067\n88  51.870\n    Dorris\n38  07.067\n88  51.870\n    Dorris\n38  07.067\n88  51.870\n    Dorris\n38  07.081\n88  51.903\n    Dorris\n38  07.081\n88  51.903\n    Dorris\n37  54.310\n88  58.084\n    Dorris\n37  54.309\n88  58.083\n    Dorris\n37  54.309\n88  58.083\n    Dorris\n37  54.310\n88  58.084\n    Dorris\n37  54.310\n88  58.084\n    Dorris\n37  58.746\n88  55.204\n    Dorris\n37  58.749\n88  55.205\n    Dorris\n37  47.990\n88  53.488\n    Dorris\n37  47.988\n88  53.489\n    Dorris\n37  51.571\n88  54.939\n    Dorris\n37  51.571\n88  54.939\n    Dorris\n37  50.787\n88  50.972\n    Dorris\n37  50.788\n88  50.971\n    Dorris\n37  50.794\n88  50.975\n    Dorris\n37  50.794\n88  50.975\n    Dorris\n37  50.786\n88  50.974\n    Dorris\n37  50.771\n88  50.986\n    Dorris\n37  50.783\n88  50.983\n    Dorris\n37  50.775\n88  50.986\n    Dorris\n37  50.775\n88  50.986\n    Dorris\n37  50.775\n88  50.980\n    Dorris\n37  50.775\n88  50.980\n    Dorris\n524\n783\n    Dorris\n528\n783\n    Drake\n36 35.870\n86 43.184\n    Dreisbach\n40o 44.177'\n75  29.596'\n    Dreisbach\n40o 44.177'\n75  29.593'\n    Everett\n38  33.026\n87  06.327\n    Farris\n37  24.687\n88  50.538\n    Farris\n37  24.678\n88  50.538\n    Finch\n44 34.662'\n37  27.129'\n    Follis\n37  51.764'\n88  56.897\n    Follis\n37  51.758'\n88  56.894\n    Follis\n37  51.761'\n88  56.893\n    Follis\n37  51.761'\n88  56.896\n    Follis\n37  51.759'\n88  56.895\n    Follis\n37  51.\n88  56\n    Follis\n37  51.758'\n88  56.896\n    Follis\n37  51.758'\n88  56.901'\n    Follis\n37  51.758'\n88  56.901'\n    Follis\n37  51.758'\n88  56.904\n    Ford\n37  52.851\n88  39.161\n    Fox\n37  48.023\n88  53.449\n    Frost\n37  17.909\n87  28.852\n    NA\n37  17.910\n87  28.852\n    Frost\n37  17.909\n87  28.854\n    Fuqua\n36 38.189\n86 51.516\n    Gregory\n38 44.609\n82 58.922'\n    Gregory\n38 44.611'\n82 58.922'\n    Hart\n37  51.757'\n88  56.900\n    Hess\n37  25.687\n88 53.947\n    Hess\n37  25.687\n88 53.949\n    Hess\n37  25.688\n88 53.952\n    Hess\n37  25.688\n88 53.952\n    Hess\n37  25.688\n88 53.952\n    Hess\n37  25.687\n88 53.947\n    Hess\n37  25.688\n88 53.952\n    Hess\n37  25.687\n88 53.948\n    Hess\n37  25.689\n88 53.952\n    Hess\n37  25.689\n88 53.952\n    Hess\n37  25.693\n88 53.949\n    Hess\n37  25.693\n88 53.947\n    Hess\n37  25.693\n88 53.949\n    Hess\n37  25.690\n88 53.951\n    Holt\nNA\nNA\n    Holt\nNA\nNA\n    Horlacher\n40o 30.928'\n75o 25.072'\n    Horlacher\n40o30.930’\n75o 25.070’\n    Horrall\n37  54.090\n88  54.218\n    Horrall\n38  33.026\n87  06.326\n    Horrall\n38  36.963\n87 11.369\n    Hurt\n36  28.804\n86  46.007\n    Jacobs\n38  21.315'\n85  41.307'\n    Jacobs\n38  21.317'\n85  41.306'\n    Johnson\n37  52.872\n88  39.183\n    Johnson\n37  52.872\n88  39.183\n    Jones\n37  47.994\n88  53.504\n    Jones\n37  47.994\n88  53.504\n    Jones\n37  47.997\n88  53.483\n    Jones\n37  47.995\n88  53.483\n    Jones\n37  47.995\n88  53.483\n    Jones\n37  48.024\n88  53.451\n    Jones\n37  48.024\n88  53.451\n    Jones\n37  48.020\n88  53.465\n    Jones\n37  48.020\n88  53.465\n    Jones\n37  51.747\n88  52.933\n    Karnes\n37  58.749\n88  55.161\n    Karnes\n37  58.749\n88  55.161\n    Keith\nNA\nNA\n    Keth\n35  09.410\n86  03.624\n    Kleppinger\n40o 44.178'\n75  29.601'\n    Lipsey\n38  33.917\n89  07.571\n    Lockwood\nNA\nNA\n    Lockwood\nNA\nNA\n    Loomis\n37  36.925\n89  12.220\n    Mensch\n40o 39.557'\n75  25.586'\n    Merrell\n35  43.945\n80  18.669\n    Merrell\n35  43.942\n80  18.671\n    Meredith\n39O 41.114’\n76O 35.858'\n    Meredith\n39O 41.115’\n76O 35.855'\n    Meredith\n39O 41.116’\n76O 35.855'\n    Meredith\n39O 41.116’\n76O 35.854'\n    Meredith\n39O 41.117’\n76O 35.853'\n    Bell\n39O 41.117’\n76O 35.853'\n    John \n39O 41.117’\n76O 35.853'\n    Meredith\n39O 41.118’\n76O 35.852'\n    Meredith\n39O 41.112’\n76O 35.857'\n    Meredith\n39O 41.112’\n76O 35.857'\n    Meredith\n39O 41.112’\n76O 35.856'\n    Meredith\n39O 41.112’\n76O 35.856'\n    Meredith\n39O 41.112’\n76O 35.855'\n    Meredith\n39O 41.112’\n76O 35.854'\n    Meredith\n39O 41.113’\n76O 35.855'\n    Meredith\n39O 41.113’\n76O 35.855'\n    Tipton\n39O 41.114’\n76O 35.855'\n    Meredith\n39O 41.114’\n76O 35.854'\n    Meredith\n39O 41.114’\n76O 35.854'\n    Mildenberger\n40o 44.194’\n75O 29.608\n    Mildenberger\n40o 44.179\n75  29.574\n    Miller\n37  48.023\n88  53.449\n    Minnich\n40o 40.757’\n75O 31.679'\n    Minnich\n40o 40.759’\n75O 31.679'\n    Mory\n40o 33.585'\n75  23.776'\n    Mory\n40o 33.586'\n75  23.776'\n    Mory\n40o 33.586'\n75  23.774'\n    Mory\n40o 33.585'\n75  23.776'\n    Nagel\n40o 33.585'\n75  23.745'\n    Nagel\n40o 44.191'\n75  29.603'\n    Nagel\n41  13.033'\n75  57.329'\n    Nagel\n40o39.575'\n75o25.555'\n    Nagel\n40o39.577'\n75o25.549'\n    Nagel\n40o 44.197'\n75O 29.605’\n    Nagel\n41  13.031'\n75  57.333'\n    Nagel\n40o 39.575'\n75  25.555'\n    Nagle\n38  44.582'\n82  58.978'\n    Nagle\n38  44.582'\n82  58.978'\n    Nagel\n38  44.582'\n82  58.978'\n    Nagel\n38  44.582'\n82  58.978'\n    Nagel\n38  44.582'\n82  58.978'\n    Nagel\n38  44.582'\n82  58.978'\n    NA\nNA\nNA\n    Nutty \n37  25.674\n88  54.020\n    Nutty\n37  25.682\n88  54.020\n    Nutty \n37  25.678\n88  54.020\n    Ritter\n37 52.861\n88 39/178\n    Ritter\n37 52.861\n88 39/178\n    Odom\n37  58.794\n88  55.324\n    Odom\n37  58.795\n88  55.326\n    Odom\n37  47.993\n88  53.510\n    Odom\n37  47.994\n88  53.510\n    NA\n37  47.992\n88  53.506\n    Odum\nNA\nNA\n    Odum\n37 47.187\n88 50.175\n    Odum\n37 47.187\n88 50.175\n    Peters\n37  47.244\n88  55.354\n    Peters\n37  47.244\n88  55.354\n    Pickard\n38  04.918\n88  52.028\n    Pickard\n38  04.919\n88  52.028\n    Pickard\n38  04.917\n88  52.028\n    Pletz\n37 44.684\n88 55.998\n    Russell\n37 44.683\n88 55.998\n    Pickard\n38  04.918\n88  54.028\n    Pickard\n38  04.919\n88  54.028\n    Pickard\n38  04.917\n88  54.028\n    Pulliam\n37  25.697\n88 53.922\n    Pulliam\n37  25.697\n88 53.922\n    Rex\n37  45.776\n88  55.111\n    Rex\n37  45.776\n88  55.111\n    Rex\n37  45.777\n88  55.110\n    Rex\n37  45.777\n88  55.115\n    Rex\n37  45.776\n88  55.112\n    Rex\n37  45.774\n88  55.109\n    Rex\n37  45.776\n88  55.108\n    Rex\n37  45.776\n88  55.108\n    Rex\n37  45.776\n88  55.108\n    Rex\n32  22 549\n90  52.100\n    Rex\n37  44.784\n88  55.855\n    Rex\n37  44.785\n88  55.855\n    Richardson\n37  44.766\n88  55.776\n    Richardson\n37  44.787\n88  55.775\n    Riegel\n37  49.828\n88  35.346\n    Riegel\n37  49.828\n88  35.346\n    Ritter\n37  52.853\n88  39.174\n    Ritter\n37  52.853\n88  39.174\n    Rockel\n40o 39.556'\n75  25.585'\n    Rockel\n40o 39.555'\n75  25.585'\n    Rockel\n40o 39.560'\n75  25.560'\n    Rockel\n40o 39.560'\n75  25.559'\n    Ross\n37  58.752\n88  58.162\n    Ross\n37  58.752\n88  58.162\n    Ruckel\nNA\nNA\n    Ruckel\nNA\nNA\n    Russell\n37 44.681\n88 55.998\n    Russell\n37 44.682\n88 55.998\n    NA\n37 44.682\n88 55.994\n    NA\n37 44.682\n88 55.994\n    NA\n37 44.682\n88 55.994\n    Siliven\n37  28.189\n88  48.007\n    Sinks\n36 14.451'\n86  43.526'\n    Sinks\n37 54.081'\n88  54.293'\n    Sinks\n37 54.081'\n88  54.293'\n    Sinks\n37 54.089'\n88  54.207'\n    Sinks\n37  52.619\n88  55.430\n    Sinks\n37  52.619\n88  55.430\n    Sinks\n37  52.619\n88  55.430\n    Sinks\n37  47.989\n88  53.489\n    Sinks\n37  47.989\n88  53.489\n    Sinks\n37  47.986\n88  53.489\n    Sinks\n37  47.985\n88  53.491\n    Sinks\n37  47.984\n88  53.490\n    Sinks\n37  47.984\n88  53.490\n    Sinks\n37  47.984\n88  53.488\n    Sinks\n37  47.982\n88  53.491\n    Sinks\n37  48.024\n88  53.464\n    Sinks\n37  48.024\n88  53.463\n    Sinks\n37  48.020\n88  53.463\n    Sinks\n37  44.702\n88  55.998\n    Sweet\n37  44.704\n88  55.995\n    Sinks\n37  44.702\n88  55.997\n    Sinks\n37  44.704\n88  55.998\n    Sinks\n37  44.704\n88  55.998\n    Sinks\n38  33.836\n89  07.580\n    Sinks\n38  33.837\n89  07.579\n    Sinks\n38  33.917\n89  07.572\n    Sinks\n38  02.272\n88  50.161\n    Sinks\n37  44.770\n88  55.779\n    Sinks\n37  44.770\n88  55.779\n    Solt\n40o 48.686'\n75  37.120\n    Solt\n40o 48.693'\n75  37.119\n    Solt\n40o 48.690'\n75  37.113\n    Sfafford\n37  52.608\n88  55.434\n    Sfafford\n37  52.608\n88  55.434\n    Steen\n38  33.025\n87  06.328\n    VanCleve\n37  25.694\n88 53.921\n    VanCleve\n37  25.694\n88 53.921\n    VanCleve\n37  33.397\n88 46.363\n    VanCleve\n37  33.397\n88 46.363\n    VanCleve\n37  33.397\n88 46.363\n    VanCleave\n38  04.924\n88  52.030\n    VanCleave\n38  04.924\n88  52.030\n    Veach\n37  29.916\n86  54.044\n    Veach\n37  29.916\n86  54.044\n    Veach\n37  29.895\n86  54.023\n    Veach\n37  29.895\n86  54.022\n    Veach\n37  29.895\n86  54.020\n    Veach\n37  25.692\n88  53.942\n    Veach\n37  26.692\n88  53.942\n    Veatch\n37  26.692\n88  50.527\n    Veatch\n37  26.692\n88  50.527\n    Veach\n37  26.693\n88  50.540\n    Veach\n37  26.692\n88  50.531\n    Veach\n37  26.692\n88  50.531\n    Veach\n37  29.895'\n88 54.022\n    Veach\n37  29.897'\n88 54.022\n    Veatch\n37  28.187'\n88 49.000'\n    Veatch\n37  28.187'\n88 48.999'\n    Veatch\n37  28.187'\n88 49.001'\n    Veatch\n37  28.186\n88  48.005\n    Veach-Nutty \n37  25.682\n88  54.017\n    Veach\n37  25.681\n88  54.017\n    Veach\n37  25.679\n88  54.017\n    Veach\n37  25.682\n88  54.017\n    Ware\n37  52.856\n88  39.186\n    Ware\n37  52.856\n88  39.186\n    Ware\n37  52.867\n88  39.176\n    Ware\n37  52.867\n88  39.176\n    Webber\n37  49.829\n88  35.336\n    Webber\n37  49.826\n88  35.336\n    Weir\n36  15.064\n86  11.669\n    Weir\n36  15.064\n86  11.669\n    Wier\n37 49.208\n88 46.787\n    Whiteside\n37  26.743\n88 50.534\n    Whiteside\n37  26.743\n88 50.534\n    Willis\n36  35.889\n86  43.203\n    Wilson\n36 26.350\n86 47.072\n    Wilson\n36 26.361\n86 47.070\n    Wilson\n36  29.553\n86  46.791\n    Wilson\n36  28.812\n86  46.023\n    Wilson\n36  28.803\n86  46.007\n    Wilson\nNA\nNA\n    Wilson\n37  48.034\n88  53.443\n    Wilson\n37  48.034\n88  53.443\n    Wilson\n36  26.351\n86 47.070\n    Wilson\n36  26.351\n86 47.070\n    Wilson\n36  26.351\n86 47.070\n    Wilson\n36  26.350\n86 47.073\n    Wilson\n36  26.350\n86 47.073\n    Wilson\n36  26.351\n86 47.071\n    Wilson\nNA\nNA\n    Wilson \nNA\nNA\n    Wilson\nNA\nNA\n    Wilson\nNA\nNA\n    Wise\n37  50.352\n88  31.612\n    Wollard\n37 54.076'\n88  54.322'\n    Woolard\n37 54.075'\n88  54.322'\n    Woolard\n37  58.721\n88  55.211\n    Woolard\n37  58.721\n88  55.211\n    Woolard\n37  58.721\n88  55.211\n    Woolard\n37  58.723\n88  55.212\n    Woolard\n37  58.721\n88  55.213\n    Woolard\n37  58.720\n88  55.213\n    Woolard\n37  58.720\n88  55.213\n    Woolard\n37  51.394\n88  41.745\n    Woolard\n37  51.395\n88  41.746\n    Woolard\n37  51.396\n88  41.747\n    Woolard\n37  51.391\n88  41.742\n    Woolard\n37  51.397\n88  41.741\n    Woolard\n37  52.853\n88  39.160\n    Woolard\n37  52.853\n88  39.161\n    Woolard\n37  52.853\n88  39.160\n    Woolard\n37  52.853\n88  39.159\n    Woolard\n37  52.854\n88  39.160\n    Woolard\n37  51.742\n88  52.935\n    Woolard\n37  51.742\n88  52.935\n  \n  \n  \n\n\n\n\nLatitude and longitude data contains some stray degree and minute symbols. The degree symbol appears both as a straight and curved apostrophe and the degree symbols appear both as o and O. This cleaning needs to be done on both N and W columns. The str_replace_all() function from stringr looks at a string, finds a pattern, and replaces it with a replacement. Here, the pattern is each of those symbols and the replacement is a space.\n\n\nStyling Tables with gt\nI’m using the gt package to format my tables. Here I’m not doing much styling, but it is super easy to make really nice tables with just a few lines of code.\nI write and code in RStudio using Quarto. This allows you to alternate text and code chunks. You can run all the code chunks normally in RStudio or you can “render” the quarto document, which runs all the code chunks and produces the html page that becomes the page I publish on my website. When just running the code chunks, I get a table with scroll bars, but when rendering the webpage, I get a multi-page table that displays everything. This is fixed by specifying the size of the container for the table. With the container, the table is truncated to a few rows and a scroll bar appears. The container.padding option just makes sure the data isn’t truncated in the middle of a row.\n\n\nCleaning up Typos in the GPS Data (strings)\nI put all my cleaned data in a new dataframe. If something unexpected happens, I can check against the original data without having to reload it. I tend to use separate mutates for operation. I know it could be all in one mutate, but even when being careful about indents, I end up missing commas and parentheses as I add and remove steps. Individual mutates makes visually checking for syntax errors much easier for me.\n\ntombstones &lt;- tombstones_raw %&gt;%\nmutate(N = str_replace_all(N, pattern = \"’\", \" \")) %&gt;%\nmutate(N = str_replace_all(N, pattern = \"O\", \" \")) %&gt;%\nmutate(N = str_replace_all(N, pattern = \"o\", \" \")) %&gt;%\nmutate(N = str_replace_all(N, pattern = \"'\", \" \")) %&gt;% \nmutate(W = str_replace_all(W, pattern = \"’\", \" \")) %&gt;%\nmutate(W = str_replace_all(W, pattern = \"O\", \" \")) %&gt;% \nmutate(W = str_replace_all(W, pattern = \"o\", \" \")) %&gt;%\nmutate(W = str_replace_all(W, pattern = \"'\", \" \")) \n\nLook at the cleaned data.\n\ntombstones %&gt;%\n  select(Surname, First.Name, N, W) %&gt;%\n  gt()  %&gt;%\n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Surname\n      First.Name\n      N\n      W\n    \n  \n  \n    Anderson\nAbraham\n36 56.472\n86 86.961\n    Anderson\nElizabeth\n36 56.472\n86 86.961\n    Anderson\nZady\n37  53.396\n88  41.321\n    Anderson\nAlbert\n37  52.856\n88  39.163\n    Anderson\nAdesia\n37  52.856\n88  39.163\n    Anderson\nMay \n37  52.855\n88  39.163\n    Anderson\nE\n37  52.853\n88  39.164\n    Anderson\nWilliam\n37  52.853\n88  39.167\n    Anderson\nNancy\n37  52.852\n88  39.165\n    Appleton\nRichard\n36  29.552\n86  46.793\n    Baldwin\nJohn \n38  33.025\n87  06.328\n    Baldwin\nWilliam\n38  33.025\n87  06.328\n    Baggett\nMahalia\n36  29.553\n86  46.793\n    Beasley\nE\n36  35.891\n86  43.204\n    Beasley\nJosephine\n36  36.755\n86  43.145\n    Beasley\nFanning\n36  36.755\n86  43.145\n    Bell\nJohn \n36  15.064\n86  11.669\n    Bell\nMary \n36  15.064\n86  11.669\n    Brazelton\nWm\n35  09.411\n86  03.624\n    Brazelton\nEsther\n35  09.410\n86  03.624\n    Brown\nElizabeth \n40  40.760 \n75  31.705 \n    Brown\nJoel\n40  40.760 \n75  31.705 \n    Bundy\nHope\n37 45.623\n \n    Bundy\nClem\n37 53.380\n88 44.474\n    Bundy\nNancy\n37 53.380\n88 44.474\n    Bundy\nW\n37 53.380\n88 44.474\n    Bundy\nCharles\n37 53.379\n88 44.474\n    Bundy\nThomas\n37  52.875\n88  39.118\n    Bundy\nOctavia\n37  52.875\n88  39.118\n    Bundy\nGeorge\n37  52.873\n88  39.188\n    Bundy\nLora\n37  52.873\n88  39.188\n    Burgess\nW\n37  49.224\n88  54.527\n    Burgess\nAlzada\n37  49.224\n88  54.527\n    Clayton\nG\n37  50.788\n88  50.968\n    Clayton\nEllen\n37  50.788\n88  50.968\n    Clayton\nL\n37  50.795\n88  50.977\n    Clayton\nMary\n37  50.795\n88  50.977\n    Chapman\nDaniel\n37  29.894\n88  54.045\n    Chapman\nElizabeth\n37  29.894\n88  54.046\n    Chapman\nCaroline\n37  25.692\n88 53.951\n    Chapman\nDaniel\n37  25.691\n88 53.949\n    Chapman\nLucretia\n37  25.691\n88 53.949\n    Chapman\nSamuel\n37  25.692\n88 53.951\n    Chapman\nElizabeth\n37  25.692\n88 53.951\n    Chapman\nLaura\n37  25.694\n88 53.951\n    Chapman\nPolly\n38  33.026\n87  06.327\n    Crockett\nMandy\n36 22.801\n86 45.985\n    Crockett\nJohn\n36 22.804\n86 45.984\n    Davis\nEzra\n37 44.682\n88 55.994\n    Davis\nLizzie\n37 44.683\n88 55.993\n    Davis\nFred\n36  14.260\n86  43.129\n    Dolch\nCatherine\n38  44.563\n82  58.988\n    Dolch\nChristian\n38  44.584\n82  58.987\n    Dolch\nPeter\n38  44.564\n82  58.987\n    Doley\nGeorge\n38  44.615\n82  58.882\n    Doley\nKatie\n38  44.615\n82  58.882\n    Doley\nMary E\n38  44.615\n82  58.882\n    Doley\nHenriettie\n38  44.615\n82  58.882\n    NA\nMED\n38  44.618\n82  58.884\n    NA\nHD\n38  44.618\n82  58.884\n    NA\nGD\n38  44.618\n82  58.885\n    NA\nMother\n38  44.618\n82  58.885\n    NA\nFather\n38  44.618\n82  58.886\n    Doley\nGeorge\n38  44.615\n82  58.882\n    Doley\nJames \n38  44.610\n82  58.923\n    Doley\nMay\n38  44.611\n82  58.922\n    Doley\nJohn\n38  44.611\n82  59.012\n    Doley\nMaggie \n38  44.611\n82  59.013\n    Doley\nWilliam \n37  49.907\n88  35.306\n    Doley\nDora\n37  49.907\n88  35.306\n    Doley\nL[eaman]\n37  49.907\n88  35.306\n    Doley\nG[uilford]\n37  58.810\n88  55.084\n    Doley\nD[ora]\n37  58.810\n88  55.084\n    Doley\nEugene\n37  58.751\n88  55.161\n    Doley\nLou \n37  58.751\n88  55.161\n    Dorris\nJ[oseph]\n36 28.798 \n86 46.011 \n    Dorris\nJoseph\n36  28.811\n86  46.008\n    Dorris\nSarah\n36  28.812\n86  46.008\n    Dorris\nW\n36  28.812\n86  46.008\n    Dorris\nA\n36  28.813\n86  46.007\n    Dorris\nJ\nNA\nNA\n    Dorris\nElizabeth\nNA\nNA\n    Dorris\nRobert \n36  26.485\n86  48.329\n    Dorris\nRebecca\n36  26.484\n86  48.329\n    Dorris\nMonroe\n38  07.067\n88  51.870\n    Dorris\nDella\n38  07.067\n88  51.870\n    Dorris\nMary M\n38  07.067\n88  51.870\n    Dorris\nHarve \n38  07.081\n88  51.903\n    Dorris\nCarrie \n38  07.081\n88  51.903\n    Dorris\nSmith \n37  54.310\n88  58.084\n    Dorris\nAda\n37  54.309\n88  58.083\n    Dorris\nWilliam\n37  54.309\n88  58.083\n    Dorris\nHarvey \n37  54.310\n88  58.084\n    Dorris\nCora\n37  54.310\n88  58.084\n    Dorris\nJohn\n37  58.746\n88  55.204\n    Dorris\nW\n37  58.749\n88  55.205\n    Dorris\nGustavus\n37  47.990\n88  53.488\n    Dorris\nSarah \n37  47.988\n88  53.489\n    Dorris\nJoseph\n37  51.571\n88  54.939\n    Dorris\nDella \n37  51.571\n88  54.939\n    Dorris\nWilliam\n37  50.787\n88  50.972\n    Dorris\nHarriet\n37  50.788\n88  50.971\n    Dorris\nWilliam\n37  50.794\n88  50.975\n    Dorris\nMary\n37  50.794\n88  50.975\n    Dorris\nJames\n37  50.786\n88  50.974\n    Dorris\nSarah \n37  50.771\n88  50.986\n    Dorris\nW[illiam]\n37  50.783\n88  50.983\n    Dorris\nE[lisha]\n37  50.775\n88  50.986\n    Dorris\nSarah \n37  50.775\n88  50.986\n    Dorris\nJames\n37  50.775\n88  50.980\n    Dorris\nGeorgia\n37  50.775\n88  50.980\n    Dorris\nWilliam\n524\n783\n    Dorris\nMalinda\n528\n783\n    Drake\nMary \n36 35.870\n86 43.184\n    Dreisbach\nCatherina\n40  44.177 \n75  29.596 \n    Dreisbach\nJohannes\n40  44.177 \n75  29.593 \n    Everett\nSemantha\n38  33.026\n87  06.327\n    Farris\nElizabeth\n37  24.687\n88  50.538\n    Farris\nElizabeth\n37  24.678\n88  50.538\n    Finch\nIsaac\n44 34.662 \n37  27.129 \n    Follis\nFawn\n37  51.764 \n88  56.897\n    Follis\nRalph \n37  51.758 \n88  56.894\n    Follis\nA\n37  51.761 \n88  56.893\n    Follis\nChristian \n37  51.761 \n88  56.896\n    Follis\nG\n37  51.759 \n88  56.895\n    Follis\nRalph \n37  51.\n88  56\n    Follis\nE\n37  51.758 \n88  56.896\n    Follis\nWilliam\n37  51.758 \n88  56.901 \n    Follis\nMartha \n37  51.758 \n88  56.901 \n    Follis\nJeff\n37  51.758 \n88  56.904\n    Ford\nFlorence\n37  52.851\n88  39.161\n    Fox\nFrances\n37  48.023\n88  53.449\n    Frost\nEbenezer\n37  17.909\n87  28.852\n    NA\nNA\n37  17.910\n87  28.852\n    Frost\nNA\n37  17.909\n87  28.854\n    Fuqua\nWilliam\n36 38.189\n86 51.516\n    Gregory\nLeonard\n38 44.609\n82 58.922 \n    Gregory\nLucille\n38 44.611 \n82 58.922 \n    Hart\nParmelia\n37  51.757 \n88  56.900\n    Hess\nAmalphus\n37  25.687\n88 53.947\n    Hess\nAdolphus\n37  25.687\n88 53.949\n    Hess\nSamuel\n37  25.688\n88 53.952\n    Hess\nAugusta\n37  25.688\n88 53.952\n    Hess\nUlysses\n37  25.688\n88 53.952\n    Hess\nUlysses\n37  25.687\n88 53.947\n    Hess\nWilliam\n37  25.688\n88 53.952\n    Hess\nWilliam\n37  25.687\n88 53.948\n    Hess\nJerome\n37  25.689\n88 53.952\n    Hess\nFranklin\n37  25.689\n88 53.952\n    Hess\nSamuel\n37  25.693\n88 53.949\n    Hess\nBernice \n37  25.693\n88 53.947\n    Hess\nCatherine\n37  25.693\n88 53.949\n    Hess\nGeorge\n37  25.690\n88 53.951\n    Holt\nLucinda\nNA\nNA\n    Holt\nWilliam\nNA\nNA\n    Horlacher\nDaniel\n40  30.928 \n75  25.072 \n    Horlacher\nMargaretha\n40 30.930 \n75  25.070 \n    Horrall\nPolly\n37  54.090\n88  54.218\n    Horrall\nJames\n38  33.026\n87  06.326\n    Horrall\nWilliam\n38  36.963\n87 11.369\n    Hurt\nElizabeth\n36  28.804\n86  46.007\n    Jacobs\nJeremiah\n38  21.315 \n85  41.307 \n    Jacobs\nRebecca\n38  21.317 \n85  41.306 \n    Johnson\nJames\n37  52.872\n88  39.183\n    Johnson\nMary\n37  52.872\n88  39.183\n    Jones\nLevi\n37  47.994\n88  53.504\n    Jones\nHester\n37  47.994\n88  53.504\n    Jones\nRidley\n37  47.997\n88  53.483\n    Jones\nJames \n37  47.995\n88  53.483\n    Jones\nTina\n37  47.995\n88  53.483\n    Jones\nEzra\n37  48.024\n88  53.451\n    Jones\nNannie\n37  48.024\n88  53.451\n    Jones\nSamuel\n37  48.020\n88  53.465\n    Jones\nMelverda\n37  48.020\n88  53.465\n    Jones\nJohn\n37  51.747\n88  52.933\n    Karnes\nWillard\n37  58.749\n88  55.161\n    Karnes\nRuth\n37  58.749\n88  55.161\n    Keith\nJames\nNA\nNA\n    Keth\nNancy\n35  09.410\n86  03.624\n    Kleppinger\nAnna\n40  44.178 \n75  29.601 \n    Lipsey\nJoe\n38  33.917\n89  07.571\n    Lockwood\nEugenia\nNA\nNA\n    Lockwood\nLeland\nNA\nNA\n    Loomis\nJon\n37  36.925\n89  12.220\n    Mensch\nAbraham\n40  39.557 \n75  25.586 \n    Merrell\nAzariah\n35  43.945\n80  18.669\n    Merrell\nAbigail\n35  43.942\n80  18.671\n    Meredith\nEleandra\n39  41.114 \n76  35.858 \n    Meredith\nMicajah\n39  41.115 \n76  35.855 \n    Meredith\nSamuel\n39  41.116 \n76  35.855 \n    Meredith\nElizabeth\n39  41.116 \n76  35.854 \n    Meredith\nRuth\n39  41.117 \n76  35.853 \n    Bell\nSarah\n39  41.117 \n76  35.853 \n    John \nBell\n39  41.117 \n76  35.853 \n    Meredith\nMary\n39  41.118 \n76  35.852 \n    Meredith\nClarence\n39  41.112 \n76  35.857 \n    Meredith\nCora\n39  41.112 \n76  35.857 \n    Meredith\nW\n39  41.112 \n76  35.856 \n    Meredith\nSusan\n39  41.112 \n76  35.856 \n    Meredith\nHannah\n39  41.112 \n76  35.855 \n    Meredith\nMary \n39  41.112 \n76  35.854 \n    Meredith\nSamuel\n39  41.113 \n76  35.855 \n    Meredith\nBelinda\n39  41.113 \n76  35.855 \n    Tipton\nSusannah\n39  41.114 \n76  35.855 \n    Meredith\nThomas\n39  41.114 \n76  35.854 \n    Meredith\nSarah\n39  41.114 \n76  35.854 \n    Mildenberger\nAnna\n40  44.194 \n75  29.608\n    Mildenberger\nNicolaus\n40  44.179\n75  29.574\n    Miller\nMyrtie\n37  48.023\n88  53.449\n    Minnich\nElizabeth\n40  40.757 \n75  31.679 \n    Minnich\nJohn\n40  40.759 \n75  31.679 \n    Mory\nCatherina\n40  33.585 \n75  23.776 \n    Mory\nGotthard\n40  33.586 \n75  23.776 \n    Mory\nMagdelena\n40  33.586 \n75  23.774 \n    Mory\nPeter\n40  33.585 \n75  23.776 \n    Nagel\nAnna\n40  33.585 \n75  23.745 \n    Nagel\nAnna\n40  44.191 \n75  29.603 \n    Nagel\nCaty\n41  13.033 \n75  57.329 \n    Nagel\nDaniel\n40 39.575 \n75 25.555 \n    Nagel\nFrederick\n40 39.577 \n75 25.549 \n    Nagel\nFriedrich\n40  44.197 \n75  29.605 \n    Nagel\nJohann\n41  13.031 \n75  57.333 \n    Nagel\nMaria\n40  39.575 \n75  25.555 \n    Nagle\nJohn\n38  44.582 \n82  58.978 \n    Nagle\nMary\n38  44.582 \n82  58.978 \n    Nagel\nHenry\n38  44.582 \n82  58.978 \n    Nagel\nMary \n38  44.582 \n82  58.978 \n    Nagel\nWill\n38  44.582 \n82  58.978 \n    Nagel\nAdeline\n38  44.582 \n82  58.978 \n    NA\nNA\nNA\nNA\n    Nutty \nJohn\n37  25.674\n88  54.020\n    Nutty\nBeatrice\n37  25.682\n88  54.020\n    Nutty \nJohn\n37  25.678\n88  54.020\n    Ritter\nNA\n37 52.861\n88 39/178\n    Ritter\nNA\n37 52.861\n88 39/178\n    Odom\nArchibald\n37  58.794\n88  55.324\n    Odom\nCynthia \n37  58.795\n88  55.326\n    Odom\nG\n37  47.993\n88  53.510\n    Odom\nSarah\n37  47.994\n88  53.510\n    NA\nThomas\n37  47.992\n88  53.506\n    Odum\nBritton\nNA\nNA\n    Odum\nWiley\n37 47.187\n88 50.175\n    Odum\nSallie A\n37 47.187\n88 50.175\n    Peters\nDaniel\n37  47.244\n88  55.354\n    Peters\nCharlotte\n37  47.244\n88  55.354\n    Pickard\nWilliam\n38  04.918\n88  52.028\n    Pickard\nHarriet \n38  04.919\n88  52.028\n    Pickard\nLouise\n38  04.917\n88  52.028\n    Pletz\nKarl\n37 44.684\n88 55.998\n    Russell\nCaroline\n37 44.683\n88 55.998\n    Pickard\nWilliam\n38  04.918\n88  54.028\n    Pickard\nHarriet\n38  04.919\n88  54.028\n    Pickard\nLouise\n38  04.917\n88  54.028\n    Pulliam\nFrieda\n37  25.697\n88 53.922\n    Pulliam\nAmos\n37  25.697\n88 53.922\n    Rex\nWilliam\n37  45.776\n88  55.111\n    Rex\nElmina\n37  45.776\n88  55.111\n    Rex\nMamie\n37  45.777\n88  55.110\n    Rex\nGeorge \n37  45.777\n88  55.115\n    Rex\nBertie\n37  45.776\n88  55.112\n    Rex\nLulie\n37  45.774\n88  55.109\n    Rex\nLily\n37  45.776\n88  55.108\n    Rex\nArthur\n37  45.776\n88  55.108\n    Rex\nGeorge \n37  45.776\n88  55.108\n    Rex\nJno\n32  22 549\n90  52.100\n    Rex\nGuy\n37  44.784\n88  55.855\n    Rex\nHarlie\n37  44.785\n88  55.855\n    Richardson\nAnnabelle\n37  44.766\n88  55.776\n    Richardson\nAlfred\n37  44.787\n88  55.775\n    Riegel\nSolomon\n37  49.828\n88  35.346\n    Riegel\nCatherine\n37  49.828\n88  35.346\n    Ritter\nJ\n37  52.853\n88  39.174\n    Ritter\nMary\n37  52.853\n88  39.174\n    Rockel\nBalzer\n40  39.556 \n75  25.585 \n    Rockel\nElisabetha\n40  39.555 \n75  25.585 \n    Rockel\nJohannes\n40  39.560 \n75  25.560 \n    Rockel\nElizabeth\n40  39.560 \n75  25.559 \n    Ross\nGeorge \n37  58.752\n88  58.162\n    Ross\nEuna\n37  58.752\n88  58.162\n    Ruckel\nMary\nNA\nNA\n    Ruckel\nMelchir\nNA\nNA\n    Russell\nJames\n37 44.681\n88 55.998\n    Russell\nAna\n37 44.682\n88 55.998\n    NA\nNA\n37 44.682\n88 55.994\n    NA\nNA\n37 44.682\n88 55.994\n    NA\nNA\n37 44.682\n88 55.994\n    Siliven\nJenniel\n37  28.189\n88  48.007\n    Sinks\nA\n36 14.451 \n86  43.526 \n    Sinks\nFrancis \n37 54.081 \n88  54.293 \n    Sinks\nDelphia\n37 54.081 \n88  54.293 \n    Sinks\nSalem\n37 54.089 \n88  54.207 \n    Sinks\nDaniel\n37  52.619\n88  55.430\n    Sinks\nMartha\n37  52.619\n88  55.430\n    Sinks\nRoy \n37  52.619\n88  55.430\n    Sinks\nElizabeth\n37  47.989\n88  53.489\n    Sinks\ninfant son\n37  47.989\n88  53.489\n    Sinks\nJohn\n37  47.986\n88  53.489\n    Sinks\nMary\n37  47.985\n88  53.491\n    Sinks\nWilliam\n37  47.984\n88  53.490\n    Sinks\nCharlotte \n37  47.984\n88  53.490\n    Sinks\nAnna\n37  47.984\n88  53.488\n    Sinks\nLeonard\n37  47.982\n88  53.491\n    Sinks\nEtta Faye\n37  48.024\n88  53.464\n    Sinks\nJohn\n37  48.024\n88  53.463\n    Sinks\nSena\n37  48.020\n88  53.463\n    Sinks\nWilliam\n37  44.702\n88  55.998\n    Sweet\nJewell \n37  44.704\n88  55.995\n    Sinks\nFrancis \n37  44.702\n88  55.997\n    Sinks\nArlie\n37  44.704\n88  55.998\n    Sinks\nViola\n37  44.704\n88  55.998\n    Sinks\nLeonard\n38  33.836\n89  07.580\n    Sinks\nMae\n38  33.837\n89  07.579\n    Sinks\nBessie\n38  33.917\n89  07.572\n    Sinks\nCaroline\n38  02.272\n88  50.161\n    Sinks\nArlie\n37  44.770\n88  55.779\n    Sinks\nEva\n37  44.770\n88  55.779\n    Solt\nConrad\n40  48.686 \n75  37.120\n    Solt\nConrad\n40  48.693 \n75  37.119\n    Solt\nMaria\n40  48.690 \n75  37.113\n    Sfafford\nTrice\n37  52.608\n88  55.434\n    Sfafford\nPhebe\n37  52.608\n88  55.434\n    Steen\nRichard\n38  33.025\n87  06.328\n    VanCleve\nMartin\n37  25.694\n88 53.921\n    VanCleve\nFlorence\n37  25.694\n88 53.921\n    VanCleve\nW\n37  33.397\n88 46.363\n    VanCleve\nNancy \n37  33.397\n88 46.363\n    VanCleve\nJ\n37  33.397\n88 46.363\n    VanCleave\nW\n38  04.924\n88  52.030\n    VanCleave\nElizabeth\n38  04.924\n88  52.030\n    Veach\nPleasant\n37  29.916\n86  54.044\n    Veach\nVictoria\n37  29.916\n86  54.044\n    Veach\nWard\n37  29.895\n86  54.023\n    Veach\nCynthia\n37  29.895\n86  54.022\n    Veach\nJames\n37  29.895\n86  54.020\n    Veach\nJames\n37  25.692\n88  53.942\n    Veach\nNannie\n37  26.692\n88  53.942\n    Veatch\nJohn\n37  26.692\n88  50.527\n    Veatch\nEleanor\n37  26.692\n88  50.527\n    Veach\nWilliam\n37  26.693\n88  50.540\n    Veach\nJames\n37  26.692\n88  50.531\n    Veach\nRachel\n37  26.692\n88  50.531\n    Veach\nPleasant\n37  29.895 \n88 54.022\n    Veach\nMary\n37  29.897 \n88 54.022\n    Veatch\nParmelia\n37  28.187 \n88 49.000 \n    Veatch\nMary\n37  28.187 \n88 48.999 \n    Veatch\nElnor \n37  28.187 \n88 49.001 \n    Veatch\nFrelin\n37  28.186\n88  48.005\n    Veach-Nutty \nNA\n37  25.682\n88  54.017\n    Veach\nJohn\n37  25.681\n88  54.017\n    Veach\nRose\n37  25.679\n88  54.017\n    Veach\nRuth\n37  25.682\n88  54.017\n    Ware\nTurner\n37  52.856\n88  39.186\n    Ware\nMartha\n37  52.856\n88  39.186\n    Ware\nJoseph\n37  52.867\n88  39.176\n    Ware\nCaroline\n37  52.867\n88  39.176\n    Webber\nDick\n37  49.829\n88  35.336\n    Webber\nPearl\n37  49.826\n88  35.336\n    Weir\nJames\n36  15.064\n86  11.669\n    Weir\nMary\n36  15.064\n86  11.669\n    Wier\nLeticia \n37 49.208\n88 46.787\n    Whiteside\nLucinda \n37  26.743\n88 50.534\n    Whiteside\nJohn\n37  26.743\n88 50.534\n    Willis\nMatha\n36  35.889\n86  43.203\n    Wilson\nJessie\n36 26.350\n86 47.072\n    Wilson\nMary \n36 26.361\n86 47.070\n    Wilson\nJoseph\n36  29.553\n86  46.791\n    Wilson\nElisha\n36  28.812\n86  46.023\n    Wilson\nSallie\n36  28.803\n86  46.007\n    Wilson\nLutetita\nNA\nNA\n    Wilson\nThomas\n37  48.034\n88  53.443\n    Wilson\nSarah\n37  48.034\n88  53.443\n    Wilson\nElisha\n36  26.351\n86 47.070\n    Wilson\nMartha\n36  26.351\n86 47.070\n    Wilson\nCharles\n36  26.351\n86 47.070\n    Wilson\nZack\n36  26.350\n86 47.073\n    Wilson\nJuritha\n36  26.350\n86 47.073\n    Wilson\nElisha\n36  26.351\n86 47.071\n    Wilson\nDrury\nNA\nNA\n    Wilson \nMary \nNA\nNA\n    Wilson\nSandifer\nNA\nNA\n    Wilson\nNancy\nNA\nNA\n    Wise\nLuvena\n37  50.352\n88  31.612\n    Wollard\nJohn\n37 54.076 \n88  54.322 \n    Woolard\nNettie\n37 54.075 \n88  54.322 \n    Woolard\nMillie\n37  58.721\n88  55.211\n    Woolard\nLawrence\n37  58.721\n88  55.211\n    Woolard\nEtta \n37  58.721\n88  55.211\n    Woolard\nJohn\n37  58.723\n88  55.212\n    Woolard\nJames\n37  58.721\n88  55.213\n    Woolard\nC\n37  58.720\n88  55.213\n    Woolard\nBlanche\n37  58.720\n88  55.213\n    Woolard\nL \n37  51.394\n88  41.745\n    Woolard\nAma\n37  51.395\n88  41.746\n    Woolard\nRobert\n37  51.396\n88  41.747\n    Woolard\nJames\n37  51.391\n88  41.742\n    Woolard\nRomey\n37  51.397\n88  41.741\n    Woolard\nAnna\n37  52.853\n88  39.160\n    Woolard\nJames\n37  52.853\n88  39.161\n    Woolard\nFrancis\n37  52.853\n88  39.160\n    Woolard\nTurner\n37  52.853\n88  39.159\n    Woolard\nWilliam\n37  52.854\n88  39.160\n    Woolard\nGeorge\n37  51.742\n88  52.935\n    Woolard\nNancy\n37  51.742\n88  52.935\n  \n  \n  \n\n\n\n\nMuch better. There is some missing data, encoded both as blanks and as NAs. There are also some coordinates that don’t make sense, like 524 (for the entry Dorris William). This will need to be dealt with.\n\n\nConverting to Decimal Coordinates (Numeric)\nNext, I’m converting the N and W data to decimal latitude and longitude. S/W should be “-” and N/E should be “+”. I split the degree/minute/second data into parts and then do the conversion. I delete the intermediate components when done. I used str_split_fixed() here, which stores the parts in a matrix in your dataframe, hence the indexing to access the parts. The related function str_split() returns a list. Both functions take the string, a pattern. str_split_fixed() also requires the number of parts (n) to split into. If it doesn’t find that many parts it will store a blank (““) rather than fail. More info about the str_split family can be found here. (A function like separate() would be more straightforward for this application. I originally included another example here where I use separate, so both methods were illustrated, but I have moved that to a module of this project that isn’t posted yet.)\nI want to break a coordinate into 3 parts. So 37 25.687 becomes 37 25 and 687. First I break the coordinate into two parts, using the space as the separator. So 37 and 25.687. I then coerce the first part (which is the degree part of the coordinate) into a numeric. I then split the second part ( 25.687) using the . as the separator and again coerce the results into numbers. The coercion does lead to warning about the generation of NAs during the process, but that is fine. I know not all the data is numeric- there were blanks and NAs to start with. Lastly, I convert my degree, minute, second coordinates to decimal coordinates using the formula degree + minute/60 + second/3600.\n\nEscaping Characters in stringr\nIt is important to note that stringr defaults to considering that patterns are written in regular expressions (regex). This means some characters are special and require escaping in the pattern. The period is one such character and the correct pattern is “\\\\.” Otherwise, using “.” will match to every character. The stringr cheat sheet has a high level overview of regular expressions on the second page.\n\n\nUsing Selectors from dplyr\nI named all the original output from the string splits such that they contained the word “part” and I can easily remove them using a helper from dplyr, in this case, contains. I highly recommend using some sort of naming scheme for intermediate variables/ fields so they can be easily removed in one go without lots of typing. I retain the original and the numeric parts so I can double check the results.\n\ntombstones &lt;- tombstones %&gt;%\n  mutate(part1N = str_split_fixed(N, pattern = \" \", n = 2) ) %&gt;%\n  mutate(N_degree = as.numeric(part1N[,1])) %&gt;%\n  mutate(part2N = str_split_fixed(part1N[,2], pattern = '\\\\.', n = 2)) %&gt;%\n  mutate(N_minute = as.numeric(part2N[,1])) %&gt;%\n  mutate(N_second = as.numeric(part2N[,2])) %&gt;%\n  mutate(lat = N_degree + N_minute/60 + N_second/3600)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `N_minute = as.numeric(part2N[, 1])`.\nCaused by warning:\n! NAs introduced by coercion\n\n#converting to decimal longitude  \ntombstones &lt;- tombstones %&gt;%\n  mutate(part1W = str_split_fixed(W, pattern = \" \", n = 2) ) %&gt;%\n  mutate(W_degree = as.numeric(part1W[,1])) %&gt;%\n  mutate(part2W = str_split_fixed(part1W[,2], pattern = '\\\\.', n = 2)) %&gt;%\n  mutate(W_minute = as.numeric(part2W[,1])) %&gt;%\n  mutate(W_second = as.numeric(part2W[,2])) %&gt;%\n  mutate(long = -(W_degree + W_minute/60 + W_second/3600)) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `W_minute = as.numeric(part2W[, 1])`.\nCaused by warning:\n! NAs introduced by coercion\n\ntombstones &lt;- tombstones %&gt;%\n  select(-contains(\"part\"))\n\nTaking a quick look at the results\n\ntombstones %&gt;%\n  select(Surname, First.Name, N, N_degree, N_minute, N_second, lat) %&gt;%\n  gt()  %&gt;%\n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Surname\n      First.Name\n      N\n      N_degree\n      N_minute\n      N_second\n      lat\n    \n  \n  \n    Anderson\nAbraham\n36 56.472\n36\n56\n472\n37.06444\n    Anderson\nElizabeth\n36 56.472\n36\n56\n472\n37.06444\n    Anderson\nZady\n37  53.396\n37\n53\n396\n37.99333\n    Anderson\nAlbert\n37  52.856\n37\n52\n856\n38.10444\n    Anderson\nAdesia\n37  52.856\n37\n52\n856\n38.10444\n    Anderson\nMay \n37  52.855\n37\n52\n855\n38.10417\n    Anderson\nE\n37  52.853\n37\n52\n853\n38.10361\n    Anderson\nWilliam\n37  52.853\n37\n52\n853\n38.10361\n    Anderson\nNancy\n37  52.852\n37\n52\n852\n38.10333\n    Appleton\nRichard\n36  29.552\n36\n29\n552\n36.63667\n    Baldwin\nJohn \n38  33.025\n38\n33\n25\n38.55694\n    Baldwin\nWilliam\n38  33.025\n38\n33\n25\n38.55694\n    Baggett\nMahalia\n36  29.553\n36\n29\n553\n36.63694\n    Beasley\nE\n36  35.891\n36\n35\n891\n36.83083\n    Beasley\nJosephine\n36  36.755\n36\n36\n755\n36.80972\n    Beasley\nFanning\n36  36.755\n36\n36\n755\n36.80972\n    Bell\nJohn \n36  15.064\n36\n15\n64\n36.26778\n    Bell\nMary \n36  15.064\n36\n15\n64\n36.26778\n    Brazelton\nWm\n35  09.411\n35\n9\n411\n35.26417\n    Brazelton\nEsther\n35  09.410\n35\n9\n410\n35.26389\n    Brown\nElizabeth \n40  40.760 \n40\n40\n760\n40.87778\n    Brown\nJoel\n40  40.760 \n40\n40\n760\n40.87778\n    Bundy\nHope\n37 45.623\n37\n45\n623\n37.92306\n    Bundy\nClem\n37 53.380\n37\n53\n380\n37.98889\n    Bundy\nNancy\n37 53.380\n37\n53\n380\n37.98889\n    Bundy\nW\n37 53.380\n37\n53\n380\n37.98889\n    Bundy\nCharles\n37 53.379\n37\n53\n379\n37.98861\n    Bundy\nThomas\n37  52.875\n37\n52\n875\n38.10972\n    Bundy\nOctavia\n37  52.875\n37\n52\n875\n38.10972\n    Bundy\nGeorge\n37  52.873\n37\n52\n873\n38.10917\n    Bundy\nLora\n37  52.873\n37\n52\n873\n38.10917\n    Burgess\nW\n37  49.224\n37\n49\n224\n37.87889\n    Burgess\nAlzada\n37  49.224\n37\n49\n224\n37.87889\n    Clayton\nG\n37  50.788\n37\n50\n788\n38.05222\n    Clayton\nEllen\n37  50.788\n37\n50\n788\n38.05222\n    Clayton\nL\n37  50.795\n37\n50\n795\n38.05417\n    Clayton\nMary\n37  50.795\n37\n50\n795\n38.05417\n    Chapman\nDaniel\n37  29.894\n37\n29\n894\n37.73167\n    Chapman\nElizabeth\n37  29.894\n37\n29\n894\n37.73167\n    Chapman\nCaroline\n37  25.692\n37\n25\n692\n37.60889\n    Chapman\nDaniel\n37  25.691\n37\n25\n691\n37.60861\n    Chapman\nLucretia\n37  25.691\n37\n25\n691\n37.60861\n    Chapman\nSamuel\n37  25.692\n37\n25\n692\n37.60889\n    Chapman\nElizabeth\n37  25.692\n37\n25\n692\n37.60889\n    Chapman\nLaura\n37  25.694\n37\n25\n694\n37.60944\n    Chapman\nPolly\n38  33.026\n38\n33\n26\n38.55722\n    Crockett\nMandy\n36 22.801\n36\n22\n801\n36.58917\n    Crockett\nJohn\n36 22.804\n36\n22\n804\n36.59000\n    Davis\nEzra\n37 44.682\n37\n44\n682\n37.92278\n    Davis\nLizzie\n37 44.683\n37\n44\n683\n37.92306\n    Davis\nFred\n36  14.260\n36\n14\n260\n36.30556\n    Dolch\nCatherine\n38  44.563\n38\n44\n563\n38.88972\n    Dolch\nChristian\n38  44.584\n38\n44\n584\n38.89556\n    Dolch\nPeter\n38  44.564\n38\n44\n564\n38.89000\n    Doley\nGeorge\n38  44.615\n38\n44\n615\n38.90417\n    Doley\nKatie\n38  44.615\n38\n44\n615\n38.90417\n    Doley\nMary E\n38  44.615\n38\n44\n615\n38.90417\n    Doley\nHenriettie\n38  44.615\n38\n44\n615\n38.90417\n    NA\nMED\n38  44.618\n38\n44\n618\n38.90500\n    NA\nHD\n38  44.618\n38\n44\n618\n38.90500\n    NA\nGD\n38  44.618\n38\n44\n618\n38.90500\n    NA\nMother\n38  44.618\n38\n44\n618\n38.90500\n    NA\nFather\n38  44.618\n38\n44\n618\n38.90500\n    Doley\nGeorge\n38  44.615\n38\n44\n615\n38.90417\n    Doley\nJames \n38  44.610\n38\n44\n610\n38.90278\n    Doley\nMay\n38  44.611\n38\n44\n611\n38.90306\n    Doley\nJohn\n38  44.611\n38\n44\n611\n38.90306\n    Doley\nMaggie \n38  44.611\n38\n44\n611\n38.90306\n    Doley\nWilliam \n37  49.907\n37\n49\n907\n38.06861\n    Doley\nDora\n37  49.907\n37\n49\n907\n38.06861\n    Doley\nL[eaman]\n37  49.907\n37\n49\n907\n38.06861\n    Doley\nG[uilford]\n37  58.810\n37\n58\n810\n38.19167\n    Doley\nD[ora]\n37  58.810\n37\n58\n810\n38.19167\n    Doley\nEugene\n37  58.751\n37\n58\n751\n38.17528\n    Doley\nLou \n37  58.751\n37\n58\n751\n38.17528\n    Dorris\nJ[oseph]\n36 28.798 \n36\n28\n798\n36.68833\n    Dorris\nJoseph\n36  28.811\n36\n28\n811\n36.69194\n    Dorris\nSarah\n36  28.812\n36\n28\n812\n36.69222\n    Dorris\nW\n36  28.812\n36\n28\n812\n36.69222\n    Dorris\nA\n36  28.813\n36\n28\n813\n36.69250\n    Dorris\nJ\nNA\nNA\nNA\nNA\nNA\n    Dorris\nElizabeth\nNA\nNA\nNA\nNA\nNA\n    Dorris\nRobert \n36  26.485\n36\n26\n485\n36.56806\n    Dorris\nRebecca\n36  26.484\n36\n26\n484\n36.56778\n    Dorris\nMonroe\n38  07.067\n38\n7\n67\n38.13528\n    Dorris\nDella\n38  07.067\n38\n7\n67\n38.13528\n    Dorris\nMary M\n38  07.067\n38\n7\n67\n38.13528\n    Dorris\nHarve \n38  07.081\n38\n7\n81\n38.13917\n    Dorris\nCarrie \n38  07.081\n38\n7\n81\n38.13917\n    Dorris\nSmith \n37  54.310\n37\n54\n310\n37.98611\n    Dorris\nAda\n37  54.309\n37\n54\n309\n37.98583\n    Dorris\nWilliam\n37  54.309\n37\n54\n309\n37.98583\n    Dorris\nHarvey \n37  54.310\n37\n54\n310\n37.98611\n    Dorris\nCora\n37  54.310\n37\n54\n310\n37.98611\n    Dorris\nJohn\n37  58.746\n37\n58\n746\n38.17389\n    Dorris\nW\n37  58.749\n37\n58\n749\n38.17472\n    Dorris\nGustavus\n37  47.990\n37\n47\n990\n38.05833\n    Dorris\nSarah \n37  47.988\n37\n47\n988\n38.05778\n    Dorris\nJoseph\n37  51.571\n37\n51\n571\n38.00861\n    Dorris\nDella \n37  51.571\n37\n51\n571\n38.00861\n    Dorris\nWilliam\n37  50.787\n37\n50\n787\n38.05194\n    Dorris\nHarriet\n37  50.788\n37\n50\n788\n38.05222\n    Dorris\nWilliam\n37  50.794\n37\n50\n794\n38.05389\n    Dorris\nMary\n37  50.794\n37\n50\n794\n38.05389\n    Dorris\nJames\n37  50.786\n37\n50\n786\n38.05167\n    Dorris\nSarah \n37  50.771\n37\n50\n771\n38.04750\n    Dorris\nW[illiam]\n37  50.783\n37\n50\n783\n38.05083\n    Dorris\nE[lisha]\n37  50.775\n37\n50\n775\n38.04861\n    Dorris\nSarah \n37  50.775\n37\n50\n775\n38.04861\n    Dorris\nJames\n37  50.775\n37\n50\n775\n38.04861\n    Dorris\nGeorgia\n37  50.775\n37\n50\n775\n38.04861\n    Dorris\nWilliam\n524\n524\nNA\nNA\nNA\n    Dorris\nMalinda\n528\n528\nNA\nNA\nNA\n    Drake\nMary \n36 35.870\n36\n35\n870\n36.82500\n    Dreisbach\nCatherina\n40  44.177 \n40\n44\n177\n40.78250\n    Dreisbach\nJohannes\n40  44.177 \n40\n44\n177\n40.78250\n    Everett\nSemantha\n38  33.026\n38\n33\n26\n38.55722\n    Farris\nElizabeth\n37  24.687\n37\n24\n687\n37.59083\n    Farris\nElizabeth\n37  24.678\n37\n24\n678\n37.58833\n    Finch\nIsaac\n44 34.662 \n44\n34\n662\n44.75056\n    Follis\nFawn\n37  51.764 \n37\n51\n764\n38.06222\n    Follis\nRalph \n37  51.758 \n37\n51\n758\n38.06056\n    Follis\nA\n37  51.761 \n37\n51\n761\n38.06139\n    Follis\nChristian \n37  51.761 \n37\n51\n761\n38.06139\n    Follis\nG\n37  51.759 \n37\n51\n759\n38.06083\n    Follis\nRalph \n37  51.\n37\n51\nNA\nNA\n    Follis\nE\n37  51.758 \n37\n51\n758\n38.06056\n    Follis\nWilliam\n37  51.758 \n37\n51\n758\n38.06056\n    Follis\nMartha \n37  51.758 \n37\n51\n758\n38.06056\n    Follis\nJeff\n37  51.758 \n37\n51\n758\n38.06056\n    Ford\nFlorence\n37  52.851\n37\n52\n851\n38.10306\n    Fox\nFrances\n37  48.023\n37\n48\n23\n37.80639\n    Frost\nEbenezer\n37  17.909\n37\n17\n909\n37.53583\n    NA\nNA\n37  17.910\n37\n17\n910\n37.53611\n    Frost\nNA\n37  17.909\n37\n17\n909\n37.53583\n    Fuqua\nWilliam\n36 38.189\n36\n38\n189\n36.68583\n    Gregory\nLeonard\n38 44.609\n38\n44\n609\n38.90250\n    Gregory\nLucille\n38 44.611 \n38\n44\n611\n38.90306\n    Hart\nParmelia\n37  51.757 \n37\n51\n757\n38.06028\n    Hess\nAmalphus\n37  25.687\n37\n25\n687\n37.60750\n    Hess\nAdolphus\n37  25.687\n37\n25\n687\n37.60750\n    Hess\nSamuel\n37  25.688\n37\n25\n688\n37.60778\n    Hess\nAugusta\n37  25.688\n37\n25\n688\n37.60778\n    Hess\nUlysses\n37  25.688\n37\n25\n688\n37.60778\n    Hess\nUlysses\n37  25.687\n37\n25\n687\n37.60750\n    Hess\nWilliam\n37  25.688\n37\n25\n688\n37.60778\n    Hess\nWilliam\n37  25.687\n37\n25\n687\n37.60750\n    Hess\nJerome\n37  25.689\n37\n25\n689\n37.60806\n    Hess\nFranklin\n37  25.689\n37\n25\n689\n37.60806\n    Hess\nSamuel\n37  25.693\n37\n25\n693\n37.60917\n    Hess\nBernice \n37  25.693\n37\n25\n693\n37.60917\n    Hess\nCatherine\n37  25.693\n37\n25\n693\n37.60917\n    Hess\nGeorge\n37  25.690\n37\n25\n690\n37.60833\n    Holt\nLucinda\nNA\nNA\nNA\nNA\nNA\n    Holt\nWilliam\nNA\nNA\nNA\nNA\nNA\n    Horlacher\nDaniel\n40  30.928 \n40\n30\n928\n40.75778\n    Horlacher\nMargaretha\n40 30.930 \n40\n30\n930\n40.75833\n    Horrall\nPolly\n37  54.090\n37\n54\n90\n37.92500\n    Horrall\nJames\n38  33.026\n38\n33\n26\n38.55722\n    Horrall\nWilliam\n38  36.963\n38\n36\n963\n38.86750\n    Hurt\nElizabeth\n36  28.804\n36\n28\n804\n36.69000\n    Jacobs\nJeremiah\n38  21.315 \n38\n21\n315\n38.43750\n    Jacobs\nRebecca\n38  21.317 \n38\n21\n317\n38.43806\n    Johnson\nJames\n37  52.872\n37\n52\n872\n38.10889\n    Johnson\nMary\n37  52.872\n37\n52\n872\n38.10889\n    Jones\nLevi\n37  47.994\n37\n47\n994\n38.05944\n    Jones\nHester\n37  47.994\n37\n47\n994\n38.05944\n    Jones\nRidley\n37  47.997\n37\n47\n997\n38.06028\n    Jones\nJames \n37  47.995\n37\n47\n995\n38.05972\n    Jones\nTina\n37  47.995\n37\n47\n995\n38.05972\n    Jones\nEzra\n37  48.024\n37\n48\n24\n37.80667\n    Jones\nNannie\n37  48.024\n37\n48\n24\n37.80667\n    Jones\nSamuel\n37  48.020\n37\n48\n20\n37.80556\n    Jones\nMelverda\n37  48.020\n37\n48\n20\n37.80556\n    Jones\nJohn\n37  51.747\n37\n51\n747\n38.05750\n    Karnes\nWillard\n37  58.749\n37\n58\n749\n38.17472\n    Karnes\nRuth\n37  58.749\n37\n58\n749\n38.17472\n    Keith\nJames\nNA\nNA\nNA\nNA\nNA\n    Keth\nNancy\n35  09.410\n35\n9\n410\n35.26389\n    Kleppinger\nAnna\n40  44.178 \n40\n44\n178\n40.78278\n    Lipsey\nJoe\n38  33.917\n38\n33\n917\n38.80472\n    Lockwood\nEugenia\nNA\nNA\nNA\nNA\nNA\n    Lockwood\nLeland\nNA\nNA\nNA\nNA\nNA\n    Loomis\nJon\n37  36.925\n37\n36\n925\n37.85694\n    Mensch\nAbraham\n40  39.557 \n40\n39\n557\n40.80472\n    Merrell\nAzariah\n35  43.945\n35\n43\n945\n35.97917\n    Merrell\nAbigail\n35  43.942\n35\n43\n942\n35.97833\n    Meredith\nEleandra\n39  41.114 \n39\n41\n114\n39.71500\n    Meredith\nMicajah\n39  41.115 \n39\n41\n115\n39.71528\n    Meredith\nSamuel\n39  41.116 \n39\n41\n116\n39.71556\n    Meredith\nElizabeth\n39  41.116 \n39\n41\n116\n39.71556\n    Meredith\nRuth\n39  41.117 \n39\n41\n117\n39.71583\n    Bell\nSarah\n39  41.117 \n39\n41\n117\n39.71583\n    John \nBell\n39  41.117 \n39\n41\n117\n39.71583\n    Meredith\nMary\n39  41.118 \n39\n41\n118\n39.71611\n    Meredith\nClarence\n39  41.112 \n39\n41\n112\n39.71444\n    Meredith\nCora\n39  41.112 \n39\n41\n112\n39.71444\n    Meredith\nW\n39  41.112 \n39\n41\n112\n39.71444\n    Meredith\nSusan\n39  41.112 \n39\n41\n112\n39.71444\n    Meredith\nHannah\n39  41.112 \n39\n41\n112\n39.71444\n    Meredith\nMary \n39  41.112 \n39\n41\n112\n39.71444\n    Meredith\nSamuel\n39  41.113 \n39\n41\n113\n39.71472\n    Meredith\nBelinda\n39  41.113 \n39\n41\n113\n39.71472\n    Tipton\nSusannah\n39  41.114 \n39\n41\n114\n39.71500\n    Meredith\nThomas\n39  41.114 \n39\n41\n114\n39.71500\n    Meredith\nSarah\n39  41.114 \n39\n41\n114\n39.71500\n    Mildenberger\nAnna\n40  44.194 \n40\n44\n194\n40.78722\n    Mildenberger\nNicolaus\n40  44.179\n40\n44\n179\n40.78306\n    Miller\nMyrtie\n37  48.023\n37\n48\n23\n37.80639\n    Minnich\nElizabeth\n40  40.757 \n40\n40\n757\n40.87694\n    Minnich\nJohn\n40  40.759 \n40\n40\n759\n40.87750\n    Mory\nCatherina\n40  33.585 \n40\n33\n585\n40.71250\n    Mory\nGotthard\n40  33.586 \n40\n33\n586\n40.71278\n    Mory\nMagdelena\n40  33.586 \n40\n33\n586\n40.71278\n    Mory\nPeter\n40  33.585 \n40\n33\n585\n40.71250\n    Nagel\nAnna\n40  33.585 \n40\n33\n585\n40.71250\n    Nagel\nAnna\n40  44.191 \n40\n44\n191\n40.78639\n    Nagel\nCaty\n41  13.033 \n41\n13\n33\n41.22583\n    Nagel\nDaniel\n40 39.575 \n40\n39\n575\n40.80972\n    Nagel\nFrederick\n40 39.577 \n40\n39\n577\n40.81028\n    Nagel\nFriedrich\n40  44.197 \n40\n44\n197\n40.78806\n    Nagel\nJohann\n41  13.031 \n41\n13\n31\n41.22528\n    Nagel\nMaria\n40  39.575 \n40\n39\n575\n40.80972\n    Nagle\nJohn\n38  44.582 \n38\n44\n582\n38.89500\n    Nagle\nMary\n38  44.582 \n38\n44\n582\n38.89500\n    Nagel\nHenry\n38  44.582 \n38\n44\n582\n38.89500\n    Nagel\nMary \n38  44.582 \n38\n44\n582\n38.89500\n    Nagel\nWill\n38  44.582 \n38\n44\n582\n38.89500\n    Nagel\nAdeline\n38  44.582 \n38\n44\n582\n38.89500\n    NA\nNA\nNA\nNA\nNA\nNA\nNA\n    Nutty \nJohn\n37  25.674\n37\n25\n674\n37.60389\n    Nutty\nBeatrice\n37  25.682\n37\n25\n682\n37.60611\n    Nutty \nJohn\n37  25.678\n37\n25\n678\n37.60500\n    Ritter\nNA\n37 52.861\n37\n52\n861\n38.10583\n    Ritter\nNA\n37 52.861\n37\n52\n861\n38.10583\n    Odom\nArchibald\n37  58.794\n37\n58\n794\n38.18722\n    Odom\nCynthia \n37  58.795\n37\n58\n795\n38.18750\n    Odom\nG\n37  47.993\n37\n47\n993\n38.05917\n    Odom\nSarah\n37  47.994\n37\n47\n994\n38.05944\n    NA\nThomas\n37  47.992\n37\n47\n992\n38.05889\n    Odum\nBritton\nNA\nNA\nNA\nNA\nNA\n    Odum\nWiley\n37 47.187\n37\n47\n187\n37.83528\n    Odum\nSallie A\n37 47.187\n37\n47\n187\n37.83528\n    Peters\nDaniel\n37  47.244\n37\n47\n244\n37.85111\n    Peters\nCharlotte\n37  47.244\n37\n47\n244\n37.85111\n    Pickard\nWilliam\n38  04.918\n38\n4\n918\n38.32167\n    Pickard\nHarriet \n38  04.919\n38\n4\n919\n38.32194\n    Pickard\nLouise\n38  04.917\n38\n4\n917\n38.32139\n    Pletz\nKarl\n37 44.684\n37\n44\n684\n37.92333\n    Russell\nCaroline\n37 44.683\n37\n44\n683\n37.92306\n    Pickard\nWilliam\n38  04.918\n38\n4\n918\n38.32167\n    Pickard\nHarriet\n38  04.919\n38\n4\n919\n38.32194\n    Pickard\nLouise\n38  04.917\n38\n4\n917\n38.32139\n    Pulliam\nFrieda\n37  25.697\n37\n25\n697\n37.61028\n    Pulliam\nAmos\n37  25.697\n37\n25\n697\n37.61028\n    Rex\nWilliam\n37  45.776\n37\n45\n776\n37.96556\n    Rex\nElmina\n37  45.776\n37\n45\n776\n37.96556\n    Rex\nMamie\n37  45.777\n37\n45\n777\n37.96583\n    Rex\nGeorge \n37  45.777\n37\n45\n777\n37.96583\n    Rex\nBertie\n37  45.776\n37\n45\n776\n37.96556\n    Rex\nLulie\n37  45.774\n37\n45\n774\n37.96500\n    Rex\nLily\n37  45.776\n37\n45\n776\n37.96556\n    Rex\nArthur\n37  45.776\n37\n45\n776\n37.96556\n    Rex\nGeorge \n37  45.776\n37\n45\n776\n37.96556\n    Rex\nJno\n32  22 549\n32\nNA\nNA\nNA\n    Rex\nGuy\n37  44.784\n37\n44\n784\n37.95111\n    Rex\nHarlie\n37  44.785\n37\n44\n785\n37.95139\n    Richardson\nAnnabelle\n37  44.766\n37\n44\n766\n37.94611\n    Richardson\nAlfred\n37  44.787\n37\n44\n787\n37.95194\n    Riegel\nSolomon\n37  49.828\n37\n49\n828\n38.04667\n    Riegel\nCatherine\n37  49.828\n37\n49\n828\n38.04667\n    Ritter\nJ\n37  52.853\n37\n52\n853\n38.10361\n    Ritter\nMary\n37  52.853\n37\n52\n853\n38.10361\n    Rockel\nBalzer\n40  39.556 \n40\n39\n556\n40.80444\n    Rockel\nElisabetha\n40  39.555 \n40\n39\n555\n40.80417\n    Rockel\nJohannes\n40  39.560 \n40\n39\n560\n40.80556\n    Rockel\nElizabeth\n40  39.560 \n40\n39\n560\n40.80556\n    Ross\nGeorge \n37  58.752\n37\n58\n752\n38.17556\n    Ross\nEuna\n37  58.752\n37\n58\n752\n38.17556\n    Ruckel\nMary\nNA\nNA\nNA\nNA\nNA\n    Ruckel\nMelchir\nNA\nNA\nNA\nNA\nNA\n    Russell\nJames\n37 44.681\n37\n44\n681\n37.92250\n    Russell\nAna\n37 44.682\n37\n44\n682\n37.92278\n    NA\nNA\n37 44.682\n37\n44\n682\n37.92278\n    NA\nNA\n37 44.682\n37\n44\n682\n37.92278\n    NA\nNA\n37 44.682\n37\n44\n682\n37.92278\n    Siliven\nJenniel\n37  28.189\n37\n28\n189\n37.51917\n    Sinks\nA\n36 14.451 \n36\n14\n451\n36.35861\n    Sinks\nFrancis \n37 54.081 \n37\n54\n81\n37.92250\n    Sinks\nDelphia\n37 54.081 \n37\n54\n81\n37.92250\n    Sinks\nSalem\n37 54.089 \n37\n54\n89\n37.92472\n    Sinks\nDaniel\n37  52.619\n37\n52\n619\n38.03861\n    Sinks\nMartha\n37  52.619\n37\n52\n619\n38.03861\n    Sinks\nRoy \n37  52.619\n37\n52\n619\n38.03861\n    Sinks\nElizabeth\n37  47.989\n37\n47\n989\n38.05806\n    Sinks\ninfant son\n37  47.989\n37\n47\n989\n38.05806\n    Sinks\nJohn\n37  47.986\n37\n47\n986\n38.05722\n    Sinks\nMary\n37  47.985\n37\n47\n985\n38.05694\n    Sinks\nWilliam\n37  47.984\n37\n47\n984\n38.05667\n    Sinks\nCharlotte \n37  47.984\n37\n47\n984\n38.05667\n    Sinks\nAnna\n37  47.984\n37\n47\n984\n38.05667\n    Sinks\nLeonard\n37  47.982\n37\n47\n982\n38.05611\n    Sinks\nEtta Faye\n37  48.024\n37\n48\n24\n37.80667\n    Sinks\nJohn\n37  48.024\n37\n48\n24\n37.80667\n    Sinks\nSena\n37  48.020\n37\n48\n20\n37.80556\n    Sinks\nWilliam\n37  44.702\n37\n44\n702\n37.92833\n    Sweet\nJewell \n37  44.704\n37\n44\n704\n37.92889\n    Sinks\nFrancis \n37  44.702\n37\n44\n702\n37.92833\n    Sinks\nArlie\n37  44.704\n37\n44\n704\n37.92889\n    Sinks\nViola\n37  44.704\n37\n44\n704\n37.92889\n    Sinks\nLeonard\n38  33.836\n38\n33\n836\n38.78222\n    Sinks\nMae\n38  33.837\n38\n33\n837\n38.78250\n    Sinks\nBessie\n38  33.917\n38\n33\n917\n38.80472\n    Sinks\nCaroline\n38  02.272\n38\n2\n272\n38.10889\n    Sinks\nArlie\n37  44.770\n37\n44\n770\n37.94722\n    Sinks\nEva\n37  44.770\n37\n44\n770\n37.94722\n    Solt\nConrad\n40  48.686 \n40\n48\n686\n40.99056\n    Solt\nConrad\n40  48.693 \n40\n48\n693\n40.99250\n    Solt\nMaria\n40  48.690 \n40\n48\n690\n40.99167\n    Sfafford\nTrice\n37  52.608\n37\n52\n608\n38.03556\n    Sfafford\nPhebe\n37  52.608\n37\n52\n608\n38.03556\n    Steen\nRichard\n38  33.025\n38\n33\n25\n38.55694\n    VanCleve\nMartin\n37  25.694\n37\n25\n694\n37.60944\n    VanCleve\nFlorence\n37  25.694\n37\n25\n694\n37.60944\n    VanCleve\nW\n37  33.397\n37\n33\n397\n37.66028\n    VanCleve\nNancy \n37  33.397\n37\n33\n397\n37.66028\n    VanCleve\nJ\n37  33.397\n37\n33\n397\n37.66028\n    VanCleave\nW\n38  04.924\n38\n4\n924\n38.32333\n    VanCleave\nElizabeth\n38  04.924\n38\n4\n924\n38.32333\n    Veach\nPleasant\n37  29.916\n37\n29\n916\n37.73778\n    Veach\nVictoria\n37  29.916\n37\n29\n916\n37.73778\n    Veach\nWard\n37  29.895\n37\n29\n895\n37.73194\n    Veach\nCynthia\n37  29.895\n37\n29\n895\n37.73194\n    Veach\nJames\n37  29.895\n37\n29\n895\n37.73194\n    Veach\nJames\n37  25.692\n37\n25\n692\n37.60889\n    Veach\nNannie\n37  26.692\n37\n26\n692\n37.62556\n    Veatch\nJohn\n37  26.692\n37\n26\n692\n37.62556\n    Veatch\nEleanor\n37  26.692\n37\n26\n692\n37.62556\n    Veach\nWilliam\n37  26.693\n37\n26\n693\n37.62583\n    Veach\nJames\n37  26.692\n37\n26\n692\n37.62556\n    Veach\nRachel\n37  26.692\n37\n26\n692\n37.62556\n    Veach\nPleasant\n37  29.895 \n37\n29\n895\n37.73194\n    Veach\nMary\n37  29.897 \n37\n29\n897\n37.73250\n    Veatch\nParmelia\n37  28.187 \n37\n28\n187\n37.51861\n    Veatch\nMary\n37  28.187 \n37\n28\n187\n37.51861\n    Veatch\nElnor \n37  28.187 \n37\n28\n187\n37.51861\n    Veatch\nFrelin\n37  28.186\n37\n28\n186\n37.51833\n    Veach-Nutty \nNA\n37  25.682\n37\n25\n682\n37.60611\n    Veach\nJohn\n37  25.681\n37\n25\n681\n37.60583\n    Veach\nRose\n37  25.679\n37\n25\n679\n37.60528\n    Veach\nRuth\n37  25.682\n37\n25\n682\n37.60611\n    Ware\nTurner\n37  52.856\n37\n52\n856\n38.10444\n    Ware\nMartha\n37  52.856\n37\n52\n856\n38.10444\n    Ware\nJoseph\n37  52.867\n37\n52\n867\n38.10750\n    Ware\nCaroline\n37  52.867\n37\n52\n867\n38.10750\n    Webber\nDick\n37  49.829\n37\n49\n829\n38.04694\n    Webber\nPearl\n37  49.826\n37\n49\n826\n38.04611\n    Weir\nJames\n36  15.064\n36\n15\n64\n36.26778\n    Weir\nMary\n36  15.064\n36\n15\n64\n36.26778\n    Wier\nLeticia \n37 49.208\n37\n49\n208\n37.87444\n    Whiteside\nLucinda \n37  26.743\n37\n26\n743\n37.63972\n    Whiteside\nJohn\n37  26.743\n37\n26\n743\n37.63972\n    Willis\nMatha\n36  35.889\n36\n35\n889\n36.83028\n    Wilson\nJessie\n36 26.350\n36\n26\n350\n36.53056\n    Wilson\nMary \n36 26.361\n36\n26\n361\n36.53361\n    Wilson\nJoseph\n36  29.553\n36\n29\n553\n36.63694\n    Wilson\nElisha\n36  28.812\n36\n28\n812\n36.69222\n    Wilson\nSallie\n36  28.803\n36\n28\n803\n36.68972\n    Wilson\nLutetita\nNA\nNA\nNA\nNA\nNA\n    Wilson\nThomas\n37  48.034\n37\n48\n34\n37.80944\n    Wilson\nSarah\n37  48.034\n37\n48\n34\n37.80944\n    Wilson\nElisha\n36  26.351\n36\n26\n351\n36.53083\n    Wilson\nMartha\n36  26.351\n36\n26\n351\n36.53083\n    Wilson\nCharles\n36  26.351\n36\n26\n351\n36.53083\n    Wilson\nZack\n36  26.350\n36\n26\n350\n36.53056\n    Wilson\nJuritha\n36  26.350\n36\n26\n350\n36.53056\n    Wilson\nElisha\n36  26.351\n36\n26\n351\n36.53083\n    Wilson\nDrury\nNA\nNA\nNA\nNA\nNA\n    Wilson \nMary \nNA\nNA\nNA\nNA\nNA\n    Wilson\nSandifer\nNA\nNA\nNA\nNA\nNA\n    Wilson\nNancy\nNA\nNA\nNA\nNA\nNA\n    Wise\nLuvena\n37  50.352\n37\n50\n352\n37.93111\n    Wollard\nJohn\n37 54.076 \n37\n54\n76\n37.92111\n    Woolard\nNettie\n37 54.075 \n37\n54\n75\n37.92083\n    Woolard\nMillie\n37  58.721\n37\n58\n721\n38.16694\n    Woolard\nLawrence\n37  58.721\n37\n58\n721\n38.16694\n    Woolard\nEtta \n37  58.721\n37\n58\n721\n38.16694\n    Woolard\nJohn\n37  58.723\n37\n58\n723\n38.16750\n    Woolard\nJames\n37  58.721\n37\n58\n721\n38.16694\n    Woolard\nC\n37  58.720\n37\n58\n720\n38.16667\n    Woolard\nBlanche\n37  58.720\n37\n58\n720\n38.16667\n    Woolard\nL \n37  51.394\n37\n51\n394\n37.95944\n    Woolard\nAma\n37  51.395\n37\n51\n395\n37.95972\n    Woolard\nRobert\n37  51.396\n37\n51\n396\n37.96000\n    Woolard\nJames\n37  51.391\n37\n51\n391\n37.95861\n    Woolard\nRomey\n37  51.397\n37\n51\n397\n37.96028\n    Woolard\nAnna\n37  52.853\n37\n52\n853\n38.10361\n    Woolard\nJames\n37  52.853\n37\n52\n853\n38.10361\n    Woolard\nFrancis\n37  52.853\n37\n52\n853\n38.10361\n    Woolard\nTurner\n37  52.853\n37\n52\n853\n38.10361\n    Woolard\nWilliam\n37  52.854\n37\n52\n854\n38.10389\n    Woolard\nGeorge\n37  51.742\n37\n51\n742\n38.05611\n    Woolard\nNancy\n37  51.742\n37\n51\n742\n38.05611\n  \n  \n  \n\n\n\n\nThe weird coordinates like William Dorris had (524) were turned into NAs by this process, so I don’t need to worry about fixing them. The leading zeros were removed from the seconds data. For this application, it doesn’t matter. For others it might, and you could pad then back using str_pad(). So that’s it for cleaning this variable."
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#cleaning-up-dates-strings",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#cleaning-up-dates-strings",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Cleaning up Dates (strings)",
    "text": "Cleaning up Dates (strings)\nNext, I’m going to clean up the dates. They imported as also imported as strings. I don’t think I’m going to use the dates in the map, but I might use them when I’m working with the web scraping data. Like I did with the GPS Data, I’m first going to cleanup the typos then convert to the format I want.\n\nViewing the Dates\n\nclass(tombstones$DOB)\n\n[1] \"character\"\n\ntombstones %&gt;% \n  select(Surname, First.Name,DOB, DOD) %&gt;%\n  gt()  %&gt;% \n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Surname\n      First.Name\n      DOB\n      DOD\n    \n  \n  \n    Anderson\nAbraham\n10 Mar 1776\n15 Aug 1838\n    Anderson\nElizabeth\n29 Jan 1782\n13 Oct 1869\n    Anderson\nZady\n18 Apr 1812\n12 Dec 1839\n    Anderson\nAlbert\n28 Nov 1809\n5 Nov 1882\n    Anderson\nAdesia\n17 Mar 1808\n26 Spt 1864\n    Anderson\nMay \nNA\n9 Aug 1887\n    Anderson\nE\n23 Spt 1877\n24 Oct 1899\n    Anderson\nWilliam\n26 Feb 1836\n31 Dec 1895\n    Anderson\nNancy\n2 Spt 1836\n18 Oct 1917\n    Appleton\nRichard\n1 Aug 1817\n6 Oct 1897\n    Baldwin\nJohn \n25 Sep 1845\nNA\n    Baldwin\nWilliam\nNA\nNA\n    Baggett\nMahalia\n3 June 1832\n6 June 1897\n    Beasley\nE\n13 May 1826\nNA\n    Beasley\nJosephine\n23 Dec 1858\n25 Aug 1899\n    Beasley\nFanning\nNA\n25 Aug 1899\n    Bell\nJohn \n1825\n1872\n    Bell\nMary \n1826\n1859\n    Brazelton\nWm\nNA\n10 Dec 1858\n    Brazelton\nEsther\nNA\n20 Sept 1840\n    Brown\nElizabeth \n1 July 1807\n2 Feb 1888\n    Brown\nJoel\n20 Dec 1803\nNA\n    Bundy\nHope\n1833\n1918\n    Bundy\nClem\nNA\nNA\n    Bundy\nNancy\nNA\nNA\n    Bundy\nW\nNA\nNA\n    Bundy\nCharles\nNA\nNA\n    Bundy\nThomas\n4 Feb 1881\n22 Apr 1892\n    Bundy\nOctavia\n16 Apr 1839\n10 Mar 1932\n    Bundy\nGeorge\n1864\n1949\n    Bundy\nLora\n1870\n1957\n    Burgess\nW\n1844\n17 Dec 1907\n    Burgess\nAlzada\n1854\n1931\n    Clayton\nG\n9 Apr 1846\n9 Mar 1919\n    Clayton\nEllen\n28 Mar 1852\n23 May 1917\n    Clayton\nL\nNA\n25 May 1909\n    Clayton\nMary\nNA\n18 Jan 1908\n    Chapman\nDaniel\nNA\nNA\n    Chapman\nElizabeth\nNA\nNA\n    Chapman\nCaroline\nNA\n24 Jan 1861\n    Chapman\nDaniel\n5 Jul 1863\nFeb 1842\n    Chapman\nLucretia\n23 Apr 1769\nAug 1849\n    Chapman\nSamuel\n14 May 1794\n15 Apr 1863\n    Chapman\nElizabeth\n23 May 1796\n30 Dec 1859\n    Chapman\nLaura\nNA\n14 June 1865\n    Chapman\nPolly\nNA\nNA\n    Crockett\nMandy\n3 Sep 1844\n5 Nov 1914\n    Crockett\nJohn\n4 Oct 1844\n25 July 1919\n    Davis\nEzra\n1887\n1949\n    Davis\nLizzie\n1886\n1955\n    Davis\nFred\nNA\nNA\n    Dolch\nCatherine\n11 Feb 1793\n14 Feb 1867\n    Dolch\nChristian\n24 Dec 1794\n29 Aug 1874\n    Dolch\nPeter\nNA\n16 July 1862\n    Doley\nGeorge\n25 July 1823\n26 Jan 1868\n    Doley\nKatie\n8 Jan 1833\n15 Spt 1927\n    Doley\nMary E\n23 Aug 1851\n18 July 1853\n    Doley\nHenriettie\n28 Feb 1853\n18 July 1854\n    NA\nMED\nNA\nNA\n    NA\nHD\nNA\nNA\n    NA\nGD\nNA\nNA\n    NA\nMother\nNA\nNA\n    NA\nFather\nNA\nNA\n    Doley\nGeorge\n14 Mar 1856\n26 Jan 1860\n    Doley\nJames \n21 Jan 1858\n11 July 1926\n    Doley\nMay\n1869\n1940\n    Doley\nJohn\n1864\n[1949?]\n    Doley\nMaggie \n1868\n1917\n    Doley\nWilliam \n20 Dec 1862\n24 Nov 1934\n    Doley\nDora\n17 Feb 1864\n9 Apr 1934\n    Doley\nL[eaman]\n17 Apr 1889\n19 Nov 1960\n    Doley\nG[uilford]\n5 Aug 1894\n1 June 1960\n    Doley\nD[ora]\n1897\n1983\n    Doley\nEugene\n8 Nov 1886\n12 June 1958\n    Doley\nLou \n29 Oct 1895\n26 June 1963\n    Dorris\nJ[oseph]\n12 Feb 1821\n8 Jan 1893\n    Dorris\nJoseph\n10 Feb 1779\n5 Nov 1866\n    Dorris\nSarah\n2 June 1790\n23 June 1862\n    Dorris\nW\n2 June 1790\n5 Nov 1873\n    Dorris\nA\nNA\n7 Aug 1853\n    Dorris\nJ\n15 Jan 1822\n28 Nov 1911\n    Dorris\nElizabeth\n7 Jan 1833\n27 Aug 1895\n    Dorris\nRobert \n24 Mar 1818\n22 Aug 1894\n    Dorris\nRebecca\n22 mar 1829\n24 Apr 1910\n    Dorris\nMonroe\n1848\n1928\n    Dorris\nDella\n1884\n1934\n    Dorris\nMary M\n1855\n1927\n    Dorris\nHarve \n13 Aug. 1889\n6 Apr 1966\n    Dorris\nCarrie \n12 July 1892\n7 Aug 1990\n    Dorris\nSmith \nNA\nNA\n    Dorris\nAda\nNA\nNA\n    Dorris\nWilliam\nNA\nNA\n    Dorris\nHarvey \nNA\nNA\n    Dorris\nCora\nNA\nNA\n    Dorris\nJohn\nNA\nNA\n    Dorris\nW\n1857\n1936\n    Dorris\nGustavus\n2 Aug 1847\n27 Feb 1923\n    Dorris\nSarah \nNA\n17 Spt 1880\n    Dorris\nJoseph\nNA\nNA\n    Dorris\nDella \nNA\nNA\n    Dorris\nWilliam\n28 Nov 1818\n17 Feb 1905\n    Dorris\nHarriet\n28 Nov 1818\n17 Feb 1905\n    Dorris\nWilliam\n1867\n1942\n    Dorris\nMary\n1872\n1960\n    Dorris\nJames\nNA\n24 Mar 1877\n    Dorris\nSarah \n1 June 1837\n6 Dec 1908\n    Dorris\nW[illiam]\n2 Jan 1835\n16 Feb 1898\n    Dorris\nE[lisha]\n17 Dec 1853\n7 May 1907\n    Dorris\nSarah \n28 Aug 1858\n20 Feb 1922\n    Dorris\nJames\n1861\n1921\n    Dorris\nGeorgia\n1860\n1908\n    Dorris\nWilliam\nNA\n10 Aug 1857\n    Dorris\nMalinda\nNA\n22 Mar 1850\n    Drake\nMary \n1843\n1909\n    Dreisbach\nCatherina\n21 Dec 1752\n20 Spt 1819\n    Dreisbach\nJohannes\n21 Aug 1752\n20 Spt 1825\n    Everett\nSemantha\nNA\nApr 1845\n    Farris\nElizabeth\nNA\n2 Feb 1866\n    Farris\nElizabeth\nNA\n28 Spt 1858\n    Finch\nIsaac\nNA\n26 Nov 1813\n    Follis\nFawn\n9 Feb 1907\n17 Nov. 1909\n    Follis\nRalph \n7 Apr 1901\n27 Feb 1906\n    Follis\nA\nNA\n17 Oct 1887\n    Follis\nChristian \nNA\n21 Oct 1892\n    Follis\nG\nNA\n20 Aug 1899\n    Follis\nRalph \n6 Oct 1892\n4 Nov 1892\n    Follis\nE\nNA\n15 Mar 1890\n    Follis\nWilliam\n20 Aug 1832\n25 Aug 1900\n    Follis\nMartha \n10 Aug 1841\n15 Aug 1881\n    Follis\nJeff\n10 Sep 1873\n25 July 1905\n    Ford\nFlorence\n3 Spt 1866\n1 July 1940\n    Fox\nFrances\n27 Spt 1904\n18 Jan 1997\n    Frost\nEbenezer\nNA\nNA\n    NA\nNA\nNA\nNA\n    Frost\nNA\nNA\nNA\n    Fuqua\nWilliam\nNA\nNA\n    Gregory\nLeonard\n15 Mar 1887\n13 June 1963\n    Gregory\nLucille\n1899\n1986\n    Hart\nParmelia\n14 May 1847\n17 Sep 1878\n    Hess\nAmalphus\nNA\n27 Mar 1872\n    Hess\nAdolphus\nNA\n23 June 1871\n    Hess\nSamuel\n23 Dec 1823\n5 Mar 1901\n    Hess\nAugusta\n17 Jan 1828\nNA\n    Hess\nUlysses\n21 Dec 1867\n16 Feb 1897\n    Hess\nUlysses\n867\n1897\n    Hess\nWilliam\n9 Dec 1851\n26 Apr 1900\n    Hess\nWilliam\n1851\n1900\n    Hess\nJerome\n1849\n1931\n    Hess\nFranklin\n1857\n1935\n    Hess\nSamuel\n1854\n1949\n    Hess\nBernice \n20 Sep 1894\n27 Sep 1978\n    Hess\nCatherine\n1856\n1906\n    Hess\nGeorge\n27 Feb 1864\n3 Dec 1942\n    Holt\nLucinda\n15 Aug 1832\n28 Aug 1888\n    Holt\nWilliam\n14 May 1818\n26 Aug 1903\n    Horlacher\nDaniel\n4 Aug 1735\n24 Spt 1804\n    Horlacher\nMargaretha\n4 Jan 1741\n22 Apr 1806\n    Horrall\nPolly\nNA\nNA\n    Horrall\nJames\nNA\n15 Apr 1848\n    Horrall\nWilliam\nNA\nNA\n    Hurt\nElizabeth\nNA\nNA\n    Jacobs\nJeremiah\nNA\n30 Dec 1824\n    Jacobs\nRebecca\nNA\n18 July 1813\n    Johnson\nJames\n25 Feb 1837\nNA\n    Johnson\nMary\n5 Spt, 1844\n12 Nov 1895\n    Jones\nLevi\n1828\n1892\n    Jones\nHester\n1841\n1910\n    Jones\nRidley\nNA\n2 Mar 1863\n    Jones\nJames \n1863\n1933\n    Jones\nTina\n1874\n1918\n    Jones\nEzra\n15 Spt 1867\n19 May 1942\n    Jones\nNannie\n18 July 1968\n18 July 1957\n    Jones\nSamuel\n11 Feb 1876\n22 Nov 1915\n    Jones\nMelverda\n1 Nov 1879\n2 Apr 1924\n    Jones\nJohn\nNA\nNA\n    Karnes\nWillard\n1 Dec 1911\n27 Oct 1990\n    Karnes\nRuth\n8 Jan 1917\n8 Jan 2000\n    Keith\nJames\nNA\n  6 Oct 1841\n    Keth\nNancy\nNA\n17 Nov 1827\n    Kleppinger\nAnna\n29 Spt 1748\n19 June 1817\n    Lipsey\nJoe\n1888\n1928\n    Lockwood\nEugenia\n11 July 1910\n5 Oct 1994\n    Lockwood\nLeland\n19 Feb. 1914\n24 Dec 1970\n    Loomis\nJon\nNA\nNA\n    Mensch\nAbraham\n6 Apr 1754\n16 Mar 1826\n    Merrell\nAzariah\n20 May 1777\n25 Jan 1844\n    Merrell\nAbigail\n1781\n11 June 1844\n    Meredith\nEleandra\nNA\n6 Oct 1875\n    Meredith\nMicajah\nNA\n26 Oct 1822\n    Meredith\nSamuel\n8 Sept 1753\n10 Oct 1825\n    Meredith\nElizabeth\n22 Jan 1757\n6 Apr 1824\n    Meredith\nRuth\nNA\n28 Aug 1856\n    Bell\nSarah\nNA\n1860\n    John \nBell\n7 July 1812\n4 July 1872\n    Meredith\nMary\n16 Apr 1793\n11 Dec 1873\n    Meredith\nClarence\n1899\n1979\n    Meredith\nCora\n1900\n1983\n    Meredith\nW\n1926\n1945\n    Meredith\nSusan\n28 July 1833\n2 Mar 1919\n    Meredith\nHannah\n15 Nov 1830\n21 Dec 1903\n    Meredith\nMary \nNA\n11 Nov 1882\n    Meredith\nSamuel\nNA\n5 Jan 1884\n    Meredith\nBelinda\nNA\n4 Aug 1889\n    Tipton\nSusannah\nNA\n3 May 1852\n    Meredith\nThomas\nNA\n20 Apr 1840\n    Meredith\nSarah\nNA\n21 Mar 1830\n    Mildenberger\nAnna\n29 Spt 1739\n11 Oct. 1777\n    Mildenberger\nNicolaus\n15 Oct 1781\n 19 Oct 1856\n    Miller\nMyrtie\n18 July 1896\n7 Dec 1991\n    Minnich\nElizabeth\nNA\nNA\n    Minnich\nJohn\nNA\nNA\n    Mory\nCatherina\n8 May 1758\n25 Aug 1837\n    Mory\nGotthard\n20 Mar 1752\n26 May 1843\n    Mory\nMagdelena\n17 Spt 1759\n26 Nov 1827\n    Mory\nPeter\n3 May 1757\n[grass blocks date]\n    Nagel\nAnna\n28 Jul 1761\n27 Mar 1840\n    Nagel\nAnna\n9 Feb 1725\n9 Spt 1790\n    Nagel\nCaty\nNA\n4 May 1817\n    Nagel\nDaniel\nNA\n  7 May 1866\n    Nagel\nFrederick\n26 Apr 1759\n10 Mar 1839\n    Nagel\nFriedrich\n1713\n22 Nov 1779\n    Nagel\nJohann\n15 Feb 1746\n3 June 1823\n    Nagel\nMaria\nNA\nNA\n    Nagle\nJohn\nNA\n23 Nov 1870\n    Nagle\nMary\nNA\n18 Mar 1870\n    Nagel\nHenry\n1834\n1913\n    Nagel\nMary \n1836\n1921\n    Nagel\nWill\n1858\n1916\n    Nagel\nAdeline\n1860\n1941\n    NA\nNA\nNA\nNA\n    Nutty \nJohn\n1907\n1977\n    Nutty\nBeatrice\n1909\n1989\n    Nutty \nJohn\n1907\n1977\n    Ritter\nNA\nNA\nNA\n    Ritter\nNA\nNA\nNA\n    Odom\nArchibald\n20 Feb 1838\n20 May 1915\n    Odom\nCynthia \nNA\nNA\n    Odom\nG\n1854\n1943\n    Odom\nSarah\n14 June 1854\n24 Oct 1910\n    NA\nThomas\nNA\n15 Nov 1887\n    Odum\nBritton\n1794\n1863\n    Odum\nWiley\n21 May 1879\n2 Mar 1937\n    Odum\nSallie A\n15 May 1881\n6 May 1946\n    Peters\nDaniel\nNA\n20 Aug 1808\n    Peters\nCharlotte\nNA\n20 June 1880\n    Pickard\nWilliam\n19 Feb 1839\n6 Jan 1907\n    Pickard\nHarriet \n1843\n1916\n    Pickard\nLouise\n1868\n1935\n    Pletz\nKarl\n1914\n1975\n    Russell\nCaroline\n1917\n1995\n    Pickard\nWilliam\n1839\n1907\n    Pickard\nHarriet\n1847\n1916\n    Pickard\nLouise\n1868\n1935\n    Pulliam\nFrieda\nNA\nNA\n    Pulliam\nAmos\nNA\nNA\n    Rex\nWilliam\nNA\n27 Apr 1909\n    Rex\nElmina\nNA\n25 Jan 1900\n    Rex\nMamie\nNA\n7 Aug 1892\n    Rex\nGeorge \nNA\n8 Dec 1891\n    Rex\nBertie\nNA\n5 Nov 1887\n    Rex\nLulie\n10 Oct 1877\n  2 Oct 1878\n    Rex\nLily\n21 Spt 1871\n11 Oct 1872\n    Rex\nArthur\n11 Aug 1870\n25 Aug 1870\n    Rex\nGeorge \nNA\nNA\n    Rex\nJno\nNA\nNA\n    Rex\nGuy\n28 May 1889\n14 Oct 1983\n    Rex\nHarlie\n9 Sep 1890\n10 May 1979\n    Richardson\nAnnabelle\n1916\n1993\n    Richardson\nAlfred\n1915\n1987\n    Riegel\nSolomon\nNA\n18 May 1827\n    Riegel\nCatherine\nNA\n27 Dec 1882\n    Ritter\nJ\nNA\n11 Nov 1917\n    Ritter\nMary\nNA\n10 Mat 1903\n    Rockel\nBalzer\n10 Nov 1707\n9 June 1800\n    Rockel\nElisabetha\n24 June 1719\n16 Oct 1794\n    Rockel\nJohannes\n23 Mar 1749\n4 Jan 1838\n    Rockel\nElizabeth\n7 Oct 1764\n1 Mar 1835\n    Ross\nGeorge \n1 Aug 1885\n8 June 1971\n    Ross\nEuna\n2 July 1885\n19 Mar 1938\n    Ruckel\nMary\nNA\nNA\n    Ruckel\nMelchir\n27 may 1769\n15 Apr 1832\n    Russell\nJames\nNA\nNA\n    Russell\nAna\nNA\nNA\n    NA\nNA\nNA\nNA\n    NA\nNA\nNA\nNA\n    NA\nNA\nNA\nNA\n    Siliven\nJenniel\nNA\n30 Aug 1873\n    Sinks\nA\nNA\nNA\n    Sinks\nFrancis \n20 Nov 1837\n17 Jan 1909\n    Sinks\nDelphia\n27 Dec 1838\n11 Dec 1931\n    Sinks\nSalem\nNA\n26 Oct 1869\n    Sinks\nDaniel\n1841\n1923\n    Sinks\nMartha\n1856\n1924\n    Sinks\nRoy \n1889\n1908\n    Sinks\nElizabeth\n10 Mar 1835\n10 Aug 1911\n    Sinks\ninfant son\n18 Dec. 1898\n18 Dec. 1898\n    Sinks\nJohn\nNA\n20 Nov 1893\n    Sinks\nMary\nNA\n23 Aug 1906\n    Sinks\nWilliam\nNA\n27 Spt 1953\n    Sinks\nCharlotte \nNA\n31 Oct 1910\n    Sinks\nAnna\nNA\n5 Aug 1909\n    Sinks\nLeonard\nNA\n26 Oct 1919\n    Sinks\nEtta Faye\n31 Mar 1910\n19 Mar 1989\n    Sinks\nJohn\nNA\n28 Jan 1918\n    Sinks\nSena\n18 Spt 1888\n13 Dec 1972\n    Sinks\nWilliam\n11 Spt 1914\n30 Spt 1986\n    Sweet\nJewell \n1912\n1990\n    Sinks\nFrancis \n1908\n1988\n    Sinks\nArlie\n1883\n1951\n    Sinks\nViola\n1882\n1966\n    Sinks\nLeonard\nNA\nNA\n    Sinks\nMae\nNA\nNA\n    Sinks\nBessie\n21 Nov 1889\n24 Apr 1979\n    Sinks\nCaroline\nNA\n17 Apr 1876\n    Sinks\nArlie\n27 July 1916\n4 July 2009\n    Sinks\nEva\n15 Jan 1914\n8 Sep 1002\n    Solt\nConrad\n29 Spt. 1758\n24 Dec 1825\n    Solt\nConrad\n20 Mar 1753\n25 Aug 1830\n    Solt\nMaria\n1760\n23 Dec 1839\n    Sfafford\nTrice\nNA\nNA\n    Sfafford\nPhebe\nNA\nNA\n    Steen\nRichard\nNA\nNA\n    VanCleve\nMartin\n1860\n1946\n    VanCleve\nFlorence\n1867\n1946\n    VanCleve\nW\n1 Jan 1813\n20 May 1886\n    VanCleve\nNancy \n26 Nov 1814\n22 Feb 1902\n    VanCleve\nJ\n20 Dec 1850\n31 Oct 1872\n    VanCleave\nW\n29 June 1838\n15 Jan 1914\n    VanCleave\nElizabeth\n3 Sep 1840\n25 May 1901\n    Veach\nPleasant\n19 Dec 1845\n23 Jan 1917\n    Veach\nVictoria\n4 Apr 1849\n10 Apr 1921\n    Veach\nWard\n1886\n1916\n    Veach\nCynthia\n1861\n1921\n    Veach\nJames\n1860\n1890\n    Veach\nJames\n1857\n1939\n    Veach\nNannie\n1863\n1953\n    Veatch\nJohn\n11 Nov 1776\n17 Spt 1844\n    Veatch\nEleanor\n26 Oct 1773\n22 July 1852\n    Veach\nWilliam\n1882\n1957\n    Veach\nJames\nNA\nNA\n    Veach\nRachel\nNA\nNA\n    Veach\nPleasant\n1 Oct 1837\n18 Jan 1895\n    Veach\nMary\nNA\n16 Nov 1876\n    Veatch\nParmelia\n9 Feb 1808\n31 Jan 1867\n    Veatch\nMary\n5 Feb 1842\nNA\n    Veatch\nElnor \nNA\n1834\n    Veatch\nFrelin\n21 Mar 1825\n18 Jan 1848\n    Veach-Nutty \nNA\nNA\nNA\n    Veach\nJohn\n1863\n1950\n    Veach\nRose\n1868\n1948\n    Veach\nRuth\n1900\n1901\n    Ware\nTurner\n14 Feb 1817\n5 Feb 1902\n    Ware\nMartha\nNA\n23 Mar 1881\n    Ware\nJoseph\nNA\nNA\n    Ware\nCaroline\nNA\nNA\n    Webber\nDick\n7 Aug 1881\n3 Dec 1942\n    Webber\nPearl\n17 Apr 1889\n10 Apr 1967\n    Weir\nJames\n1799\n1879\n    Weir\nMary\n1801\n1886\n    Wier\nLeticia \n29 Dec 1836\n26 Mar 1865\n    Whiteside\nLucinda \n21 Apr 1858\n31 Jan 1936\n    Whiteside\nJohn\n2 Apr 1852\n30 Sep 1913\n    Willis\nMatha\n10 Mar 1830\n13 Oct 1925\n    Wilson\nJessie\n14 Apr 1924\n16 May 1996\n    Wilson\nMary \n7 Dec 1927\n2 June 1978\n    Wilson\nJoseph\n26 Feb 1825\nJan 1862\n    Wilson\nElisha\n24 May 1800\n9 July 1873\n    Wilson\nSallie\n15 Oct 1807\n4 Apr 1866\n    Wilson\nLutetita\n1826\n1907\n    Wilson\nThomas\nNA\n1922\n    Wilson\nSarah\nNA\nNA\n    Wilson\nElisha\n1841\n1907\n    Wilson\nMartha\n1840\n1929\n    Wilson\nCharles\n23 Jan 1908\n19 Apr 1955\n    Wilson\nZack\n1848\n28 Sep 1918\n    Wilson\nJuritha\n6 Aug 1854\n28 Sep 1916\n    Wilson\nElisha\n1841\n1907\n    Wilson\nDrury\n10 Dec 1827\n5 Feb 1911\n    Wilson \nMary \n17 Jan 1837\n1 Mar 1900\n    Wilson\nSandifer\n15 Oct 1834\n8 Jan 1929\n    Wilson\nNancy\n26 Oct 1842\n1 Apr 1913\n    Wise\nLuvena\nNA\nNA\n    Wollard\nJohn\n12 Jul 1846\n9 Jan 1918\n    Woolard\nNettie\n8 Spt 1874\n28 Nov 1910\n    Woolard\nMillie\n13 Nov 1837\n19 Apr 1932\n    Woolard\nLawrence\n25 Dec 1877\n12 May 1923\n    Woolard\nEtta \n14 July 1883\n7 Dec 1945\n    Woolard\nJohn\n21 Jan 1872\n19 Oct 1936                                                                           \n    Woolard\nJames\n30 June 1908\n12 Oct 1966\n    Woolard\nC\n2 Oct 1882\n9 Dec 1953\n    Woolard\nBlanche\n1 Jan 1885\n25 July 1959\n    Woolard\nL \n15 Spt. 1811\n9 May 1878\n    Woolard\nAma\n23 May 1819\n5 Dec 1891\n    Woolard\nRobert\n1863\n1938\n    Woolard\nJames\n12 Apr 1848\n7 Spt 1888\n    Woolard\nRomey\n23 May 1922\n12 Spt. 1944\n    Woolard\nAnna\nNA\n30 Spt 1887\n    Woolard\nJames\nNA\n27 Spt 1878\n    Woolard\nFrancis\nNA\n20 Feb. 1867\n    Woolard\nTurner\nNA\n20 Oct 1861\n    Woolard\nWilliam\n22 Aug 1857\n2 Jan 1858\n    Woolard\nGeorge\n28 Mar 1867\n5 Jan 1935\n    Woolard\nNancy\n1869\n1955\n  \n  \n  \n\n\n\n\nA few things to note on first glance. The majority of the dates use day month(abbreviated) year. Some entries only have the year. There are NAs. A few abbreviations have a period (Nov. in the Follis Fawn entry). September is abbreviated as both Spt and Sep. There are some dates that are guesses ([1949?] in the Doley John record.) I will replace Spt with Sep and the period with “” using str_replace_all() as above. I’m making the corrections in the original columns (DOB, DOD) but putting the date version in new variables. That way it is easy to check that the conversions were done correctly.\nThe date type must have a day, month, and year, so the years only records will become NAs. I don’t think this matters for my application, but if it did, I’d probably make an integer column for the year data. The dates would need to be split into numeric month, day, and year field or possibly date and numeric year fields.\n\n\nCleaning Typos and Converting to Dates\nI’m using base R as.Date() for the conversion. This let’s you specify the format. A detailed explanation of specifying dates and the code can be found at the Epidemiologist R Handbook. Various functions from tidyverse package lubridate could also be used to parse the dates. If I were planning on doing more with the dates I’d probably use lubridate functions exclusively.\n\ntombstones &lt;- tombstones %&gt;%\n  mutate(DOB = str_replace_all(DOB, \"Spt\", \"Sep\")) %&gt;%\n  mutate(DOD = str_replace_all(DOD, \"Spt\", \"Sep\")) %&gt;%\n  mutate(DOB = str_replace_all(DOB, \"\\\\.\", \"\")) %&gt;%\n  mutate(DOD = str_replace_all(DOD, \"\\\\.\", \"\")) %&gt;%\n  mutate(DOB_date = as.Date(DOB, format = \"%d %b %Y\")) %&gt;%\n  mutate(DOD_date = as.Date(DOD, format = \"%d %b %Y\"))\n\nChecking that everything worked.\n\ntombstones %&gt;%\n  select(Surname, First.Name,DOB, DOD, DOB_date, DOD_date) %&gt;%\n  gt()  %&gt;% \n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Surname\n      First.Name\n      DOB\n      DOD\n      DOB_date\n      DOD_date\n    \n  \n  \n    Anderson\nAbraham\n10 Mar 1776\n15 Aug 1838\n1776-03-10\n1838-08-15\n    Anderson\nElizabeth\n29 Jan 1782\n13 Oct 1869\n1782-01-29\n1869-10-13\n    Anderson\nZady\n18 Apr 1812\n12 Dec 1839\n1812-04-18\n1839-12-12\n    Anderson\nAlbert\n28 Nov 1809\n5 Nov 1882\n1809-11-28\n1882-11-05\n    Anderson\nAdesia\n17 Mar 1808\n26 Sep 1864\n1808-03-17\n1864-09-26\n    Anderson\nMay \nNA\n9 Aug 1887\nNA\n1887-08-09\n    Anderson\nE\n23 Sep 1877\n24 Oct 1899\n1877-09-23\n1899-10-24\n    Anderson\nWilliam\n26 Feb 1836\n31 Dec 1895\n1836-02-26\n1895-12-31\n    Anderson\nNancy\n2 Sep 1836\n18 Oct 1917\n1836-09-02\n1917-10-18\n    Appleton\nRichard\n1 Aug 1817\n6 Oct 1897\n1817-08-01\n1897-10-06\n    Baldwin\nJohn \n25 Sep 1845\nNA\n1845-09-25\nNA\n    Baldwin\nWilliam\nNA\nNA\nNA\nNA\n    Baggett\nMahalia\n3 June 1832\n6 June 1897\n1832-06-03\n1897-06-06\n    Beasley\nE\n13 May 1826\nNA\n1826-05-13\nNA\n    Beasley\nJosephine\n23 Dec 1858\n25 Aug 1899\n1858-12-23\n1899-08-25\n    Beasley\nFanning\nNA\n25 Aug 1899\nNA\n1899-08-25\n    Bell\nJohn \n1825\n1872\nNA\nNA\n    Bell\nMary \n1826\n1859\nNA\nNA\n    Brazelton\nWm\nNA\n10 Dec 1858\nNA\n1858-12-10\n    Brazelton\nEsther\nNA\n20 Sept 1840\nNA\nNA\n    Brown\nElizabeth \n1 July 1807\n2 Feb 1888\n1807-07-01\n1888-02-02\n    Brown\nJoel\n20 Dec 1803\nNA\n1803-12-20\nNA\n    Bundy\nHope\n1833\n1918\nNA\nNA\n    Bundy\nClem\nNA\nNA\nNA\nNA\n    Bundy\nNancy\nNA\nNA\nNA\nNA\n    Bundy\nW\nNA\nNA\nNA\nNA\n    Bundy\nCharles\nNA\nNA\nNA\nNA\n    Bundy\nThomas\n4 Feb 1881\n22 Apr 1892\n1881-02-04\n1892-04-22\n    Bundy\nOctavia\n16 Apr 1839\n10 Mar 1932\n1839-04-16\n1932-03-10\n    Bundy\nGeorge\n1864\n1949\nNA\nNA\n    Bundy\nLora\n1870\n1957\nNA\nNA\n    Burgess\nW\n1844\n17 Dec 1907\nNA\n1907-12-17\n    Burgess\nAlzada\n1854\n1931\nNA\nNA\n    Clayton\nG\n9 Apr 1846\n9 Mar 1919\n1846-04-09\n1919-03-09\n    Clayton\nEllen\n28 Mar 1852\n23 May 1917\n1852-03-28\n1917-05-23\n    Clayton\nL\nNA\n25 May 1909\nNA\n1909-05-25\n    Clayton\nMary\nNA\n18 Jan 1908\nNA\n1908-01-18\n    Chapman\nDaniel\nNA\nNA\nNA\nNA\n    Chapman\nElizabeth\nNA\nNA\nNA\nNA\n    Chapman\nCaroline\nNA\n24 Jan 1861\nNA\n1861-01-24\n    Chapman\nDaniel\n5 Jul 1863\nFeb 1842\n1863-07-05\nNA\n    Chapman\nLucretia\n23 Apr 1769\nAug 1849\n1769-04-23\nNA\n    Chapman\nSamuel\n14 May 1794\n15 Apr 1863\n1794-05-14\n1863-04-15\n    Chapman\nElizabeth\n23 May 1796\n30 Dec 1859\n1796-05-23\n1859-12-30\n    Chapman\nLaura\nNA\n14 June 1865\nNA\n1865-06-14\n    Chapman\nPolly\nNA\nNA\nNA\nNA\n    Crockett\nMandy\n3 Sep 1844\n5 Nov 1914\n1844-09-03\n1914-11-05\n    Crockett\nJohn\n4 Oct 1844\n25 July 1919\n1844-10-04\n1919-07-25\n    Davis\nEzra\n1887\n1949\nNA\nNA\n    Davis\nLizzie\n1886\n1955\nNA\nNA\n    Davis\nFred\nNA\nNA\nNA\nNA\n    Dolch\nCatherine\n11 Feb 1793\n14 Feb 1867\n1793-02-11\n1867-02-14\n    Dolch\nChristian\n24 Dec 1794\n29 Aug 1874\n1794-12-24\n1874-08-29\n    Dolch\nPeter\nNA\n16 July 1862\nNA\n1862-07-16\n    Doley\nGeorge\n25 July 1823\n26 Jan 1868\n1823-07-25\n1868-01-26\n    Doley\nKatie\n8 Jan 1833\n15 Sep 1927\n1833-01-08\n1927-09-15\n    Doley\nMary E\n23 Aug 1851\n18 July 1853\n1851-08-23\n1853-07-18\n    Doley\nHenriettie\n28 Feb 1853\n18 July 1854\n1853-02-28\n1854-07-18\n    NA\nMED\nNA\nNA\nNA\nNA\n    NA\nHD\nNA\nNA\nNA\nNA\n    NA\nGD\nNA\nNA\nNA\nNA\n    NA\nMother\nNA\nNA\nNA\nNA\n    NA\nFather\nNA\nNA\nNA\nNA\n    Doley\nGeorge\n14 Mar 1856\n26 Jan 1860\n1856-03-14\n1860-01-26\n    Doley\nJames \n21 Jan 1858\n11 July 1926\n1858-01-21\n1926-07-11\n    Doley\nMay\n1869\n1940\nNA\nNA\n    Doley\nJohn\n1864\n[1949?]\nNA\nNA\n    Doley\nMaggie \n1868\n1917\nNA\nNA\n    Doley\nWilliam \n20 Dec 1862\n24 Nov 1934\n1862-12-20\n1934-11-24\n    Doley\nDora\n17 Feb 1864\n9 Apr 1934\n1864-02-17\n1934-04-09\n    Doley\nL[eaman]\n17 Apr 1889\n19 Nov 1960\n1889-04-17\n1960-11-19\n    Doley\nG[uilford]\n5 Aug 1894\n1 June 1960\n1894-08-05\n1960-06-01\n    Doley\nD[ora]\n1897\n1983\nNA\nNA\n    Doley\nEugene\n8 Nov 1886\n12 June 1958\n1886-11-08\n1958-06-12\n    Doley\nLou \n29 Oct 1895\n26 June 1963\n1895-10-29\n1963-06-26\n    Dorris\nJ[oseph]\n12 Feb 1821\n8 Jan 1893\n1821-02-12\n1893-01-08\n    Dorris\nJoseph\n10 Feb 1779\n5 Nov 1866\n1779-02-10\n1866-11-05\n    Dorris\nSarah\n2 June 1790\n23 June 1862\n1790-06-02\n1862-06-23\n    Dorris\nW\n2 June 1790\n5 Nov 1873\n1790-06-02\n1873-11-05\n    Dorris\nA\nNA\n7 Aug 1853\nNA\n1853-08-07\n    Dorris\nJ\n15 Jan 1822\n28 Nov 1911\n1822-01-15\n1911-11-28\n    Dorris\nElizabeth\n7 Jan 1833\n27 Aug 1895\n1833-01-07\n1895-08-27\n    Dorris\nRobert \n24 Mar 1818\n22 Aug 1894\n1818-03-24\n1894-08-22\n    Dorris\nRebecca\n22 mar 1829\n24 Apr 1910\n1829-03-22\n1910-04-24\n    Dorris\nMonroe\n1848\n1928\nNA\nNA\n    Dorris\nDella\n1884\n1934\nNA\nNA\n    Dorris\nMary M\n1855\n1927\nNA\nNA\n    Dorris\nHarve \n13 Aug 1889\n6 Apr 1966\n1889-08-13\n1966-04-06\n    Dorris\nCarrie \n12 July 1892\n7 Aug 1990\n1892-07-12\n1990-08-07\n    Dorris\nSmith \nNA\nNA\nNA\nNA\n    Dorris\nAda\nNA\nNA\nNA\nNA\n    Dorris\nWilliam\nNA\nNA\nNA\nNA\n    Dorris\nHarvey \nNA\nNA\nNA\nNA\n    Dorris\nCora\nNA\nNA\nNA\nNA\n    Dorris\nJohn\nNA\nNA\nNA\nNA\n    Dorris\nW\n1857\n1936\nNA\nNA\n    Dorris\nGustavus\n2 Aug 1847\n27 Feb 1923\n1847-08-02\n1923-02-27\n    Dorris\nSarah \nNA\n17 Sep 1880\nNA\n1880-09-17\n    Dorris\nJoseph\nNA\nNA\nNA\nNA\n    Dorris\nDella \nNA\nNA\nNA\nNA\n    Dorris\nWilliam\n28 Nov 1818\n17 Feb 1905\n1818-11-28\n1905-02-17\n    Dorris\nHarriet\n28 Nov 1818\n17 Feb 1905\n1818-11-28\n1905-02-17\n    Dorris\nWilliam\n1867\n1942\nNA\nNA\n    Dorris\nMary\n1872\n1960\nNA\nNA\n    Dorris\nJames\nNA\n24 Mar 1877\nNA\n1877-03-24\n    Dorris\nSarah \n1 June 1837\n6 Dec 1908\n1837-06-01\n1908-12-06\n    Dorris\nW[illiam]\n2 Jan 1835\n16 Feb 1898\n1835-01-02\n1898-02-16\n    Dorris\nE[lisha]\n17 Dec 1853\n7 May 1907\n1853-12-17\n1907-05-07\n    Dorris\nSarah \n28 Aug 1858\n20 Feb 1922\n1858-08-28\n1922-02-20\n    Dorris\nJames\n1861\n1921\nNA\nNA\n    Dorris\nGeorgia\n1860\n1908\nNA\nNA\n    Dorris\nWilliam\nNA\n10 Aug 1857\nNA\n1857-08-10\n    Dorris\nMalinda\nNA\n22 Mar 1850\nNA\n1850-03-22\n    Drake\nMary \n1843\n1909\nNA\nNA\n    Dreisbach\nCatherina\n21 Dec 1752\n20 Sep 1819\n1752-12-21\n1819-09-20\n    Dreisbach\nJohannes\n21 Aug 1752\n20 Sep 1825\n1752-08-21\n1825-09-20\n    Everett\nSemantha\nNA\nApr 1845\nNA\nNA\n    Farris\nElizabeth\nNA\n2 Feb 1866\nNA\n1866-02-02\n    Farris\nElizabeth\nNA\n28 Sep 1858\nNA\n1858-09-28\n    Finch\nIsaac\nNA\n26 Nov 1813\nNA\n1813-11-26\n    Follis\nFawn\n9 Feb 1907\n17 Nov 1909\n1907-02-09\n1909-11-17\n    Follis\nRalph \n7 Apr 1901\n27 Feb 1906\n1901-04-07\n1906-02-27\n    Follis\nA\nNA\n17 Oct 1887\nNA\n1887-10-17\n    Follis\nChristian \nNA\n21 Oct 1892\nNA\n1892-10-21\n    Follis\nG\nNA\n20 Aug 1899\nNA\n1899-08-20\n    Follis\nRalph \n6 Oct 1892\n4 Nov 1892\n1892-10-06\n1892-11-04\n    Follis\nE\nNA\n15 Mar 1890\nNA\n1890-03-15\n    Follis\nWilliam\n20 Aug 1832\n25 Aug 1900\n1832-08-20\n1900-08-25\n    Follis\nMartha \n10 Aug 1841\n15 Aug 1881\n1841-08-10\n1881-08-15\n    Follis\nJeff\n10 Sep 1873\n25 July 1905\n1873-09-10\n1905-07-25\n    Ford\nFlorence\n3 Sep 1866\n1 July 1940\n1866-09-03\n1940-07-01\n    Fox\nFrances\n27 Sep 1904\n18 Jan 1997\n1904-09-27\n1997-01-18\n    Frost\nEbenezer\nNA\nNA\nNA\nNA\n    NA\nNA\nNA\nNA\nNA\nNA\n    Frost\nNA\nNA\nNA\nNA\nNA\n    Fuqua\nWilliam\nNA\nNA\nNA\nNA\n    Gregory\nLeonard\n15 Mar 1887\n13 June 1963\n1887-03-15\n1963-06-13\n    Gregory\nLucille\n1899\n1986\nNA\nNA\n    Hart\nParmelia\n14 May 1847\n17 Sep 1878\n1847-05-14\n1878-09-17\n    Hess\nAmalphus\nNA\n27 Mar 1872\nNA\n1872-03-27\n    Hess\nAdolphus\nNA\n23 June 1871\nNA\n1871-06-23\n    Hess\nSamuel\n23 Dec 1823\n5 Mar 1901\n1823-12-23\n1901-03-05\n    Hess\nAugusta\n17 Jan 1828\nNA\n1828-01-17\nNA\n    Hess\nUlysses\n21 Dec 1867\n16 Feb 1897\n1867-12-21\n1897-02-16\n    Hess\nUlysses\n867\n1897\nNA\nNA\n    Hess\nWilliam\n9 Dec 1851\n26 Apr 1900\n1851-12-09\n1900-04-26\n    Hess\nWilliam\n1851\n1900\nNA\nNA\n    Hess\nJerome\n1849\n1931\nNA\nNA\n    Hess\nFranklin\n1857\n1935\nNA\nNA\n    Hess\nSamuel\n1854\n1949\nNA\nNA\n    Hess\nBernice \n20 Sep 1894\n27 Sep 1978\n1894-09-20\n1978-09-27\n    Hess\nCatherine\n1856\n1906\nNA\nNA\n    Hess\nGeorge\n27 Feb 1864\n3 Dec 1942\n1864-02-27\n1942-12-03\n    Holt\nLucinda\n15 Aug 1832\n28 Aug 1888\n1832-08-15\n1888-08-28\n    Holt\nWilliam\n14 May 1818\n26 Aug 1903\n1818-05-14\n1903-08-26\n    Horlacher\nDaniel\n4 Aug 1735\n24 Sep 1804\n1735-08-04\n1804-09-24\n    Horlacher\nMargaretha\n4 Jan 1741\n22 Apr 1806\n1741-01-04\n1806-04-22\n    Horrall\nPolly\nNA\nNA\nNA\nNA\n    Horrall\nJames\nNA\n15 Apr 1848\nNA\n1848-04-15\n    Horrall\nWilliam\nNA\nNA\nNA\nNA\n    Hurt\nElizabeth\nNA\nNA\nNA\nNA\n    Jacobs\nJeremiah\nNA\n30 Dec 1824\nNA\n1824-12-30\n    Jacobs\nRebecca\nNA\n18 July 1813\nNA\n1813-07-18\n    Johnson\nJames\n25 Feb 1837\nNA\n1837-02-25\nNA\n    Johnson\nMary\n5 Sep, 1844\n12 Nov 1895\nNA\n1895-11-12\n    Jones\nLevi\n1828\n1892\nNA\nNA\n    Jones\nHester\n1841\n1910\nNA\nNA\n    Jones\nRidley\nNA\n2 Mar 1863\nNA\n1863-03-02\n    Jones\nJames \n1863\n1933\nNA\nNA\n    Jones\nTina\n1874\n1918\nNA\nNA\n    Jones\nEzra\n15 Sep 1867\n19 May 1942\n1867-09-15\n1942-05-19\n    Jones\nNannie\n18 July 1968\n18 July 1957\n1968-07-18\n1957-07-18\n    Jones\nSamuel\n11 Feb 1876\n22 Nov 1915\n1876-02-11\n1915-11-22\n    Jones\nMelverda\n1 Nov 1879\n2 Apr 1924\n1879-11-01\n1924-04-02\n    Jones\nJohn\nNA\nNA\nNA\nNA\n    Karnes\nWillard\n1 Dec 1911\n27 Oct 1990\n1911-12-01\n1990-10-27\n    Karnes\nRuth\n8 Jan 1917\n8 Jan 2000\n1917-01-08\n2000-01-08\n    Keith\nJames\nNA\n  6 Oct 1841\nNA\n1841-10-06\n    Keth\nNancy\nNA\n17 Nov 1827\nNA\n1827-11-17\n    Kleppinger\nAnna\n29 Sep 1748\n19 June 1817\n1748-09-29\n1817-06-19\n    Lipsey\nJoe\n1888\n1928\nNA\nNA\n    Lockwood\nEugenia\n11 July 1910\n5 Oct 1994\n1910-07-11\n1994-10-05\n    Lockwood\nLeland\n19 Feb 1914\n24 Dec 1970\n1914-02-19\n1970-12-24\n    Loomis\nJon\nNA\nNA\nNA\nNA\n    Mensch\nAbraham\n6 Apr 1754\n16 Mar 1826\n1754-04-06\n1826-03-16\n    Merrell\nAzariah\n20 May 1777\n25 Jan 1844\n1777-05-20\n1844-01-25\n    Merrell\nAbigail\n1781\n11 June 1844\nNA\n1844-06-11\n    Meredith\nEleandra\nNA\n6 Oct 1875\nNA\n1875-10-06\n    Meredith\nMicajah\nNA\n26 Oct 1822\nNA\n1822-10-26\n    Meredith\nSamuel\n8 Sept 1753\n10 Oct 1825\nNA\n1825-10-10\n    Meredith\nElizabeth\n22 Jan 1757\n6 Apr 1824\n1757-01-22\n1824-04-06\n    Meredith\nRuth\nNA\n28 Aug 1856\nNA\n1856-08-28\n    Bell\nSarah\nNA\n1860\nNA\nNA\n    John \nBell\n7 July 1812\n4 July 1872\n1812-07-07\n1872-07-04\n    Meredith\nMary\n16 Apr 1793\n11 Dec 1873\n1793-04-16\n1873-12-11\n    Meredith\nClarence\n1899\n1979\nNA\nNA\n    Meredith\nCora\n1900\n1983\nNA\nNA\n    Meredith\nW\n1926\n1945\nNA\nNA\n    Meredith\nSusan\n28 July 1833\n2 Mar 1919\n1833-07-28\n1919-03-02\n    Meredith\nHannah\n15 Nov 1830\n21 Dec 1903\n1830-11-15\n1903-12-21\n    Meredith\nMary \nNA\n11 Nov 1882\nNA\n1882-11-11\n    Meredith\nSamuel\nNA\n5 Jan 1884\nNA\n1884-01-05\n    Meredith\nBelinda\nNA\n4 Aug 1889\nNA\n1889-08-04\n    Tipton\nSusannah\nNA\n3 May 1852\nNA\n1852-05-03\n    Meredith\nThomas\nNA\n20 Apr 1840\nNA\n1840-04-20\n    Meredith\nSarah\nNA\n21 Mar 1830\nNA\n1830-03-21\n    Mildenberger\nAnna\n29 Sep 1739\n11 Oct 1777\n1739-09-29\n1777-10-11\n    Mildenberger\nNicolaus\n15 Oct 1781\n 19 Oct 1856\n1781-10-15\n1856-10-19\n    Miller\nMyrtie\n18 July 1896\n7 Dec 1991\n1896-07-18\n1991-12-07\n    Minnich\nElizabeth\nNA\nNA\nNA\nNA\n    Minnich\nJohn\nNA\nNA\nNA\nNA\n    Mory\nCatherina\n8 May 1758\n25 Aug 1837\n1758-05-08\n1837-08-25\n    Mory\nGotthard\n20 Mar 1752\n26 May 1843\n1752-03-20\n1843-05-26\n    Mory\nMagdelena\n17 Sep 1759\n26 Nov 1827\n1759-09-17\n1827-11-26\n    Mory\nPeter\n3 May 1757\n[grass blocks date]\n1757-05-03\nNA\n    Nagel\nAnna\n28 Jul 1761\n27 Mar 1840\n1761-07-28\n1840-03-27\n    Nagel\nAnna\n9 Feb 1725\n9 Sep 1790\n1725-02-09\n1790-09-09\n    Nagel\nCaty\nNA\n4 May 1817\nNA\n1817-05-04\n    Nagel\nDaniel\nNA\n  7 May 1866\nNA\n1866-05-07\n    Nagel\nFrederick\n26 Apr 1759\n10 Mar 1839\n1759-04-26\n1839-03-10\n    Nagel\nFriedrich\n1713\n22 Nov 1779\nNA\n1779-11-22\n    Nagel\nJohann\n15 Feb 1746\n3 June 1823\n1746-02-15\n1823-06-03\n    Nagel\nMaria\nNA\nNA\nNA\nNA\n    Nagle\nJohn\nNA\n23 Nov 1870\nNA\n1870-11-23\n    Nagle\nMary\nNA\n18 Mar 1870\nNA\n1870-03-18\n    Nagel\nHenry\n1834\n1913\nNA\nNA\n    Nagel\nMary \n1836\n1921\nNA\nNA\n    Nagel\nWill\n1858\n1916\nNA\nNA\n    Nagel\nAdeline\n1860\n1941\nNA\nNA\n    NA\nNA\nNA\nNA\nNA\nNA\n    Nutty \nJohn\n1907\n1977\nNA\nNA\n    Nutty\nBeatrice\n1909\n1989\nNA\nNA\n    Nutty \nJohn\n1907\n1977\nNA\nNA\n    Ritter\nNA\nNA\nNA\nNA\nNA\n    Ritter\nNA\nNA\nNA\nNA\nNA\n    Odom\nArchibald\n20 Feb 1838\n20 May 1915\n1838-02-20\n1915-05-20\n    Odom\nCynthia \nNA\nNA\nNA\nNA\n    Odom\nG\n1854\n1943\nNA\nNA\n    Odom\nSarah\n14 June 1854\n24 Oct 1910\n1854-06-14\n1910-10-24\n    NA\nThomas\nNA\n15 Nov 1887\nNA\n1887-11-15\n    Odum\nBritton\n1794\n1863\nNA\nNA\n    Odum\nWiley\n21 May 1879\n2 Mar 1937\n1879-05-21\n1937-03-02\n    Odum\nSallie A\n15 May 1881\n6 May 1946\n1881-05-15\n1946-05-06\n    Peters\nDaniel\nNA\n20 Aug 1808\nNA\n1808-08-20\n    Peters\nCharlotte\nNA\n20 June 1880\nNA\n1880-06-20\n    Pickard\nWilliam\n19 Feb 1839\n6 Jan 1907\n1839-02-19\n1907-01-06\n    Pickard\nHarriet \n1843\n1916\nNA\nNA\n    Pickard\nLouise\n1868\n1935\nNA\nNA\n    Pletz\nKarl\n1914\n1975\nNA\nNA\n    Russell\nCaroline\n1917\n1995\nNA\nNA\n    Pickard\nWilliam\n1839\n1907\nNA\nNA\n    Pickard\nHarriet\n1847\n1916\nNA\nNA\n    Pickard\nLouise\n1868\n1935\nNA\nNA\n    Pulliam\nFrieda\nNA\nNA\nNA\nNA\n    Pulliam\nAmos\nNA\nNA\nNA\nNA\n    Rex\nWilliam\nNA\n27 Apr 1909\nNA\n1909-04-27\n    Rex\nElmina\nNA\n25 Jan 1900\nNA\n1900-01-25\n    Rex\nMamie\nNA\n7 Aug 1892\nNA\n1892-08-07\n    Rex\nGeorge \nNA\n8 Dec 1891\nNA\n1891-12-08\n    Rex\nBertie\nNA\n5 Nov 1887\nNA\n1887-11-05\n    Rex\nLulie\n10 Oct 1877\n  2 Oct 1878\n1877-10-10\n1878-10-02\n    Rex\nLily\n21 Sep 1871\n11 Oct 1872\n1871-09-21\n1872-10-11\n    Rex\nArthur\n11 Aug 1870\n25 Aug 1870\n1870-08-11\n1870-08-25\n    Rex\nGeorge \nNA\nNA\nNA\nNA\n    Rex\nJno\nNA\nNA\nNA\nNA\n    Rex\nGuy\n28 May 1889\n14 Oct 1983\n1889-05-28\n1983-10-14\n    Rex\nHarlie\n9 Sep 1890\n10 May 1979\n1890-09-09\n1979-05-10\n    Richardson\nAnnabelle\n1916\n1993\nNA\nNA\n    Richardson\nAlfred\n1915\n1987\nNA\nNA\n    Riegel\nSolomon\nNA\n18 May 1827\nNA\n1827-05-18\n    Riegel\nCatherine\nNA\n27 Dec 1882\nNA\n1882-12-27\n    Ritter\nJ\nNA\n11 Nov 1917\nNA\n1917-11-11\n    Ritter\nMary\nNA\n10 Mat 1903\nNA\nNA\n    Rockel\nBalzer\n10 Nov 1707\n9 June 1800\n1707-11-10\n1800-06-09\n    Rockel\nElisabetha\n24 June 1719\n16 Oct 1794\n1719-06-24\n1794-10-16\n    Rockel\nJohannes\n23 Mar 1749\n4 Jan 1838\n1749-03-23\n1838-01-04\n    Rockel\nElizabeth\n7 Oct 1764\n1 Mar 1835\n1764-10-07\n1835-03-01\n    Ross\nGeorge \n1 Aug 1885\n8 June 1971\n1885-08-01\n1971-06-08\n    Ross\nEuna\n2 July 1885\n19 Mar 1938\n1885-07-02\n1938-03-19\n    Ruckel\nMary\nNA\nNA\nNA\nNA\n    Ruckel\nMelchir\n27 may 1769\n15 Apr 1832\n1769-05-27\n1832-04-15\n    Russell\nJames\nNA\nNA\nNA\nNA\n    Russell\nAna\nNA\nNA\nNA\nNA\n    NA\nNA\nNA\nNA\nNA\nNA\n    NA\nNA\nNA\nNA\nNA\nNA\n    NA\nNA\nNA\nNA\nNA\nNA\n    Siliven\nJenniel\nNA\n30 Aug 1873\nNA\n1873-08-30\n    Sinks\nA\nNA\nNA\nNA\nNA\n    Sinks\nFrancis \n20 Nov 1837\n17 Jan 1909\n1837-11-20\n1909-01-17\n    Sinks\nDelphia\n27 Dec 1838\n11 Dec 1931\n1838-12-27\n1931-12-11\n    Sinks\nSalem\nNA\n26 Oct 1869\nNA\n1869-10-26\n    Sinks\nDaniel\n1841\n1923\nNA\nNA\n    Sinks\nMartha\n1856\n1924\nNA\nNA\n    Sinks\nRoy \n1889\n1908\nNA\nNA\n    Sinks\nElizabeth\n10 Mar 1835\n10 Aug 1911\n1835-03-10\n1911-08-10\n    Sinks\ninfant son\n18 Dec 1898\n18 Dec 1898\n1898-12-18\n1898-12-18\n    Sinks\nJohn\nNA\n20 Nov 1893\nNA\n1893-11-20\n    Sinks\nMary\nNA\n23 Aug 1906\nNA\n1906-08-23\n    Sinks\nWilliam\nNA\n27 Sep 1953\nNA\n1953-09-27\n    Sinks\nCharlotte \nNA\n31 Oct 1910\nNA\n1910-10-31\n    Sinks\nAnna\nNA\n5 Aug 1909\nNA\n1909-08-05\n    Sinks\nLeonard\nNA\n26 Oct 1919\nNA\n1919-10-26\n    Sinks\nEtta Faye\n31 Mar 1910\n19 Mar 1989\n1910-03-31\n1989-03-19\n    Sinks\nJohn\nNA\n28 Jan 1918\nNA\n1918-01-28\n    Sinks\nSena\n18 Sep 1888\n13 Dec 1972\n1888-09-18\n1972-12-13\n    Sinks\nWilliam\n11 Sep 1914\n30 Sep 1986\n1914-09-11\n1986-09-30\n    Sweet\nJewell \n1912\n1990\nNA\nNA\n    Sinks\nFrancis \n1908\n1988\nNA\nNA\n    Sinks\nArlie\n1883\n1951\nNA\nNA\n    Sinks\nViola\n1882\n1966\nNA\nNA\n    Sinks\nLeonard\nNA\nNA\nNA\nNA\n    Sinks\nMae\nNA\nNA\nNA\nNA\n    Sinks\nBessie\n21 Nov 1889\n24 Apr 1979\n1889-11-21\n1979-04-24\n    Sinks\nCaroline\nNA\n17 Apr 1876\nNA\n1876-04-17\n    Sinks\nArlie\n27 July 1916\n4 July 2009\n1916-07-27\n2009-07-04\n    Sinks\nEva\n15 Jan 1914\n8 Sep 1002\n1914-01-15\n1002-09-08\n    Solt\nConrad\n29 Sep 1758\n24 Dec 1825\n1758-09-29\n1825-12-24\n    Solt\nConrad\n20 Mar 1753\n25 Aug 1830\n1753-03-20\n1830-08-25\n    Solt\nMaria\n1760\n23 Dec 1839\nNA\n1839-12-23\n    Sfafford\nTrice\nNA\nNA\nNA\nNA\n    Sfafford\nPhebe\nNA\nNA\nNA\nNA\n    Steen\nRichard\nNA\nNA\nNA\nNA\n    VanCleve\nMartin\n1860\n1946\nNA\nNA\n    VanCleve\nFlorence\n1867\n1946\nNA\nNA\n    VanCleve\nW\n1 Jan 1813\n20 May 1886\n1813-01-01\n1886-05-20\n    VanCleve\nNancy \n26 Nov 1814\n22 Feb 1902\n1814-11-26\n1902-02-22\n    VanCleve\nJ\n20 Dec 1850\n31 Oct 1872\n1850-12-20\n1872-10-31\n    VanCleave\nW\n29 June 1838\n15 Jan 1914\n1838-06-29\n1914-01-15\n    VanCleave\nElizabeth\n3 Sep 1840\n25 May 1901\n1840-09-03\n1901-05-25\n    Veach\nPleasant\n19 Dec 1845\n23 Jan 1917\n1845-12-19\n1917-01-23\n    Veach\nVictoria\n4 Apr 1849\n10 Apr 1921\n1849-04-04\n1921-04-10\n    Veach\nWard\n1886\n1916\nNA\nNA\n    Veach\nCynthia\n1861\n1921\nNA\nNA\n    Veach\nJames\n1860\n1890\nNA\nNA\n    Veach\nJames\n1857\n1939\nNA\nNA\n    Veach\nNannie\n1863\n1953\nNA\nNA\n    Veatch\nJohn\n11 Nov 1776\n17 Sep 1844\n1776-11-11\n1844-09-17\n    Veatch\nEleanor\n26 Oct 1773\n22 July 1852\n1773-10-26\n1852-07-22\n    Veach\nWilliam\n1882\n1957\nNA\nNA\n    Veach\nJames\nNA\nNA\nNA\nNA\n    Veach\nRachel\nNA\nNA\nNA\nNA\n    Veach\nPleasant\n1 Oct 1837\n18 Jan 1895\n1837-10-01\n1895-01-18\n    Veach\nMary\nNA\n16 Nov 1876\nNA\n1876-11-16\n    Veatch\nParmelia\n9 Feb 1808\n31 Jan 1867\n1808-02-09\n1867-01-31\n    Veatch\nMary\n5 Feb 1842\nNA\n1842-02-05\nNA\n    Veatch\nElnor \nNA\n1834\nNA\nNA\n    Veatch\nFrelin\n21 Mar 1825\n18 Jan 1848\n1825-03-21\n1848-01-18\n    Veach-Nutty \nNA\nNA\nNA\nNA\nNA\n    Veach\nJohn\n1863\n1950\nNA\nNA\n    Veach\nRose\n1868\n1948\nNA\nNA\n    Veach\nRuth\n1900\n1901\nNA\nNA\n    Ware\nTurner\n14 Feb 1817\n5 Feb 1902\n1817-02-14\n1902-02-05\n    Ware\nMartha\nNA\n23 Mar 1881\nNA\n1881-03-23\n    Ware\nJoseph\nNA\nNA\nNA\nNA\n    Ware\nCaroline\nNA\nNA\nNA\nNA\n    Webber\nDick\n7 Aug 1881\n3 Dec 1942\n1881-08-07\n1942-12-03\n    Webber\nPearl\n17 Apr 1889\n10 Apr 1967\n1889-04-17\n1967-04-10\n    Weir\nJames\n1799\n1879\nNA\nNA\n    Weir\nMary\n1801\n1886\nNA\nNA\n    Wier\nLeticia \n29 Dec 1836\n26 Mar 1865\n1836-12-29\n1865-03-26\n    Whiteside\nLucinda \n21 Apr 1858\n31 Jan 1936\n1858-04-21\n1936-01-31\n    Whiteside\nJohn\n2 Apr 1852\n30 Sep 1913\n1852-04-02\n1913-09-30\n    Willis\nMatha\n10 Mar 1830\n13 Oct 1925\n1830-03-10\n1925-10-13\n    Wilson\nJessie\n14 Apr 1924\n16 May 1996\n1924-04-14\n1996-05-16\n    Wilson\nMary \n7 Dec 1927\n2 June 1978\n1927-12-07\n1978-06-02\n    Wilson\nJoseph\n26 Feb 1825\nJan 1862\n1825-02-26\nNA\n    Wilson\nElisha\n24 May 1800\n9 July 1873\n1800-05-24\n1873-07-09\n    Wilson\nSallie\n15 Oct 1807\n4 Apr 1866\n1807-10-15\n1866-04-04\n    Wilson\nLutetita\n1826\n1907\nNA\nNA\n    Wilson\nThomas\nNA\n1922\nNA\nNA\n    Wilson\nSarah\nNA\nNA\nNA\nNA\n    Wilson\nElisha\n1841\n1907\nNA\nNA\n    Wilson\nMartha\n1840\n1929\nNA\nNA\n    Wilson\nCharles\n23 Jan 1908\n19 Apr 1955\n1908-01-23\n1955-04-19\n    Wilson\nZack\n1848\n28 Sep 1918\nNA\n1918-09-28\n    Wilson\nJuritha\n6 Aug 1854\n28 Sep 1916\n1854-08-06\n1916-09-28\n    Wilson\nElisha\n1841\n1907\nNA\nNA\n    Wilson\nDrury\n10 Dec 1827\n5 Feb 1911\n1827-12-10\n1911-02-05\n    Wilson \nMary \n17 Jan 1837\n1 Mar 1900\n1837-01-17\n1900-03-01\n    Wilson\nSandifer\n15 Oct 1834\n8 Jan 1929\n1834-10-15\n1929-01-08\n    Wilson\nNancy\n26 Oct 1842\n1 Apr 1913\n1842-10-26\n1913-04-01\n    Wise\nLuvena\nNA\nNA\nNA\nNA\n    Wollard\nJohn\n12 Jul 1846\n9 Jan 1918\n1846-07-12\n1918-01-09\n    Woolard\nNettie\n8 Sep 1874\n28 Nov 1910\n1874-09-08\n1910-11-28\n    Woolard\nMillie\n13 Nov 1837\n19 Apr 1932\n1837-11-13\n1932-04-19\n    Woolard\nLawrence\n25 Dec 1877\n12 May 1923\n1877-12-25\n1923-05-12\n    Woolard\nEtta \n14 July 1883\n7 Dec 1945\n1883-07-14\n1945-12-07\n    Woolard\nJohn\n21 Jan 1872\n19 Oct 1936                                                                           \n1872-01-21\n1936-10-19\n    Woolard\nJames\n30 June 1908\n12 Oct 1966\n1908-06-30\n1966-10-12\n    Woolard\nC\n2 Oct 1882\n9 Dec 1953\n1882-10-02\n1953-12-09\n    Woolard\nBlanche\n1 Jan 1885\n25 July 1959\n1885-01-01\n1959-07-25\n    Woolard\nL \n15 Sep 1811\n9 May 1878\n1811-09-15\n1878-05-09\n    Woolard\nAma\n23 May 1819\n5 Dec 1891\n1819-05-23\n1891-12-05\n    Woolard\nRobert\n1863\n1938\nNA\nNA\n    Woolard\nJames\n12 Apr 1848\n7 Sep 1888\n1848-04-12\n1888-09-07\n    Woolard\nRomey\n23 May 1922\n12 Sep 1944\n1922-05-23\n1944-09-12\n    Woolard\nAnna\nNA\n30 Sep 1887\nNA\n1887-09-30\n    Woolard\nJames\nNA\n27 Sep 1878\nNA\n1878-09-27\n    Woolard\nFrancis\nNA\n20 Feb 1867\nNA\n1867-02-20\n    Woolard\nTurner\nNA\n20 Oct 1861\nNA\n1861-10-20\n    Woolard\nWilliam\n22 Aug 1857\n2 Jan 1858\n1857-08-22\n1858-01-02\n    Woolard\nGeorge\n28 Mar 1867\n5 Jan 1935\n1867-03-28\n1935-01-05\n    Woolard\nNancy\n1869\n1955\nNA\nNA"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#cleaning-up-cemetery-names-strings",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#cleaning-up-cemetery-names-strings",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Cleaning up Cemetery Names (strings)",
    "text": "Cleaning up Cemetery Names (strings)\nThe string/ character data is more difficult to clean, since the possibilities are endless. Almost anything could be correct. Correcting this data requires some subject matter expertise. I think I would like to group tombstones by cemetery in my map, so I do want to clean this up. However, I’m generally going to take a light touch with this.\n\nFiguring Out the Types of Typos\nHere, I expect that there is a limited set of cemeteries, so I can group the data to look for typos.\n\ncems_unique &lt;- tombstones %&gt;%\n  distinct(Cemetery) %&gt;%\n  arrange(Cemetery)\n\ncems_unique %&gt;%\n  gt() %&gt;% \n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Cemetery\n    \n  \n  \n    Baggett Cem\n    Baldwin Cem\n    Bethlehem\n    Bethlehem Baptist Church\n    Bethlehem Cem\n    Blockhouse \n    Boner Cem\n    Britton Odum Farm Commerce\n    Campground Cem\n    Casey Springs\n    Casey Springs Cem\n    Chapman-Veatch Cemeteryem\n    Christ Church\n    Cole Cem\n    Corinth Zion Meth Cem\n    Crocker Cem\n    Denning\n    Ebenezer Bapt. Ch. Cem.\n    Ebenezer Cumberland Presbyterian Church Cem\n    Eld SM Williams\n    Ewing Cem\n    Follis Cemetery \n    Fredonia\n    Friedensville Evangelican Lutheran Church\n    Gladdish-Anderson\n    Goshen Cumberland Presbyterian Church Cem\n    Grapevine Cem\n    Green Cem\n    Greenbrier Cem\n    Greenlawn\n    Hanover Green\n    Harmony Free Will Baptist Ch Cem\n    Harmony Free Will Baptist Church\n    Hartwell\n    Hidlay Cem.\n    Hudgens Cem.\n    Jersey Baptis Ch\n    Johnson Cem\n    Johnston City Cem\n    Lake Creek Cem.\n    Lebanon Cem\n    Lebanon Cumberland Presbyterian Ch\n    Lebanon Cumberland Presbyterian Church\n    Liberty Meth Cem\n    Maplewood Cem\n    Masonic\n    Masonic & Oddfellows Cem\n    Miller Cem\n    Mt Olive Cem\n    Mt Olive Meth Ch Cem\n    Mt. Olive Cem.\n    Nashville National Cem.\n    New Chapel Methodist Church\n    Oak Grove Cem\n    Oddfellows\n    Oddfellows Cem\n    Old Bethel Cem\n    Orlinda\n    Orlinda Cem\n    Pleasant Valley Bapt. Church Cem.\n    Robinson Cem\n    Rose Hill\n    Russell Cem\n    Spillertown Cem.\n    Spring Hill\n    St. John's Lutheran Church\n    St. Paul's Lutheran \"Blue\" Church\n    Trinity\n    Trinity Methodist Church\n    Union Grove Cem\n    Vicksburg National Cem\n    Vienna Fraternal Cem\n    Webber Campground\n    Weber-Campground\n    Weir Cem.\n    White Ash\n    Williams Prairie Baptist Ch Cem\n    Zion Stone Ch\n    NA\n  \n  \n  \n\n\n\n\nThere are 79 unique names. I see some obvious issues. Sometimes the word cemetery is added to the end (or an abbreviation like Cem and Cem.). Sometimes it isn’t. Church is rendered as Ch. in some cases.\n\n\nCleaning the Easy Typos and Inconsistencies\nSince all of these are cemeteries, I’m just going to remove that from the name. I will change all Ch. to Church and I will clean out all periods. Using str_replace_all() as usual.\nOrder of operation matters here, I think. I want to replace Ch and Ch. with Church but I don’t want the Church to be replaced. So I either need a regex or I need to include something else in my pattern. I think I can use “Ch” and “Ch.” as the pattern. I can’t just clear out the periods and replace them with spaces and use “Ch” because some of the periods occur in the middle of the string and I’ll end up with extra spaces I need to clear out. Similarly, Cemetary should be replaced before Cem, otherwise I’ll have to clean out etary. Meth should be Methodist. Bapt. should be Baptist. There is a Cemeterem, which is clearly a typo\n(The fact that I’m putting this in a new dataframe is a hint that I’m not as clever as I thought. Can you spot what I did wrong?)\n\ntombstones2 &lt;- tombstones %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Ch \", \"Church\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Ch.\", \"Church\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"\\\\.\", \"\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Cemeteryem\", \"\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Cemetery\", \"\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Cem\", \"\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Meth \", \"Methodist \")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Bapt \", \"Baptist \")) \n\nSee how I did…\n\ncems_unique &lt;- tombstones2 %&gt;%\n  distinct(Cemetery) %&gt;%\n  arrange(Cemetery)\n\ncems_unique %&gt;%\n  gt() %&gt;% \n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Cemetery\n    \n  \n  \n    Baggett \n    Baldwin \n    Bethlehem\n    Bethlehem \n    Bethlehem Baptist Churchrch\n    Blockhouse \n    Boner \n    Britton Odum Farm Commerce\n    Campground \n    Casey Springs\n    Casey Springs \n    Churchist Churchrch\n    Churchpman-Veatch \n    Cole \n    Corinth Zion Methodist \n    Crocker \n    Denning\n    Ebenezer Baptist Church \n    Ebenezer Cumberland Presbyterian Churchrch \n    Eld SM Williams\n    Ewing \n    Follis  \n    Fredonia\n    Friedensville Evangelican Lutheran Churchrch\n    Gladdish-Anderson\n    Goshen Cumberland Presbyterian Churchrch \n    Grapevine \n    Green \n    Greenbrier \n    Greenlawn\n    Hanover Green\n    Harmony Free Will Baptist Churchrch\n    Hartwell\n    Hidlay \n    Hudgens \n    Jersey Baptis Ch\n    Johnson \n    Johnston City \n    Lake Creek \n    Lebanon \n    Lebanon Cumberland Presbyterian Ch\n    Lebanon Cumberland Presbyterian Churchrch\n    Liberty Methodist \n    Maplewood \n    Masonic\n    Masonic & Oddfellows \n    Miller \n    Mt Olive \n    Mt Olive Methodist Churchrch\n    Nashville National \n    New Churchpel Methodist Churchrch\n    Oak Grove \n    Oddfellows\n    Oddfellows \n    Old Bethel \n    Orlinda\n    Orlinda \n    Pleasant Valley Baptist Churchrch \n    Robinson \n    Rose Hill\n    Russell \n    Spillertown \n    Spring Hill\n    St John's Lutheran Churchrch\n    St Paul's Lutheran \"Blue\" Churchrch\n    Trinity\n    Trinity Methodist Churchrch\n    Union Grove \n    Vicksburg National \n    Vienna Fraternal \n    Webber Campground\n    Weber-Campground\n    Weir \n    White Ash\n    Williams Prairie Baptist Churchrch\n    Zion Stone Ch\n    NA\n  \n  \n  \n\n\n\n\nBadly! Almost like I should read my own digression about regex and escape characters. Even the period in Ch. needs to be escaped, not just single periods or leading periods. So \"Ch\\\\.\" not \"Ch.\" as the pattern.\n\ntombstones3 &lt;- tombstones %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Ch \", \"Church\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Ch\\\\.\", \"Church\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"\\\\.\", \"\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Cemeteryem\", \"\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Cemetery\", \"\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Cem\", \"\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Meth \", \"Methodist \")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Bapt \", \"Baptist \"))\n\nDid that work?\n\ncems_unique &lt;- tombstones3 %&gt;%\n  distinct(Cemetery) %&gt;%\n  arrange(Cemetery)\n\ncems_unique %&gt;% \n  gt() %&gt;% \n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Cemetery\n    \n  \n  \n    Baggett \n    Baldwin \n    Bethlehem\n    Bethlehem \n    Bethlehem Baptist Church\n    Blockhouse \n    Boner \n    Britton Odum Farm Commerce\n    Campground \n    Casey Springs\n    Casey Springs \n    Chapman-Veatch \n    Christ Church\n    Cole \n    Corinth Zion Methodist \n    Crocker \n    Denning\n    Ebenezer Baptist Church \n    Ebenezer Cumberland Presbyterian Church \n    Eld SM Williams\n    Ewing \n    Follis  \n    Fredonia\n    Friedensville Evangelican Lutheran Church\n    Gladdish-Anderson\n    Goshen Cumberland Presbyterian Church \n    Grapevine \n    Green \n    Greenbrier \n    Greenlawn\n    Hanover Green\n    Harmony Free Will Baptist Church\n    Hartwell\n    Hidlay \n    Hudgens \n    Jersey Baptis Ch\n    Johnson \n    Johnston City \n    Lake Creek \n    Lebanon \n    Lebanon Cumberland Presbyterian Ch\n    Lebanon Cumberland Presbyterian Church\n    Liberty Methodist \n    Maplewood \n    Masonic\n    Masonic & Oddfellows \n    Miller \n    Mt Olive \n    Mt Olive Methodist Church\n    Nashville National \n    New Chapel Methodist Church\n    Oak Grove \n    Oddfellows\n    Oddfellows \n    Old Bethel \n    Orlinda\n    Orlinda \n    Pleasant Valley Baptist Church \n    Robinson \n    Rose Hill\n    Russell \n    Spillertown \n    Spring Hill\n    St John's Lutheran Church\n    St Paul's Lutheran \"Blue\" Church\n    Trinity\n    Trinity Methodist Church\n    Union Grove \n    Vicksburg National \n    Vienna Fraternal \n    Webber Campground\n    Weber-Campground\n    Weir \n    White Ash\n    Williams Prairie Baptist Church\n    Zion Stone Ch\n    NA\n  \n  \n  \n\n\n\n\nBetter, but some names that look the same are coming up as distinct entries, like Bethlehem. This probably means that there are extraneous spaces floating around. These can be removed from the front and back of a string using the stringr function str_trim() with the side set to both.\n\ntombstones3 &lt;- tombstones3 %&gt;%\n  mutate(Cemetery = str_trim(Cemetery, side = c(\"both\")))\n\nChecking…\n\ncems_unique &lt;- tombstones3 %&gt;%\n  distinct(Cemetery) %&gt;%\n  arrange(Cemetery)\n\ncems_unique %&gt;% \n  gt() %&gt;% \n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Cemetery\n    \n  \n  \n    Baggett\n    Baldwin\n    Bethlehem\n    Bethlehem Baptist Church\n    Blockhouse\n    Boner\n    Britton Odum Farm Commerce\n    Campground\n    Casey Springs\n    Chapman-Veatch\n    Christ Church\n    Cole\n    Corinth Zion Methodist\n    Crocker\n    Denning\n    Ebenezer Baptist Church\n    Ebenezer Cumberland Presbyterian Church\n    Eld SM Williams\n    Ewing\n    Follis\n    Fredonia\n    Friedensville Evangelican Lutheran Church\n    Gladdish-Anderson\n    Goshen Cumberland Presbyterian Church\n    Grapevine\n    Green\n    Greenbrier\n    Greenlawn\n    Hanover Green\n    Harmony Free Will Baptist Church\n    Hartwell\n    Hidlay\n    Hudgens\n    Jersey Baptis Ch\n    Johnson\n    Johnston City\n    Lake Creek\n    Lebanon\n    Lebanon Cumberland Presbyterian Ch\n    Lebanon Cumberland Presbyterian Church\n    Liberty Methodist\n    Maplewood\n    Masonic\n    Masonic & Oddfellows\n    Miller\n    Mt Olive\n    Mt Olive Methodist Church\n    Nashville National\n    New Chapel Methodist Church\n    Oak Grove\n    Oddfellows\n    Old Bethel\n    Orlinda\n    Pleasant Valley Baptist Church\n    Robinson\n    Rose Hill\n    Russell\n    Spillertown\n    Spring Hill\n    St John's Lutheran Church\n    St Paul's Lutheran \"Blue\" Church\n    Trinity\n    Trinity Methodist Church\n    Union Grove\n    Vicksburg National\n    Vienna Fraternal\n    Webber Campground\n    Weber-Campground\n    Weir\n    White Ash\n    Williams Prairie Baptist Church\n    Zion Stone Ch\n    NA\n  \n  \n  \n\n\n\n\n\n\nUsing Regex Anchors to Help Clean\nThe abbreviation Ch at the end of the line doesn’t have a space after it, so it isn’t replaced. Here we can use an anchor in our regex to specify that we want to match the pattern at the end of a string. “Ch$” will match at the end and “^Ch” will match at the start of a string.\n\ntombstones3 &lt;- tombstones3 %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Ch$\", \"Church\")) \n\nChecking again.\n\ncems_unique &lt;- tombstones3 %&gt;%\n  distinct(Cemetery) %&gt;% \n  arrange(Cemetery)\n\ncems_unique %&gt;% \n  gt() %&gt;% \n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Cemetery\n    \n  \n  \n    Baggett\n    Baldwin\n    Bethlehem\n    Bethlehem Baptist Church\n    Blockhouse\n    Boner\n    Britton Odum Farm Commerce\n    Campground\n    Casey Springs\n    Chapman-Veatch\n    Christ Church\n    Cole\n    Corinth Zion Methodist\n    Crocker\n    Denning\n    Ebenezer Baptist Church\n    Ebenezer Cumberland Presbyterian Church\n    Eld SM Williams\n    Ewing\n    Follis\n    Fredonia\n    Friedensville Evangelican Lutheran Church\n    Gladdish-Anderson\n    Goshen Cumberland Presbyterian Church\n    Grapevine\n    Green\n    Greenbrier\n    Greenlawn\n    Hanover Green\n    Harmony Free Will Baptist Church\n    Hartwell\n    Hidlay\n    Hudgens\n    Jersey Baptis Church\n    Johnson\n    Johnston City\n    Lake Creek\n    Lebanon\n    Lebanon Cumberland Presbyterian Church\n    Liberty Methodist\n    Maplewood\n    Masonic\n    Masonic & Oddfellows\n    Miller\n    Mt Olive\n    Mt Olive Methodist Church\n    Nashville National\n    New Chapel Methodist Church\n    Oak Grove\n    Oddfellows\n    Old Bethel\n    Orlinda\n    Pleasant Valley Baptist Church\n    Robinson\n    Rose Hill\n    Russell\n    Spillertown\n    Spring Hill\n    St John's Lutheran Church\n    St Paul's Lutheran \"Blue\" Church\n    Trinity\n    Trinity Methodist Church\n    Union Grove\n    Vicksburg National\n    Vienna Fraternal\n    Webber Campground\n    Weber-Campground\n    Weir\n    White Ash\n    Williams Prairie Baptist Church\n    Zion Stone Church\n    NA\n  \n  \n  \n\n\n\n\n\n\nUsing the Geographic Data to Match Cemeteries\nNow, I do have both geographic data and a subject matter expert (my father) to further refine this list. Let’s start with geography. Bethlehem and Bethlehem Baptist Church could be the same place.\n\ntombstones3 %&gt;%\n  filter(Cemetery == \"Bethlehem\" |\n           Cemetery == \"Bethlehem Baptist Church\") %&gt;%\n  select(Cemetery, City, State, lat, long) %&gt;%\n  gt() %&gt;%\n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Cemetery\n      City\n      State\n      lat\n      long\n    \n  \n  \n    Bethlehem\nRobertson Co\nTN\n36.58917\n-87.02361\n    Bethlehem\nRobertson Co\nTN\n36.59000\n-87.02333\n    Bethlehem\nSpringfield\nTN\n36.68833\n-86.76972\n    Bethlehem\nSpringfield\nTN\n36.69194\n-86.76889\n    Bethlehem\nSpringfield\nTN\n36.69222\n-86.76889\n    Bethlehem\nSpringfield\nTN\n36.69222\n-86.76889\n    Bethlehem\nSpringfield\nTN\n36.69250\n-86.76861\n    Bethlehem\nSpringfield\nTN\nNA\nNA\n    Bethlehem\nSpringfield\nTN\nNA\nNA\n    Bethlehem Baptist Church\nSpringfield\nTN\n36.69000\n-86.76861\n    Bethlehem Baptist Church\nSpringfield\nTN\n36.69222\n-86.77306\n    Bethlehem Baptist Church\nSpringfield\nTN\n36.68972\n-86.76861\n    Bethlehem Baptist Church\nSpringfield\nTN\nNA\nNA\n    Bethlehem\nRobertson Co.\nTN\nNA\nNA\n    Bethlehem\nRobertson Co.\nTN\nNA\nNA\n  \n  \n  \n\n\n\n\nIt turns out that Roberston County’s county seat is Springfield, so these entries are all the same. I’d say this is a pretty common type of problem that you might run into. The spreadsheet column says City, but what it really means is something like “the local level of geography that is meaningful to me”. So thinking about what the data represents rather than how it is labeled can help guide how you handle it.\nSo, perhaps you could check if cemeteries were the same by calculating the distance between the two sets of coordinates. There is a calculator here, which gives a distance as &lt;0.2 mi for one comparison between Bethlehem and Bethlehem Baptist Church. You could also calculate the distance programmatically, see code block 14 in this blog post. You could pull in other geographic data and correct/ add data so you have both city and county if applicable.\nFor now, I’m going to make the correction so all entries say Bethlehem Baptist Church. I’m not going to correct the city/county issue.\n\ntombstones3 &lt;- tombstones3 %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Bethlehem$\", \"Bethlehem Baptist Church\"))\n\n\ntombstones3 %&gt;% filter(Cemetery == \"Bethlehem\" |\n                         Cemetery == \"Bethlehem Baptist Church\") %&gt;% \n  select(Cemetery, City, State, lat, long) %&gt;% \n  gt() %&gt;%\n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Cemetery\n      City\n      State\n      lat\n      long\n    \n  \n  \n    Bethlehem Baptist Church\nRobertson Co\nTN\n36.58917\n-87.02361\n    Bethlehem Baptist Church\nRobertson Co\nTN\n36.59000\n-87.02333\n    Bethlehem Baptist Church\nSpringfield\nTN\n36.68833\n-86.76972\n    Bethlehem Baptist Church\nSpringfield\nTN\n36.69194\n-86.76889\n    Bethlehem Baptist Church\nSpringfield\nTN\n36.69222\n-86.76889\n    Bethlehem Baptist Church\nSpringfield\nTN\n36.69222\n-86.76889\n    Bethlehem Baptist Church\nSpringfield\nTN\n36.69250\n-86.76861\n    Bethlehem Baptist Church\nSpringfield\nTN\nNA\nNA\n    Bethlehem Baptist Church\nSpringfield\nTN\nNA\nNA\n    Bethlehem Baptist Church\nSpringfield\nTN\n36.69000\n-86.76861\n    Bethlehem Baptist Church\nSpringfield\nTN\n36.69222\n-86.77306\n    Bethlehem Baptist Church\nSpringfield\nTN\n36.68972\n-86.76861\n    Bethlehem Baptist Church\nSpringfield\nTN\nNA\nNA\n    Bethlehem Baptist Church\nRobertson Co.\nTN\nNA\nNA\n    Bethlehem Baptist Church\nRobertson Co.\nTN\nNA\nNA\n  \n  \n  \n\n\n\n\nThere aren’t that many possible duplicates.\n\n\n\nLebanon\n\n\nLebanon Cumberland Presbyterian Church\n\n\nMt Olive\n\n\nMt Olive Methodist Church\n\n\nTrinity\n\n\nTrinity Methodist Church\n\n\nWebber Campground\n\n\nWeber-Campground\n\n\n\n\n\nTalking to the Subject Matter Expert\nSometimes it is easier just to go talk to the subject matter expert than it is to come up with complicated programmatic solutions. Sometimes that isn’t possible; maybe you don’t even know who generated the data. Or perhaps their time is much more valuable than yours. That’s why I also came up with a solution (using geographic data to match cemeteries) that could be done independently, even though I can ask the expert.\nFor me, a 5 minute conversation eliminated the need to write lots more code. It turns out that Webber (correct spelling) Campground Cemetery, Weber Campground and Campground are all the same. I wouldn’t have caught Campground as being a match, but my father mentioned it. These entries also switch between city and county in the location part of the table, making it even trickier. It turns out all of these pairs are the same, so I’m going to correct them. I’m also writing this back to the main tombstones dataframe, rather than continuing with intermediate variables. My father also told me that Johnston City Cemetery is actually Shakerag Masonic Cemetery. He also stated that Oddfellows was Oddfellow and Masonic and Oddfellows was Masonic and Odd Fellows. So I’ll correct these too.\n\ntombstones &lt;- tombstones3 %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Lebanon$\", \"Lebanon Cumberland Presbyterian Church\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Mt Olive$\", \"Mt Olive Methodist Church\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Trinity$\", \"Trinity Methodist Church\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Weber-Campground\", \"Webber Campground\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"^Campground\", \"Webber Campground\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Johnston City\", \"Shakerag Masonic\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Masonic & Oddfellows\", \"Masonic & Odd Fellows\")) %&gt;%\n  mutate(Cemetery = str_replace_all(Cemetery, \"Oddfellows\", \"Oddfellow\"))\n\nFinal check.\n\ncems_unique &lt;- tombstones %&gt;%\n  distinct(Cemetery) %&gt;%\n  arrange(Cemetery)\n\ncems_unique %&gt;%\n  gt() %&gt;%\n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      Cemetery\n    \n  \n  \n    Baggett\n    Baldwin\n    Bethlehem Baptist Church\n    Blockhouse\n    Boner\n    Britton Odum Farm Commerce\n    Casey Springs\n    Chapman-Veatch\n    Christ Church\n    Cole\n    Corinth Zion Methodist\n    Crocker\n    Denning\n    Ebenezer Baptist Church\n    Ebenezer Cumberland Presbyterian Church\n    Eld SM Williams\n    Ewing\n    Follis\n    Fredonia\n    Friedensville Evangelican Lutheran Church\n    Gladdish-Anderson\n    Goshen Cumberland Presbyterian Church\n    Grapevine\n    Green\n    Greenbrier\n    Greenlawn\n    Hanover Green\n    Harmony Free Will Baptist Church\n    Hartwell\n    Hidlay\n    Hudgens\n    Jersey Baptis Church\n    Johnson\n    Lake Creek\n    Lebanon Cumberland Presbyterian Church\n    Liberty Methodist\n    Maplewood\n    Masonic\n    Masonic & Odd Fellows\n    Miller\n    Mt Olive Methodist Church\n    Nashville National\n    New Chapel Methodist Church\n    Oak Grove\n    Oddfellow\n    Old Bethel\n    Orlinda\n    Pleasant Valley Baptist Church\n    Robinson\n    Rose Hill\n    Russell\n    Shakerag Masonic\n    Spillertown\n    Spring Hill\n    St John's Lutheran Church\n    St Paul's Lutheran \"Blue\" Church\n    Trinity Methodist Church\n    Union Grove\n    Vicksburg National\n    Vienna Fraternal\n    Webber Campground\n    Weir\n    White Ash\n    Williams Prairie Baptist Church\n    Zion Stone Church\n    NA\n  \n  \n  \n\n\n\n\nLastly, I’m going to replace the NAs with a blank. This will make things nicer if I use this for a label.\n\ntombstones &lt;- tombstones %&gt;%\n  mutate(Cemetery = ifelse(is.na(Cemetery), \"\", Cemetery))"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#cleaning-up-names-strings",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#cleaning-up-names-strings",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Cleaning Up Names (strings)",
    "text": "Cleaning Up Names (strings)\nIn addition to cleaning up typos and such from the names, I’m going to construct a some compound names following my father’s photo naming pattern.\nMy father’s photo naming convention is mostly “last name first name middle initial”. He often includes other information, like if the photo was a close up or distance or if the stone was rubbed with chalk to enhance. So I can’t use exact pattern matching, but rather need to use partial pattern matching.\nThis is another situation where having some clear guidelines about how data will be handled is important. I have decided to prioritize correct matches over more matches. Take my name- Louise Elaine Sinks. It could be rendered as LE Sinks, L E Sinks, L Sinks, Louise E Sinks, and Louise Sinks. I could capture all occurrences of my name by matching on all of these variations, but I’m also more likely to get false matches. In the context of this dataset, names and name patterns are often reused within a family, making false matches even more likely. (I inherited my grandfather’s nameplate and have it on my desk- L E Sinks Jr- since I share the same name pattern with him.)\nI will create the following patterns for matching:\n\nFull Name with whatever is in the Middle Name column (e.g. Louise Elaine Sinks)\nIf First and Middle name are just initials, I will create a version with and without spaces, since I see it done both ways in the photo names (e.g. LE Sinks and L E Sinks)\nIf no middle name is available, I will use First and Last Name (e.g. Louise Sinks)\n\nI will not:\n\nOmit the middle name if it is available. (e.g. Louise Elaine Sinks will never be Louise Sinks)\nTruncate a name to an initial. (e.g. If the entry says Louise Elaine Sinks, I will not use the pattern L E Sinks or Louise E Sinks)\n\nThese rules seem complicated, but when you have a set of unique individuals with names like A T Sinks, Arlie T Sinks and Arlie Sinks, you need to think very carefully about how you will distinguish them when you are doing partial matching.\nWith that said, let’s clean up some names.\n\nCleaning up First Names\nLook at what we are dealing with.\n\ntombstones_raw %&gt;%\n  select(First.Name) %&gt;%\n  gt() %&gt;%\n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      First.Name\n    \n  \n  \n    Abraham\n    Elizabeth\n    Zady\n    Albert\n    Adesia\n    May \n    E\n    William\n    Nancy\n    Richard\n    John \n    William\n    Mahalia\n    E\n    Josephine\n    Fanning\n    John \n    Mary \n    Wm\n    Esther\n    Elizabeth \n    Joel\n    Hope\n    Clem\n    Nancy\n    W\n    Charles\n    Thomas\n    Octavia\n    George\n    Lora\n    W\n    Alzada\n    G\n    Ellen\n    L\n    Mary\n    Daniel\n    Elizabeth\n    Caroline\n    Daniel\n    Lucretia\n    Samuel\n    Elizabeth\n    Laura\n    Polly\n    Mandy\n    John\n    Ezra\n    Lizzie\n    Fred\n    Catherine\n    Christian\n    Peter\n    George\n    Katie\n    Mary E\n    Henriettie\n    MED\n    HD\n    GD\n    Mother\n    Father\n    George\n    James \n    May\n    John\n    Maggie \n    William \n    Dora\n    L[eaman]\n    G[uilford]\n    D[ora]\n    Eugene\n    Lou \n    J[oseph]\n    Joseph\n    Sarah\n    W\n    A\n    J\n    Elizabeth\n    Robert \n    Rebecca\n    Monroe\n    Della\n    Mary M\n    Harve \n    Carrie \n    Smith \n    Ada\n    William\n    Harvey \n    Cora\n    John\n    W\n    Gustavus\n    Sarah \n    Joseph\n    Della \n    William\n    Harriet\n    William\n    Mary\n    James\n    Sarah \n    W[illiam]\n    E[lisha]\n    Sarah \n    James\n    Georgia\n    William\n    Malinda\n    Mary \n    Catherina\n    Johannes\n    Semantha\n    Elizabeth\n    Elizabeth\n    Isaac\n    Fawn\n    Ralph \n    A\n    Christian \n    G\n    Ralph \n    E\n    William\n    Martha \n    Jeff\n    Florence\n    Frances\n    Ebenezer\n    NA\n    NA\n    William\n    Leonard\n    Lucille\n    Parmelia\n    Amalphus\n    Adolphus\n    Samuel\n    Augusta\n    Ulysses\n    Ulysses\n    William\n    William\n    Jerome\n    Franklin\n    Samuel\n    Bernice \n    Catherine\n    George\n    Lucinda\n    William\n    Daniel\n    Margaretha\n    Polly\n    James\n    William\n    Elizabeth\n    Jeremiah\n    Rebecca\n    James\n    Mary\n    Levi\n    Hester\n    Ridley\n    James \n    Tina\n    Ezra\n    Nannie\n    Samuel\n    Melverda\n    John\n    Willard\n    Ruth\n    James\n    Nancy\n    Anna\n    Joe\n    Eugenia\n    Leland\n    Jon\n    Abraham\n    Azariah\n    Abigail\n    Eleandra\n    Micajah\n    Samuel\n    Elizabeth\n    Ruth\n    Sarah\n    Bell\n    Mary\n    Clarence\n    Cora\n    W\n    Susan\n    Hannah\n    Mary \n    Samuel\n    Belinda\n    Susannah\n    Thomas\n    Sarah\n    Anna\n    Nicolaus\n    Myrtie\n    Elizabeth\n    John\n    Catherina\n    Gotthard\n    Magdelena\n    Peter\n    Anna\n    Anna\n    Caty\n    Daniel\n    Frederick\n    Friedrich\n    Johann\n    Maria\n    John\n    Mary\n    Henry\n    Mary \n    Will\n    Adeline\n    NA\n    John\n    Beatrice\n    John\n    NA\n    NA\n    Archibald\n    Cynthia \n    G\n    Sarah\n    Thomas\n    Britton\n    Wiley\n    Sallie A\n    Daniel\n    Charlotte\n    William\n    Harriet \n    Louise\n    Karl\n    Caroline\n    William\n    Harriet\n    Louise\n    Frieda\n    Amos\n    William\n    Elmina\n    Mamie\n    George \n    Bertie\n    Lulie\n    Lily\n    Arthur\n    George \n    Jno\n    Guy\n    Harlie\n    Annabelle\n    Alfred\n    Solomon\n    Catherine\n    J\n    Mary\n    Balzer\n    Elisabetha\n    Johannes\n    Elizabeth\n    George \n    Euna\n    Mary\n    Melchir\n    James\n    Ana\n    NA\n    NA\n    NA\n    Jenniel\n    A\n    Francis \n    Delphia\n    Salem\n    Daniel\n    Martha\n    Roy \n    Elizabeth\n    infant son\n    John\n    Mary\n    William\n    Charlotte \n    Anna\n    Leonard\n    Etta Faye\n    John\n    Sena\n    William\n    Jewell \n    Francis \n    Arlie\n    Viola\n    Leonard\n    Mae\n    Bessie\n    Caroline\n    Arlie\n    Eva\n    Conrad\n    Conrad\n    Maria\n    Trice\n    Phebe\n    Richard\n    Martin\n    Florence\n    W\n    Nancy \n    J\n    W\n    Elizabeth\n    Pleasant\n    Victoria\n    Ward\n    Cynthia\n    James\n    James\n    Nannie\n    John\n    Eleanor\n    William\n    James\n    Rachel\n    Pleasant\n    Mary\n    Parmelia\n    Mary\n    Elnor \n    Frelin\n    NA\n    John\n    Rose\n    Ruth\n    Turner\n    Martha\n    Joseph\n    Caroline\n    Dick\n    Pearl\n    James\n    Mary\n    Leticia \n    Lucinda \n    John\n    Matha\n    Jessie\n    Mary \n    Joseph\n    Elisha\n    Sallie\n    Lutetita\n    Thomas\n    Sarah\n    Elisha\n    Martha\n    Charles\n    Zack\n    Juritha\n    Elisha\n    Drury\n    Mary \n    Sandifer\n    Nancy\n    Luvena\n    John\n    Nettie\n    Millie\n    Lawrence\n    Etta \n    John\n    James\n    C\n    Blanche\n    L \n    Ama\n    Robert\n    James\n    Romey\n    Anna\n    James\n    Francis\n    Turner\n    William\n    George\n    Nancy\n  \n  \n  \n\n\n\n\nSometimes the tombstone only has the initial, but my father knows the full name through other means. This would be rendered as L[ouise]. The photos will usually use L not Louise, so I’m just removing this extra data and storing it elsewhere. I do this with str_extract() and then I replace it with a blank using str_replace_all(). There are also extra spaces that I will trim off as before with str_trim().\n\nUsing Regex Quantity Codes\nThe regex for getting rid of the bracketed information is a bit more complicated than what I’ve used before. I want something that looks for [ followed by any number of other characters followed by ]. The brackets are special characters, so they need to be escaped. So that would be “\\\\[ \\\\]” for the front and back of the pattern. The middle can be anything. This is where regex is so powerful. As I mentioned before, an un-escaped period will match to any character. There are also quantity codes- a plus sign will match one or more. So I use “\\\\[.+\\\\]” which will match one or more characters inside brackets. (I could use the quantity symbol * which matches 0 or more, but there will never be empty brackets in this dataset.)\n\ntombstones &lt;- tombstones %&gt;%\n  mutate(Extra_First_Name = str_extract(First.Name, \"\\\\[.+\\\\]\")) %&gt;%\n  mutate(First.Name = str_replace(First.Name, \"\\\\[.+\\\\]\", \"\")) %&gt;%\n  mutate(First.Name = str_replace(First.Name, \"\\\\.\", \"\")) %&gt;%\n  mutate(First.Name = str_trim(First.Name, side = c(\"both\"))) %&gt;%\n  drop_na(First.Name)\n\nLook at the data.\n\ntombstones %&gt;%\n  select(First.Name) %&gt;%\n  gt() %&gt;%\n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      First.Name\n    \n  \n  \n    Abraham\n    Elizabeth\n    Zady\n    Albert\n    Adesia\n    May\n    E\n    William\n    Nancy\n    Richard\n    John\n    William\n    Mahalia\n    E\n    Josephine\n    Fanning\n    John\n    Mary\n    Wm\n    Esther\n    Elizabeth\n    Joel\n    Hope\n    Clem\n    Nancy\n    W\n    Charles\n    Thomas\n    Octavia\n    George\n    Lora\n    W\n    Alzada\n    G\n    Ellen\n    L\n    Mary\n    Daniel\n    Elizabeth\n    Caroline\n    Daniel\n    Lucretia\n    Samuel\n    Elizabeth\n    Laura\n    Polly\n    Mandy\n    John\n    Ezra\n    Lizzie\n    Fred\n    Catherine\n    Christian\n    Peter\n    George\n    Katie\n    Mary E\n    Henriettie\n    MED\n    HD\n    GD\n    Mother\n    Father\n    George\n    James\n    May\n    John\n    Maggie\n    William\n    Dora\n    L\n    G\n    D\n    Eugene\n    Lou\n    J\n    Joseph\n    Sarah\n    W\n    A\n    J\n    Elizabeth\n    Robert\n    Rebecca\n    Monroe\n    Della\n    Mary M\n    Harve\n    Carrie\n    Smith\n    Ada\n    William\n    Harvey\n    Cora\n    John\n    W\n    Gustavus\n    Sarah\n    Joseph\n    Della\n    William\n    Harriet\n    William\n    Mary\n    James\n    Sarah\n    W\n    E\n    Sarah\n    James\n    Georgia\n    William\n    Malinda\n    Mary\n    Catherina\n    Johannes\n    Semantha\n    Elizabeth\n    Elizabeth\n    Isaac\n    Fawn\n    Ralph\n    A\n    Christian\n    G\n    Ralph\n    E\n    William\n    Martha\n    Jeff\n    Florence\n    Frances\n    Ebenezer\n    William\n    Leonard\n    Lucille\n    Parmelia\n    Amalphus\n    Adolphus\n    Samuel\n    Augusta\n    Ulysses\n    Ulysses\n    William\n    William\n    Jerome\n    Franklin\n    Samuel\n    Bernice\n    Catherine\n    George\n    Lucinda\n    William\n    Daniel\n    Margaretha\n    Polly\n    James\n    William\n    Elizabeth\n    Jeremiah\n    Rebecca\n    James\n    Mary\n    Levi\n    Hester\n    Ridley\n    James\n    Tina\n    Ezra\n    Nannie\n    Samuel\n    Melverda\n    John\n    Willard\n    Ruth\n    James\n    Nancy\n    Anna\n    Joe\n    Eugenia\n    Leland\n    Jon\n    Abraham\n    Azariah\n    Abigail\n    Eleandra\n    Micajah\n    Samuel\n    Elizabeth\n    Ruth\n    Sarah\n    Bell\n    Mary\n    Clarence\n    Cora\n    W\n    Susan\n    Hannah\n    Mary\n    Samuel\n    Belinda\n    Susannah\n    Thomas\n    Sarah\n    Anna\n    Nicolaus\n    Myrtie\n    Elizabeth\n    John\n    Catherina\n    Gotthard\n    Magdelena\n    Peter\n    Anna\n    Anna\n    Caty\n    Daniel\n    Frederick\n    Friedrich\n    Johann\n    Maria\n    John\n    Mary\n    Henry\n    Mary\n    Will\n    Adeline\n    John\n    Beatrice\n    John\n    Archibald\n    Cynthia\n    G\n    Sarah\n    Thomas\n    Britton\n    Wiley\n    Sallie A\n    Daniel\n    Charlotte\n    William\n    Harriet\n    Louise\n    Karl\n    Caroline\n    William\n    Harriet\n    Louise\n    Frieda\n    Amos\n    William\n    Elmina\n    Mamie\n    George\n    Bertie\n    Lulie\n    Lily\n    Arthur\n    George\n    Jno\n    Guy\n    Harlie\n    Annabelle\n    Alfred\n    Solomon\n    Catherine\n    J\n    Mary\n    Balzer\n    Elisabetha\n    Johannes\n    Elizabeth\n    George\n    Euna\n    Mary\n    Melchir\n    James\n    Ana\n    Jenniel\n    A\n    Francis\n    Delphia\n    Salem\n    Daniel\n    Martha\n    Roy\n    Elizabeth\n    infant son\n    John\n    Mary\n    William\n    Charlotte\n    Anna\n    Leonard\n    Etta Faye\n    John\n    Sena\n    William\n    Jewell\n    Francis\n    Arlie\n    Viola\n    Leonard\n    Mae\n    Bessie\n    Caroline\n    Arlie\n    Eva\n    Conrad\n    Conrad\n    Maria\n    Trice\n    Phebe\n    Richard\n    Martin\n    Florence\n    W\n    Nancy\n    J\n    W\n    Elizabeth\n    Pleasant\n    Victoria\n    Ward\n    Cynthia\n    James\n    James\n    Nannie\n    John\n    Eleanor\n    William\n    James\n    Rachel\n    Pleasant\n    Mary\n    Parmelia\n    Mary\n    Elnor\n    Frelin\n    John\n    Rose\n    Ruth\n    Turner\n    Martha\n    Joseph\n    Caroline\n    Dick\n    Pearl\n    James\n    Mary\n    Leticia\n    Lucinda\n    John\n    Matha\n    Jessie\n    Mary\n    Joseph\n    Elisha\n    Sallie\n    Lutetita\n    Thomas\n    Sarah\n    Elisha\n    Martha\n    Charles\n    Zack\n    Juritha\n    Elisha\n    Drury\n    Mary\n    Sandifer\n    Nancy\n    Luvena\n    John\n    Nettie\n    Millie\n    Lawrence\n    Etta\n    John\n    James\n    C\n    Blanche\n    L\n    Ama\n    Robert\n    James\n    Romey\n    Anna\n    James\n    Francis\n    Turner\n    William\n    George\n    Nancy\n  \n  \n  \n\n\n\n\nThere is an unnamed child (infant son) and maybe a typo’d name (Jno). I’ll keep these cases in mind as I move forward.\n\n\n\nCleaning up Middle Names\nThe middle name column is very problematic. Sometimes it is the middle name. Sometimes it is a note like “shared family tombstone”. Sometimes it is the middle initial from the gravestone, but with the full name filled in with brackets like A[lvis]. Sometimes it is a maiden name or the name of the spouse.\nMost of this stuff is similar to what I’ve done with other fields. I’ll strip out extra spaces, deal with the bracketed info, periods and the abbreviation ux (Latin for wife). I’m taking out everything after ux, since that should be the name of the wife.\nI’m also changing all blanks to NAs and I will use this to partition my dataset when I am doing the photo matching. na_if() from dplyr will let you replace any specific value with NA.\n\ntombstones &lt;- tombstones %&gt;%\n  mutate(Extra_Middle_Name1 = str_extract(Middle.Name, \"ux .+\")) %&gt;%\n  mutate(Middle.Name = str_replace(Middle.Name, \"ux .+\", \"\")) %&gt;%\n  mutate(Extra_Middle_Name2 = str_extract(Middle.Name, \"\\\\[.+\\\\]\")) %&gt;%\n  mutate(Middle.Name = str_replace(Middle.Name, \"\\\\[.+\\\\]\", \"\")) %&gt;%\n  mutate(Middle.Name = str_replace(Middle.Name, \"\\\\.\", \"\")) %&gt;%\n  mutate(Middle.Name = str_trim(Middle.Name, side = c(\"both\"))) %&gt;%\n  mutate(Middle.Name = na_if(Middle.Name, \"\")) \n\nThis fixes most of the issues, but not all. I’m going to see how the rest of this going and then come back and fix problem cases if needed. (Turns out this is fine.)\nNow I’m making the full name with Middle Name/initial (e.g. Sinks Louise Elaine). His photo naming format is almost always last name first. Nothing too exotic here. I do use paste() to glue together my pieces, using space as the separator.\n\ntombstones &lt;- tombstones %&gt;%\n  mutate(full_name_MI = ifelse(\n    is.na(Middle.Name) == TRUE,\n    Middle.Name,\n    paste(Surname, First.Name, Middle.Name, sep = \" \")\n  )) %&gt;%\n  mutate(full_name = paste(Surname, First.Name, sep = \" \"))\n\nNow to deal with the case where first and middle names that are only letters, sometimes the photo name is Sinks LE rather than Sinks L E (which is created above) . So I need a second middle name column for these cases. (There are also cases with initials in the middle name itself. I don’t know if I need to clean that up. See Hess Ulysses S G.)\nThis may not be elegant, but I’m just slamming together first and middle name without spaces. So I’m getting names like Sinks LouiseElaine also.\n\ntombstones &lt;- tombstones %&gt;%\n  mutate(full_name_MI_no_space = ifelse(\n    is.na(Middle.Name) == TRUE,\n    Middle.Name,\n    paste(Surname, paste0(First.Name, Middle.Name), sep = \" \")\n  )) \n\nAnd here I filter out first + middle name combo that is longer than three characters and setting the middle name without spaces column to NA. So LouiseElaine gets set to NA but LE remains. I chose 3 because I did see some folks with double initials in the middle name (e.g. Ulysses SG Hess) and I wasn’t sure if there were some fully initialed names that might be like that. I think in practice 2 is fine. This whole thing is a bit sloppy. The more rigorous way to do this is to check the patterns of the first and middle name and only created the combined name if they matched the pattern of being a single letter or two single letters separated by a space. Sometimes though, quick and dirty gets the job done.\n\ntombstones &lt;- tombstones %&gt;%\n  mutate(full_name_MI_no_space = ifelse(\n    nchar(paste0(First.Name, Middle.Name)) &gt; 3,\n    \" \",\n    full_name_MI_no_space\n  )) %&gt;%\n  mutate(full_name_MI_no_space = na_if(full_name_MI_no_space, \" \"))\n\nThere is another part of this project that I’m working on separately, so I’m saving a copy of the cleaned dataset to be used in that module.\n\nsaveRDS(tombstones, \"tombstones_cleaned.RDS\")"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#reset-photo-folders",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#reset-photo-folders",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Reset Photo Folders",
    "text": "Reset Photo Folders\nTesting the matching is an iterative process. I also ended up going back and adding more cleaning steps based on what I was seeing during the matching. There are about 500 photos and they get sorted into various folders. I was manually resetting all the folders, but that got old quickly. I wrote a code chunk to reset the folders. I move everything into a folder called trash- please do this rather than actually deleting the files until you are sure your code works! Then I copy the originals into a starting folder (Unmatched Photos)\nHow this works is I generate a list of files in each folder using list.files() and then I pass that list to the file.rename() or file.copy() functions. The file functions report TRUE for each file they correct find an act on and FALSE otherwise. The output should be all TRUEs.\n(I thought I could make this a function, since I end up using it again below, but it doesn’t work. I didn’t return which is invalid and I ended up locking up Studio. Then I returned TRUE, but that seems to not work either. I ended up terminating R. It seems to hang when copying the files back in.)\n\n# first moving all the photos to the trash folder, folder by folder\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Unmatched Photos\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Unmatched Photos\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[106] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[136] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[151] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[166] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[181] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[196] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Archived Photos\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Archived Photos\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[106] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[136] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[151] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Matched_Round1\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Matched_Round1\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE\n\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Matched_Round2\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Matched_Round2\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Matched_Round3\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Matched_Round3\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Matched_Round4\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Matched_Round4\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Map\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Map\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[106] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[136] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[151] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[166] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[181] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[196] TRUE TRUE TRUE TRUE\n\n# now I move a copy of all the photos into the starting folder.\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Original Photos\"))\n\nfile.copy(\n  from = here(blog_folder, photo_folder, \"Original Photos\", my_file_list),\n  to = here(blog_folder, photo_folder, unmatched_folder, my_file_list)\n)\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[106] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[136] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[151] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[166] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[181] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[196] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[211] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[226] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[241] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[256] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[271] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[286] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[301] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[316] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[331] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[346] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[361] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[376] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[391] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[406] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[421] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[436] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[451] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[466] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[481] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[496] TRUE TRUE TRUE TRUE"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#split-out-middle-names-and-no-middle-names",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#split-out-middle-names-and-no-middle-names",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Split out Middle Names and No Middle Names",
    "text": "Split out Middle Names and No Middle Names\nOkay, now I am creating the two main data sets- with and without middle name entries.\n\nunmatched_middle &lt;- tombstones %&gt;%\n  filter(is.na(Surname) == FALSE) %&gt;%\n  filter(is.na(Middle.Name) == FALSE)\n\nunmatched_no_middle &lt;-  tombstones %&gt;%\n  filter(is.na(Surname) == FALSE) %&gt;%\n  filter(is.na(Middle.Name) == TRUE)\n\nNow a function to do the matching."
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#matching_photos-function",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#matching_photos-function",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Matching_Photos() Function",
    "text": "Matching_Photos() Function\nThis function is a beast and should probably be broken into smaller functions. When you have to comment all the different steps in a function, then it is likely doing too many things! As I mentioned, I originally just tested this in non-function form, and I just took the whole chunk and put it in a function. I might clean this up later, but since this is a one off project, it may not be worth the time.\nThere are a few things to note about this function. First, for debugging inside a quarto file, use browser() inside the function to allow you to step through and observe the variables in the function. Not all of the regular debugging tools work in quarto (compared to a regular R file).\nSecondly, accessing the value of the parameters inside the function required using functions like get(). Filter on name_to_match and nothing matches, because nothing has that name. Using get(name_to_match) actual returns the column name to match on.\nOh, and I use loops for everything! If I were to clean this up, I’d definitely get rid of loops using purrr or something.\nSo what does this function do?\nIt takes in a dataframe (df_to_match) and what column of names it should be matching against (name_to_match). It also takes in which match round this is.\n\nGenerates the list of unmatched photos.\nFinds duplicated names.\nand filters them out so that matching only occurs on the unique names in the set.\nDoes the matching with a fuzzyjoin. More about fuzzyjoin here and here.\nMove all the photos that matched to the appropriate folder for the round.\nRemove all the unmatched names from the dataframe.\nGenerate a list of people who have multiple photos associated with their name.\nMake a panel/composite photo from the individual photos for each person with multiples and move the original to the folder archive. The panel photos are made using the magick package. I got started using this solution from stack overflow.\nReturn the dataframe with the matches. This is useful for troubleshooting, but it is not the final results dataframe because it has the original (multiple) photos listed and not the new panel photos. Functions in R must return something, and I did use this returned dataframe extensively when troubleshooting, but in other cases, I’d probably silence the output or just return TRUE/FALSE (with some error checking) to reflect if the function executed properly. It could also return the duplicate names df which could be sent to my father to manually match photos to those names. Or it could call Update_Photo_Names() as the last part, which would then let Matching_Photos() return the dataframe with the correct file names.\n\n\nMatching_Photos &lt;-\n  function(df_to_match, name_to_match, match_folder) {\n    # browser()\n    #Step 1: list of unmatched photos\n    photo_names = list.files(here(blog_folder, photo_folder, unmatched_folder))\n    photo_df = as.data.frame(photo_names)\n    #Step 2: generate duplicate names\n    duplicate_names &lt;- df_to_match %&gt;%\n      group_by_at(name_to_match) %&gt;% count(sort = TRUE) %&gt;% filter(n &gt; 1)\n    # return(duplicate_names) #this was for troubleshooting\n    #Step 3: remove duplicate names\n    tombstones_unique_names &lt;- df_to_match %&gt;%\n      anti_join(duplicate_names)\n    #step 4: do the matching\n    tombstones_merged &lt;-\n      fuzzy_right_join(\n        photo_df,\n        tombstones_unique_names,\n        by = c(\"photo_names\" = name_to_match),\n        match_fun = str_detect\n      )\n    #step 5: Moving all the photos that match to the correct match folder\n    matched_this_round &lt;- inner_join(photo_df, tombstones_merged)\n    index &lt;- 1\n    for (index in seq(1:nrow(matched_this_round))) {\n      file.rename(\n        from = here(\n          blog_folder,\n          photo_folder,\n          unmatched_folder,\n          matched_this_round$photo_names[index]\n        ),\n        to = here(\n          blog_folder,\n          photo_folder,\n          match_folder,\n          matched_this_round$photo_names[index]\n        )\n      )\n    }\n    #step 6: Remove any unmatched names\n    tombstones_merged &lt;- tombstones_merged %&gt;%\n      drop_na(photo_names)\n    #step 7: generate the list of folks with multiple photos\n    multiple_photos &lt;-\n      tombstones_merged %&gt;% group_by_at(name_to_match) %&gt;% count(sort = TRUE) %&gt;% filter(n &gt; 1)\n    #step 8: make the panel photos and move the originals to archive. (function?)\n    if (nrow(multiple_photos) &gt; 0) {\n      index &lt;- 1\n      for (index in seq(1:nrow(multiple_photos))) {\n        df_temp &lt;- tombstones_merged %&gt;%\n          filter(get(name_to_match) == multiple_photos[[name_to_match]][index])\n        these &lt;-\n          as.list(here(\n            blog_folder,\n            photo_folder,\n            match_folder,\n            df_temp$photo_names\n          ))\n        photo_panel &lt;-\n          image_append(do.call(\"c\", lapply(these, image_read)))\n        image_write(\n          photo_panel,\n          path =  here(\n            blog_folder,\n            photo_folder,\n            match_folder,\n            paste0(df_temp[[name_to_match]][1], \"_panel.png\")\n            #paste0(df_temp$full_name_MI[1], \"_panel.png\")\n          ),\n          format = \"jpeg\"\n        )\n        index2 &lt;- 1\n        for (index2 in seq(1:nrow(df_temp))) {\n          file.rename(\n            from = here(\n              blog_folder,\n              photo_folder,\n              match_folder,\n              df_temp$photo_names[index2]\n            ),\n            to = here(\n              blog_folder,\n              photo_folder,\n              archive_folder,\n              df_temp$photo_names[index2]\n            )\n          )\n        }\n      }\n    }\n    #Step 9: return\n    return(tombstones_merged)\n  }"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#a-false-start",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#a-false-start",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "A False Start",
    "text": "A False Start\nDoes the function work?\n\ntester &lt;- Matching_Photos(unmatched_middle, \"full_name_MI\", match1)\n\nJoining with `by = join_by(full_name_MI)`\nJoining with `by = join_by(photo_names)`\n\n\nWarning in file.rename(from = here(blog_folder, photo_folder, unmatched_folder,\n: cannot rename file 'C:/Users/drsin/OneDrive/Documents/R\nProjects/lsinks.github.io/posts/2023-08-04-data-cleaning-tombstone/Photos/Unmatched\nPhotos/Jones Levi A Jones Hester J.JPG' to 'C:/Users/drsin/OneDrive/Documents/R\nProjects/lsinks.github.io/posts/2023-08-04-data-cleaning-tombstone/Photos/Matched_Round1/Jones\nLevi A Jones Hester J.JPG', reason 'The system cannot find the file specified'\n\n\nWarning in file.rename(from = here(blog_folder, photo_folder, unmatched_folder,\n: cannot rename file 'C:/Users/drsin/OneDrive/Documents/R\nProjects/lsinks.github.io/posts/2023-08-04-data-cleaning-tombstone/Photos/Unmatched\nPhotos/Pickard William Epps.JPG' to 'C:/Users/drsin/OneDrive/Documents/R\nProjects/lsinks.github.io/posts/2023-08-04-data-cleaning-tombstone/Photos/Matched_Round1/Pickard\nWilliam Epps.JPG', reason 'The system cannot find the file specified'\n\n\nYes, it works! However, there are a couple of warnings kicked up. The warnings are generated when the file to be moved doesn’t exist. This means that it has matched to another name and has already been moved. So these aren’t a problem with the function, but rather a problem with my logic/ partitioning of data into the different rounds.\nThe first message is about “Jones Levi A Jones Hester J.JPG”. The tombstone is for both the husband and wife and it should match to two names- Levi A Jones and Hester J Jones. So this is okay and not an error. The second one error arises from “Pickard William Epps.jpg”. This is a problem. There is apparently both a William E Pickard and a William Epps Pickard. The strings that are being matched against the photos are Pickard William E and Pickard William Epps. The first will match to the second when doing a partial match. Notice that if the naming convention were First Middle Last and not Last First Middle there would not be a problem. William E Pickard and William Epps Pickard will not match with each other in my method. This also means I don’t need to worry about a similar scenario for the first name, only the middle name/initial because it is at the end of the string."
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#reset-photos-again",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#reset-photos-again",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Reset Photos Again",
    "text": "Reset Photos Again\nI’m suppressing the output, so you won’t see pages of TRUES.\n\n# first moving all the photos to the trash folder, folder by folder\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Unmatched Photos\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Unmatched Photos\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Archived Photos\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Archived Photos\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Matched_Round1\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Matched_Round1\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Matched_Round2\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Matched_Round2\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Matched_Round3\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Matched_Round3\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Matched_Round4\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Matched_Round4\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\n\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Map\"))\nfile.rename(\n  from = here(blog_folder, photo_folder, \"Map\", my_file_list),\n  to = here(blog_folder, photo_folder, \"Trash\", my_file_list)\n)\n\n# now I move a copy of all the photos into the starting folder.\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, \"Original Photos\"))\n\nfile.copy(\n  from = here(blog_folder, photo_folder, \"Original Photos\", my_file_list),\n  to = here(blog_folder, photo_folder, unmatched_folder, my_file_list)\n)"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#the-full-scheme-a-flowchart",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#the-full-scheme-a-flowchart",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "The Full Scheme: A Flowchart",
    "text": "The Full Scheme: A Flowchart\nSo to fix this, I can split this into two new middle name dataframes. One with middle names longer than one character (or maybe 2) and those with initials for middle names. Again, the more complete version needs to be matched first.\nI made a nice flowchart to explain. (I worked off of handwritten charts while I was coding. This was too complex not to have a reference.) Dark blue squares are the various dataframes as they are filtered and split before matching. Light blue squares are the results of the various rounds of matching. The magenta boxes are the matching protocols for each round. At the end, all the results_r# dataframes will be merged."
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#round-1-long-middle-names",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#round-1-long-middle-names",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Round 1: Long Middle Names",
    "text": "Round 1: Long Middle Names\n\nSplitting : Long and Initials Middle Names\n\nmiddle_names_long &lt;- unmatched_middle %&gt;%\n  filter(nchar(Middle.Name) &gt; 2)\n  \nmiddle_names_short &lt;- unmatched_middle %&gt;%\n  filter(nchar(Middle.Name) &lt;= 2)\n\n\n\nMatching Round 1: Full Middle Names\n\ntester &lt;- Matching_Photos(middle_names_long, \"full_name_MI\", match1)\n\nJoining with `by = join_by(full_name_MI)`\nJoining with `by = join_by(photo_names)`\n\n\nNo errors or warnings. Now I’m generating the new list of photos. This will have the panel photos instead of the individual photos. I’m matching back to the original input dataframe to the match function, not the output (tester). The output of the renaming function is our results dataframe for this round.\n\n\nUpdate_Photo_Names() : Rename and Moves Photos Function\nThis function updates the photo list to include the panel photos instead of the original files. It also moves all the photos from this round into the proper folder for the round.\n\nUpdate_Photo_Names &lt;- function(df, name_to_match, match_folder) {\n  photo_names &lt;-\n    list.files(here(blog_folder, photo_folder, match_folder))\n  photo_df = as.data.frame(photo_names)\n  df_updated &lt;- fuzzy_right_join(photo_df,\n                                 df,\n                                 by = c(\"photo_names\" = name_to_match),\n                                 match_fun = str_detect)\n  #new_name &lt;- paste(\"photos\", match_folder, sep = \"_\") this was for debugging\n  \n  df_updated &lt;- df_updated %&gt;%\n    rename_with(.fn = ~ paste(\"photos\", match_folder, sep = \"_\"),\n                .cols = photo_names)\n  return(df_updated)\n}\n\n\n\nRename Round 1: Generate first results DF\n\nresults_r1 &lt;- Update_Photo_Names(middle_names_long, \"full_name_MI\", match1)\n\nAnd now we just go through this again for each round."
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#round-2-short-middle-names",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#round-2-short-middle-names",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Round 2: Short Middle Names",
    "text": "Round 2: Short Middle Names\n\ntester2 &lt;- Matching_Photos(middle_names_short, \"full_name_MI\", match2)\n\nJoining with `by = join_by(full_name_MI)`\nJoining with `by = join_by(photo_names)`\n\n\nWarning in file.rename(from = here(blog_folder, photo_folder, unmatched_folder,\n: cannot rename file 'C:/Users/drsin/OneDrive/Documents/R\nProjects/lsinks.github.io/posts/2023-08-04-data-cleaning-tombstone/Photos/Unmatched\nPhotos/Jones Levi A Jones Hester J.JPG' to 'C:/Users/drsin/OneDrive/Documents/R\nProjects/lsinks.github.io/posts/2023-08-04-data-cleaning-tombstone/Photos/Matched_Round2/Jones\nLevi A Jones Hester J.JPG', reason 'The system cannot find the file specified'\n\n\nOnly get the Hester and Levi Jones warning, which is fine.\n\nresults_r2 &lt;- Update_Photo_Names(middle_names_short, \"full_name_MI\", match2)"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#round-3-middle-names-with-no-space",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#round-3-middle-names-with-no-space",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Round 3: Middle Names with No Space",
    "text": "Round 3: Middle Names with No Space\nNow I need to pull out the names that are only initials and match the no space version. These should only be contained in the results_r2 dataframe (since the other dataframe (r1) only has long middle names anyway.)\n\nmiddle_name_initials &lt;- results_r2 %&gt;%\n  mutate(full_name_MI_no_space = na_if(full_name_MI_no_space, \" \")) %&gt;%\n  filter(is.na(full_name_MI_no_space) == FALSE) %&gt;%\n  filter(is.na(photos_Matched_Round2) == TRUE)\n\nOkay, 17 didn’t match.\n\ntester &lt;- Matching_Photos(middle_name_initials, \"full_name_MI_no_space\", match3)\n\nJoining with `by = join_by(full_name_MI_no_space)`\nJoining with `by = join_by(photo_names)`\n\n\nNo warnings or errors.\n\nresults_r3 &lt;-\n  Update_Photo_Names(middle_name_initials, \"full_name_MI_no_space\", match3)"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#round-4-no-middle-initials",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#round-4-no-middle-initials",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Round 4: No Middle Initials",
    "text": "Round 4: No Middle Initials\nAlmost done. Now I’m matching the folks with no middle names. This group has more panel pictures than the others and it runs much slower the other match rounds.\n\ntester3 &lt;- Matching_Photos(unmatched_no_middle, \"full_name\", match4)\n\nJoining with `by = join_by(full_name)`\nJoining with `by = join_by(photo_names)`\n\n\nNow we generate the new names.\n\nresults_r4 &lt;- Update_Photo_Names(unmatched_no_middle, \"full_name\", match4)"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#join-all-the-results",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#join-all-the-results",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Join all the Results",
    "text": "Join all the Results\nNow I full_join all the results dataframes together.\n\ntombstones_matched &lt;- results_r1 %&gt;%\n  full_join(results_r2) %&gt;%\n  full_join(results_r3) %&gt;%\n  full_join(results_r4)\n\nNow generate the final photo list. Because of the way I segmented the data and how careful I was about order of matching, there should be only one match per name. And this is true.\nHowever, by also being careful about what order I create the final photo list, I can put in a safeguard against incorrect matching for future work. The file name should always be taking from the earliest match round if for some reason it matches in multiple rounds. This might be cleaner written as a case statement, but I wrote this section when I only had two rounds of matching and the nested ifelse was very clear.\n\ntombstones_matched_final &lt;- tombstones_matched %&gt;%\n  mutate(photo_list = ifelse(\n    is.na(photos_Matched_Round1) == TRUE,\n    ifelse(\n      is.na(photos_Matched_Round2) == TRUE,\n      ifelse(\n        is.na(photos_Matched_Round3) == TRUE,\n        photos_Matched_Round4,\n        photos_Matched_Round3\n      ),\n      photos_Matched_Round2\n    )\n    ,\n    photos_Matched_Round1\n  ))\n\nNow I just double check that the photo_list was generated properly.\n\ntombstones_matched_checker &lt;- tombstones_matched_final %&gt;%\n  select(photo_list, contains(\"photos_Matched\"))"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#move-to-all-photos-to-the-map-folder",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#move-to-all-photos-to-the-map-folder",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Move to all Photos to the Map Folder",
    "text": "Move to all Photos to the Map Folder\nLastly, I move all the matched photos to a single folder. I’m putting it in a folder called “Map” since I’m using this to make my leaflet map. Again, I’ve suppressed the output for this block, but when troubleshooting you do want to make sure you are getting all trues.\n\n\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, match1))\n\nfile.copy(\n  from = here(blog_folder, photo_folder, match1, my_file_list),\n  to = here(blog_folder, photo_folder, \"Map\", my_file_list)\n)\n\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, match2))\n\nfile.copy(\n  from = here(blog_folder, photo_folder, match2, my_file_list),\n  to = here(blog_folder, photo_folder, \"Map\", my_file_list)\n)\n\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, match3))\n\nfile.copy(\n  from = here(blog_folder, photo_folder, match3, my_file_list),\n  to = here(blog_folder, photo_folder, \"Map\", my_file_list)\n)\n\nmy_file_list &lt;-\n  list.files(here(blog_folder, photo_folder, match4))\n\nfile.copy(\n  from = here(blog_folder, photo_folder, match4, my_file_list),\n  to = here(blog_folder, photo_folder, \"Map\", my_file_list)\n)"
  },
  {
    "objectID": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#making-labels-for-the-map",
    "href": "posts/2023-08-04-data-cleaning-tombstone/tombstones_data_cleaning.html#making-labels-for-the-map",
    "title": "Data Cleaning for the Tombstone Project",
    "section": "Making Labels for the Map",
    "text": "Making Labels for the Map\n\nMaking a Complete Name Field\nNow I need to make a column of the most complete name. This will be used as the label in the map. If the person has a middle name entry, I use the full name with middle name, and if not, I use the first and last. Note that I’m generating this by pasting together the individual name parts rather than using all the name variations I generated to match with. I want this label to be in conventional order and not the last name first format that I used for matching.\n\ntombstones_matched_final &lt;- tombstones_matched_final %&gt;%\n  mutate(complete_name = ifelse(\n    is.na(full_name_MI) == FALSE,\n    paste(First.Name, Middle.Name, Surname, sep = \" \"),\n    paste(First.Name, Surname, sep = \" \"))\n  )\n\n\n\nMaking a Cemetery Label\n\ntombstones_matched_final &lt;- tombstones_matched_final %&gt;%\n  mutate(cemetery_name = ifelse(Cemetery == \"\", \"\", paste(Cemetery, \"Cemetery\", sep = \" \"))\n  )\n\nSaving the file for use in another part of this project.\n\nsaveRDS(tombstones_matched_final, \"tombstones_matched_final.RDS\")"
  },
  {
    "objectID": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html",
    "href": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html",
    "title": "Tidy Tuesday Twofer (32 and 33)",
    "section": "",
    "text": "Last week I played around with the TidyTuesday data on hot sauces, but I didn’t polish anything or write any text. This week’s TidyTuesday data concerns spam email. I’m going to cover them both in this blog post."
  },
  {
    "objectID": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#loading-the-data",
    "href": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#loading-the-data",
    "title": "Tidy Tuesday Twofer (32 and 33)",
    "section": "Loading the data",
    "text": "Loading the data\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 32)\n\n--- Compiling #TidyTuesday Information for 2023-08-08 ----\n\n\n--- There are 3 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `episodes.csv`\n    Downloading file 2 of 3: `sauces.csv`\n    Downloading file 3 of 3: `seasons.csv`\n\n\n--- Download complete ---\n\nepisodes &lt;- tuesdata$episodes\nsauces &lt;- tuesdata$sauces\nseasons &lt;- tuesdata$seasons\n\nThe data is taken from some show where apparently people are interviewed while eating hot sauce. As the interview proceeds (as measured by the question number), the hot sauces get hotter."
  },
  {
    "objectID": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#how-much-hotter",
    "href": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#how-much-hotter",
    "title": "Tidy Tuesday Twofer (32 and 33)",
    "section": "How much hotter?",
    "text": "How much hotter?\nI made a factor out of the sauce_number/ question number. You can see the x-axis label is nicer for the version with the factor sauce_number2.\n\nColumn Plot\n\n#making a factor\nsauces &lt;- sauces %&gt;%\n    mutate(sauce_number2 = factor(sauce_number))\n\n#numeric\nggplot(sauces, aes(sauce_number, scoville, color = season)) +\n    geom_col(position = \"dodge2\") \n\n\n\n#factor\nggplot(sauces, aes(sauce_number2, scoville, color = season)) +\n    geom_col(position = \"dodge2\") \n\n\n\n\nAnd having that variable as a factor allows for a really nice box plot as well. ggplot generates a box plot for each level of the factor and displays them in a single plot. Using the numeric form of the variable gives a warning that it is expecting a group and puts everything into a single box plot. (You can add group = sauce_number to the aes to recreate the plot you get straight out of the box with the factor version.)\n\n\nHistogram\n\n#numeric\n  ggplot(sauces, aes(sauce_number, scoville)) + \n    scale_y_log10() + \n    geom_boxplot()\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n#factor\n  ggplot(sauces, aes(sauce_number2, scoville)) + \n    scale_y_log10() + \n    geom_boxplot()\n\n\n\n\nThe increase in heat level as the questions proceed looks like it has exponential behavior to me. Also looks like some questions # have more variation in the heat level. Questions 8 and 10 seem to have settled in on a specific sauce after the first few seasons.\n\n\nQuestion 10 Sauces\n\nsauces %&gt;% filter(sauce_number == 10) %&gt;% \n  select(season, sauce_name, scoville) %&gt;%\n  gt()\n\n\n\n\n\n  \n    \n    \n      season\n      sauce_name\n      scoville\n    \n  \n  \n    1\nMad Dog 357\n357000\n    2\nBlair's Mega Death Sauce\n550000\n    3\nBlair's Mega Death Sauce\n550000\n    4\nThe Last Dab\n2000000\n    5\nHot Ones – The Last Dab (Reaper Edition)\n2000000\n    6\nHot Ones – The Last Dab Reduxx\n2000000\n    7\nHot Ones – The Last Dab Reduxx\n2000000\n    8\nHot Ones – The Last Dab: Reduxx\n2000000\n    9\nHot Ones – The Last Dab Reduxx\n2000000\n    10\nHot Ones – The Last Dab XXX\n2000000\n    11\nHot Ones – The Last Dab XXX\n2000000\n    12\nHot Ones – The Last Dab XXX\n2000000\n    13\nHot Ones – The Last Dab: Apollo\n2000000\n    14\nHot Ones – The Last Dab: Apollo\n2000000\n    15\nHot Ones – The Last Dab: Apollo\n2000000\n    16\nHot Ones – The Last Dab: Apollo\n2000000\n    17\nHot Ones – The Last Dab: Apollo\n2000000\n    18\nHot Ones – The Last Dab: Apollo\n2000000\n    19\nHot Ones – The Last Dab: Apollo\n2000000\n    20\nHot Ones – The Last Dab: Apollo\n2000000\n    21\nHot Ones – The Last Dab: Apollo\n2000000\n  \n  \n  \n\n\n\n\nSo it looks like once they found a 2 million scoville sauce they used variations of it or rebranded it as a show tie in for the remaining seasons.\n\n\nLog Scale Column Plot\nFor exponential data, you can plot on a log scale to see more details. (Season 8 and 10 really stand out with their flat tops.)\n\nggplot(sauces, aes(sauce_number2, scoville, color = season)) +\n  geom_col(position = \"dodge2\") +\n  scale_y_log10()\n\n\n\n\nIt looks like there are a few different regimes. The first three or four questions, the heat level rises sharply with each question. Then for the middle questions, the increase is somewhat more gradual. For the last two or three questions, the heat level again rises steeply.\n\n\nAverage Heat per Question\nThis might be more easily seen by plotting the average heat for each question across all seasons.\n\naverage_sauce &lt;- sauces %&gt;% group_by(sauce_number) %&gt;% summarize(hot = mean(scoville))\n\nggplot(average_sauce, aes(x= sauce_number, y = hot)) +\n    geom_point() +\n   scale_y_log10()\n\n\n\n\nThat seems pretty clear that there are 3 regions.\n\n\nExponential Fit\nBut, we get a decent-ish fit just assuming an exponential increase. I’m not doing anything fancy here. I’m just using geom_smooth and passing it an exponential formula. This isn’t serious model fitting, this is more a guide to the eye.\n\nggplot(sauces, aes(x = sauce_number, y =  scoville)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = (y ~ exp(x)))\n\n\n\nggplot(average_sauce, aes(x = sauce_number, y = hot)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = (y ~ exp(x)))"
  },
  {
    "objectID": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#what-does-this-mean",
    "href": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#what-does-this-mean",
    "title": "Tidy Tuesday Twofer (32 and 33)",
    "section": "What does this mean?",
    "text": "What does this mean?\nHonestly, probably nothing. :) It is possible that the producers were trying to have some sort of exponential increase in the heat level, so the experience got much worse with each question. But I doubt anyone sat down and simulated what Scoville levels they needed for each question."
  },
  {
    "objectID": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#load-the-data",
    "href": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#load-the-data",
    "title": "Tidy Tuesday Twofer (32 and 33)",
    "section": "Load the data",
    "text": "Load the data\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 33)\n\n--- Compiling #TidyTuesday Information for 2023-08-15 ----\n\n\n--- There is 1 file available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 1: `spam.csv`\n\n\n--- Download complete ---\n\nspam &lt;- tuesdata$spam\n\nAll the variables are complete. This is a subset of the data submitted to the UCI Machine Learning Repository. Looking at the data dictionary, we might expect all the variable to be positively correlated with spam.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncrl.tot\ndouble\nTotal length of uninterrupted sequences of capitals\n\n\ndollar\ndouble\nOccurrences of the dollar sign, as percent of total number of characters\n\n\nbang\ndouble\nOccurrences of ‘!’, as percent of total number of characters\n\n\nmoney\ndouble\nOccurrences of ‘money’, as percent of total number of characters\n\n\nn000\ndouble\nOccurrences of the string ‘000’, as percent of total number of words\n\n\nmake\ndouble\nOccurrences of ‘make’, as a percent of total number of words\n\n\nyesno\ncharacter\nOutcome variable, a factor with levels ‘n’ not spam, ‘y’ spam\n\n\n\nI’m using skim to examine the data. I’ve discussed it before here; it is a great tool that gives more information than summary.\n\nskim(spam)\n\n\nData summary\n\n\nName\nspam\n\n\nNumber of rows\n4601\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nyesno\n0\n1\n1\n1\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncrl.tot\n0\n1\n283.29\n606.35\n1\n35\n95\n266.00\n15841.00\n▇▁▁▁▁\n\n\ndollar\n0\n1\n0.08\n0.25\n0\n0\n0\n0.05\n6.00\n▇▁▁▁▁\n\n\nbang\n0\n1\n0.27\n0.82\n0\n0\n0\n0.32\n32.48\n▇▁▁▁▁\n\n\nmoney\n0\n1\n0.09\n0.44\n0\n0\n0\n0.00\n12.50\n▇▁▁▁▁\n\n\nn000\n0\n1\n0.10\n0.35\n0\n0\n0\n0.00\n5.45\n▇▁▁▁▁\n\n\nmake\n0\n1\n0.10\n0.31\n0\n0\n0\n0.00\n4.54\n▇▁▁▁▁"
  },
  {
    "objectID": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#is-this-an-imbalanced-data-set",
    "href": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#is-this-an-imbalanced-data-set",
    "title": "Tidy Tuesday Twofer (32 and 33)",
    "section": "Is this an imbalanced data set?",
    "text": "Is this an imbalanced data set?\nOften classification data sets have much more normal data than abnormal data. Are there reasonable numbers of spam entries in this collection?\n\nggplot(spam, aes(yesno)) + geom_bar()\n\n\n\n\nThat’s not too bad. I’m going to calculate the percentage of spam messages by converting this to a numerical variable and taking the mean. I need a number anyway for my correlation plot.\n\nspam &lt;- spam %&gt;%\n  mutate(yesno_num = ifelse(yesno== 'y', 1, 0))\n\nmean(spam$yesno_num)\n\n[1] 0.3940448"
  },
  {
    "objectID": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#correlation-plot",
    "href": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#correlation-plot",
    "title": "Tidy Tuesday Twofer (32 and 33)",
    "section": "Correlation plot",
    "text": "Correlation plot\nOne of my all time favorite packages is corrplot. Correlations can suggest what variables are likely to be important to the outcome and they can also flag potential issues that could arise from multicollinearity among the predictors. I’m normally default to a table over a viz, but corrplot presents the data so beautifully that I just can’t resist using it.\nA correlation plot is pretty technical, so I probably would not use it in most circumstances. I use it in my own EDA but I wouldn’t include it in a communication to a general audience. If I were sharing this, I’d clean up the variable names to be clearer.\nCorrelations need to be calculated between numeric variables, so I am removing the categorical yesno and keeping my numerical one.\nCorrplot has so many different customizations. I’ve annotated my code to reflect what the different parameters do, but there are dozens of others that can be used for more customization. I like to include the actual numerical values (addCoef.col), omit the diagonal since it will be all ones (diag) and only show one half of the matrix ( type = ‘lower’ or ‘upper’). I also like to have the magnitude (abs value ) reflected by the size of the circle and the value (including sign reflected by the color). The features in this data set are all positively correlated with each other and\nSometimes labels get cropped. This might need to be fixed via the margin parameter (mar) within the call to corrplot or via the par statement before the call.\n\npar(xpd = TRUE) # allows corrplot labels into the margin. fixes clipping\n\nspam %&gt;% select(-yesno) %&gt;%\n  cor %&gt;%\n  {.[order(abs(.[, 7]), decreasing = TRUE),\n      order(abs(.[, 7]), decreasing = TRUE)]} %&gt;%\n  corrplot(\n    method = 'circle', #circle is default and I think it is the best anyway\n    type = 'lower', # upper, lower, or full\n    tl.col = 'black', #color of text label\n    addCoef.col = 'black',#color of the coefficients\n    cl.ratio = 0.2, #size of the color bar legend\n    tl.srt = 15, # this sets the angle of the text\n    col = COL2('PuOr', 10),   #this sets the color palette, COL2 is diverging\n    diag = FALSE, # don't show diag\n    mar = c(1, 1, 4, 1), \n    title = \"What features are correlated with Spam?\",\n  )\n    title(sub= \"Data from UCI Machine Learning Repository via Rdatasets\")\n\n\n\n\nAll of them have some positive correlation. None of the predictors look strongly correlated with each other either."
  },
  {
    "objectID": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#what-would-i-do-next-if-i-were-going-to-model-this-data-set",
    "href": "posts/2023-08-15-TidyTuesday-Twofer/tidytuesday-twofer.html#what-would-i-do-next-if-i-were-going-to-model-this-data-set",
    "title": "Tidy Tuesday Twofer (32 and 33)",
    "section": "What would I do next if I were going to model this data set?",
    "text": "What would I do next if I were going to model this data set?\nI’ve written about classification problems before and I’d probably start with the fitting methods I used there.\nAll of the numerical variables had pretty skewed distributions based on the skim output. Lots of models require more normally distributed data. I’d transform the data and scale and normalize it as well. There is a great table in the Tidy Modeling with R which goes over which preprocessing steps are required or beneficial for different types of fitting."
  },
  {
    "objectID": "posts/2023-08-29-TidyTuesday-fair-use/tidytuesday35-fair-use.html",
    "href": "posts/2023-08-29-TidyTuesday-fair-use/tidytuesday35-fair-use.html",
    "title": "TidyTuesday 35: Exploring Fair Use Cases",
    "section": "",
    "text": "Today’s TidyTuesday concerns US copyright law. Fair use is the right to use copyrighted materials in some instances. Fair use law isn’t always clear, and there is often litigation to decide whether something is fair use. This week’s TidyTuesday uses a data set created by web scraping to get information about federal court cases on fair use. This week’s data comes with the following warning:\n\nThere are two datasets this week for which the rows align, but the values might not precisely line up for a clean join -- a case you often have to deal with in real-world data.\n\nI think I want to do a visualization around that comment- what percentage of the data doesn’t precisely align, and are there common types of mismatches?"
  },
  {
    "objectID": "posts/2023-08-29-TidyTuesday-fair-use/tidytuesday35-fair-use.html#introoverview",
    "href": "posts/2023-08-29-TidyTuesday-fair-use/tidytuesday35-fair-use.html#introoverview",
    "title": "TidyTuesday 35: Exploring Fair Use Cases",
    "section": "",
    "text": "Today’s TidyTuesday concerns US copyright law. Fair use is the right to use copyrighted materials in some instances. Fair use law isn’t always clear, and there is often litigation to decide whether something is fair use. This week’s TidyTuesday uses a data set created by web scraping to get information about federal court cases on fair use. This week’s data comes with the following warning:\n\nThere are two datasets this week for which the rows align, but the values might not precisely line up for a clean join -- a case you often have to deal with in real-world data.\n\nI think I want to do a visualization around that comment- what percentage of the data doesn’t precisely align, and are there common types of mismatches?"
  },
  {
    "objectID": "posts/2023-08-29-TidyTuesday-fair-use/tidytuesday35-fair-use.html#setting-up",
    "href": "posts/2023-08-29-TidyTuesday-fair-use/tidytuesday35-fair-use.html#setting-up",
    "title": "TidyTuesday 35: Exploring Fair Use Cases",
    "section": "Setting Up",
    "text": "Setting Up\n\nLoading Libraries\n\nlibrary(tidyverse) # Who doesn't want to be tidy?\nlibrary(gt) # for nice tables\nlibrary(cowplot) # for composite graphs\nlibrary(ggthemes) # themes for ggplot\n\n\n\nLoading Data\nReading in the data using the tidytuesday package.\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 35)\n\n--- Compiling #TidyTuesday Information for 2023-08-29 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `fair_use_cases.csv`\n    Downloading file 2 of 2: `fair_use_findings.csv`\n\n\n--- Download complete ---\n\nfair_use_cases &lt;- tuesdata$fair_use_cases\nfair_use_findings &lt;- tuesdata$fair_use_findings"
  },
  {
    "objectID": "posts/2023-08-29-TidyTuesday-fair-use/tidytuesday35-fair-use.html#preliminary-exploratory-data-analysis",
    "href": "posts/2023-08-29-TidyTuesday-fair-use/tidytuesday35-fair-use.html#preliminary-exploratory-data-analysis",
    "title": "TidyTuesday 35: Exploring Fair Use Cases",
    "section": "Preliminary Exploratory Data Analysis",
    "text": "Preliminary Exploratory Data Analysis\nLooking at the cleaning script, it appears that the fair_use_cases comes from the table found at copyright.gov. This table has some summary information about the case and a link to the court rulings (a pdf). The fair_use_findings dataframe contains additional information about the case extracted from the pdf of the court rulings. Indeed, there are 251 rows in each dataframe, and the way the dataframes are generated, they should match up.\n\nfair_use_cases %&gt;% \n  head() %&gt;%\n  gt() \n\n\n\n\n\n  \n    \n    \n      case\n      year\n      court\n      jurisdiction\n      categories\n      outcome\n      fair_use_found\n    \n  \n  \n    De Fontbrune v. Wofsy, 39 F.4th 1214 (9th Cir. 2022)\n2022\n9th Circuit\n9th Circuit\nEducation/Scholarship/Research; Photograph\nFair use not found\nFALSE\n    Sedlik v. Von Drachenberg, No. CV 21-1102 (C.D. Cal. May 31, 2022)\n2022\nC.D. Cal.\n9th Circuit\nPainting/Drawing/Graphic; Photograph\nPreliminary finding; Fair use not found\nFALSE\n    Sketchworks Indus. Strength Comedy, Inc. v. Jacobs, No. 19-CV-7470-LTS-VF (S.D.N.Y. May 12, 2022)\n2022\nS.D.N.Y.\n2nd Circuit\nFilm/Audiovisual; Music; Parody/Satire; Review/Commentary\nFair use found\nTRUE\n    Am. Soc'y for Testing & Materials v. Public.Resource.Org, Inc., No. 13-cv-1215 (D.D.C. Mar. 31, 2022)\n2022\nD.D.C.\nDistrict of Columbia Circuit\nEducation/Scholarship/Research; Textual Work; Used in government proceeding\nMixed Result\nFALSE\n    Yang v. Mic Network Inc., Nos. 20-4097-cv, 20-4201-cv (2d Cir. Mar. 29, 2022)\n2022\n2d Circuit\n2nd Circuit\nNews reporting; Photography\nFair use found\nTRUE\n    Viacom Int’l v. Pixi Universal, Civ. Action No H-21-2612 (S.D. Tex. Mar. 25, 2022)\n2022\nS.D. Tex.\n5th Circuit\nPainting/Drawing/Graphic; Parody/Satire\nFair use not found\nFALSE\n  \n  \n  \n\n\n\n\nNow the other dataframe. This one has some huge blocks of text in some fields, so I’m going to add a container to the table to make it have a vertical scroll bar.\n\nfair_use_findings %&gt;% \n  head() %&gt;% \n  gt() %&gt;% \n  tab_options(container.height = px(300), container.padding.y = px(24))\n\n\n\n\n\n  \n    \n    \n      title\n      case_number\n      year\n      court\n      key_facts\n      issue\n      holding\n      tags\n      outcome\n    \n  \n  \n    De Fontbrune v. Wofsy\n39 F.4th 1214 (9th Cir. 2022)\n2022\nUnited States Court of Appeals for the Ninth Circuit\nPlaintiffs own the rights to a catalogue comprised of 16,000 photographs of Pablo Picasso’s work, which was originally compiled by Picasso’s friend in 1932 (the “Zervos Catalogue”). In 1995, after obtaining permission from Picasso’s estate to publish a work illustrating and describing works by Picasso, Defendants Alan Wofsy and his company Alan Wofsy & Associates began publishing The Picasso Project—–a series of volumes reproducing images of Picasso’s work, including 1,492 photographs from the Zervos Catalogue. Plaintiffs sued for copyright infringement. A French court held the photographs were protected by copyright because they “added creative features through deliberate choices of lighting, the lens, filters, [and] framing or angle of view.” In 2001, Plaintiffs obtained a judgment in France that subjected Defendants to damages for any further acts of infringement. In 2012, after discovering copies of The Picasso Project in a French bookstore, Plaintiffs enforced their judgment in France and were awarded €2 million. Plaintiffs sought recognition of the judgment in the U.S. courts. The district court granted summary judgment for Defendants, determining that the French judgment was “repugnant to U.S. public policy protecting free expression” because it failed to provide a fair use defense. Plaintiffs appealed; and Defendants cross-appealed on other defenses.\nWhether reproduction of photographs documenting artwork in a reference book that was sold commercially is a fair use.\nThe panel held that the first factor, the purpose and character of the use, weighed against fair use because Defendants conceded that The Picasso Project was a commercial venture and the use at issue—reproduction of the photographs in a book illustrating Picasso’s works—was not transformative. Specifically, the court noted that Defendants’ use “did not serve an ‘entirely different function’ than the originals,” but had overlapping purposes, and the insertion of informative captions did not “necessarily” transform the works. The second factor, the nature of the copyrighted work, did not favor fair use because, although the works were published and documentary in nature, the French court had concluded that the photographs exhibited creative elements. The court determined that the third factor, the amount and substantiality of the work used, weighed against fair use because Defendants failed to demonstrate that “copying the entirety of each photograph was necessary.” The fourth factor, the effect of the use upon the potential market for or value of the copyrighted work, also weighed against fair use because there is a presumption of market harm when the use is commercial and non-transformative. Although Defendants presented evidence that auction prices for the Zervos Catalogue increased while The Picasso Project was on the market, Defendants had not provided evidence that “widespread appropriation” of the works would not harm the market for the photographs. Weighing all the factors, the court had “serious doubts” that fair use would protect Defendants’ use, and, accordingly, granted summary judgment to Plaintiffs on the public policy defense.\nEducation/Scholarship/Research; Photograph\nFair use not found\n    Sedlik v. Von Drachenberg\nNo. CV 21-1102, 2022 WL 2784818 (C.D. Cal. May 31, 2022)\n2022\nUnited States District Court for the Southern District of New York\nPlaintiff Jeffrey Sedlik is a photographer who created an iconic portrait of musician Miles Davis, which he has licensed for various uses (the “Portrait”). Defendant Katherine Von Drachenberg, professionally known as Kat Von D, is a celebrity tattooist. In 2017, Kat Von D inked a tattoo on the arm of a friend, Blake Farmer, as a gift. Farmer chose the Portrait as the reference image for his tattoo. Kat Von D traced a printout of the Portrait to create a line drawing and stencil to transfer to Farmer’s arm, then completed the tattoo freehand. Kat Von D and the tattoo shop hosting her both posted a photograph of Kat Von D tattooing Farmer’s arm with the Portrait in the background, as well as a photograph of the finished tattoo. Kat Von D also posted a short video of herself inking the tattoo. Sedlik sued and moved for summary judgment on his claim of copyright infringement. Defendants moved for summary judgment, asserting fair use.\nWhether use of a photograph as the reference image for a realistic tattoo is fair use.\nConsidering the first fair use factor, the purpose and character of the use, the court found triable issues as to the transformativeness and commercial nature of the work. The court rejected Kat Von D’s arguments that the tattoo was transformative because of Farmer’s personal connection to the image in the Portrait and by nature of it being permanently imprinted on Farmer’s body. The court concluded that a jury should determine whether the visual differences between the Portrait and the tattoo, such as additional shading and the elimination of the black background, are significant enough to render the tattoo transformative. The court also stated that a jury should decide whether Kat Von D’s use of the work was commercial, noting that she did not charge Farmer for the tattoo but could have derived an indirect economic benefit from promotion of the tattoo on social media. The court found that the second factor, the nature of the copyrighted work, favored fair use because although the Portrait is creative, it was published several decades ago and has been widely disseminated. The third factor, the amount and substantiality of the portion used, weighed against fair use because Kat Von D chose to copy certain expressive elements of the Portrait that were not necessary to achieve her stated purpose of expressing “a sentiment of melancholy.” On the fourth factor, the effect of the use upon the potential market for or value of the copyrighted work, while the court concluded the tattoo was not a substitute in the primary market for the Portrait, it found that Sedlik raised a triable issue as to whether a future market exists for licensing the Portrait for use in creating tattoos. Because the court found triable issues concerning the statutory factors, it declined to address a non-statutory factor raised by defendants—“fundamental rights of bodily integrity and personal expression”—and concluded that fair use in this case should be decided by a jury.\nPainting/Drawing/Graphic; Photograph\nPreliminary finding; Fair use not found\n    Sketchworks Indus. Strength Comedy, Inc. v. Jacobs\nNo. 19-CV-7470-LTS-VF, 2022 U.S. Dist. LEXIS 86331 (S.D.N.Y. May 12, 2022)\n2022\nUnited States District Court for the Southern District of New York\nPlaintiff Sketchworks Industrial Strength Comedy, Inc. (“Sketchworks”) is a sketch comedy company that created and owns a copyright in Vape, a stage musical that is an alleged parody of the theatrical work and film, Grease. Vape follows the same characters along roughly the same story-arc and in the same setting as Grease and incorporates portions of the film’s music. Defendants are the trustees for the individual trusts of the co-authors of Grease. Just before Vape was scheduled to be performed, Defendants sent Sketchworks and the theater where Vape was to be performed cease and desist letters, and the performances were cancelled. Sketchworks brought an action against Defendants seeking a declaratory judgment that Vape constitutes fair use of Grease, asserting that Vape is a parody that uses millennial slang, pop culture references, and exaggeration to comment on Grease and criticize its misogynistic and sexist elements. Defendants disputed that Vape is a parody and asserted that it infringes their copyright in Grease. The parties cross- moved for judgment on the pleadings.\nWhether the use of protected elements, including music, plot, characters, dialogue, and setting, from a theatrical work and film to create a parodic stage musical is fair use.\nThe court found that the first factor, the purpose and character of the use, favored fair use because Vape is a parody of Grease and is therefore transformative. Critical to this determination, the court found that Vape juxtaposed “familiar elements from Grease, such as the main characters and the plot arc, with alterations to the script and song lyrics” to highlight the experiences of Grease’s female characters and “comment on how misogynistic tendencies have both evolved since Grease was developed and remain the same.” The second factor, the nature of the copyrighted work, disfavored fair use because Grease’s creative expression fell “within the core of the copyright’s protective purposes.” The court, however, declined to give this factor much weight, reasoning that parodies typically copy publicly known, expressive works. The third factor, the amount and substantiality of the use, weighed in favor of fair use. Although Vape took “substantial elements” from Grease, the court found that “the taking was not excessive” because use of those elements was necessary to achieve Vape’s parodic purpose and communicate its criticism of certain aspects of Grease. The fourth factor, the effect of the use on the potential market for or value of the copyrighted work, also weighed in favor of fair use because any potential harm to Grease’s market value for derivatives was likely “minimal.” While Vape updated some of the language and cultural references from Grease, the court found that Vape “cannot be reasonably viewed as a derivative ‘sequel to, . . . or updated remake, of Grease,’” because its updates were done “in a spirit of mockery.” Further, the court commented that any effect on the demand for derivatives attributable to Vape’s “critical nature” is not remediable under copyright law. Weighing the four factors together, the court concluded that Vape constitutes a fair use of Grease and granted Sketchworks’ motion for judgment on the pleadings.\nFilm/Audiovisual; Music; Parody/Satire; Review/Commentary\nFair use found\n    Am. Soc'y for Testing & Materials v. Public.Resource.Org, Inc.\nNo. 13-cv-1215 (TSC), 2022 U.S. Dist. LEXIS 60922 (D.D.C. Mar. 31, 2022)\n2022\nUnited States District Court for the District of Columbia\nDefendant Public.Resource.Org, Inc., a non-profit organization, has a mission to make the “law and other government materials more widely available so that people, businesses, and organizations can easily read and discuss [the] laws and the operations of government.” Plaintiffs consist of three non-profit standards-developing organizations: (1) “ASTM,” which is focused on industry-related technical and safety standards; (2) “NFPA,” which is focused on safety standards; and (3) “ASHRAE,” which is focused on construction-related standards. Plaintiffs own copyrights in various “voluntary consensus standards,” which are developed by numerous subject matter experts under Plaintiffs’ guidance. Plaintiffs sell PDFs and hard copies of their standards and maintain reading rooms for viewing the standards. Defendant purchased hard copies of Plaintiffs’ standards and, without authorization, scanned and made digital, verbatim, copies freely available online to the public. This case concerns 191 ASTM standards, 23 NFPA standards, and 3 ASHRAE standards that Defendant claims have been incorporated by reference into federal law. Plaintiffs brought copyright, trademark, and unfair competition claims; Defendant countersued, seeking declaratory judgment. The parties filed motions for summary judgment. In 2017, the district court found that all factors weighed against fair use. On appeal, the court of appeals reversed in part and remanded the case back to the district court without a detailed discussion of the fair use factors for additional factual development. On remand, both parties again moved for summary judgment.\nWhether it is fair use to make available online for free a verbatim copy of privately developed standards, which have been incorporated by reference into law, without obtaining authorization from the copyright owner.\nAs directed by the court of appeals, the district court conducted a four-step fair use analysis for each of the 217 allegations of infringement, concluding that Defendant’s reproduction of 184 standards was fair use, reproduction of 32 standards was not fair use, and that portions of the reproduction of 1 standard was fair use. For all 217 standards, the court found that the fourth factor, the effect of the use upon the potential market for or value of the work, favored fair use. Having found that Defendant’s use was noncommercial, the court determined that Plaintiffs did not provide sufficient evidence to show some meaningful likelihood of future harm exists. The court noted that it was “less deferential” to Plaintiffs’ “conclusory opinions” about market harm given that, during the elapsed time since the alleged infringement and the commencement of the litigation Plaintiffs could have provided “economic data and analysis” supporting their arguments. The court also found that Defendant’s reproductions did not have a “substantially adverse impact on the potential market for the originals.” Regarding the 184 standards that the court found Defendant reproduced fairly, the court determined that 153 were incorporated by reference into law and that the other 31 were identical in text to standards incorporated by reference. The court concluded that the first factor, the purpose and character of the use, generally favored fair use because Defendant did not “stand to profit” from the reproduction and that its purpose was “to inform the public about the law and facilitate public debate.” The court noted that Defendant’s use qualified as one that “furthere[d] the purposes” of fair use, and generally provided information “essential for a private entity to comprehend its legal duties,” which weighed “heavily in favor” of fair use. In assessing the second factor, the nature of the copyrighted work, the court considered that “the express text of the law falls plainly outside the realm of copyright protection” and determined that consequently the standards incorporated by reference “are, at best, at the outer edge of ‘copyright's protective purposes.’” Thus, this factor weighed “heavily in favor” of fair use. The court explained that the 184 standards were incorporated into law “without limitation” such that “the consequence of the incorporation by reference is virtually indistinguishable from a situation in which the standard had been expressly copied into law.” The third factor, the amount and substantiality of the portion used, also favored fair use as the court found that “a greater amount of the standard's text might be fairly reproduced” because the incorporating regulations did “not specify” whether certain provisions, or the entire text, of the standards were incorporated by reference into law and did not indicate which specific provisions were “relevant for regulatory guidance.” Balancing the factors, the court found fair use and denied Plaintiffs’ motion for summary judgment regarding these 184 standards. Regarding the 32 standards that the court found were not reproduced fairly, the court noted that these standards were not shown to be incorporated by reference into law and “differ[ed] in substantive ways from those incorporated by reference into law.” Discussing the first factor, the court found that this factor weighed slightly against fair use because Defendant’s purpose of “inform[ing] the public about the law” was not “significantly furthered” by publishing standards with substantive differences from the standards that were incorporated by reference. The second factor weighed against fair use because there was no evidence showing that the standards were incorporated into law. And, although the standards were more factual than creative, the court concluded that these works “fall more squarely within the realm of copyright protection” than standards incorporated into law. The third factor weighed against fair use, as Defendant’s purpose of informing the public about the law “could be achieved with a paraphrase or summary.” The court also noted that “[i]ncorporating one standard by reference does not justify posting provisions of a different version that has not been incorporated into law.” Balancing these factors, the court did not find fair use and denied Defendant’s motion for summary judgment regarding these 32 standards. Regarding the 1 standard where the court found that portions of the reproduced standard were used fairly, only the parts incorporated by reference into a regulation were found to be fair use. In its second factor analysis, distinguishing the portions not incorporated into law, the court found that Defendant’s “wholesale reproduction” of the standard was “harder to justify” because only parts of the standard were incorporated into law.\nEducation/Scholarship/Research; Textual Work; Used in government proceeding\nMixed Result\n    Yang v. Mic Network Inc.\nNos. 20-4097-cv(L), 20-4201-cv (XAP), 2022 U.S. App. LEXIS 8195 (2d Cir. Mar. 29, 2022)\n2022\nUnited States Court of Appeals for the Second Circuit\nPlaintiff Stephen Yang (“Yang”) licensed a photograph he took of Dan Rochkind (“Rochkind”) to the New York Post, which ran the photograph in an article about Rochkind entitled “Why I Won’t Date Hot Women Anymore.” Defendant Mic Network, Inc. (“Mic”) posted its own article entitled “Twitter Is Skewering the 'New York Post' for a Piece on Why a Man ‘Won't Date Hot Women’.” The Mic article included a screenshot of the Post article that captured the headline and a portion of Yang’s photograph. Mic did not obtain a license to use the photograph. In response, Yang sued Mic for copyright infringement, and Mic moved to dismiss, asserting fair use. The district court granted Mic’s motion, concluding that its use of Yang’s photograph was fair use. Yang appealed the order and judgment.\nWhether using a screenshot from an article, including part of a photograph, to report on and criticize the article constitutes fair use of the photograph.\nOn appeal, the court decided that the first factor, the purpose and character of the use, weighed in favor of fair use. As an initial matter, the panel held that it was not error for the district to decide transformativeness on a motion to dismiss in this case because the only two pieces of evidence needed were the original and secondary works. The court held that, in addition to identifying the subject of Mic’s criticism, Mic, also transformed the photograph by critiquing and providing commentary on the Post article. Mic did not use the photograph “merely as an illustrative aid,” and thus its use was for different purpose than the original. The second factor, the nature of the copyrighted work, had limited weight in the court’s analysis after it held that the use was transformative and thus “d[id] not counsel against a finding of fair use.” Likewise, the third factor, the amount and substantiality of the work used, did not disfavor fair use as the court agreed with the district court’s conclusion that Mic’s use of the image was reasonable to satirize the Post article. The court determined that the fourth factor, the effect of the use on the potential market for or value of the work, also favored fair use. The court concluded that Mic’s screenshot was not a competing substitute for Yang’s work because Mic did not simply republish the photograph, but instead used a screenshot consisting of a “significantly” cropped version of the work along with the Post headline. Further, Yang failed to plausibly allege that a market exists for “photographs that happen to be featured in news articles criticizing the original article in which the photograph appeared.” Weighing the factors together, the court concluded that the district court properly dismissed Yang’s copyright infringement claim on fair use grounds.\nNews Reporting; Photography\nFair use found\n    Viacom Int’l v. Pixi Universal\nCiv. Action No H-21-2612, 2022 U.S. Dist. LEXIS 57400 (S.D. Tex. Mar. 25, 2022)\n2022\nUnited States District Court for the Southern District of Texas\nPlaintiff Viacom International Inc. (“Viacom”) is the owner of the SpongeBob SquarePants (“SpongeBob”) entertainment franchise and holds over 400 copyright registrations for SpongeBob works, including the animated television series, feature films, two-dimensional drawings, and stylebooks featuring extensive artwork from the franchise (the “Works”). The SpongeBob television series features a fictional fast-food restaurant called the “Krusty Krab,” which Viacom once recreated as an immersive experience at a comics convention in 2019. Defendant Pixi Universal, LLC (“Pixi”) operates themed “pop-up” restaurants and bars. In 2021, Pixi recreated the Krusty Krab as a pop-up called “The Rusty Krab,” which featured recreations of the fictional restaurant and other elements of the SpongeBob series. Pixi charged a fee for admission and for food and drink. After demanding that Pixi cease using SpongeBob intellectual property, Viacom brought copyright and trademark infringement claims and sought injunctive relief. Pixi asserted fair use as its sole defense to the copyright claim.\nWhether using elements of copyrighted works to create a themed “pop-up” business based on those works is fair use.\nThe court decided that the first factor, the purpose and character of the use, weighed against fair use. The court found that Pixi’s use was wholly commercial and that it did not transform the Works by merely changing the medium of expression. Instead, Pixi’s use shared the same purpose as the original, that is, “light-hearted entertainment.” Rejecting Pixi’s “post-hoc characterization” of its pop-up as a parody, the court found that Pixi embraced, replicated, and intended to profit from the Works. The second factor, the nature of the copyrighted work, also weighed against fair use because of the “fictional, imaginative nature” of the Works. The third factor, the amount and substantiality of the work used, likewise weighed against fair use. The court determined that, although Pixi did not copy all the details of the SpongeBob universe or even specific episode storylines, Pixi’s use of “central, principal characters and iconic locals and backgrounds from the series” was nonetheless “substantial” as it used the “heart” of the Works. Lastly, the court determined that the fourth factor, the effect of the use on the potential market for or value of the work, also weighed against fair use. The court found that the type of immersive experience that Pixi created, which was based exclusively on Viacom’s SpongeBob franchise, affected the potential market for Viacom to venture into creating or licensing derivative immersive works. Because the factors together weighed against a finding of fair use, the court concluded that Viacom was likely to succeed on its copyright infringement claim and granted its motion for preliminary injunctive relief.\nPainting/Drawing/Graphic; Parody/Satire\nFair use not found\n  \n  \n  \n\n\n\n\nThe case field in fair_use_cases should contain the information found in the fields title and case_number in the fair_use_findings dataframe. The court and jurisdiction from fair_use_cases should also somehow relate to the court listed in fair_use_findings.\nI’m going to start with the case, title, and case_number.\n\nfair_use_cases %&gt;% \n  select(case) %&gt;% \n  head() %&gt;%\n  gt()\n\n\n\n\n\n  \n    \n    \n      case\n    \n  \n  \n    De Fontbrune v. Wofsy, 39 F.4th 1214 (9th Cir. 2022)\n    Sedlik v. Von Drachenberg, No. CV 21-1102 (C.D. Cal. May 31, 2022)\n    Sketchworks Indus. Strength Comedy, Inc. v. Jacobs, No. 19-CV-7470-LTS-VF (S.D.N.Y. May 12, 2022)\n    Am. Soc'y for Testing & Materials v. Public.Resource.Org, Inc., No. 13-cv-1215 (D.D.C. Mar. 31, 2022)\n    Yang v. Mic Network Inc., Nos. 20-4097-cv, 20-4201-cv (2d Cir. Mar. 29, 2022)\n    Viacom Int’l v. Pixi Universal, Civ. Action No H-21-2612 (S.D. Tex. Mar. 25, 2022)\n  \n  \n  \n\n\n\n\nIt looks like the case number is formatted differently across the entries. This may be due to different style guides in different circuits/ courts.\nNow look at the same information from the other dataframe.\n\nfair_use_findings %&gt;% \n  select(title, case_number) %&gt;% \n  head() %&gt;%\n  gt()\n\n\n\n\n\n  \n    \n    \n      title\n      case_number\n    \n  \n  \n    De Fontbrune v. Wofsy\n39 F.4th 1214 (9th Cir. 2022)\n    Sedlik v. Von Drachenberg\nNo. CV 21-1102, 2022 WL 2784818 (C.D. Cal. May 31, 2022)\n    Sketchworks Indus. Strength Comedy, Inc. v. Jacobs\nNo. 19-CV-7470-LTS-VF, 2022 U.S. Dist. LEXIS 86331 (S.D.N.Y. May 12, 2022)\n    Am. Soc'y for Testing & Materials v. Public.Resource.Org, Inc.\nNo. 13-cv-1215 (TSC), 2022 U.S. Dist. LEXIS 60922 (D.D.C. Mar. 31, 2022)\n    Yang v. Mic Network Inc.\nNos. 20-4097-cv(L), 20-4201-cv (XAP), 2022 U.S. App. LEXIS 8195 (2d Cir. Mar. 29, 2022)\n    Viacom Int’l v. Pixi Universal\nCiv. Action No H-21-2612, 2022 U.S. Dist. LEXIS 57400 (S.D. Tex. Mar. 25, 2022)"
  },
  {
    "objectID": "posts/2023-08-29-TidyTuesday-fair-use/tidytuesday35-fair-use.html#matching-the-case-titles",
    "href": "posts/2023-08-29-TidyTuesday-fair-use/tidytuesday35-fair-use.html#matching-the-case-titles",
    "title": "TidyTuesday 35: Exploring Fair Use Cases",
    "section": "Matching the Case Titles",
    "text": "Matching the Case Titles\nIt looks like the titles are styled in the same way. If the five entries here are representative of the dataset as a whole, then I don’t think joining these two files will be difficult.\nI’m going to use str_detect() to match the title from the fair_use_findings data frame (the pattern) with the case name from the fair_use_cases (the string). This will return TRUE if the pattern is contained within the string and a FALSE otherwise. Note that this needs to be an exact match. There can be extra characters before or after the pattern in the string, but the pattern portion must match exactly.\nI talked about this in more depth in a previous tidytuesday, where I joined data frames from Arlington County and the Historical Markers TidyTuesday dataset based on partial matches like these.\n\nstr_detect(string = fair_use_cases$case,  pattern = fair_use_findings$title)\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [61] FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [73]  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[109] FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[169]  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[181]  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[205]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n[217] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n[229]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n\n\nOkay, so some of them don’t match. How many and why?\n\nmatches_title &lt;- str_detect(string = fair_use_cases$case,  pattern = fair_use_findings$title) \n\nHow many? Remember, you can treat booleans as numbers, so sum will give you the total number of correct matches, and mean will give you the percentage of matches.\n\nnrow(fair_use_cases) - sum(matches_title)\n\n[1] 33\n\n\n\nWhat Case Names don’t Match?\nOnly 33 cases didn’t match. I’m going to look at the ones that don’t match.\n\nfair_use_cases$case[!matches_title]\n\n [1] \"Nat'l Acad. of TV Arts & Scis., Inc., v. Multimedia Sys. Design, Inc. No. 20-CV-7269 (VEC) (S.D.N.Y. July 30, 2021)\"    \n [2] \"Tresóna Multimedia v. Burbank High School Vocal Music Ass’n, Nos. 17-56006, 17-56417, 17-56419 (9th Cir. Mar. 24, 2020)\"\n [3] \"Cambridge Univ. Press v. Becker, No. No. 1:08-cv-1425-ODE (N.D. Ga. Mar. 2, 2020)\"                                      \n [4] \"Barcroft Media, Ltd. v. Coed Media Group, LLC, No. 16-CV-7634 (JMF) (S.D.N.Y. Nov. 2, 2017)\"                            \n [5] \"Penguin Random House v. Colting, No. 17-cv-386 (S.D.N.Y. Sept. 8, 2017)\"                                                \n [6] \"Peteski Productions, Inc. v. Rothman, No. 5:17-CV-00122 (E.D. Tex. Aug. 30, 2017)\"                                      \n [7] \"Hosseinzadeh v. Klein, No. 16-CV-3081 (S.D.N.Y. Aug. 23, 2017)\"                                                         \n [8] \"Paramount Pictures, Corp. v. Axanar Prods., Inc., No. 2:15-cv-09938-RGK-E (C.D. Cal. Jan. 3, 2017)\"                     \n [9] \"Cambridge University Press v. Becker, 1:08-cv-01425-ODE (N.D. Ga. March 31, 2016)\"                                      \n[10] \"Katz v. Google Inc., No. 14-14525 (11th Cir. Sept. 17, 2015)\"                                                           \n[11] \"Penguin Grp. (USA), Inc. v. Am. Buddha, No. 4:13-cv-02075-JGZ (D. Ariz. May 11, 2015)\"                                  \n[12] \"Bouchat v. Balt. Ravens Ltd. P'ship, 737 F.3d 932 (4th Cir. 2013)\"                                                      \n[13] \"Fox Broad. Co. v. Dish Network, L.L.C., 723 F.3d 1067 (9th Cir. 2013)\"                                                  \n[14] \"Faulkner Literary Rights, L.L.C. v. Sony Pictures Classics, Inc., 953 F. Supp. 2d 701 (N.D. Miss. 2013)\"                \n[15] \"Brownmark Films, L.L.C. v. Comedy Partners, 682 F.3d 687 (7th Cir. 2012)\"                                               \n[16] \"Northland Family Planning Clinic, Inc. v. Crt. for Bio-Ethical Reform, 868 F. Supp. 2d 962 (C.D. Cal. 2012)\"            \n[17] \"CCA and B, LLC v. F + W Media, Inc., 819 F. Supp. 2d 1310 (Ν.D. Ga. 2011)\"                                              \n[18] \"Bouchat v. Balt. Ravens Ltd. P'ship, 619 F.3d 301 (4th Cir. 2010)\"                                                      \n[19] \"Veeck v. S. Bldg. Code Cong. Int’l, 241 F.3d 398 (5th Cir. 2001)\"                                                       \n[20] \"Sony Computer Entm't. Inc. v. Connectix Corp., 203 F. 3d 596 (9th Cir. 2000)\"                                           \n[21] \"Castle Rock Entm’t, Inc. v. Carol Publ’g Grp., Inc., 150 F.3d 132 (2d Cir. 1998)\"                                       \n[22] \"Sundeman v. Seajay Soc’y, Inc., 142 F.3d 194 (4th Cir. 1998)\"                                                           \n[23] \"L.A. News Serv. v. Reuters Television Int'l, Ltd., 149 F.3d 987 (9th Cir. 1998)\"                                        \n[24] \"Dr. Seuss Enters., L.P. v. Penguin Books USA, Inc., 109 F.3d 1394 (9th Cir. 1997)\"                                      \n[25] \"Metro-Goldwyn-Mayer, Inc. v. Am. Honda Motor Co., 900 F. Supp. 1287 (C.D. Cal. 1995)\"                                   \n[26] \"Rotbart v. J.R. O’Dwyer Co., No. 94 Civ. 2091 (JSM) (S.D.N.Y. Feb. 7, 1995)\"                                            \n[27] \"Nat’l Rifle Ass’n of Am. v. Handgun Control Fed’n of Ohio, 15 F.3d 559 (6th Cir. 1994)\"                                 \n[28] \"Educ. Testing Servs. v. Katzman, 793 F.2d 533 (3d Cir. 1986)\"                                                           \n[29] \"Harper & Row Publishers, Inc. v. Nation Enters., 471 U.S. 539 (1985)\"                                                   \n[30] \"Consumers Union, Inc. v. Gen. Signal Corp., 724 F.2d 1044 (2d Cir. 1983)\"                                               \n[31] \"Dow Jones & Co. v. Brd. of Trade of Chi., 546 F. Supp. 113 (S.D.N.Y. 1982)\"                                             \n[32] \"DC Comics, Inc. v. Crazy Eddie, Inc., No. 79 Civ. 3786 (PNL) (S.D.N.Y. Aug. 3, 1979)\"                                   \n[33] \"Eisenschiml v. Fawcett Publ’ns, Inc., 246 F.2d 598 (7th Cir. 1957)\"                                                     \n\n\nLooking at these titles, there isn’t an obvious commonality. Some of them have accent marks or apostrophes in the names, but not all of them.\nWe know how these files were generated, so we might suspect that the same indexing would work for the other dataframe.\n\nfair_use_findings$title[!matches_title]\n\n [1] \"Nat'l Acad. of TV Arts & Scis., Inc. v. Multimedia Sys. Design, Inc.\"                       \n [2] \"Tresóna Multimedia, LLC v. Burbank High School Vocal Music Ass’n\"                           \n [3] \"Cambridge University Press v. Becker\"                                                       \n [4] \"Barcroft Media, Ltd. V. Coed Media Group, LLC\"                                              \n [5] \"Penguin Random House LLC, et al. v. Frederik Colting and Melissa Medina, d/b/a Moppet Books\"\n [6] \"Peteski Productions, Inc. v. Leah Rothman\"                                                  \n [7] \"Matt Hosseinzadeh v. Ethan Klein and Hila Klein\"                                            \n [8] \"Paramount Pictures Corp. v. Axanar Prods., Inc.\"                                            \n [9] \"Cambridge University Press v. Mark P. Becker\"                                               \n[10] \"Katz v. Google, Inc.\"                                                                       \n[11] \"Penguin Grp. (USA), Inc. v. Am. Buddha,\"                                                    \n[12] \"Bouchat v. Balt. Ravens Ltd. P’ship,\"                                                       \n[13] \"Fox Broad. Co. v. Dish Network, LLC,\"                                                       \n[14] \"Faulkner Literary Rights, LLC v. Sony Pictures Classics, Inc.,\"                             \n[15] \"Brownmark Films, LLC v. Comedy Partners,\"                                                   \n[16] \"Northland Family Planning Clinic, Inc. v. Ctr. for Bio-Ethical Reform,\"                     \n[17] \"CCA and B, LLC v. F + W Media, Inc.,\"                                                       \n[18] \"Bouchat v. Balt. Ravens Ltd. P’ship,\"                                                       \n[19] \"Veeck v. S. Bldg. Code Congress Int’l,\"                                                     \n[20] \"Sony Computer Entm’t, Inc. v. Connectix Corp.,\"                                             \n[21] \"Castle Rock Entm’t, Inc. v. Carol Publ. Group, Inc.,\"                                       \n[22] \"Sundeman v. The Seajay Soc’y, Inc.,\"                                                        \n[23] \"L.A. News Serv. v. Reuters Television Int’l, Ltd.,\"                                         \n[24] \"Dr. Seuss Enters., LP v. Penguin Books USA, Inc.,\"                                          \n[25] \"Metro-Goldwyn-Mayer, Inc. v. Am. Honda Motor Co., Inc.,\"                                    \n[26] \"Rotbart v. J.R. O’Dwyer Co., Inc.,\"                                                         \n[27] \"Nat'l Rifle Ass’n of Am. v. Handgun Control Fed’n of Ohio,\"                                 \n[28] \"Educ. Testing Serv. v. Katzman,\"                                                            \n[29] \"Harper & Row Publishers, Inc. v. Nation Enterprises,\"                                       \n[30] \"Consumers Union of U.S., Inc. v. Gen. Signal Corp.,\"                                        \n[31] \"Dow Jones & Co., Inc. v. Bd. of Trade of the City of Chi.,\"                                 \n[32] \"DC Comics Inc. v. Crazy Eddie, Inc.,\"                                                       \n[33] \"Eisenschiml v. Fawcett Publ’n, Inc.,\"                                                       \n\n\nSo, the index does work. Entry 33 is pretty clear- one has “Publ’ns, Inc.” and one has “Publ’n, Inc.”. The first entry has an extra comma in one file and not the other.\n\n\nAre Mistakes in Punctuation the Main Problem?\nIt actually looks like the comma thing might be a common reason they don’t match.\nIf I remove all commas, does my matching improve?\n\ncases_no_comma &lt;- fair_use_cases %&gt;%\n  mutate(case = str_replace_all(case, \",\", \"\"))\n\nfindings_no_comma &lt;- fair_use_findings %&gt;%\n  mutate(title = str_replace_all(title, \",\", \"\"))\n\nmatch_no_comma &lt;- str_detect(string = cases_no_comma$case,  pattern = findings_no_comma$title)\n\nHow many?\n\n251- sum(match_no_comma)\n\n[1] 29\n\n\nOkay, so that is four more matches.\nI could go through and remove punctuation one by one. That’s pretty tedious.\n\n\nDoes Matching on the First 10 Characters Improve the Match?\nWhat if I match on the first ten letters? Most of the punctuation is towards the middle or the end of the title. Ten is a wild guess based on looking at the data. I can’t match on the first word because something like De would likely match on multiple names.\n\nfindings_trunc_10 &lt;- fair_use_findings %&gt;%\n  mutate(title_10 = str_sub(title, 1, 10))\n\nNow match.\n\nmatch_trunc_10 &lt;- str_detect(string = fair_use_cases$case,  pattern = findings_trunc_10$title_10)\n\nHow many?\n\n251 - sum(match_trunc_10)\n\n[1] 3\n\n\nOkay, so that seems to catch most of the errors."
  },
  {
    "objectID": "posts/2023-08-29-TidyTuesday-fair-use/tidytuesday35-fair-use.html#are-longer-case-titles-more-likely-to-have-typos",
    "href": "posts/2023-08-29-TidyTuesday-fair-use/tidytuesday35-fair-use.html#are-longer-case-titles-more-likely-to-have-typos",
    "title": "TidyTuesday 35: Exploring Fair Use Cases",
    "section": "Are Longer Case Titles More Likely to Have Typos?",
    "text": "Are Longer Case Titles More Likely to Have Typos?\nSo, as the title gets longer and more complicated, is it more likely to contain typos? I’m going to look at the length of the title vs. the chances it doesn’t match. I’m going to combine the two datasets, so I’m working with a single dataframe. Again, I know how the two datasets were created (generated from the same table), so I know I can just cbind them. If the data came from different places, I’d need a different strategy.\nFirst, I’m going to rename the columns that are shared between the two.\n\nfair_use_cases &lt;- fair_use_cases %&gt;%\n  rename(year_cases = year) %&gt;%\n  rename(court_cases = court)\n\nfair_use_findings &lt;- fair_use_findings %&gt;%\n  rename(year_findings = year) %&gt;%\n  rename(court_findings = court)\n\nNow, combining them.\n\ncombined &lt;- fair_use_cases %&gt;%\n  cbind(fair_use_findings)\n\nI’m going to remove a bunch of columns I don’t care about.\n\ncombined &lt;- combined %&gt;%\n  select( -categories, -outcome, -fair_use_found, -key_facts, -issue, -holding, -tags, -outcome )\n\nNow, I calculate the length of the title.\n\ncombined &lt;- combined %&gt;%\n  mutate(title_length = nchar(title))\n\nWhat is the distribution of lengths?\n\ncombined %&gt;%\n  ggplot(aes(title_length)) +\n  geom_bar()\n\n\n\n\nI probably need to bin the lengths. I’m using floor, but you could use ceiling or round as well.\n\ncombined &lt;- combined %&gt;%\n  mutate(title_length_floor = floor(title_length/10))\n\nPlot the distribution again.\n\ncombined %&gt;%\n  ggplot(aes(title_length_floor * 10)) +\n  geom_bar()\n\n\n\n\nThat’s not a terrible distribution.\nNow, I’m creating a column to indicate if they match.\n\ncombined &lt;- combined %&gt;%\n  mutate(match = str_detect(case, title))\n\nNow, group by the length. I’m calculating the number of records in each bin (n), the mismatch rate as a %, and the number of mismatches.\n\ncombined2 &lt;- combined %&gt;%\n  group_by(title_length_floor) %&gt;%\n  summarise(mismatch_rate = (1 - mean(match))*100, n = n()) %&gt;%\n  mutate(mismatch_count = (n * mismatch_rate/100))\n\n\nA Graph of Title Length And Mismatch Rate\nDual axes plots can be tricky to understand and easy to mislead with. Just like a correlation plot, this is another type of plot that I’d probably reserve for internal use only.\nThe count and rate data share the same x aesthetic, so I define that in the ggplot call. The individual y aesthetics are defined in the geom calls for each layer.\nHere’s the mismatch rate. Coincidentally, the percentage range (0-100 %) and the count range (0-79) are roughly on the same scale, so I don’t need to do anything too weird with my dual axes. If I did, then the sec.axis can be scaled by a formula relative to the first. Here, it is just scaled by one. The left and right axes can be styled differently, so I am color-coding them to match the color of the data points.\n\np1 &lt;- combined2 %&gt;%\n  ggplot(aes(x = title_length_floor * 10)) +\n  geom_point(aes(y = mismatch_rate), color = \"blue\") +\n  geom_line(aes(y = mismatch_rate), color = \"blue\") +\n  geom_col(aes(y = n),  alpha = 0.5) +\n  scale_y_continuous(name = \"% Mismatch\",\n                     # Add a second axis and specify its features\n                     sec.axis = sec_axis(trans = ~ . * 1, name = \"# of Cases\")) +\n  theme_pander() +\n  theme(\n    axis.title.y.left = element_text(colour = \"blue\"),\n    axis.line.y.left = element_line(color = \"blue\"),\n    axis.ticks.y.left = element_line(color = \"blue\"),\n    axis.text.y.left = element_text(colour = \"blue\"),\n    axis.title.y.right = element_text(colour = \"gray28\"),\n    axis.line.y.right = element_line(color = \"gray28\"),\n    axis.ticks.y.right = element_line(color = \"gray28\"),\n    axis.text.y.right = element_text(colour = \"gray28\")\n  ) +\n  xlab(\"Length of the Case Title\") +\n  ggtitle(\"Mismatch Rate Increases as Case Title Length Increases\",\n          subtitle = \"Data from U.S. Copyright Office Fair Use Index\")\n\np1\n\n\n\n\nThe two largest bins have very few records, so I wouldn’t expect that extremely high error rate to persist as more data is added. However, there does seem to be an increasing mismatch rate with length of the title.\n\ncombined2 %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      title_length_floor\n      mismatch_rate\n      n\n      mismatch_count\n    \n  \n  \n    1\n0.000000\n27\n0\n    2\n2.083333\n48\n1\n    3\n15.189873\n79\n12\n    4\n15.686275\n51\n8\n    5\n24.137931\n29\n7\n    6\n21.428571\n14\n3\n    7\n50.000000\n2\n1\n    9\n100.000000\n1\n1\n  \n  \n  \n\n\n\n\n\n\nA Graph of Title Length And Number of Mismatches\nHere’s looking at the same information, but just in terms of counts.\n\np2 &lt;- combined2 %&gt;%\n  ggplot(aes(x = title_length_floor)) +\n  geom_point(aes(y = mismatch_count), color = \"blue\") +\n  geom_col(aes(y = n),  alpha = 0.5) +\n  scale_y_continuous(name = \"# of Mismatches\",\n                     # Add a second axis and specify its features\n                     sec.axis = sec_axis(trans = ~ . * 1, name = \"# of Cases\")) +\n  theme_pander() +\n  theme(\n    axis.title.y.left = element_text(colour = \"blue\"),\n    axis.line.y.left = element_line(color = \"blue\"),\n    axis.ticks.y.left = element_line(color = \"blue\"),\n    axis.text.y.left = element_text(colour = \"blue\"),\n    axis.title.y.right = element_text(colour = \"gray28\"),\n    axis.line.y.right = element_line(color = \"gray28\"),\n    axis.ticks.y.right = element_line(color = \"gray28\"),\n    axis.text.y.right = element_text(colour = \"gray28\")\n  ) +\n  xlab(\"Length of the Case Title\") +\n  ggtitle(\"Number of Mismatches as Case Title Length Increases\",\n          subtitle = \"Data from U.S. Copyright Office Fair Use Index\")\np2\n\n\n\n\n\n\nMaking a Composite/ Panel Figure with cowplot\nMake a composite figure with cowplot.\n\nplot_grid(p1, p2,  ncol = 1)"
  },
  {
    "objectID": "posts/2023-09-05-TidyTuesday-Labor/labor.html",
    "href": "posts/2023-09-05-TidyTuesday-Labor/labor.html",
    "title": "TidyTuesday 36: Visualizing Worker Demographic Information with Treemaps",
    "section": "",
    "text": "This week’s TidyTuesday presents data taken from the Union Membership and Coverage Database from the CPS (Unionstats.com) created by Barry T. Hirsch, David A. Macpherson, and William E. Even. This database contains data about the wages of union and non-union workers from 1973 until today.\nThere is a companion paper:\n\nMacpherson, David A. and Barry T. Hirsch. 2023. “Five decades of CPS wages, methods, and union-nonunion wage gaps at Unionstats.com.” Industrial Relations: A Journal of Economy and Society 62: 439–452. https://doi.org/10.1111/ irel.12330\n\nI highly recommend reading the paper as it clearly illustrates the challenges of working with real-world data collected by 3rd parties. The source data is government survey data. For some key questions, a third of respondents didn’t answer. Imputation was performed (but not always noted) by the government agency, but in such a way that it obscured the trend about wage gaps between union and non-union labor. (Likely, the survey was designed to study something else, and the imputation method was fine for that question. Some years did not even ask about participation in labor unions.) The survey also didn’t always collect detailed information about the salary of the highest earners, simply marking them as being “above the cap.” The paper details numerous such issues and explains how the data was handled to standardize the results over the five decades the database covers. It is a beautiful example of how to handle messy data.\nToday, I will make two types of treemaps using some of the demographic data from this dataset. A treemap is a way of visualizing how various parts relate to the whole. The demographic data seems all related and could be viewed in a treemap form. So, I’m going to pull that out. The UnionStats website also warns, “Note: CPS sample sizes are very small for some cells.  Use such estimates cautiously.” So, we can use the treemap to visualize that caution for the demographic subsets."
  },
  {
    "objectID": "posts/2023-09-05-TidyTuesday-Labor/labor.html#introoverview-to-tidytuesday-36-union-membership-in-the-united-states",
    "href": "posts/2023-09-05-TidyTuesday-Labor/labor.html#introoverview-to-tidytuesday-36-union-membership-in-the-united-states",
    "title": "TidyTuesday 36: Visualizing Worker Demographic Information with Treemaps",
    "section": "",
    "text": "This week’s TidyTuesday presents data taken from the Union Membership and Coverage Database from the CPS (Unionstats.com) created by Barry T. Hirsch, David A. Macpherson, and William E. Even. This database contains data about the wages of union and non-union workers from 1973 until today.\nThere is a companion paper:\n\nMacpherson, David A. and Barry T. Hirsch. 2023. “Five decades of CPS wages, methods, and union-nonunion wage gaps at Unionstats.com.” Industrial Relations: A Journal of Economy and Society 62: 439–452. https://doi.org/10.1111/ irel.12330\n\nI highly recommend reading the paper as it clearly illustrates the challenges of working with real-world data collected by 3rd parties. The source data is government survey data. For some key questions, a third of respondents didn’t answer. Imputation was performed (but not always noted) by the government agency, but in such a way that it obscured the trend about wage gaps between union and non-union labor. (Likely, the survey was designed to study something else, and the imputation method was fine for that question. Some years did not even ask about participation in labor unions.) The survey also didn’t always collect detailed information about the salary of the highest earners, simply marking them as being “above the cap.” The paper details numerous such issues and explains how the data was handled to standardize the results over the five decades the database covers. It is a beautiful example of how to handle messy data.\nToday, I will make two types of treemaps using some of the demographic data from this dataset. A treemap is a way of visualizing how various parts relate to the whole. The demographic data seems all related and could be viewed in a treemap form. So, I’m going to pull that out. The UnionStats website also warns, “Note: CPS sample sizes are very small for some cells.  Use such estimates cautiously.” So, we can use the treemap to visualize that caution for the demographic subsets."
  },
  {
    "objectID": "posts/2023-09-05-TidyTuesday-Labor/labor.html#setting-up",
    "href": "posts/2023-09-05-TidyTuesday-Labor/labor.html#setting-up",
    "title": "TidyTuesday 36: Visualizing Worker Demographic Information with Treemaps",
    "section": "Setting Up",
    "text": "Setting Up\n\nLoading Libraries\n\nlibrary(tidyverse) # who doesn't want to be tidy?\nlibrary(gt) # for nice tables\nlibrary(treemap) # for treemap\nlibrary(d3treeR) # for interactive treemaps\n\n\n\nLoading Data\nThis week’s data is not loaded in the usual way! While the TidyTuesday page was up on Monday, the TidyTuesday package insisted the data was unavailable by either week number or date. So, instead, I loaded it directly from Git Hub.\n\ndemographics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-09-05/demographics.csv')\nwages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-09-05/wages.csv')\nstates &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-09-05/states.csv')\n\nI will work with the 2019 data, since it is recent and pre-pandemic. The demographic data is found in the facet variable and other subcategories (public vs. private sector, industry, etc.).\n\nwages_2019 &lt;- wages %&gt;%\n  filter(year == 2019) \n\nwages_2019 %&gt;% select(facet) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      facet\n    \n  \n  \n    all wage and salary workers\n    construction\n    all wage and salary workers\n    wholesale/retail\n    all wage and salary workers\n    all wage and salary workers\n    all wage and salary workers\n    public administration\n    private sector: all\n    private sector: nonagricultural\n    private sector: construction\n    private sector: manufacturing\n    public sector: all\n    public sector: federal\n    public sector: state government\n    public sector: local government\n    demographics: less than college\n    demographics: college or more\n    demographics: male\n    demographics: female\n    demographics: white male\n    demographics: white female\n    demographics: black male\n    demographics: black female\n    demographics: hispanic male\n    demographics: hispanic female\n  \n  \n  \n\n\n\n\nInterestingly, the overall data (“all wage and salary workers”) appears 5 times."
  },
  {
    "objectID": "posts/2023-09-05-TidyTuesday-Labor/labor.html#selecting-the-demographic-data",
    "href": "posts/2023-09-05-TidyTuesday-Labor/labor.html#selecting-the-demographic-data",
    "title": "TidyTuesday 36: Visualizing Worker Demographic Information with Treemaps",
    "section": "Selecting the Demographic Data",
    "text": "Selecting the Demographic Data\nI will view how the sample size changes over the sex/race demographics. I’m going to use a treemap to do so.\nFirst, I will select the rows where the facet has “demographics” or the “all wage and salary workers”. This is done with str_detect from the stringr package.\n\nwages_2019_demo &lt;- wages_2019 %&gt;%\n  filter(\n    str_detect(facet , \"demographics\") == TRUE |\n      str_detect(facet, \"all wage and salary workers\") == TRUE\n  )\n\nRemove the first four rows- this contains duplicate information about all workers. The college data is also removed since it isn’t related to the other demographic data in a way we can define.\n\nwages_2019_demo2 &lt;- wages_2019_demo[5:15,]\n\nwages_2019_demo2 &lt;- wages_2019_demo2 %&gt;%\n  filter(str_detect(facet , \"college\") == FALSE)\n\nwages_2019_demo2 %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      year\n      sample_size\n      wage\n      at_cap\n      union_wage\n      nonunion_wage\n      union_wage_premium_raw\n      union_wage_premium_adjusted\n      facet\n    \n  \n  \n    2019\n94818\n28.09587\n0.05071279\n31.70222\n27.64805\n0.14663510\n0.14290078\nall wage and salary workers\n    2019\n48157\n31.31306\n0.07093566\n34.02119\n30.96146\n0.09882383\n0.17838623\ndemographics: male\n    2019\n46661\n24.64259\n0.02903222\n28.99576\n24.12817\n0.20173885\n0.10255232\ndemographics: female\n    2019\n40352\n31.72107\n0.07324656\n34.46735\n31.36370\n0.09895673\n0.17714998\ndemographics: white male\n    2019\n37864\n24.57163\n0.02741190\n28.91303\n24.06818\n0.20129693\n0.09296709\ndemographics: white female\n    2019\n3607\n23.72498\n0.02696504\n30.11012\n22.74304\n0.32392681\n0.17447826\ndemographics: black male\n    2019\n4577\n21.96607\n0.02158116\n26.77524\n21.29592\n0.25729427\n0.14210089\ndemographics: black female\n    2019\n6897\n22.11324\n0.02291996\n30.18838\n21.20103\n0.42391109\n0.24975224\ndemographics: hispanic male\n    2019\n5947\n18.81279\n0.01247475\n25.42001\n18.14904\n0.40062550\n0.15435517\ndemographics: hispanic female\n  \n  \n  \n\n\n\n\nLet’s remove “demographics:” and the all jobs data and then split the facet description into race and sex. I’m replacing \"demographics: \" with an empty string using str_replace.\nFrom tidyR, separate() does a pretty good job of guessing how to split the data, but you can always give it the pattern to split on. Note that separate() has superseded in favour of separate_wider_position() and separate_wider_delim(), but it still supported.\n\nwages_2019_demo3 &lt;- wages_2019_demo2[2:9,]\n\nwages_2019_demo3 &lt;- wages_2019_demo3 %&gt;%\n  mutate(facet = str_replace(facet, \"demographics: \", \"\")) %&gt;%\n  separate(facet, c(\"race\", \"sex\"))\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 2 rows [1, 2].\n\nwages_2019_demo2 %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      year\n      sample_size\n      wage\n      at_cap\n      union_wage\n      nonunion_wage\n      union_wage_premium_raw\n      union_wage_premium_adjusted\n      facet\n    \n  \n  \n    2019\n94818\n28.09587\n0.05071279\n31.70222\n27.64805\n0.14663510\n0.14290078\nall wage and salary workers\n    2019\n48157\n31.31306\n0.07093566\n34.02119\n30.96146\n0.09882383\n0.17838623\ndemographics: male\n    2019\n46661\n24.64259\n0.02903222\n28.99576\n24.12817\n0.20173885\n0.10255232\ndemographics: female\n    2019\n40352\n31.72107\n0.07324656\n34.46735\n31.36370\n0.09895673\n0.17714998\ndemographics: white male\n    2019\n37864\n24.57163\n0.02741190\n28.91303\n24.06818\n0.20129693\n0.09296709\ndemographics: white female\n    2019\n3607\n23.72498\n0.02696504\n30.11012\n22.74304\n0.32392681\n0.17447826\ndemographics: black male\n    2019\n4577\n21.96607\n0.02158116\n26.77524\n21.29592\n0.25729427\n0.14210089\ndemographics: black female\n    2019\n6897\n22.11324\n0.02291996\n30.18838\n21.20103\n0.42391109\n0.24975224\ndemographics: hispanic male\n    2019\n5947\n18.81279\n0.01247475\n25.42001\n18.14904\n0.40062550\n0.15435517\ndemographics: hispanic female\n  \n  \n  \n\n\n\n\nNow, I don’t need male and female total rows except to check the numbers (the first two rows). I see the subcategories don’t add up to these upper-level demographic categories. Male + female does equal the total number of workers (“all wage and salary workers”), but the individual subgroups of male and female don’t add up to the totals (hispanic male + white male + black male &gt; male). One explanation is that some survey participants chose to identify in multiple categories. This data isn’t the greatest choice for a treemap since some participants will appear in multiple boxes, but I will proceed with the graph.\nRemove the first two rows. Treemap will generate the totals using the sample size from each subcategory.\n\nwages_2019_demo3a &lt;- wages_2019_demo3[3:8, ]"
  },
  {
    "objectID": "posts/2023-09-05-TidyTuesday-Labor/labor.html#making-a-treemap-with-the-treemap-package",
    "href": "posts/2023-09-05-TidyTuesday-Labor/labor.html#making-a-treemap-with-the-treemap-package",
    "title": "TidyTuesday 36: Visualizing Worker Demographic Information with Treemaps",
    "section": "Making a Treemap with the treemap package",
    "text": "Making a Treemap with the treemap package\nUsing the treemap package as recommended by The R Graph Gallery. There isn’t a built-in way to display the numerical data, so I will construct that as a separate field. It will consist of the subgroup name, a new line, and then the sample size for that subgroup.\nI’m keeping many of the defaults for the treemap. You can specify the palette and details about how the blocks will be arranged, but I’m not tweaking those parameters. The package also doesn’t have a caption or subtitle option, so I’m including the data source in the title.\n\n# Make the label with the subgroup and sample size.\nwages_2019_demo3a &lt;- wages_2019_demo3a %&gt;%\n  mutate(race_sample_size = paste(race, sample_size, sep = \"\\n\"))\n\n# Making the tree map\ntreemap(\n  wages_2019_demo3a,\n  index = c(\"sex\", \"race_sample_size\"),\n  vSize = \"sample_size\",\n  type = \"index\",\n  bg.labels = c(\"white\"),\n  align.labels = list(c(\"center\", \"center\"),\n                      c(\"right\", \"bottom\")),\n  title = \"Sample Size of Different Demographic Groups (2019) [data from unionstats.com]\",\n  fontsize.title = 12\n)"
  },
  {
    "objectID": "posts/2023-09-05-TidyTuesday-Labor/labor.html#interactive-treemap-with-d3treer",
    "href": "posts/2023-09-05-TidyTuesday-Labor/labor.html#interactive-treemap-with-d3treer",
    "title": "TidyTuesday 36: Visualizing Worker Demographic Information with Treemaps",
    "section": "Interactive Treemap with d3treeR",
    "text": "Interactive Treemap with d3treeR\nI can also make an interactive treemap using the d3treeR package. Like many of the other interactive graphs I’ve created, this is a wrapper for a javscript module and can be additionally interacted with using htmlwidgets.\nThis package needs to be installed via devtools::install_github(\"d3treeR/d3treeR\"). There is a similarly named package on CRAN (d3Tree), but that isn’t what you need.\nHere, I made a new treemap without the numerical data label I made for the static map. The d3tree2 function will automatically display that information as you hover over the boxes.\n\ninter &lt;- d3tree2(\n  treemap(\n    wages_2019_demo3a,\n    index = c(\"sex\", \"race\"),\n    vSize = \"sample_size\",\n    type = \"index\",\n    bg.labels = c(\"white\"),\n    align.labels = list(c(\"center\", \"center\"),\n                        c(\"right\", \"bottom\"))\n  ),\n  rootname = \"Sample Size of Different Demographic Groups (from unionstats.com)\"\n)\n\nWeirdly, when you run the above codeblock, the static map created by the treemap call is displayed, even though the interactive map isn’t. This can be mitigated by setting the output of the code block to false if you are using markdown or quarto.\nAnd then you can display the interactive map.\n\ninter"
  },
  {
    "objectID": "posts/2023-09-05-TidyTuesday-Labor/labor.html#conclusions",
    "href": "posts/2023-09-05-TidyTuesday-Labor/labor.html#conclusions",
    "title": "TidyTuesday 36: Visualizing Worker Demographic Information with Treemaps",
    "section": "Conclusions",
    "text": "Conclusions\nSome demographic subgroups do represent under 5% of the total workforce surveyed. For the race category, it appears that some respondents chose more than one category, which could be a problem for imputed data based on matching demographic characteristics."
  },
  {
    "objectID": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html",
    "href": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html",
    "title": "An Introduction to Web Scraping Using rvest",
    "section": "",
    "text": "I’m working on a project for my father that will culminate in a website for his genealogy research. There are a couple of different parts that I’m working on independently. In a previous part of the project, I matched an Excel sheet with GPS coordinates and some biographic data with photos of various tombstones. I then used that data to make a leaflet map of various family grave sites.\nI wanted to add more information beyond the information on the tombstone. My father suggested that I add information from applications he submitted for membership into the SAR. SAR, Sons of the American Revolution, is a society of men who can trace their lineage back to someone who served in the American Revolutionary War. Some of the application material is available on their website. He indicated that adding the links to his spreadsheet would be a lot of typing. I saw this material was openly available on the SAR website, so this would be an ideal task to solve with web scraping. (It turns out that there aren’t that many links after all, but it was helpful as a learning exercise.\nThis blog post deals with scraping the data from the website, wrangling it into a format where it could be matched against my father’s spreadsheet, and then looking for matches. (Every part of this series has ended up being more data wrangling and cleaning than I could have envisioned with just a touch of whatever neat thing I actually wanted to do.) I’ll also talk a bit about including some code from Google Bard.\nThe thumbnail for this post is the gravestone of Ebenezer Frost, one of the potential matches. You can see why birth and death dates are difficult to extract from the stone."
  },
  {
    "objectID": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#tombstone-project-overview",
    "href": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#tombstone-project-overview",
    "title": "An Introduction to Web Scraping Using rvest",
    "section": "",
    "text": "I’m working on a project for my father that will culminate in a website for his genealogy research. There are a couple of different parts that I’m working on independently. In a previous part of the project, I matched an Excel sheet with GPS coordinates and some biographic data with photos of various tombstones. I then used that data to make a leaflet map of various family grave sites.\nI wanted to add more information beyond the information on the tombstone. My father suggested that I add information from applications he submitted for membership into the SAR. SAR, Sons of the American Revolution, is a society of men who can trace their lineage back to someone who served in the American Revolutionary War. Some of the application material is available on their website. He indicated that adding the links to his spreadsheet would be a lot of typing. I saw this material was openly available on the SAR website, so this would be an ideal task to solve with web scraping. (It turns out that there aren’t that many links after all, but it was helpful as a learning exercise.\nThis blog post deals with scraping the data from the website, wrangling it into a format where it could be matched against my father’s spreadsheet, and then looking for matches. (Every part of this series has ended up being more data wrangling and cleaning than I could have envisioned with just a touch of whatever neat thing I actually wanted to do.) I’ll also talk a bit about including some code from Google Bard.\nThe thumbnail for this post is the gravestone of Ebenezer Frost, one of the potential matches. You can see why birth and death dates are difficult to extract from the stone."
  },
  {
    "objectID": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#loading-libraries-and-data",
    "href": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#loading-libraries-and-data",
    "title": "An Introduction to Web Scraping Using rvest",
    "section": "Loading Libraries and Data",
    "text": "Loading Libraries and Data\n\nlibrary(tidyverse) # who doesn't want to be tidy?\nlibrary(here) # relative file paths\nlibrary(rvest) # web scraping package\nlibrary(gt) # for nicer tables\n\nThis is my father’s spreadsheet after some clean-up. I go into the process in detail here.\n\ndata_file &lt;-\n  readRDS(\n    here(\n      \"posts\",\n      \"2023-08-04-data-cleaning-tombstone\",\n      \"tombstones_matched_final.RDS\"\n    )\n  )"
  },
  {
    "objectID": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#conceptual-overview-of-web-scraping",
    "href": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#conceptual-overview-of-web-scraping",
    "title": "An Introduction to Web Scraping Using rvest",
    "section": "Conceptual Overview of Web Scraping",
    "text": "Conceptual Overview of Web Scraping\nWeb scraping is extracting data from a website. If you vaguely know what web scraping is, you probably envision high-volume automated processes that extract large quantities of data. This type of web scraping has some ethical and legal concerns, which I’ll discuss in a later section. However, many of the tools and processes you need to understand to web scrape effectively can be practiced in a low-volume, non-automated way. This is the skill set I’m going to focus on in this tutorial.\n\nAcquire the webpage\nIdentify the HTML elements you wish to extract\nClean and wrangle the data into a format you can work with\nAutomate by looping through web pages to acquire additional information\n\n\nEthics and Legality of Web Scraping\nDo your research before you start a web scraping project, particularly if you intend to collect a high volume of data. If there is an alternate way to get the data, such as through an API, you should probably use that method over web scraping. Small-volume, publically accessible data scraped for non-commercial use is likely fine, both legally and ethically.\nR for Data Science (2e) by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund has a really nice overview of potential issues to consider before starting a web scraping project. Luke Barousse has a great YouTube video about a web scraping project he did that scraped data from LinkedIn that ended with his account access being restricted.\nThe data involved in this project involves information about people long dead, except for information I’ve shared about myself or my father, which I have permission to share. This is a very small project involving collecting data from a single webpage and is for non-commercial use.\n\n\nAnatomy of a Webpage: Why do I need to know about CSS Selectors?\nModern webpages consist of HTML tags, which define the structure; CSS files, which define the style; and JavaScript files, which create interactivity. HTML tags are usually a pair of tags that enclose content and attributes. The html element is the tag, content, and attributes. The opening tag looks like &lt;tag&gt; while the closing tag looks like &lt;/tag&gt;. In between the tags, you can have content, attributes, and other tags. HTML is hierarchical, and the relationship between tags is described using the language of families. The tag within a tag would be a child. The higher level tag would be a parent. Sibling tags are children of the same parent tag. All of this is important because to select the information you want, you will often need to describe the hierarchy to get to the correct content.\nThe formatting and styling of a webpage are generated with cascading styling sheets (CSS). These files describe how specific elements of the HTML should look. For example, you might want to make the h1 header quite large. You would do that in the CSS file with code like this:\nh1 {   font-size 72px; }\nThe CSS file selects the HTML elements using something called CSS selectors. The format of CSS selectors allows you to create complicated selectors- for example, to select the first entry of a list or all children of a table. Many web scraping tools use CSS selectors to extract the desired elements from the webpage. I initially found this confusing since I was trying to extract HTML tags, and it wasn’t made clear that this was using CSS selectors. It made googling for more information difficult as well since I wasn’t using the correct terminology. So, even though you may not be doing any web design, you should be familiar with CSS selectors if you intend to web scrape.\nThere is a cheat sheet here that shows some of the more advanced selectors and a great resource showing all different types of CSS selectors here."
  },
  {
    "objectID": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#web-scraping-with-rvest",
    "href": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#web-scraping-with-rvest",
    "title": "An Introduction to Web Scraping Using rvest",
    "section": "Web Scraping with rvest",
    "text": "Web Scraping with rvest\nI decided to use the rvest package for web scraping. I was introduced to it through TidyTuesday, specifically the London Marathon dataset, which was drawn from the London Marathon package. The author of the package, Nicola Rennie, has a tutorial on how she used web scraping to create the package.\nrvest is a tidyverse package and is installed with the installation of tidyverse. However, it is not a core tidyverse package and does need to be loaded separately with a call to library().\nFor politeness’s sake, I am working on a downloaded copy of the webpage. The function read_html() can also take a URL to read directly from a website. You don’t want to repeatedly be pulling the same data from a website as you are getting your code to work. So you can either work with a downloaded copy or use read_html once and then perform all your manipulations on a copy so you don’t have to pull the data again if you mess up your cleaning/wrangling.\nThere is a simple example on the webscraping using rvest. This vignette illustrates basic HMTL and CSS selectors to extract them. The rvest webpage recommends using the SelectorGadget to identify the elements you want to extract. There are also instructions at the SelectorGadget homepage. I didn’t use SelectorGadget when writing my code, but I did go back and try it when I was writing the tutorial, and I didn’t have much luck. I think you still need to know a decent amount about HTML in order to be able to get useful answers from it. In my case, I don’t think there is a selector to extract only the information I wanted. I think you have to select lots of information and do additional extracting in the code.\n\nReading in the Web Page\n\nsar &lt;-\n  read_html(\"Display Member - 121743 - John Douglas Sinks Ph.D..html\")\n\nWhat I have now in sar is an XML document object. This seems like a simple list, but it is actually a very complicated object consisting of multiple nested lists.\n\n\nDeciding which HTML elements to Extract\nThe next part is the part that I haven’t found a great tutorial on that I can refer you to. Nor do I have a great idea of how to teach this- other than trial and error. This has been the main block to publishing this section of the project. So, I will walk through what I did as an example.\nOkay, so how did I actually find the proper CSS selector to extract the HTML elements I wanted, given that GadgetSelector didn’t work well?\nI viewed the source code for the webpage (usually by right-clicking in the browser and clicking view source) and just looked for the elements I wanted.\nMost tutorials on CSS selectors illustrate them with very simple HTML pages; the rvest vignette is a perfect case in point. However, when you actually go to scrape a page, there is a huge jump in (apparent) complexity.\n\n\nA Real Webpage- Rendered and Source\nThis is what the webpage I want to scrape from looks like. It doesn’t look particularly complicated. There are 74 rows in the table.\n\nThen you view the source code:\n\nThere are 2260 lines of code involved in making that webpage!\nSo, I searched (Ctrl-f in Chrome) in the source code page for the first name in the table, PHELPS. It shows up at line 451. This is the entire line:\n\n&lt;td&gt;&lt;a href=“https://sarpatriots.sar.org/patriot/display/335209”&gt;&lt;span style=“display:inline;color: #003366”&gt; &lt;span style=“color:#003366” class=“glyphicon glyphicon-user”&gt;&lt;/span&gt; John PHELPS &lt;small&gt; (P-335209)&lt;/small&gt;&lt;/span&gt;&lt;/a&gt; &lt;/td&gt;\n\nI see that it is wrapped in td tags. This means this item is being coded as a table cell element. That’s good news- if all the data is actually encoded as a table, then it is much easier to extract. I can look up how a table should be formatted in HTML. I found a great resource at the W3Schools: here is their page on tables.\nA table should be tagged as a table with &lt;table&gt; and the closing tag &lt;/table&gt;.\nEach row should be tagged with &lt;tr&gt; and each cell with &lt;td&gt;\n\n&lt;table&gt;\n  &lt;tr&gt;\n    &lt;td&gt;Cell A&lt;/td&gt;\n    &lt;td&gt;Cell B&lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/table&gt;\n\nKnowing this, I can now search the source code for table. Why do I need to do this? If there are multiple tables on the page, I want to make sure I extract the correct one. Luckily for me, there is only one table.\nIf you only cared about the text in a table, you could easily extract it with html_table(). I want the text and the links, so I need to identify selectors to get to the hyperlinks.\nI also know that there are a lot of hyperlinks on this page, so my selector might involve children of the table, or it might involve something related to the cell itself (td). I can’t just choose every hyperlink, or I’d have a pile of trash to sort through.\nThis is how you’d pull out all the hyperlinks.\n\nall_hyperlinks &lt;- sar %&gt;%\n  html_nodes(css = \"a\")\n\nhead(all_hyperlinks)\n\n{xml_nodeset (6)}\n[1] &lt;a class=\"navbar-brand\" style=\"padding:10px 15px\" href=\"https://sarpatrio ...\n[2] &lt;a href=\"https://sarpatriots.sar.org/patriot/search/\" data-toggle=\"toolti ...\n[3] &lt;a href=\"https://sarpatriots.sar.org/member/search/\" data-toggle=\"tooltip ...\n[4] &lt;a href=\"https://sarpatriots.sar.org/application/SearchDescendants/\" data ...\n[5] &lt;a href=\"https://sarpatriots.sar.org/Biographies/search/\" data-toggle=\"to ...\n[6] &lt;a href=\"https://sarpatriots.sar.org/cemetery/search/\" data-toggle=\"toolt ...\n\n\nThe first few are related to the navigation information at the top of the page. So, I do want a more sophisticated selector.\nI need to understand how the table and rows are structured so I can construct the best selector. A row contains 5 cells. The first row is a header row.\n\nThe &lt;th&gt; tag is a child element of &lt;tr&gt; and represents table header elements. It should be used instead of &lt;td&gt; for the header elements. Again, looking at the source code and searching for that tag, I can verify that is true. The header row could just be coded as regular table cells, in which case I would need to discard that information during the data cleaning. Again, I’m lucky, and the header is properly encoded with `th’ tags.\nMy goal is to extract the patriot name and the link associated with it. There are actually 3 columns with links:\n\nPatriot Name, which goes to another page with biographical information\nView Application Detail, which goes to a page with an abbreviated genealogy.\nClick to Purchase, which goes to a page allowing you to order the full application if you are a member\n\nFor my purpose, the link under Patriot Name has the most interesting information and is the one I want. But I’m going to end up with all the links, so I could use any of them.\nSince I know I want a hyperlink, I need to understand how that type of element is encoded. Again, the W3Schools has a great explanation for hyperlinks.\nA hyperlink uses the a' tag. Thea’ tag has two parts: the hyperlink, which is the attribute (comes after href = ), and the text. The plain text part follows the URL and may also include styling. If we go back to the cell I initially extracted for PHELPS, the href = URL is clear. Then there are some codes for styling, and then the text John PHELPS, more styling, and then a number. The name and the number (P-335209) are both in the text of this element.\n\n&lt;td&gt;&lt;a href=“https://sarpatriots.sar.org/patriot/display/335209”&gt;&lt;span style=“display:inline;color: #003366”&gt; &lt;span style=“color:#003366” class=“glyphicon glyphicon-user”&gt;&lt;/span&gt; John PHELPS &lt;small&gt; (P-335209)&lt;/small&gt;&lt;/span&gt;&lt;/a&gt; &lt;/td&gt;\n\nNote that when I played with the SelectorGadget, I could never get it to give me the selector for the hyperlinks. I could get the cell element &lt;td&gt;, or I could get &lt;span&gt;, which you can see is a child of the hyperlink. Even if you could get the hyperlink, I don’t know that it is possible to get just the column of hyperlinks that you wanted. I certainly couldn’t make it give me just the &lt;span&gt; of the column I wanted. If you think SelectorGadget will get you to exactly and only the content you want, then you might be frustrated.\n\n\nExtracting the Links with html_nodes\nTo get the links that I wanted, I found an example on Stack Overflow. (As a side note, I just learned that you could annotate code in Quarto. If you like the annotations better than free-form explanations of the code, please let me know in a comment.)\nI’ve been talking about CSS selectors, but you can also extract elements using a notation called XPATH. That is what the Stack Overflow solution uses. I believe you can build much more complicated selectors using XPATH, but for something basic, it is probably overkill. Here is an overview/cheat sheet. Datacamp also has a course on web scraping in R where they go through selecting with CSS selectors and XPath, but I honestly found it very confusing. I took the course twice (and did the coding for this project in between) and still couldn’t solve many of the problems.\nAnyway, here we go:\n\n1link_nodes &lt;- sar %&gt;% html_nodes(xpath = \"//table//a\")\n2#link_nodes &lt;- sar %&gt;% html_nodes(\"table a\")\n\n3link_text  &lt;- link_nodes %&gt;% html_text()\n4text_df = as.data.frame(link_text)\n\nindex &lt;- 1\nfor (index in seq(1:nrow(text_df))) {\n  text_df$url[index] &lt;- link_nodes[index] %&gt;%  html_attr(\"href\")\n5}\n\n\n1\n\nSelect the elements we want out of the HTML using rvest::html_nodes(). The XPath code specifies that I want `a’ tags that are children of the table. Store it in link_nodes.\n\n2\n\nAn alternate method is to use the more standard CSS selectors, but again, we want the &lt;a&gt; elements that are children of the table. It is commented out here, but it produces identical results as 1.\n\n3\n\nThe link_node contains both the URL and the text. Here, I extract the text. There are two methods in the rvest package: html_text() is faster but might include formatting information relating to whitespace. html_text2() is slower but produces cleaner output.\n\n4\n\nNow, I store the text in a dataframe.\n\n5\n\nNow, I loop through the text dataframe and add on the matching URL that we extract from link_nodes using html_attr(\"href\"). I had mentioned that the elements could contain attributes- attributes are additional features of the element and are expressed in the code as attribute = value. In this case, the format is something like href= \"https://sarpatriots.sar.org/patriot/display/335209\"\n\n\n\n\nSo now I have a dataframe with the text and the URL stored separately.\n\nhead(text_df)\n\n                                                    link_text\n1                                   John  PHELPS   (P-335209)\n2                                                            \n3 Click to Purchase\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n4                                  Moses   ALLEN   (P-102709)\n5                                                            \n6 Click to Purchase\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n                                                      url\n1      https://sarpatriots.sar.org/patriot/display/335209\n2  https://sarpatriots.sar.org/application/display/246742\n3 https://sarpatriots.sar.org/application/purchase/246742\n4      https://sarpatriots.sar.org/patriot/display/102709\n5  https://sarpatriots.sar.org/application/display/246757\n6 https://sarpatriots.sar.org/application/purchase/246757\n\n\nThere is a bunch of whitespace (\\n, \\t) information in some of the entries (new line and tab markers). This perhaps would have been handled with html_text2. We’ve also lost some of the structure that the table had. Each hyperlink is extracted into its own row in the text_df, but 3 of these text_df rows correspond to one row in our webpage table. As I mentioned, I want the link associated with the Patriot name, so I can just take every third row of this text_df table and discard the rest. If I wanted a different link associated with the name, I could do that too.\nNow, I’m creating an index to subset my rows. (This doesn’t need to be a separate variable. I’m just doing that for clarity.\n\nsubset_index &lt;- seq(1, nrow(text_df), 3)\n\nhead(subset_index)\n\n[1]  1  4  7 10 13 16\n\n\nThen, I am subsetting the dataframe using subset_index for the rows and taking all columns.\n\nraw_data &lt;- text_df[subset_index,]\n\nhead(raw_data)\n\n                                        link_text\n1                       John  PHELPS   (P-335209)\n4                      Moses   ALLEN   (P-102709)\n7         William  HARRELL/HORRALL Jr  (P-185027)\n10      Aaron   VAN CLEVE/VAN CLEAVE   (P-310164)\n13     William   VAN CLEVE/VANCLEAVE   (P-309334)\n16           Richard   VEATCH/VEETCH   (P-310578)\n                                                  url\n1  https://sarpatriots.sar.org/patriot/display/335209\n4  https://sarpatriots.sar.org/patriot/display/102709\n7  https://sarpatriots.sar.org/patriot/display/185027\n10 https://sarpatriots.sar.org/patriot/display/310164\n13 https://sarpatriots.sar.org/patriot/display/309334\n16 https://sarpatriots.sar.org/patriot/display/310578\n\n\n\n\nExtracting the Links with html_elements\nThe html_nodes() function used above has been superseded by html_elements(). They should function exactly the same since the notes say that the function is merely renamed. It is generally best to use the most current version of the functions, so here is the above code updated. I’ve also switched to the CSS Selector instead of the XPATH, just to illustrate it produces the same results.\n\nlink_nodes2 &lt;- sar %&gt;%\n  html_elements(\"table a\")\n\nlink_text2  &lt;- link_nodes2 %&gt;%\n  html_text2()\n\ntext_df2 = as.data.frame(link_text2)\n\nindex &lt;- 1\nfor (index in seq(1:nrow(text_df2))) {\n  text_df2$url[index] &lt;- link_nodes2[index] %&gt;%\n    html_attr(\"href\")\n}\n\nraw_data2 &lt;- text_df2[subset_index, ]\n\nhead(raw_data2)\n\n                               link_text2\n1                  John PHELPS (P-335209)\n4                  Moses ALLEN (P-102709)\n7   William HARRELL/HORRALL Jr (P-185027)\n10  Aaron VAN CLEVE/VAN CLEAVE (P-310164)\n13 William VAN CLEVE/VANCLEAVE (P-309334)\n16       Richard VEATCH/VEETCH (P-310578)\n                                                  url\n1  https://sarpatriots.sar.org/patriot/display/335209\n4  https://sarpatriots.sar.org/patriot/display/102709\n7  https://sarpatriots.sar.org/patriot/display/185027\n10 https://sarpatriots.sar.org/patriot/display/310164\n13 https://sarpatriots.sar.org/patriot/display/309334\n16 https://sarpatriots.sar.org/patriot/display/310578\n\n\nYou can see it produces the same results. I did use html_text2(), which did handle the whitespace issues.\n\n\nMore than One Selector Works\nMany times, there are several different selectors that work. The URLs I want are also children of the table cell elements (&lt;td&gt;), so you could use that instead of children of the table. If you were scraping over multiple pages, you might want to check and see if one selector was more universal than others. Here, I think “table a” is the better choice because it gives an idea of what is going on even if you don’t know HTML or CSS selectors. “td” just isn’t as obvious.\n\nlink_nodes3 &lt;- sar %&gt;%\n  html_elements(\"td a\")  \n\nlink_text3 &lt;- link_nodes3 %&gt;%\n  html_text2()  \n\ntext_df3 = as.data.frame(link_text3) \n\nindex &lt;- 1\nfor (index in seq(1:nrow(text_df3))) {\n  text_df2$url[index] &lt;- link_nodes3[index] %&gt;%\n    html_attr(\"href\")\n}\n\nraw_data3 &lt;- text_df2[subset_index,]\n\nhead(raw_data3)\n\n                               link_text2\n1                  John PHELPS (P-335209)\n4                  Moses ALLEN (P-102709)\n7   William HARRELL/HORRALL Jr (P-185027)\n10  Aaron VAN CLEVE/VAN CLEAVE (P-310164)\n13 William VAN CLEVE/VANCLEAVE (P-309334)\n16       Richard VEATCH/VEETCH (P-310578)\n                                                  url\n1  https://sarpatriots.sar.org/patriot/display/335209\n4  https://sarpatriots.sar.org/patriot/display/102709\n7  https://sarpatriots.sar.org/patriot/display/185027\n10 https://sarpatriots.sar.org/patriot/display/310164\n13 https://sarpatriots.sar.org/patriot/display/309334\n16 https://sarpatriots.sar.org/patriot/display/310578\n\n\n\n\nWeb Scraping Larger Volumes of Data\nThis is everything I need, but if I were working on a problem where I needed to scrape multiple pages, this is what I’d do. The actual URL of the webpage is https://sarpatriots.sar.org/member/display/121743. It saved the webpage with information about my father’s name, but that isn’t actually in the URL. So, I’d try a few URLs with different 6-digit numbers and see if they produced web pages with the same data. They do, with the exception that some pages say, “No known ancestors within this Online Database.”\nI’d try to figure out what the range of valid numbers is. For example, 021743 isn’t valid and produces a page that says, “This is an invalid Member Number. Please check your records and try again.”\nThen, I’d loop through all the valid member numbers and get the data. I’d have error handling for pages without a table. And I’d use a package like polite to make sure that the automated requests were being made using best practices."
  },
  {
    "objectID": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#dataframes-arent-tibbles",
    "href": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#dataframes-arent-tibbles",
    "title": "An Introduction to Web Scraping Using rvest",
    "section": "Dataframes aren’t Tibbles!",
    "text": "Dataframes aren’t Tibbles!\nTibbles and data frames are usually entirely interchangeable, but sometimes you do run into the differences. The tidyverse doesn’t approve of row names or numbers, so they aren’t used. Dataframes do have row numbers, and you can see that R kept the indexing of the raw_data, so the row numbers are by 3s. However, indexing is, as usual, by position. So rev_war$name_unclean[3] returns the name associated with the row number 7. This is annoying and makes it difficult to pull out test cases. So, this should be fixed. This issue will pop up when you use data frames but not when you use tibbles.\nSo, two ways to fix this:\n\nConvert to a tibble using as_tibble().\n\n\nrev_war_tibble &lt;- as_tibble(raw_data)\n\nhead(rev_war_tibble)\n\n# A tibble: 6 × 2\n  link_text                                        url                          \n  &lt;chr&gt;                                            &lt;chr&gt;                        \n1 \"    John  PHELPS   (P-335209)\"                  https://sarpatriots.sar.org/…\n2 \"    Moses   ALLEN   (P-102709)\"                 https://sarpatriots.sar.org/…\n3 \"    William  HARRELL/HORRALL Jr  (P-185027)\"    https://sarpatriots.sar.org/…\n4 \"    Aaron   VAN CLEVE/VAN CLEAVE   (P-310164)\"  https://sarpatriots.sar.org/…\n5 \"    William   VAN CLEVE/VANCLEAVE   (P-309334)\" https://sarpatriots.sar.org/…\n6 \"    Richard   VEATCH/VEETCH   (P-310578)\"       https://sarpatriots.sar.org/…\n\n\n\nReassign the indexing of the data frame.\n\n\nhead(raw_data)\n\n                                        link_text\n1                       John  PHELPS   (P-335209)\n4                      Moses   ALLEN   (P-102709)\n7         William  HARRELL/HORRALL Jr  (P-185027)\n10      Aaron   VAN CLEVE/VAN CLEAVE   (P-310164)\n13     William   VAN CLEVE/VANCLEAVE   (P-309334)\n16           Richard   VEATCH/VEETCH   (P-310578)\n                                                  url\n1  https://sarpatriots.sar.org/patriot/display/335209\n4  https://sarpatriots.sar.org/patriot/display/102709\n7  https://sarpatriots.sar.org/patriot/display/185027\n10 https://sarpatriots.sar.org/patriot/display/310164\n13 https://sarpatriots.sar.org/patriot/display/309334\n16 https://sarpatriots.sar.org/patriot/display/310578\n\nrownames(raw_data) = seq(length = nrow(raw_data))\n\nhead(raw_data)\n\n                                       link_text\n1                      John  PHELPS   (P-335209)\n2                     Moses   ALLEN   (P-102709)\n3        William  HARRELL/HORRALL Jr  (P-185027)\n4      Aaron   VAN CLEVE/VAN CLEAVE   (P-310164)\n5     William   VAN CLEVE/VANCLEAVE   (P-309334)\n6           Richard   VEATCH/VEETCH   (P-310578)\n                                                 url\n1 https://sarpatriots.sar.org/patriot/display/335209\n2 https://sarpatriots.sar.org/patriot/display/102709\n3 https://sarpatriots.sar.org/patriot/display/185027\n4 https://sarpatriots.sar.org/patriot/display/310164\n5 https://sarpatriots.sar.org/patriot/display/309334\n6 https://sarpatriots.sar.org/patriot/display/310578\n\n\nI’m going to continue with the reindexed data frame."
  },
  {
    "objectID": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#cleaning-and-reformating-the-data",
    "href": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#cleaning-and-reformating-the-data",
    "title": "An Introduction to Web Scraping Using rvest",
    "section": "Cleaning and Reformating the Data",
    "text": "Cleaning and Reformating the Data\nNow, to wrangle the text data into a usable form. I did more extensive data cleaning in the first part of the project, which you can find here.\nThe link_text contains the name and an ID number, like John PHELPS (P-335209). This can be split into two components using ( as a separator. Remember that special characters like ( need to be escaped. So the actual separator is \" \\\\(\". I took the space also. Then I removed the other parenthesis with str_replace(). I used transmute to do the final mutation and select the columns I wanted moving forward.\n\n# first, clean up the names\nrev_war &lt;- raw_data %&gt;%\n  separate(link_text, sep = \" \\\\(\", into = c(\"name_unclean\", \"id_unclean\") ) \n\n# Clean up the ID number\nrev_war &lt;- rev_war %&gt;%\n  transmute(name_unclean, url, SAR_ID = str_replace(id_unclean, \"\\\\)\", \"\"))\n\nThe names are a mess. Women have “Mrs” prepended. The last names are in all caps and sometimes have multiple variations separated by a /. There are also Jr and Sr at the end of some. There are different numbers of leading and trailing spaces on the names.\nThe whitespace, Jr/Sr, and Mrs issues are easy to deal with using tools from the stringr package. The “Mrs” is not useful at all, so I’m just going to replace it with” “via str_replace(). I’m going to remove the whitespace using str_trim(). I generally like to keep the original data so I can check the transformations, so these operations are in a new column name. I also made a suffix column for Jr/Sr and removed that info from the name column. I’m not changing the names to title case now because I’m going to use the fact that the last names are in upper case as part of my pattern matching later.\n\n#remove Mrs\nrev_war &lt;- rev_war %&gt;%\n  mutate(name =\n    str_replace(name_unclean, \"Mrs\", \"\")\n  ) %&gt;%\n\n# There appear to be leading and tailing strings on the name\n  mutate(name =\n           str_trim(name, side = c(\"both\"))\n  ) %&gt;%\n\n# Deal with Jr/Sr first\n  mutate(suffix =\n           case_when(\n             str_detect(name, \"Jr\") == TRUE ~ \"Jr\",\n             str_detect(name, \"Sr\") == TRUE ~ \"Sr\",\n             TRUE ~\"\"\n           )\n  ) %&gt;%\n\n# Now remove the Jr and Sr from the name\n  mutate(name =\n           str_replace(name, \"Jr\", \"\")\n  ) %&gt;%\n  mutate(name =\n           str_replace(name, \"Sr\", \"\")\n  ) %&gt;%\n\n#double check the white space issue\n  mutate(name =\n           str_trim(name, side = c(\"both\"))\n  )\n\nThat is all pretty straightforward.\nNow, how do we break this up?\n\nrev_war$name[3]\n\n[1] \"William  HARRELL/HORRALL\"\n\n\nThe obvious choice would be to use the space(s) between the name to separate the string into two parts. This fails because of names like this:\n\nrev_war$name[27]\n\n[1] \"Friderick William NAGEL/NAGLE\"\n\n\nI admit I used trial and error to figure this out. First note- using str_split() from stringr is not the way to go. I got hyper-focused on stringr functions since I was using the package so heavily. It creates a matrix of results in your data frame and not new columns of data. Use something from the separate() family from tidyr instead.\nI decided to do this stepwise. So the remove = FALSE flag needs to be set in separate(), so I keep the original data. First, I got the first names by splitting on the pattern of space and then two or more capital letters. The two or more is necessary because of names like Friderick William. This gives the complete first name(s) and an incomplete last name because the separator is discarded. So you end up with AGEL/NAGLE, which I dumped in a column named trash. Here, I immediately deleted it, but it was handy for troubleshooting.\n\n# Bard gave me a hint to get the regex!\nrev_war &lt;- rev_war %&gt;%\n  separate(\n    name,\n    into = c(\"first_name\", \"trash\"),\n    remove = FALSE,\n    sep = \"\\\\s+[A-Z]{2,}\"\n  )\n\nWarning: Expected 2 pieces. Additional pieces discarded in 3 rows [4, 5, 25].\n\nhead(rev_war)\n\n                         name_unclean\n1                      John  PHELPS  \n2                     Moses   ALLEN  \n3        William  HARRELL/HORRALL Jr \n4      Aaron   VAN CLEVE/VAN CLEAVE  \n5     William   VAN CLEVE/VANCLEAVE  \n6           Richard   VEATCH/VEETCH  \n                                                 url   SAR_ID\n1 https://sarpatriots.sar.org/patriot/display/335209 P-335209\n2 https://sarpatriots.sar.org/patriot/display/102709 P-102709\n3 https://sarpatriots.sar.org/patriot/display/185027 P-185027\n4 https://sarpatriots.sar.org/patriot/display/310164 P-310164\n5 https://sarpatriots.sar.org/patriot/display/309334 P-309334\n6 https://sarpatriots.sar.org/patriot/display/310578 P-310578\n                           name first_name    trash suffix\n1                  John  PHELPS       John                \n2                 Moses   ALLEN      Moses                \n3      William  HARRELL/HORRALL    William /HORRALL     Jr\n4  Aaron   VAN CLEVE/VAN CLEAVE      Aaron                \n5 William   VAN CLEVE/VANCLEAVE    William                \n6       Richard   VEATCH/VEETCH    Richard  /VEETCH       \n\n# Delete the trash columns\nrev_war &lt;- rev_war %&gt;%\n  select(-trash)\n\nGetting the last name was trickier because any separator I could think of would also catch the multiple first-name people. Back to stringr to use str_extract(). Basically, any chunk of text with two or more capital letters in a row followed by any number of any other type of character is extracted.\n\nrev_war &lt;- rev_war %&gt;%\n  mutate(Last_name = str_extract(name, \"[A-Z]{2,}.+\"))\n\nhead(rev_war)\n\n                         name_unclean\n1                      John  PHELPS  \n2                     Moses   ALLEN  \n3        William  HARRELL/HORRALL Jr \n4      Aaron   VAN CLEVE/VAN CLEAVE  \n5     William   VAN CLEVE/VANCLEAVE  \n6           Richard   VEATCH/VEETCH  \n                                                 url   SAR_ID\n1 https://sarpatriots.sar.org/patriot/display/335209 P-335209\n2 https://sarpatriots.sar.org/patriot/display/102709 P-102709\n3 https://sarpatriots.sar.org/patriot/display/185027 P-185027\n4 https://sarpatriots.sar.org/patriot/display/310164 P-310164\n5 https://sarpatriots.sar.org/patriot/display/309334 P-309334\n6 https://sarpatriots.sar.org/patriot/display/310578 P-310578\n                           name first_name suffix            Last_name\n1                  John  PHELPS       John                      PHELPS\n2                 Moses   ALLEN      Moses                       ALLEN\n3      William  HARRELL/HORRALL    William     Jr      HARRELL/HORRALL\n4  Aaron   VAN CLEVE/VAN CLEAVE      Aaron        VAN CLEVE/VAN CLEAVE\n5 William   VAN CLEVE/VANCLEAVE    William         VAN CLEVE/VANCLEAVE\n6       Richard   VEATCH/VEETCH    Richard               VEATCH/VEETCH\n\n\nNow to split on /. In this dataset, we can have 1-3 possible last names. The separate function requires that you know how many parts you are splitting the string into. If you tell the separate() function that you have 3 parts, it will create a warning for any strings that don’t have 3 parts. It will execute though, and it will fill in the missing parts with NAs and discard the extra parts. This actually happened when I separated out the first name above, but the discarded parts were all from the last name part, which I was trashing anyway.\nSeparate has been superseded by other separate functions, so I’m going to demonstrate the use of the preferred function, separate_wider_delim() . separate_wider_delim(), unlike separate(), will kick up an error if you don’t have the right number of parts and fail to execute. However, you can run it in debug mode, and it will perform the splits and then tell you on a row-by-row basis if there were the right number of parts. I exploited this and then threw away the debugging info, removed the NAs, and changed the last names to title case. (Note that the first name column should be handled the same way since there is one name that is listed with two variations: Johannes/John .)\n\n#Now we need to split the names with /\nrev_war_test &lt;- rev_war %&gt;%\n  separate_wider_delim(\n    Last_name,\n    names = c(\"V1\",\n              \"V2\",\n              \"V3\"),\n    delim = \"/\",\n    too_few = c(\"debug\"),\n    too_many = c(\"debug\"),\n    cols_remove = FALSE\n  )\n\nWarning: Debug mode activated: adding variables `Last_name_ok`, `Last_name_pieces`, and\n`Last_name_remainder`.\n\nrev_war_test &lt;- rev_war_test %&gt;%\n  select(name_unclean, first_name, V1, V2, V3, suffix, url, SAR_ID)\n\nrev_war_test &lt;- rev_war_test %&gt;%\n  mutate(V2 = ifelse(is.na(V2), \"\", V2),\n         V3 = ifelse(is.na(V3), \"\", V3),\n         V1 = str_to_title(V1),\n         V2 = str_to_title(V2),\n         V3 = str_to_title(V3)\n         )\n\nrev_war_test\n\n# A tibble: 74 × 8\n   name_unclean                 first_name V1    V2    V3    suffix url   SAR_ID\n   &lt;chr&gt;                        &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; \n 1 \"    John  PHELPS  \"         John       Phel… \"\"    \"\"    \"\"     http… P-335…\n 2 \"    Moses   ALLEN  \"        Moses      Allen \"\"    \"\"    \"\"     http… P-102…\n 3 \"    William  HARRELL/HORRA… William    Harr… \"Hor… \"\"    \"Jr\"   http… P-185…\n 4 \"    Aaron   VAN CLEVE/VAN … Aaron      Van … \"Van… \"\"    \"\"     http… P-310…\n 5 \"    William   VAN CLEVE/VA… William    Van … \"Van… \"\"    \"\"     http… P-309…\n 6 \"    Richard   VEATCH/VEETC… Richard    Veat… \"Vee… \"\"    \"\"     http… P-310…\n 7 \"    Daniel   CHAPMAN  \"     Daniel     Chap… \"\"    \"\"    \"\"     http… P-131…\n 8 \"    William  HORRALL Sr \"   William    Horr… \"\"    \"\"    \"Sr\"   http… P-335…\n 9 \"    William   BALDWIN  \"    William    Bald… \"\"    \"\"    \"\"     http… P-107…\n10 \"    John  GOODALL/GOODALE … John       Good… \"Goo… \"\"    \"\"     http… P-167…\n# ℹ 64 more rows"
  },
  {
    "objectID": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#matching-to-excel",
    "href": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#matching-to-excel",
    "title": "An Introduction to Web Scraping Using rvest",
    "section": "Matching to Excel",
    "text": "Matching to Excel\nSo the ultimate goal is to match the URLs with names from my father’s spreadsheet to save him some typing [6,000 words later, I might be getting close to saving him a few keystrokes. :) ]\nI’m not going to go into too much detail here since the main point of this tutorial was web scraping and data cleaning.\nSo, I have up to three variations on the name based on the last names. My plan was to match on the first variation, then for those that failed to match, match on the second variation, and so on.\nI was mildly hopeful that the first variation was the variation that my father used in his Excel table. So, I did a quick check using an inner join. My father had mentioned that there wouldn’t be many matches- maybe 5 or 6, because he hadn’t found most of the graves yet.\n\nrev_war_test &lt;- rev_war_test %&gt;%\n  mutate(match_1 = paste0(V1, \" \", first_name))\n\nmatched_records &lt;- rev_war_test %&gt;% \n  inner_join(data_file, by = c(\"match_1\" = \"full_name\")) %&gt;%\n  select(match_1,\n         Surname,\n         First.Name,\n         Middle.Name,\n         DOB_date,\n         DOD_date)\n\nmatched_records %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      match_1\n      Surname\n      First.Name\n      Middle.Name\n      DOB_date\n      DOD_date\n    \n  \n  \n    Chapman Daniel\nChapman\nDaniel\nNA\nNA\nNA\n    Chapman Daniel\nChapman\nDaniel\nNA\n1863-07-05\nNA\n    Horrall William\nHorrall\nWilliam\nNA\nNA\nNA\n    Baldwin William\nBaldwin\nWilliam\nNA\nNA\nNA\n    Frost Ebenezer\nFrost\nEbenezer\nNA\nNA\nNA\n    Mory Gotthard\nMory\nGotthard\nNA\n1752-03-20\n1843-05-26\n    Horlacher Daniel\nHorlacher\nDaniel\nNA\n1735-08-04\n1804-09-24\n    Rockel Balzer\nRockel\nBalzer\nNA\n1707-11-10\n1800-06-09\n    Finch Isaac\nFinch\nIsaac\nNA\nNA\n1813-11-26\n    Woolard John\nWoolard\nJohn\nEdward\n1872-01-21\n1936-10-19\n    Meredith Samuel\nMeredith\nSamuel\nNA\nNA\n1825-10-10\n    Meredith Samuel\nMeredith\nSamuel\nNA\nNA\n1884-01-05\n    Anderson Abraham\nAnderson\nAbraham\nNA\n1776-03-10\n1838-08-15\n    Dorris William\nDorris\nWilliam\nScott\nNA\nNA\n    Dorris William\nDorris\nWilliam\nT\nNA\nNA\n    Dorris William\nDorris\nWilliam\nH\nNA\n1857-08-10\n    Dorris William\nDorris\nWilliam\nNA\n1818-11-28\n1905-02-17\n  \n  \n  \n\n\n\n\nWhat we see is that many names are reused in the family, an issue I dealt with extensively while cleaning up my father’s spreadsheet. Some, but not all, of the people in my father’s spreadsheet have birthdate (DOB) or death date (DOD) information. This could be used to rule out potential matches (for example, if someone was born after the War or if they were only 3 years old during the Revolutionary War). Since there is so much missing data, using dates to rule people in would not work. So, if I select only people alive during the American Revolutionary War and then match, I am potentially excluding the correct person if their DOB or DOD is NA in the spreadsheet."
  },
  {
    "objectID": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#coding-with-ai-a-useful-tool-to-generate-regex",
    "href": "posts/2023-09-08-web-scraping-tombstone/web-scraping.html#coding-with-ai-a-useful-tool-to-generate-regex",
    "title": "An Introduction to Web Scraping Using rvest",
    "section": "Coding with AI: A Useful Tool to Generate Regex?",
    "text": "Coding with AI: A Useful Tool to Generate Regex?\nLarge Language Models (LLM) like ChatGPT and Google Bard are trendy right now. I’ve seen a lot of people say that these AI models can be used to increase productivity across a variety of domains. All sorts of tools and services now have AI bonus features. Datacamp has AI in their workspaces and also for some of the course coding exercises. I haven’t been too impressed. Generally, syntax errors are caught, corrected, and explained, but deeper problems aren’t. I also found that the results often hyperfocus on syntax that is technically correct but not necessarily best or common practice. For example, in a dpylr join, you don’t need to specify the columns to join on if they have the same name in both dataframes. However, the AI would insist that not specifying the columns in the code was the problem.\nI’ve also played around with Google Bard for writing and summarizing writing, and the results ranged from generic to ridiculous.\nAnyway, as I was staring at the stringr cheatsheet trying to figure out the regex for the pattern space and then a capital letter, I thought that this might be the perfect use case for Bard.\nSo:\n\nhi I need the regular expression for one or more spaces followed by a capital letter\n\n\nSure, here is the regular expression for one or more spaces followed by a capital letter:\nCode snippet\n\\s+[A-Z]\nUse code with caution. Learn more\nThe \\s metacharacter matches a single space character, and the + quantifier means that the space character can be matched one or more times. The [A-Z] character class matches any capital letter.\nSo, this regular expression will match any string that contains one or more spaces followed by a capital letter. For example, it would match the strings “  A”, ”   B”, and “C”.\nHere is an example of how to use this regular expression in Python:\nPython\nimport re\nstring = “  A”\nmatch = re.search(r”\\s+[A-Z]“, string)\nif match:\n    print(match.group())\nUse code with caution. Learn more\nThis code will print the capital letter that is matched by the regular expression. In this case, the code will print the letter “A”.\nI hope this helps! Let me know if you have any other questions.\n\nI “read” regex much better than I can write it, so \"\\s+[A-Z]\" was enough for me. It isn’t right, but it is almost right. The correct regex is \"\\\\s+[A-Z]\" .\nJust for fun, I asked Bard for the R code. I was using voice to text so my question is a bit garbled. Comma should be , of course.\n\ncan you give me an example of how to use it in r comma for example in the tidyr function separate\nSure, here is an example of how to use the regular expression in R, using the separate function from the tidyr package:\nR\nlibrary(tidyr)\ndata &lt;- data.frame(\n  text = c(“  A”, ”   B”, “C”),\n  stringsAsFactors = FALSE\n)\ndata %&gt;% separate(text, into = c(“prefix”, “letter”), sep = r”\\s+[A-Z]“)\n#&gt; # A tibble: 3 x 2\n#&gt;   prefix letter\n#&gt;   &lt;chr&gt;   &lt;chr&gt;\n#&gt; 1   &lt;NA&gt;     A\n#&gt; 2       B     B\n#&gt; 3   &lt;NA&gt;     C\nUse code with caution. Learn more\nThis code will first load the tidyr package. Then, it will create a data frame called data with three rows, each containing a string with one or more spaces followed by a capital letter. The separate function will then be used to split the strings in the text column into two columns: prefix and letter. The sep argument is used to specify the regular expression that will be used to split the strings. In this case, the regular expression is \\s+[A-Z], which matches one or more spaces followed by a capital letter.\nThe output of the code is a data frame with three rows and two columns. The prefix column contains the string that precedes the capital letter, and the letter column contains the capital letter.\nI hope this helps! Let me know if you have any other questions.\n\nDoes this run? No, of course not! First, this is a terrible example anyway because none of the strings can be split into two parts based on that regex. (‘A A A’ or ‘a B a’ or something like that would be a much better choice.) You certainly won’t get the output that Bard predicts. Remember, the separator gets discarded. For row one, the separator is “  A” and there is no part one and no part two. Same with row 2. C should be the prefix, and the letter should be NA. There is no separator in this case. Lots of warnings are going to be generated by this…\nI’m not sure where the extra r in sep = r\" \\s+[A-Z]\" came from. You can fix that and get it to run.\n\ndata &lt;- data.frame(\ntext = c(\"  A\", \"   B\", \"C\"),\nstringsAsFactors = FALSE\n)\n\ndata %&gt;% separate(text, into = c(\"prefix\", \"letter\"), sep = \"\\\\s+[A-Z]\")\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [3].\n\n\n  prefix letter\n1              \n2              \n3      C   &lt;NA&gt;\n\n\nBut it doesn’t give the answer Bard claims.\nI was feeling amused, so I argued back and forth with Bard about the need for the escape character. It finally conceded my point and gave me the following code:\ndata %&gt;% separate(text, into = c(\"prefix\", \"letter\"), sep = r\"\\\\\\\\\\s+[A-Z]\", fixed = FALSE)\nAgain, this also doesn’t run, even with the r removed, because \\\\\\\\\\s is not \\\\s. Adding extra random slashes does not improve your code!\nIn the end, it did save me a bit of time in generating the regex. I think it can be a useful tool, but it is probably more valuable if you have a good idea of what you are doing and can tell if the answer is correct or not. Or if you have the skills to debug what you are given. If you are a learner, I think it can potentially be very time-consuming and frustrating to use."
  },
  {
    "objectID": "past_work.html",
    "href": "past_work.html",
    "title": "Portfolio",
    "section": "",
    "text": "Nanomaterials at USNano LLC\n\n\n\n\n\nSynthesis of semiconducting nanowires, formulating nanowire inks, and creating various printed electronic devices using these inks\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/USNano/usnano.html",
    "href": "portfolio/USNano/usnano.html",
    "title": "Nanowires at USNano LLC",
    "section": "",
    "text": "US Nano was a nanomaterials start-up working on producing printed electronics. The work consisted of scaling up the synthesis of semiconducting nanowires, formulating nanowire inks, and creating various printed electronic devices using these inks. Introduction of applied statistical methods such as Design of Experiments (DOE) and statistical process control accelerated development across all facets of the work."
  },
  {
    "objectID": "portfolio/USNano/usnano.html#scale-up-of-nanowire-synthesis",
    "href": "portfolio/USNano/usnano.html#scale-up-of-nanowire-synthesis",
    "title": "Nanowires at USNano LLC",
    "section": "Scale-up of Nanowire Synthesis",
    "text": "Scale-up of Nanowire Synthesis\nUS Nano developed a robust process for large-scale manufacturing of inorganic semiconductor nanowires. While various laboratory scale methods exist, they tend to have high materials cost, produce very small quantities (milligrams) of product, and do not scale well.\n\n\n\nWe successfully reduced the materials cost 140-fold. We improved the reliability of the synthesis, thus allowing a scale-up of nanowire production to gram scale. This method was robust and used to produce various semiconductor compositions. DOE methods such as fractional factorial design and response surface methodology allowed us to vary multiple predictors at once and iterate to new materials 3X faster than the initial work using one predictor/factor at a time experimentation."
  },
  {
    "objectID": "portfolio/USNano/usnano.html#ink-formulation-and-printing",
    "href": "portfolio/USNano/usnano.html#ink-formulation-and-printing",
    "title": "Nanowires at USNano LLC",
    "section": "Ink Formulation and Printing",
    "text": "Ink Formulation and Printing\nThe resulting nanowires were incorporated into functional inks for printed/flexible electronics. In addition to DOE-based ink formulation projects, quality assurance measures such as statistical process control were implemented to keep our nanomaterials printer fully operational.\n\n\n\nExperimental inks can deposit residue on the inkjet nozzle that isn’t easily detected. Running regular jetting tests with known inks can determine when the system performance is starting to degrade, even when the residue or build-up is undetectable by eye.\nJetting tests involve collecting thousands of jetting images and analyzing the drop speed and velocity for deviation from normal. Home-written Excel VBA macros reduced analysis time from hours to under five minutes, allowing the tests to run more frequently."
  },
  {
    "objectID": "portfolio/USNano/usnano.html#nanowire-devices",
    "href": "portfolio/USNano/usnano.html#nanowire-devices",
    "title": "Nanowires at USNano LLC",
    "section": "Nanowire Devices",
    "text": "Nanowire Devices\nPrinting electronics allows for an almost infinite number of device configurations since the cost of prototyping is minimal. A prototype printed photosensor was produced as a demonstration. The demonstration device validates that inorganic semiconducting nanowires can be incorporated into flexible/printed electronics using standard plastic substrates and commercial printing methods. The technology developed in this project will enable the creation of a variety of additional devices on various substrates, including flexible films, such as plastics and metal foils, in addition to conventional rigid substrates, such as glass or semiconductor wafers. We have successfully printed our semiconductor inks on paper, plastics, and metal foils.\n\n\n\nBy iterating through designs, we produced a device with a million-fold better photocurrent than the initial simple gap devices. Again, DOE methods allowed us to quickly hone in on optimal designs without testing every configuration."
  },
  {
    "objectID": "portfolio/USNano/usnano.html#patents-and-papers",
    "href": "portfolio/USNano/usnano.html#patents-and-papers",
    "title": "Nanowires at USNano LLC",
    "section": "Patents and Papers",
    "text": "Patents and Papers\nApparatus and methods for continuous flow synthesis of semiconductor nanowires, US9306110B2\nSynthesis of CdSe/ZnS core/shell semiconductor nanowires, US9627200B2\nPetchsang N, McDonald MP, Sinks LE, Kuno M. Light induced nanowire assembly: the electrostatic alignment of semiconductor nanowires into functional macroscopic yarns. Adv Mater. 2013 Jan 25;25(4):601-5. doi: 10.1002/adma.201202722. Epub 2012 Oct 2. PMID: 23027517."
  },
  {
    "objectID": "portfolio/USNano/usnano.html#press",
    "href": "portfolio/USNano/usnano.html#press",
    "title": "Nanowires at USNano LLC",
    "section": "Press",
    "text": "Press\nNanowerk Blog, Assembling functional nanowire yarns with light\nFlorida High Tech Corridor 2016 Faces of Technology.\nPrint article, p. 38: http://usnanollc.com/wp-content/uploads/2016/04/FHT-2016-compressed.pdf\nVideo Interview:"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Advanced Femtosecond Spectroscopy Techniques\n\n\n\n\n\nExploring electron and energy transfer in inorganic functional materials and protein systems\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nNanowires at USNano LLC\n\n\n\n\n\nSynthesis of semiconducting nanowires, formulating nanowire inks, and creating various printed electronic devices using these inks\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPhosphorescence Lifetime Imaging\n\n\n\n\n\nDevelopment of novel oxygen imaging technique based on two-photon excitation microscopy. Modeling kinetic events and simulation of excitation volume with Matlab.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/Nanomaterials_USNano/Nanomaterials_USNano.html",
    "href": "portfolio/Nanomaterials_USNano/Nanomaterials_USNano.html",
    "title": "Nanowires at USNano LLC",
    "section": "",
    "text": "US Nano was a nanomaterials start-up working on producing printed electronics. The work consisted of scaling up the synthesis of semiconducting nanowires, formulating nanowire inks, and creating various printed electronic devices using these inks. Introduction of applied statistical methods such as Design of Experiments (DOE) and statistical process control accelerated development across all facets of the work."
  },
  {
    "objectID": "portfolio/Nanomaterials_USNano/Nanomaterials_USNano.html#scale-up-of-nanowire-synthesis",
    "href": "portfolio/Nanomaterials_USNano/Nanomaterials_USNano.html#scale-up-of-nanowire-synthesis",
    "title": "Nanowires at USNano LLC",
    "section": "Scale-up of Nanowire Synthesis",
    "text": "Scale-up of Nanowire Synthesis\nUS Nano developed a robust process for large-scale manufacturing of inorganic semiconductor nanowires. While various laboratory scale methods exist, they tend to have high materials cost, produce very small quantities (milligrams) of product, and do not scale well.\n\n\n\nWe successfully reduced the materials cost 140-fold. We improved the reliability of the synthesis, thus allowing a scale-up of nanowire production to gram scale. This method was robust and used to produce various semiconductor compositions. DOE methods such as fractional factorial design and response surface methodology allowed us to vary multiple predictors at once and iterate to new materials 3X faster than the initial work using one predictor/factor at a time experimentation."
  },
  {
    "objectID": "portfolio/Nanomaterials_USNano/Nanomaterials_USNano.html#ink-formulation-and-printing",
    "href": "portfolio/Nanomaterials_USNano/Nanomaterials_USNano.html#ink-formulation-and-printing",
    "title": "Nanowires at USNano LLC",
    "section": "Ink Formulation and Printing",
    "text": "Ink Formulation and Printing\nThe resulting nanowires were incorporated into functional inks for printed/flexible electronics. In addition to DOE-based ink formulation projects, quality assurance measures such as statistical process control were implemented to keep our nanomaterials printer fully operational.\n\n\n\nExperimental inks can deposit residue on the inkjet nozzle that isn’t easily detected. Running regular jetting tests with known inks can determine when the system performance is starting to degrade, even when the residue or build-up is undetectable by eye.\nJetting tests involve collecting thousands of jetting images and analyzing the drop speed and velocity for deviation from normal. Home-written Excel VBA macros reduced analysis time from hours to under five minutes, allowing the tests to run more frequently."
  },
  {
    "objectID": "portfolio/Nanomaterials_USNano/Nanomaterials_USNano.html#nanowire-devices",
    "href": "portfolio/Nanomaterials_USNano/Nanomaterials_USNano.html#nanowire-devices",
    "title": "Nanowires at USNano LLC",
    "section": "Nanowire Devices",
    "text": "Nanowire Devices\nPrinting electronics allows for an almost infinite number of device configurations since the cost of prototyping is minimal. A prototype printed photosensor was produced as a demonstration. The demonstration device validates that inorganic semiconducting nanowires can be incorporated into flexible/printed electronics using standard plastic substrates and commercial printing methods. The technology developed in this project will enable the creation of a variety of additional devices on various substrates, including flexible films, such as plastics and metal foils, in addition to conventional rigid substrates, such as glass or semiconductor wafers. We have successfully printed our semiconductor inks on paper, plastics, and metal foils.\n\n\n\nBy iterating through designs, we produced a device with a million-fold better photocurrent than the initial simple gap devices. Again, DOE methods allowed us to quickly hone in on optimal designs without testing every configuration."
  },
  {
    "objectID": "portfolio/Nanomaterials_USNano/Nanomaterials_USNano.html#patents-and-papers",
    "href": "portfolio/Nanomaterials_USNano/Nanomaterials_USNano.html#patents-and-papers",
    "title": "Nanowires at USNano LLC",
    "section": "Patents and Papers",
    "text": "Patents and Papers\nApparatus and methods for continuous flow synthesis of semiconductor nanowires, US9306110B2\nSynthesis of CdSe/ZnS core/shell semiconductor nanowires, US9627200B2\nPetchsang N, McDonald MP, Sinks LE, Kuno M. Light induced nanowire assembly: the electrostatic alignment of semiconductor nanowires into functional macroscopic yarns. Adv Mater. 2013 Jan 25;25(4):601-5. doi: 10.1002/adma.201202722. Epub 2012 Oct 2. PMID: 23027517."
  },
  {
    "objectID": "portfolio/Nanomaterials_USNano/Nanomaterials_USNano.html#press",
    "href": "portfolio/Nanomaterials_USNano/Nanomaterials_USNano.html#press",
    "title": "Nanowires at USNano LLC",
    "section": "Press",
    "text": "Press\nNanowerk Blog, Assembling functional nanowire yarns with light\nFlorida High Tech Corridor 2016 Faces of Technology.\nPrint article, p. 38: http://usnanollc.com/wp-content/uploads/2016/04/FHT-2016-compressed.pdf\nVideo Interview:"
  },
  {
    "objectID": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html",
    "href": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html",
    "title": "Phosphorescence Lifetime Imaging",
    "section": "",
    "text": "The Vinogradov Lab at the University of Pennsylvania is a multi-disciplinary group that develops new molecules and instrumentation for analyte detection and imaging. My work focused on developing a new two-photon lifetime imaging microscopy techniques and photophysical characterization of new probe molecules."
  },
  {
    "objectID": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html#development-of-two-photon-phosphorescence-lifetime-imaging-microscope",
    "href": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html#development-of-two-photon-phosphorescence-lifetime-imaging-microscope",
    "title": "Phosphorescence Lifetime Imaging",
    "section": "Development of Two-Photon Phosphorescence Lifetime Imaging Microscope",
    "text": "Development of Two-Photon Phosphorescence Lifetime Imaging Microscope\nI coupled ultrafast laser spectroscopy techniques with two-photon microscopy to create a novel phosphorescence lifetime imaging microscope. This method allows the optical sectioning of 3D samples while also obtaining information about the local chemical environment. Using this instrument, I obtained the first 3D phosphorescent lifetime and intensity images. These images were of an oxygen-sensing probe encapsulated in a polymersome. Phosphorescent molecules glow more or less intensely based on the amount of oxygen in the area, so they can be used to measure local oxygen concentrations.\n\n\n\nTwo-photon microscopy achieves super-resolution through a contracted point spread function of excitation compared to one-photon resolution. While the resolution was fairly well understood for fluorescent probes, the novel phosphorescent probes developed by the Vinogradov lab had additional properties (emission lifetimes that are several orders of magnitude longer than for fluorophores) that could impact resolution. I simulated the point spread function with Matlab to determine the resolution under various operating conditions.\n\n\n\n\n\n\n\n(a) One Photon Excitation Volume\n\n\n\n\n\n\n\n(b) Two Photon Excitation Volume\n\n\n\n\nFigure 1: Excitation volumes under one and two photon absorption regimes. Graphs are of the XZ plane."
  },
  {
    "objectID": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html#general-photophysical-studies-of-novel-probe-molecules",
    "href": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html#general-photophysical-studies-of-novel-probe-molecules",
    "title": "Phosphorescence Lifetime Imaging",
    "section": "General Photophysical Studies of Novel Probe Molecules",
    "text": "General Photophysical Studies of Novel Probe Molecules\nI measured various photophysical properties of probe molecules using both femtosecond spectroscopy and conventional steady-state techniques for absorption and emission. In addition, I measured the photostability of many probes- the best probe is no good if it is degraded by room light before you even get to the microscope!\n\n\n\nIn photophysics, models for various processes have often been derived mathematically from fundamental principles or experimentally validated in other systems. Fitting the data then becomes simpler since you only have to fit against a few equations. However, fitting parameters are related to the intrinsic properties of molecules and materials, so it is vital to ensure you are fitting with the correct model. Sometimes, a poorer fit to a more scientifically defensible model is more correct than a better fit to an invalid model. Lots of cross-checking and additional experimentation are needed beyond just fitting."
  },
  {
    "objectID": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html#in-vitro-imaging",
    "href": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html#in-vitro-imaging",
    "title": "Phosphorescence Lifetime Imaging",
    "section": "In Vitro Imaging",
    "text": "In Vitro Imaging\nThe probe molecules were developed for in vitro and in vivo use. Lots of fundamental questions about cellular functioning remain unanswered due to the lack of high-resolution data. Is there an oxygen gradient within a cell as the mitochondria consume oxygen? How exactly do zinc levels change as insulin is released?\nI collaborated with multiple groups at the University of Pennsylvania on the problem of loading our probes into the cell. At one point, I collaborated with research groups from every experimental school at Penn- Arts & Science, Medicine, Veterinary Medicine, and Engineering! We worked with a variety of cell lines- HeLa, fibroblasts, and macrophages. I did much of the cell culture myself. Various loading techniques were tested, such as microporation, transfection agents, scrape loading, endocytosis of vesicles, and polymersomes decorated with transfection agents. I would then characterize the probe within the cell using Two-Photon Phosphorescent Lifetime Imaging. Generally, the molecules behaved well, but often, the loading process led to many dead cells.\n\n\n\n\n\n\n\n(a) Bright Field Image of HeLa cells\n\n\n\n\n\n\n\n(b) Phosphorescence Image of HeLa cells\n\n\n\n\nFigure 2: Bright-field and Phosphorescent Images of HeLa cells. Probe loading was via microporation. Probe appears not to enter the nucleus."
  },
  {
    "objectID": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html#center-for-oxygen-microscopic-imaging-university-of-aarhus",
    "href": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html#center-for-oxygen-microscopic-imaging-university-of-aarhus",
    "title": "Phosphorescence Lifetime Imaging",
    "section": "Center for Oxygen Microscopic Imaging University of Aarhus",
    "text": "Center for Oxygen Microscopic Imaging University of Aarhus\nI spent two months at the University of Aarhus in Denmark working on singlet oxygen imagining. I also studied the cytotoxic effects of fluorescent probes developed in the Vinogradov lab. I learned and performed patch-clamp technique (whole-cell electrophysiological measurements) to study the effect of localized reactive oxygen species generation on ion channel function in mouse neurons. In addition, I shared my simulations on excitation volume and resolution, allowing them to understand the trade-off between resolution and signal-to-noise. Many experiments were performed at higher powers, leading to better signal to noise but degraded resolution. The simulations helped clarify exactly how much resolution was lost."
  },
  {
    "objectID": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html#selected-publications",
    "href": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html#selected-publications",
    "title": "Phosphorescence Lifetime Imaging",
    "section": "Selected Publications",
    "text": "Selected Publications\nDevor, A., Sakadžić, S; Yaseen, M. A.; Roussakis,E.; Tian, P.; Slovin, H.; Vanzetta, I.; Teng, I.; Saisan, P. A.; Sinks, L. E.; Dale, A. M.; Vinogradov, S.V.; Boas, D.A. Functional imaging of cerebral oxygenation with intrinsic optical contrast and phosphorescent probes, Optical Imaging of Neocortical Dynamics 2014\nPedersen, B. W.; Sinks, L. E.; Breitenbach, T.; Schack, N. B.; Vinogradov, S. A.; Ogilby, P. R. Single cell responses to spatially controlled photosensitized production of extracellular singlet oxygen. Photochemistry and Photobiology 2011, 87, 1077–1091.\nSinks, L. E.; Robbins, G. P.; Roussakis, E.; Troxler, T.; Hammer, D. A.; Vinogradov, S. A., Two-Photon Microscopy of Oxygen: Polymersomes as Probe Carrier Vehicles. J. Phys. Chem. B, 2010, 114(45), 14373-14382.\nSinks, L. E.; Roussakis, E.; Esipova Tatiana, V.; Vinogradov Sergei, A., Synthesis and calibration of phosphorescent nanoprobes for oxygen imaging in biological systems. Journal of visualized experiments : JoVE 2010, (37)."
  },
  {
    "objectID": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html#selected-posters-and-presentations",
    "href": "portfolio/Phosphorescence_Lifetime_Imaging/Phosphorescence_Lifetime_Imaging.html#selected-posters-and-presentations",
    "title": "Phosphorescence Lifetime Imaging",
    "section": "Selected Posters and Presentations",
    "text": "Selected Posters and Presentations\nSinks, L. E.; Finikova, O. S.; Vinogradov, S. A. Oxygen Microscopy with Two-Photon-Enhanced Phosphorescent Nanoprobes. In Novel Techniques in Microscopy; Optical Society of America, 2009 presented at the Optical Society of America Spring Congress: Novel Techniques in Microscopy Vancouver, Canada (talk)\n\nSinks, L. E. and Vinogradov, S. A. Oxygen Microscopy with Two-Photon-Enhanced Phosphorescent Nanoprobes at Howard Hughes Medical Institute Janelia Research Campus’s Symposium on Multiphoton Imaging: The Next 6X10^23 Femtoseconds, April 3 - 6, 2011"
  },
  {
    "objectID": "portfolio/Femtosecond_Laser_Spectroscopy/Femtosecond_Laser_Spectroscopy.html",
    "href": "portfolio/Femtosecond_Laser_Spectroscopy/Femtosecond_Laser_Spectroscopy.html",
    "title": "Advanced Femtosecond Spectroscopy Techniques",
    "section": "",
    "text": "I had a joint post-doctoral appointment with the late Prof. Robin Hochstrasser and Prof. Michael Therien at the University of Pennsylvania, where I researched the role of the environment on electron and energy transfer processes. Prof. Therien, now at Duke University, is a leading expert on functional materials for energy and photonic applications. His lab produced various exciting materials that I was lucky enough to study. Prof. Hochstrasser developed cutting-edge spectroscopic techniques, and I could perform multiple femtosecond spectroscopies in his laser lab."
  },
  {
    "objectID": "portfolio/Femtosecond_Laser_Spectroscopy/Femtosecond_Laser_Spectroscopy.html#maintaining-and-improving-the-femtosecond-laser-spectrometer",
    "href": "portfolio/Femtosecond_Laser_Spectroscopy/Femtosecond_Laser_Spectroscopy.html#maintaining-and-improving-the-femtosecond-laser-spectrometer",
    "title": "Advanced Femtosecond Spectroscopy Techniques",
    "section": "Maintaining and Improving the Femtosecond Laser Spectrometer",
    "text": "Maintaining and Improving the Femtosecond Laser Spectrometer\nThe femtosecond spectroscopy system was a complicated instrument designed to perform a variety of experiments for both labs: visible pump-probe, infrared pump-probe, and two-dimensional infrared (2DIR) spectroscopies. I continually improved the system throughout my post-doc. I improved the pump-probe experiment by reducing the temporal chirp 5-fold by redesigning the supercontinuum generation module and compensating for the spatial chirp by adjusting the relative size of the pump and probe beams. Upgrades to the optical train for the infrared beam path improved the power of the pump and probe beam."
  },
  {
    "objectID": "portfolio/Femtosecond_Laser_Spectroscopy/Femtosecond_Laser_Spectroscopy.html#ultrafast-pump-probe-spectroscopy",
    "href": "portfolio/Femtosecond_Laser_Spectroscopy/Femtosecond_Laser_Spectroscopy.html#ultrafast-pump-probe-spectroscopy",
    "title": "Advanced Femtosecond Spectroscopy Techniques",
    "section": "Ultrafast Pump-Probe Spectroscopy",
    "text": "Ultrafast Pump-Probe Spectroscopy\nMost of my spectroscopic work was performing visible pump and visible or near IR probe spectroscopy on the novel materials produced by the Therien Group. Since the materials were generated for various applications and with a range of physical properties, many different models were employed to analyze the data. Most fitting was performed with OriginPro, some with the built-in fitting, but some with custom functions I wrote using the built-in scripting language. The sheer variety of projects I got to participate in was a joy."
  },
  {
    "objectID": "portfolio/Femtosecond_Laser_Spectroscopy/Femtosecond_Laser_Spectroscopy.html#femtosecond-2d-ir-spectroscopy",
    "href": "portfolio/Femtosecond_Laser_Spectroscopy/Femtosecond_Laser_Spectroscopy.html#femtosecond-2d-ir-spectroscopy",
    "title": "Advanced Femtosecond Spectroscopy Techniques",
    "section": "Femtosecond 2D-IR Spectroscopy",
    "text": "Femtosecond 2D-IR Spectroscopy\nUltrafast infrared spectroscopy is the most challenging spectroscopic technique I have ever performed. Not only are all the beams invisible, making alignment of the system a massive challenge, but ambient atmospheric components, such as humidity, strongly absorb most beams.\n\n\n\nMy main research focus was understanding non-traditional hydrogen bonds. These hydrogen bonds are postulated to be critically important for forming and stabilizing complicated protein structures. While these hydrogen bonds are much weaker than conventional ones, a protein structure can have dozens of them, leading to substantial stability of the folded protein. I studied these hydrogen bonds with my colleague Dr. Kumar in a simple model system based on the molecule formamide. Later, I worked on hydrogen bonding within the structure of the trans-membrane protein gylcophorin A."
  },
  {
    "objectID": "portfolio/Femtosecond_Laser_Spectroscopy/Femtosecond_Laser_Spectroscopy.html#selected-publications",
    "href": "portfolio/Femtosecond_Laser_Spectroscopy/Femtosecond_Laser_Spectroscopy.html#selected-publications",
    "title": "Advanced Femtosecond Spectroscopy Techniques",
    "section": "Selected Publications",
    "text": "Selected Publications\nPark, J.; Park, T.-H.; Sinks, L. E.; Deria, P.; Park, J.; Baik, M.-H.; Therien, M. J. Unusual Solvent Polarity Dependent Excitation Relaxation Dynamics of a Bis [p-Ethynyldithiobenzoato] Pd-Linked Bis [(Porphinato) Zinc] Complex. Molecular Systems Design & Engineering 2018, 3 (1), 275–284.\nFry, H. C.; Lehmann, A.; Sinks, L. E.; Asselberghs, I.; Tronin, A.; Krishnan, V.; Blasie, J. K.; Clays, K.; DeGrado, W. F.; Saven, J. G.; Therien, M. J. Computational de novo design and characterization of a protein that selectively binds a highly hyperpolarizable abiological chromophore. J. Am. Chem. Soc. 2013, 135, 13914–26.\nIshizuka, T.; Sinks, L. E.; Song, K.; Hung, S.-T.; Nayak, A.; Clays, K.; Therien, M. J. The roles of molecular structure and effective optical symmetry in evolving dipolar chromophoric building blocks to potent octopolar nonlinear optical chromophores. J. Am. Chem. Soc. 2010, 133, 2884–2896.\nDeria, Pravas; Sinks, Louise; Park, Tae-Hong; Tomezsko, Diana; Brukman, Matthew; Bonnell, Dawn; Therien, Michael, Phase Transfer Catalysts Drive Diverse Organic Solvent Solubility of Single-Walled Carbon Nanotubes Helically Wrapped by Ionic, Semi-Conducting Polymers. Nano Letters, 2010, 10(10), 4192-4199.\nKumar, K.; Sinks, L. E.; Wang, J.; Kim, Y. S.; Hochstrasser, R. M., Coupling between C-D and CO motions using dual-frequency two-dimensional IR photon echo spectroscopy. Chem. Phys. Lett. 2006, 432 (1-3), 122-27.\nDuncan, T. V.; Susumu, K.; Sinks, L. E.; Therien, M. J., Exceptional Near-Infrared Fluorescence Quantum Yields and Excited-State Absorptivity of Highly Conjugated Porphyrin Arrays. J. Am. Chem. Soc. 2006, 128 (28), 9000-01."
  },
  {
    "objectID": "portfolio/Femtosecond_Laser_Spectroscopy/Femtosecond_Laser_Spectroscopy.html#selected-posters-and-presentations",
    "href": "portfolio/Femtosecond_Laser_Spectroscopy/Femtosecond_Laser_Spectroscopy.html#selected-posters-and-presentations",
    "title": "Advanced Femtosecond Spectroscopy Techniques",
    "section": "Selected Posters and Presentations",
    "text": "Selected Posters and Presentations\nSinks, L. E.; Frail, P. R.; Therien, M. J. SYMPOSIUM LECTURES-Photophysics of Porphyrins in Solution and in Films. Journal of Porphyrins and Phthalocyanines 2006, 10 (4–6), 399–399 presented at the International Conference on Porphyrins and Phthalocyanines Rome, Italy July 2006\n\n“Visible Pump/ IR Probe Technique”, 1st Annual Ultrafast Spectroscopy Workshop Sarasota, FL January 2010"
  },
  {
    "objectID": "posts/2023-11-01-website-building/website.html",
    "href": "posts/2023-11-01-website-building/website.html",
    "title": "Thoughts on Building a Quarto Website",
    "section": "",
    "text": "I’ve been using Quarto to blog for about seven months now, and I’d like to write a quick summary of my experiences and give a few tips and tricks on how to use it to publish a website or blog. This post will come from the R/ RStudio perspective, which has tight Quarto integration. Python folks, you too can have a quarto blog and website, but I have yet to find a good tutorial to link you to.\nI followed a couple of great tutorials, and I won’t duplicate that material here. I recommend you follow the tutorials by Sam Csik (website set-up and blog set-up) and Andrew Rapp (and I’ll get more specific about which parts in a minute), but I recommend reading this blog first. Some of this stuff won’t make sense until you do it, but it will help you follow the tutorials better. You can follow the tutorials without knowing any of this (I did), but I did run into some problems with the GitHub part of things, which are memorialized in my sad first post. I have found a workaround to that, which I discuss in the GitHub/Versioning section of this post.\nI like Quarto websites because they let you have a decent looking website with minimal effort to set it up. The content is the most important part of your website/ porfolio."
  },
  {
    "objectID": "posts/2023-11-01-website-building/website.html#overview-of-quarto-and-websites",
    "href": "posts/2023-11-01-website-building/website.html#overview-of-quarto-and-websites",
    "title": "Thoughts on Building a Quarto Website",
    "section": "Overview of Quarto and Websites",
    "text": "Overview of Quarto and Websites\nFrom the Quarto website, Quarto is:\n\nAn open-source scientific and technical publishing system\n\nAuthor using Jupyter notebooks or with plain text markdown in your favorite editor.\nCreate dynamic content with Python, R, Julia, and Observable.\n\n\n\nYour Quarto Files Are Real Coding Projects\nIf you’ve used any method of mixing code and text blocks, Quarto’s style will feel familiar to you. I want to emphasize this part a bit more in case you haven’t used markdown or Jupyter Notebooks. If you download any of my Quarto documents with code blocks you can open it and run the code. (You may need to change file paths or install the libraries I used). All my datasets are included in the repo for the website. This isn’t just blog of me going blah, blah, blah (this post aside)- the majority of my blog posts are complete projects with fully executable code.\n\n\nCreating a Website Project in RStudio\nAs an R user, I chose to implement my website through RStudio. Quarto is a project supported by Posit (the company that makes RStudio), so the integration is seamless. You can create a project in RStudio and designate it as a Quarto website, and it will set things up for you. You manage it like any other project. There are numerous tutorials for how to set up the website in RStudio. The official documentation for this process is here. There is less information about how to set it up for Python folks.\nIn RStudio, choosing the website project will create some basic files you need- a YAML file and an index.qmd file. The YAML file gives information about what the output of rendering a quarto file (*.qmd) should be. In this case, it is set to produce HTML, but Quarto is a scientific publishing system, so your YAML could tell it to render a PDF or a PowerPoint or any number of outputs.\n\n\nThe Difference between Executing Code and Rendering the Document\nCode written in a Quarto document (a *.qmd file) can be executed as normal in the RStudio environment. The variables you create show up in the environment pane. You can view them and interact with them in the console as usual. You execute the code using the run button at the top of the screen or cell block by cell block with the run button included in each code block. At this point, things behave like standard code.\nThere is an additional button at the top of the file that says render. Pushing this button executes the code and the instructions in the YAML headers to create the type of document you want. In this case, it generates the HTML files and copies related assets (jpg, etc.) to the doc folder in the project. These are the files that are used to generate the webpage.\nNote that rendering the file does not create any variables that you can access in the environment or console. So, usually, you want to execute the code as you are working on it, not render it. When you render, the finalized webpage will pop up in your web browser. (Sometimes it doesn’t, and I’m not sure why, but you can find the HTML file in the docs folder and open it directly.) You do want to render the page at least once in draft mode to make sure all the formatting looks good before you publish it for real. I try to make sure my tables and code blocks don’t have horizontal scroll bars, so that is something I check for. Then, I’d reformat the line breaks in the code or the table to narrow it.\nThere is also a render website button in the build tab in the pane that contains the Environment, History, Connections, and Git windows. This re-renders every page on your website (but may or may not re-execute code depending on the “freeze” setting in the YAML header or file). The best practice is to re-render the website whenever you change anything. This got very slow, very quickly, as I started to generate more content. In practice, you can probably just render the relevant pages. If you change the navigation bar or something else that applies to all pages, then you certainly need to re-render everything.\n\n\nWhy is Everything Named index.qmd?\nIf you go to a URL that doesn’t have a specific page listed (just paths/folders), the browser will automatically serve you the index file. So https://lsinks.github.io/index.html and https://lsinks.github.io/ give you the same page (the index.html file found in the root directory of the website). Many tutorials recommend that you name all your blog posts index.qmd as well. Using index.html makes the URLs shorter. The downside is you end up with a million index.qmd files in your project. I tend to reuse sections of code, so I frequently had multiple index files open in the editor, and it was confusing to navigate between them. I switched to giving each blog qmd file a short and unique name. This practice means your URL needs to be the full path + file name (e.g., https://lsinks.github.io/posts/2023-09-08-web-scraping-tombstone/web-scraping.html) whenever you use it."
  },
  {
    "objectID": "posts/2023-11-01-website-building/website.html#getting-the-basic-website-up--specific-hints-for-following-the-sam-csik-tutorials",
    "href": "posts/2023-11-01-website-building/website.html#getting-the-basic-website-up--specific-hints-for-following-the-sam-csik-tutorials",
    "title": "Thoughts on Building a Quarto Website",
    "section": "Getting the Basic Website Up- Specific Hints for Following the Sam Csik Tutorials",
    "text": "Getting the Basic Website Up- Specific Hints for Following the Sam Csik Tutorials\nStart with the tutorial on setting up your website. The tutorial assumes you have Git set up, have a GitHub account, and know how to get your credentials so you can push stuff from your local machine to the GitHub website. If you don’t, I suggest looking at the resource Happy Git and GitHub for the UseR, specifically the Installation section, which starts in Chapter 4.\n\nGitHub and Versioning in RStudio\nMy website is published on GitHub pages. The web pages only appear once I push them to GitHub and GitHub publishes them. RStudio has built-in Git/GitHub integration that should make it super easy to do versioning. (Not just for a webpage, but in general.) I have had horrible luck with this feature. I frequently get versioning conflicts that are very difficult to resolve. Searching the error messages indicates that the error is related to someone else editing the files, which isn’t the case at all.\nI first ran into this issue when setting up the website. I got to the “Publish your site with GitHub Pages” in Sam Csik’s tutorial, and you stage, commit, and push your changes to the GitHub repo. I ended up in cycles of cryptic error messages about unresolvable conflicts that I couldn’t figure out how to resolve other than by deleting the entire repository and starting over. (No great loss since there wasn’t any content at that point.) At the time, I thought it was something specific to the website project, but since then, I have run into this issue with regular projects and repos, too. Anything where I try to handle versioning within RStudio has a high chance of going catastrophically wrong.\nSo, I always handle versioning outside of RStudio, and as an extra layer of paranoia, I only pull or push changes when RStudio is closed. GitHub Desktop has a nice user interface if you aren’t comfortable with a terminal window. Happy Git and GitHub for the UseR recommends a different client, GitKracken, but I haven’t used it.\nThis might very well be a me problem; I certainly didn’t have much luck finding others with this issue when I googled. If you’ve been using the Git tab in RStudio and have had non-fast-forward errors before (without an apparent reason), then maybe you want to do what I’ve done and not use the RStudio Git interface.\n\n\nAdding the Blog\nAdding a blog to your webpage is covered by another Sam Csik tutorial. This tutorial is straightforward if you’ve completed the first tutorial.\nA blog’s index page (the listing page) automatically updates all the entries when you generate them and styles the listing in specific ways common to blogs. My website has two blogs- the actual blog that you would recognize as a blog (and the section under which this post appears) and my portfolio page. I wanted my past work to have nice thumbnail pictures, so I just set it up as a blog and had Quarto style it automatically. Keep this trick in mind as you are building out your page. A stealth blog can add a very sophisticated look to a page. And since the listing page is created dynamically, you don’t have to update it if you need to add additional content.\n\n\nNames and Middle Names in the YAML header\nThe only issue I ran into, and which took me a ridiculously long time to sort out, is that she used the author and citation tag in the YAML header to automatically create a “For attribution, please cite this work as: {citation}” at the bottom of every blog post.\nI dutifully entered “name: Louise E. Sinks” in the header and have been generating incorrectly formatted citations for months:\nE. Sinks, Louise. 2023. “Creating a Blog.” March 14, 2023. https://lsinks.github.io/2023-03-14_tester-post.\nThe solution, which is not so easy to find in the Quarto documentation, is to use:\n- name:\ngiven: Louise E.\nfamily: Sinks\nThe citation then handles the middle name/initial properly and not as part of the last name. (This issue may or may not be fixed on earlier posts when you read this, since I will have to go back and fix every post.)\nOf course, you may not want to have a citation for your webpage, so you can always omit that section.\n\n\nAdding Additional Pages is Easy\nThe steps you used to add the blog page can be extended to other pages. Both additional static pages and listing pages for the blog can be added to the quarto YAML file. You can see mine here. In fact, you can view the entire structure of my website on Git Hub. Feel free to look at the *.qmd files and the _quarto.yml file to see how the website is structured.\n\n\nGive GitHub Pages Time!\nSometimes, the changes take a while to show up live on the internet. This may give you the impression that what you did isn’t working. I know I changed a bunch of things initially and got very frustrated because I didn’t see what I expected on the live website. GitHub will email you if your deployment fails. Until you get that message, assume that the changes are just slowly propagating across the internet. Don’t panic and change things randomly like I did.\n\n\nDecide on a Convention for Categories\nDecide how you are going to name your categories. If you look at mine, they are a mix of upper and lower case. You should probably pick one from the start, so you don’t end up with multiple versions of the same category. Also think of what types of things you’ll include. If I use and explain a particular package heavily, I include the package in my category list. If I just use the package, then I don’t. Having some rules from the start will make things a bit easier in the long run, but don’t get too caught up in designing the perfect categorization system."
  },
  {
    "objectID": "posts/2023-11-01-website-building/website.html#adding-more-features",
    "href": "posts/2023-11-01-website-building/website.html#adding-more-features",
    "title": "Thoughts on Building a Quarto Website",
    "section": "Adding More Features",
    "text": "Adding More Features\nThese are all strictly optional, but might be nice to have.\n\nFeatures Suggested by Albert Rapp\nAlbert Rapp’s “The ultimate guide to starting a Quarto blog” is an invaluable resource for adding advanced functionality to your blog/website. You can skip all this stuff for now or forever, too. The sections I did implement were:\n\nadding social cards (called Twitter cards in the tutorial), which is when a thumbnail and the headline are shown when you share a link (or no headline if you are posting on X).\nadding a comments section\nadding Google Analytics and a cookies notice\nadding renv, which I did completely wrong. I’ve been using renv, but you do need to follow his detailed instructions on linking a specific snapshot to one particular blog post.\n\nHe also has sections on customizing the styling with CSS and also creating series (which is another example of using the blog feature to create customized pages). He publishes his blog through Netlify rather than GitHub pages.\n\n\nAlt Text\nAlt text lets you add a written description to pictures and figures. Alt text makes your content more accessible to visually impaired visitors. I recently became aware that Quarto has support for this feature and have implemented it going forward. (All my portfolio projects have alt text, but my blog posts before this one don’t.)\n\n\nSteal Some Code!\nIf you see something cool on a Quarto/ GitHub pages website and you can’t figure out how to do it, go look at the source code in their repo. Do you like the embedded video in my blog post on web scraping? Go look at that folder at GitHub. See where I stored the asset (*.mp4) and how I called it in the qmd file. Then, implement that yourself. (Here’s the official documentation for embedding video if you prefer.)\nObviously, I’m not advocating that you plagiarize anyone’s website! But taking code snippets and adapting them to your needs is fine. And it is nice to give a link or a shoutout to the source you found helpful."
  },
  {
    "objectID": "posts/2023-11-01-website-building/website.html#tips-for-improved-workflow",
    "href": "posts/2023-11-01-website-building/website.html#tips-for-improved-workflow",
    "title": "Thoughts on Building a Quarto Website",
    "section": "Tips for Improved Workflow",
    "text": "Tips for Improved Workflow\nThis section isn’t relevant to setting up the website but might be helpful to keep in mind as you start producing content. Everyone’s workflow is different, so there is also a huge dose of “your mileage will vary.”\n\nSpell Checking\nSpelling and grammar checking, or lack of, is one of the most significant weaknesses of the process. RStudio does have a built-in spell checker, but it could be better. Sometimes, I live dangerously and just use that and post. But I do like a more thorough editing process.\nI’m not the only one with this complaint. There are several proposed solutions out there (grammar checker R package, another package, and writing a ChatGPT shiny app to spell check) but the ones I tried didn’t work well for me.\nFor most of my other writing, I use the Grammarly spelling and grammar checker. The Grammarly plugin works terribly inside RStudio, so I usually end up copying and pasting text into the online Grammarly site and then copying it back down to the qmd file. I don’t like this process. I end up with multiple copies of the code/text in various places. You must also be careful to prevent it from correcting the code blocks, which is probably true for most checkers.\nI do try not do extensive formatting (styling code in text, adding lots of headers, figures/alt text, and links) until after I’ve used Grammarly. Pasting the text back into RStudio often screws up the markdown and Grammarly often wants to spell-check formatting codes and mess them up.\n\n\nClean Up Old Files\nDraft posts and pages with the draft: true setting in the header will be rendered at HMTL in the docs folder but not published. This means if you end up renaming the page or deleting it, you need to clean out the rendered files in the doc folder as well as the qmd file. I run into this most frequently with blog posts. I often drafted them with a guess on the publication date (and my folder structure includes the date). If the date changes, the older version won’t be automatically deleted from docs, even if the file that it is rendered from is deleted and you re-render the website. If this doesn’t make sense now, don’t worry about it.\n\n\nDraft Complicated Things in Another Project\nIncomplete and error-ridden code will cause the rendering of your website to fail, even if the offending file is marked a draft. So, for more complicated code, draft it elsewhere and copy it into the website project when you are ready to go.\nThe huge caveat to this is that you really need to pay attention to your file paths and folder structures. Your tester project likely has a simplified structure compared to your website. (Your tester project might be completely flat, while its final location in your website project might be in a subfolder of your blog folder.)\nComplicating this a bit is that in RStudio, the relative path starts in the project directory for code and in the local folder of the qmd file for rendering. The most straightforward approach is to use here to reference all your paths back to the same starting point (the root directory of the project.)"
  },
  {
    "objectID": "posts/2023-11-01-website-building/website.html#the-turtle-says-just-type",
    "href": "posts/2023-11-01-website-building/website.html#the-turtle-says-just-type",
    "title": "Thoughts on Building a Quarto Website",
    "section": "The Turtle Says Just Type",
    "text": "The Turtle Says Just Type\nI was supported by the best Technical Turtle during this process. When I wrote my first post, I didn’t really have a good thumbnail, so I just included a picture of him. He does occasionally take a more active role in my coding, by crawling across the keyboard and generating a bunch of letters (as you see in the thumbnail). And to some extent, you should take that lesson to heart- just type something. Your first post(s) are going to be terrible. Look at mine. I couldn’t figure out how to make hyperlinks, so I just dumped them as text. I didn’t even grab the right URL for one of the tutorials (I used a link to a specific section rather than the tutorial as a whole). That post is mostly a rant about how I can’t figure out GitHub.\nLots of stuff isn’t perfect here, even now. But I post them anyway. And I promote them on social media as well. It is terrifying to publicize work that you know could be better, but you can never achieve perfection.\nQuarto enables you to start publishing content quickly. Do not get bogged down in customizing your style sheets and tweaking all sorts of things. Use an off the shelf theme and just start coding."
  }
]